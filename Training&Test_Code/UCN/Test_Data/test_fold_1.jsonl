{"id": "P9_D2_P9_High_T1_26_Right", "images": ["Image/P9_D2_P9_High_T1_26_Right/1.png", "Image/P9_D2_P9_High_T1_26_Right/2.png", "Image/P9_D2_P9_High_T1_26_Right/3.png", "Image/P9_D2_P9_High_T1_26_Right/4.png", "Image/P9_D2_P9_High_T1_26_Right/5.png", "Image/P9_D2_P9_High_T1_26_Right/6.png", "Image/P9_D2_P9_High_T1_26_Right/7.png", "Image/P9_D2_P9_High_T1_26_Right/8.png", "Image/P9_D2_P9_High_T1_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the white cup to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 3154.1448 seconds to 3169.0033 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P9_Low_T1_22_Mid", "images": ["Image/P19_D1_P9_Low_T1_22_Mid/1.png", "Image/P19_D1_P9_Low_T1_22_Mid/2.png", "Image/P19_D1_P9_Low_T1_22_Mid/3.png", "Image/P19_D1_P9_Low_T1_22_Mid/4.png", "Image/P19_D1_P9_Low_T1_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this flowerpot here.\"\nThe frames roughly correspond to the speech interval from 1206.3918 seconds to 1220.1855999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P6_Medium_T3_6_Right", "images": ["Image/P17_D1_P6_Medium_T3_6_Right/1.png", "Image/P17_D1_P6_Medium_T3_6_Right/2.png", "Image/P17_D1_P6_Medium_T3_6_Right/3.png", "Image/P17_D1_P6_Medium_T3_6_Right/4.png", "Image/P17_D1_P6_Medium_T3_6_Right/5.png", "Image/P17_D1_P6_Medium_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this plant right here? Finish\"\nThe frames roughly correspond to the speech interval from 2013.038 seconds to 2024.9538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P2_Medium_T3_11_Left", "images": ["Image/P3_D1_P2_Medium_T3_11_Left/1.png", "Image/P3_D1_P2_Medium_T3_11_Left/2.png", "Image/P3_D1_P2_Medium_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1575.8361 seconds to 1585.0231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P6_Medium_T1_11_Right", "images": ["Image/P26_D3_P6_Medium_T1_11_Right/1.png", "Image/P26_D3_P6_Medium_T1_11_Right/2.png", "Image/P26_D3_P6_Medium_T1_11_Right/3.png", "Image/P26_D3_P6_Medium_T1_11_Right/4.png", "Image/P26_D3_P6_Medium_T1_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right base to the middle of that desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1110.7305 seconds to 1122.9165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P5_Medium_T3_22_Mid", "images": ["Image/P9_D2_P5_Medium_T3_22_Mid/1.png", "Image/P9_D2_P5_Medium_T3_22_Mid/2.png", "Image/P9_D2_P5_Medium_T3_22_Mid/3.png", "Image/P9_D2_P5_Medium_T3_22_Mid/4.png", "Image/P9_D2_P5_Medium_T3_22_Mid/5.png", "Image/P9_D2_P5_Medium_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the middle plant on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2990.3266 seconds to 3002.9202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P5_High_T3_9_Right", "images": ["Image/P24_D1_P5_High_T3_9_Right/1.png", "Image/P24_D1_P5_High_T3_9_Right/2.png", "Image/P24_D1_P5_High_T3_9_Right/3.png", "Image/P24_D1_P5_High_T3_9_Right/4.png", "Image/P24_D1_P5_High_T3_9_Right/5.png", "Image/P24_D1_P5_High_T3_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1049.1012999999998 seconds to 1060.849 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P2_High_T3_19_Right", "images": ["Image/P28_D2_P2_High_T3_19_Right/1.png", "Image/P28_D2_P2_High_T3_19_Right/2.png", "Image/P28_D2_P2_High_T3_19_Right/3.png", "Image/P28_D2_P2_High_T3_19_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that white base. Finish.\"\nThe frames roughly correspond to the speech interval from 1046.5433 seconds to 1055.5915 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P7_High_T3_7_Mid", "images": ["Image/P14_D3_P7_High_T3_7_Mid/1.png", "Image/P14_D3_P7_High_T3_7_Mid/2.png", "Image/P14_D3_P7_High_T3_7_Mid/3.png", "Image/P14_D3_P7_High_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that green mug up. Finish.\"\nThe frames roughly correspond to the speech interval from 1042.99827 seconds to 1052.92531 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P9_Low_T1_20_Mid", "images": ["Image/P2_D1_P9_Low_T1_20_Mid/1.png", "Image/P2_D1_P9_Low_T1_20_Mid/2.png", "Image/P2_D1_P9_Low_T1_20_Mid/3.png", "Image/P2_D1_P9_Low_T1_20_Mid/4.png", "Image/P2_D1_P9_Low_T1_20_Mid/5.png", "Image/P2_D1_P9_Low_T1_20_Mid/6.png", "Image/P2_D1_P9_Low_T1_20_Mid/7.png", "Image/P2_D1_P9_Low_T1_20_Mid/8.png", "Image/P2_D1_P9_Low_T1_20_Mid/9.png", "Image/P2_D1_P9_Low_T1_20_Mid/10.png", "Image/P2_D1_P9_Low_T1_20_Mid/11.png", "Image/P2_D1_P9_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 565.7004 seconds to 583.0835999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P5_Low_T1_13_Mid", "images": ["Image/P5_D2_P5_Low_T1_13_Mid/1.png", "Image/P5_D2_P5_Low_T1_13_Mid/2.png", "Image/P5_D2_P5_Low_T1_13_Mid/3.png", "Image/P5_D2_P5_Low_T1_13_Mid/4.png", "Image/P5_D2_P5_Low_T1_13_Mid/5.png", "Image/P5_D2_P5_Low_T1_13_Mid/6.png", "Image/P5_D2_P5_Low_T1_13_Mid/7.png", "Image/P5_D2_P5_Low_T1_13_Mid/8.png", "Image/P5_D2_P5_Low_T1_13_Mid/9.png", "Image/P5_D2_P5_Low_T1_13_Mid/10.png", "Image/P5_D2_P5_Low_T1_13_Mid/11.png", "Image/P5_D2_P5_Low_T1_13_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move the plant in front of me to the shelf that is close to the kitchen\"\nThe frames roughly correspond to the speech interval from 585.7999 seconds to 606.8351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P3_Medium_T3_4_Left", "images": ["Image/P1_D3_P3_Medium_T3_4_Left/1.png", "Image/P1_D3_P3_Medium_T3_4_Left/2.png", "Image/P1_D3_P3_Medium_T3_4_Left/3.png", "Image/P1_D3_P3_Medium_T3_4_Left/4.png", "Image/P1_D3_P3_Medium_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1350.06884 seconds to 1360.91367 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P2_Low_T1_15_Mid", "images": ["Image/P16_D2_P2_Low_T1_15_Mid/1.png", "Image/P16_D2_P2_Low_T1_15_Mid/2.png", "Image/P16_D2_P2_Low_T1_15_Mid/3.png", "Image/P16_D2_P2_Low_T1_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please move that plant up there\"\nThe frames roughly correspond to the speech interval from 662.1201900000001 seconds to 677.1397400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P3_Medium_T3_4_Left", "images": ["Image/P4_D3_P3_Medium_T3_4_Left/1.png", "Image/P4_D3_P3_Medium_T3_4_Left/2.png", "Image/P4_D3_P3_Medium_T3_4_Left/3.png", "Image/P4_D3_P3_Medium_T3_4_Left/4.png", "Image/P4_D3_P3_Medium_T3_4_Left/5.png", "Image/P4_D3_P3_Medium_T3_4_Left/6.png", "Image/P4_D3_P3_Medium_T3_4_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 242.67311999999993 seconds to 257.7912299999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P5_High_T2_8_Left", "images": ["Image/P28_D3_P5_High_T2_8_Left/1.png", "Image/P28_D3_P5_High_T2_8_Left/2.png", "Image/P28_D3_P5_High_T2_8_Left/3.png", "Image/P28_D3_P5_High_T2_8_Left/4.png", "Image/P28_D3_P5_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 433.0716 seconds to 444.5966 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P3_Medium_T2_8_Left", "images": ["Image/P28_D3_P3_Medium_T2_8_Left/1.png", "Image/P28_D3_P3_Medium_T2_8_Left/2.png", "Image/P28_D3_P3_Medium_T2_8_Left/3.png", "Image/P28_D3_P3_Medium_T2_8_Left/4.png", "Image/P28_D3_P3_Medium_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 453.046 seconds to 462.8603 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P2_Medium_T2_16_Left", "images": ["Image/P5_D1_P2_Medium_T2_16_Left/1.png", "Image/P5_D1_P2_Medium_T2_16_Left/2.png", "Image/P5_D1_P2_Medium_T2_16_Left/3.png", "Image/P5_D1_P2_Medium_T2_16_Left/4.png", "Image/P5_D1_P2_Medium_T2_16_Left/5.png", "Image/P5_D1_P2_Medium_T2_16_Left/6.png", "Image/P5_D1_P2_Medium_T2_16_Left/7.png", "Image/P5_D1_P2_Medium_T2_16_Left/8.png", "Image/P5_D1_P2_Medium_T2_16_Left/9.png", "Image/P5_D1_P2_Medium_T2_16_Left/10.png", "Image/P5_D1_P2_Medium_T2_16_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup that's in the middle of the shelf or in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1695.7323 seconds to 1716.3453 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P5_Low_T2_25_Right", "images": ["Image/P27_D2_P5_Low_T2_25_Right/1.png", "Image/P27_D2_P5_Low_T2_25_Right/2.png", "Image/P27_D2_P5_Low_T2_25_Right/3.png", "Image/P27_D2_P5_Low_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put water in that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1617.4096 seconds to 1626.9157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P6_High_T3_5_Left", "images": ["Image/P15_D3_P6_High_T3_5_Left/1.png", "Image/P15_D3_P6_High_T3_5_Left/2.png", "Image/P15_D3_P6_High_T3_5_Left/3.png", "Image/P15_D3_P6_High_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 751.0558 seconds to 761.6205 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P7_Medium_T1_2_Right", "images": ["Image/P20_D2_P7_Medium_T1_2_Right/1.png", "Image/P20_D2_P7_Medium_T1_2_Right/2.png", "Image/P20_D2_P7_Medium_T1_2_Right/3.png", "Image/P20_D2_P7_Medium_T1_2_Right/4.png", "Image/P20_D2_P7_Medium_T1_2_Right/5.png", "Image/P20_D2_P7_Medium_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the right bottle to the top of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3274.7838 seconds to 3288.4528 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P7_High_T3_11_Mid", "images": ["Image/P24_D3_P7_High_T3_11_Mid/1.png", "Image/P24_D3_P7_High_T3_11_Mid/2.png", "Image/P24_D3_P7_High_T3_11_Mid/3.png", "Image/P24_D3_P7_High_T3_11_Mid/4.png", "Image/P24_D3_P7_High_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1672.8591 seconds to 1683.492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P9_Medium_T3_13_Right", "images": ["Image/P25_D3_P9_Medium_T3_13_Right/1.png", "Image/P25_D3_P9_Medium_T3_13_Right/2.png", "Image/P25_D3_P9_Medium_T3_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1853.145 seconds to 1861.4829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P9_High_T3_3_Left", "images": ["Image/P16_D3_P9_High_T3_3_Left/1.png", "Image/P16_D3_P9_High_T3_3_Left/2.png", "Image/P16_D3_P9_High_T3_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 483.29189999999994 seconds to 492.0054 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P6_High_T1_3_Right", "images": ["Image/P7_D3_P6_High_T1_3_Right/1.png", "Image/P7_D3_P6_High_T1_3_Right/2.png", "Image/P7_D3_P6_High_T1_3_Right/3.png", "Image/P7_D3_P6_High_T1_3_Right/4.png", "Image/P7_D3_P6_High_T1_3_Right/5.png", "Image/P7_D3_P6_High_T1_3_Right/6.png", "Image/P7_D3_P6_High_T1_3_Right/7.png", "Image/P7_D3_P6_High_T1_3_Right/8.png", "Image/P7_D3_P6_High_T1_3_Right/9.png", "Image/P7_D3_P6_High_T1_3_Right/10.png", "Image/P7_D3_P6_High_T1_3_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Say Friday. Move the green bottle to the disk on my left-hand side. Finish.\"\nThe frames roughly correspond to the speech interval from 1695.9311 seconds to 1716.5618000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P4_High_T3_23_Mid", "images": ["Image/P17_D1_P4_High_T3_23_Mid/1.png", "Image/P17_D1_P4_High_T3_23_Mid/2.png", "Image/P17_D1_P4_High_T3_23_Mid/3.png", "Image/P17_D1_P4_High_T3_23_Mid/4.png", "Image/P17_D1_P4_High_T3_23_Mid/5.png", "Image/P17_D1_P4_High_T3_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this white pot with the plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1708.4157 seconds to 1720.4706 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P8_High_T1_5_Right", "images": ["Image/P27_D2_P8_High_T1_5_Right/1.png", "Image/P27_D2_P8_High_T1_5_Right/2.png", "Image/P27_D2_P8_High_T1_5_Right/3.png", "Image/P27_D2_P8_High_T1_5_Right/4.png", "Image/P27_D2_P8_High_T1_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put that plant on top of that shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1481.1046 seconds to 1492.6574999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P2_High_T2_15_Right", "images": ["Image/P9_D3_P2_High_T2_15_Right/1.png", "Image/P9_D3_P2_High_T2_15_Right/2.png", "Image/P9_D3_P2_High_T2_15_Right/3.png", "Image/P9_D3_P2_High_T2_15_Right/4.png", "Image/P9_D3_P2_High_T2_15_Right/5.png", "Image/P9_D3_P2_High_T2_15_Right/6.png", "Image/P9_D3_P2_High_T2_15_Right/7.png", "Image/P9_D3_P2_High_T2_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 501.4281 seconds to 516.3026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P5_Low_T3_3_Left", "images": ["Image/P8_D1_P5_Low_T3_3_Left/1.png", "Image/P8_D1_P5_Low_T3_3_Left/2.png", "Image/P8_D1_P5_Low_T3_3_Left/3.png", "Image/P8_D1_P5_Low_T3_3_Left/4.png", "Image/P8_D1_P5_Low_T3_3_Left/5.png", "Image/P8_D1_P5_Low_T3_3_Left/6.png", "Image/P8_D1_P5_Low_T3_3_Left/7.png", "Image/P8_D1_P5_Low_T3_3_Left/8.png", "Image/P8_D1_P5_Low_T3_3_Left/9.png", "Image/P8_D1_P5_Low_T3_3_Left/10.png", "Image/P8_D1_P5_Low_T3_3_Left/11.png", "Image/P8_D1_P5_Low_T3_3_Left/12.png", "Image/P8_D1_P5_Low_T3_3_Left/13.png", "Image/P8_D1_P5_Low_T3_3_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 422.3067999999999 seconds to 444.78899999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P5_Low_T1_25_Mid", "images": ["Image/P30_D3_P5_Low_T1_25_Mid/1.png", "Image/P30_D3_P5_Low_T1_25_Mid/2.png", "Image/P30_D3_P5_Low_T1_25_Mid/3.png", "Image/P30_D3_P5_Low_T1_25_Mid/4.png", "Image/P30_D3_P5_Low_T1_25_Mid/5.png", "Image/P30_D3_P5_Low_T1_25_Mid/6.png", "Image/P30_D3_P5_Low_T1_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1225.6736 seconds to 1240.8324 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P2_High_T2_15_Left", "images": ["Image/P15_D2_P2_High_T2_15_Left/1.png", "Image/P15_D2_P2_High_T2_15_Left/2.png", "Image/P15_D2_P2_High_T2_15_Left/3.png", "Image/P15_D2_P2_High_T2_15_Left/4.png", "Image/P15_D2_P2_High_T2_15_Left/5.png", "Image/P15_D2_P2_High_T2_15_Left/6.png", "Image/P15_D2_P2_High_T2_15_Left/7.png", "Image/P15_D2_P2_High_T2_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2587.09254 seconds to 2601.0261299999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P3_High_T2_23_Right", "images": ["Image/P12_D2_P3_High_T2_23_Right/1.png", "Image/P12_D2_P3_High_T2_23_Right/2.png", "Image/P12_D2_P3_High_T2_23_Right/3.png", "Image/P12_D2_P3_High_T2_23_Right/4.png", "Image/P12_D2_P3_High_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the white plant. Finish.\"\nThe frames roughly correspond to the speech interval from 655.0482 seconds to 665.6409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P1_Low_T1_18_Mid", "images": ["Image/P14_D2_P1_Low_T1_18_Mid/1.png", "Image/P14_D2_P1_Low_T1_18_Mid/2.png", "Image/P14_D2_P1_Low_T1_18_Mid/3.png", "Image/P14_D2_P1_Low_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup to the bottom. Finish.\"\nThe frames roughly correspond to the speech interval from 1683.6990999999998 seconds to 1692.7602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P4_Low_T1_3_Right", "images": ["Image/P30_D1_P4_Low_T1_3_Right/1.png", "Image/P30_D1_P4_Low_T1_3_Right/2.png", "Image/P30_D1_P4_Low_T1_3_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 811.8419000000001 seconds to 820.2918000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P3_High_T1_25_Mid", "images": ["Image/P12_D3_P3_High_T1_25_Mid/1.png", "Image/P12_D3_P3_High_T1_25_Mid/2.png", "Image/P12_D3_P3_High_T1_25_Mid/3.png", "Image/P12_D3_P3_High_T1_25_Mid/4.png", "Image/P12_D3_P3_High_T1_25_Mid/5.png", "Image/P12_D3_P3_High_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that white mug to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2715.76989 seconds to 2727.23809 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P4_High_T1_12_Left", "images": ["Image/P19_D2_P4_High_T1_12_Left/1.png", "Image/P19_D2_P4_High_T1_12_Left/2.png", "Image/P19_D2_P4_High_T1_12_Left/3.png", "Image/P19_D2_P4_High_T1_12_Left/4.png", "Image/P19_D2_P4_High_T1_12_Left/5.png", "Image/P19_D2_P4_High_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this space here. Finish.\"\nThe frames roughly correspond to the speech interval from 1757.8729999999998 seconds to 1770.971 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P5_Medium_T2_15_Left", "images": ["Image/P11_D2_P5_Medium_T2_15_Left/1.png", "Image/P11_D2_P5_Medium_T2_15_Left/2.png", "Image/P11_D2_P5_Medium_T2_15_Left/3.png", "Image/P11_D2_P5_Medium_T2_15_Left/4.png", "Image/P11_D2_P5_Medium_T2_15_Left/5.png", "Image/P11_D2_P5_Medium_T2_15_Left/6.png", "Image/P11_D2_P5_Medium_T2_15_Left/7.png", "Image/P11_D2_P5_Medium_T2_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the first plant from the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 969.1403 seconds to 983.873 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P5_High_T2_1_Mid", "images": ["Image/P18_D3_P5_High_T2_1_Mid/1.png", "Image/P18_D3_P5_High_T2_1_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1178.1268300000002 seconds to 1186.61801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P9_Medium_T2_15_Right", "images": ["Image/P10_D1_P9_Medium_T2_15_Right/1.png", "Image/P10_D1_P9_Medium_T2_15_Right/2.png", "Image/P10_D1_P9_Medium_T2_15_Right/3.png", "Image/P10_D1_P9_Medium_T2_15_Right/4.png", "Image/P10_D1_P9_Medium_T2_15_Right/5.png", "Image/P10_D1_P9_Medium_T2_15_Right/6.png", "Image/P10_D1_P9_Medium_T2_15_Right/7.png", "Image/P10_D1_P9_Medium_T2_15_Right/8.png", "Image/P10_D1_P9_Medium_T2_15_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2317.395 seconds to 2331.7592999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P8_High_T2_5_Right", "images": ["Image/P24_D3_P8_High_T2_5_Right/1.png", "Image/P24_D3_P8_High_T2_5_Right/2.png", "Image/P24_D3_P8_High_T2_5_Right/3.png", "Image/P24_D3_P8_High_T2_5_Right/4.png", "Image/P24_D3_P8_High_T2_5_Right/5.png", "Image/P24_D3_P8_High_T2_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1690.9249 seconds to 1702.6039 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P7_High_T1_4_Left", "images": ["Image/P11_D3_P7_High_T1_4_Left/1.png", "Image/P11_D3_P7_High_T1_4_Left/2.png", "Image/P11_D3_P7_High_T1_4_Left/3.png", "Image/P11_D3_P7_High_T1_4_Left/4.png", "Image/P11_D3_P7_High_T1_4_Left/5.png", "Image/P11_D3_P7_High_T1_4_Left/6.png", "Image/P11_D3_P7_High_T1_4_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant to the top of the shelf on the same side. Finish.\"\nThe frames roughly correspond to the speech interval from 1837.4444 seconds to 1851.0559 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P9_High_T2_27_Left", "images": ["Image/P3_D2_P9_High_T2_27_Left/1.png", "Image/P3_D2_P9_High_T2_27_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item? Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2305.6249000000003 seconds to 2315.8141 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P6_High_T3_10_Right", "images": ["Image/P7_D2_P6_High_T3_10_Right/1.png", "Image/P7_D2_P6_High_T3_10_Right/2.png", "Image/P7_D2_P6_High_T3_10_Right/3.png", "Image/P7_D2_P6_High_T3_10_Right/4.png", "Image/P7_D2_P6_High_T3_10_Right/5.png", "Image/P7_D2_P6_High_T3_10_Right/6.png", "Image/P7_D2_P6_High_T3_10_Right/7.png", "Image/P7_D2_P6_High_T3_10_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1102.4762 seconds to 1118.3300000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P1_High_T1_9_Mid", "images": ["Image/P21_D1_P1_High_T1_9_Mid/1.png", "Image/P21_D1_P1_High_T1_9_Mid/2.png", "Image/P21_D1_P1_High_T1_9_Mid/3.png", "Image/P21_D1_P1_High_T1_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1194.5824 seconds to 1206.4696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P2_Low_T1_5_Left", "images": ["Image/P7_D3_P2_Low_T1_5_Left/1.png", "Image/P7_D3_P2_Low_T1_5_Left/2.png", "Image/P7_D3_P2_Low_T1_5_Left/3.png", "Image/P7_D3_P2_Low_T1_5_Left/4.png", "Image/P7_D3_P2_Low_T1_5_Left/5.png", "Image/P7_D3_P2_Low_T1_5_Left/6.png", "Image/P7_D3_P2_Low_T1_5_Left/7.png", "Image/P7_D3_P2_Low_T1_5_Left/8.png", "Image/P7_D3_P2_Low_T1_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. If I remove the plant to the ground. Finish.\"\nThe frames roughly correspond to the speech interval from 1674.49508 seconds to 1690.2381 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P3_High_T1_2_Right", "images": ["Image/P19_D3_P3_High_T1_2_Right/1.png", "Image/P19_D3_P3_High_T1_2_Right/2.png", "Image/P19_D3_P3_High_T1_2_Right/3.png", "Image/P19_D3_P3_High_T1_2_Right/4.png", "Image/P19_D3_P3_High_T1_2_Right/5.png", "Image/P19_D3_P3_High_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that container down here. Finish.\"\nThe frames roughly correspond to the speech interval from 2593.3428 seconds to 2607.7915999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P6_High_T3_7_Left", "images": ["Image/P1_D1_P6_High_T3_7_Left/1.png", "Image/P1_D1_P6_High_T3_7_Left/2.png", "Image/P1_D1_P6_High_T3_7_Left/3.png", "Image/P1_D1_P6_High_T3_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 192.4017 seconds to 201.7979 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P3_High_T2_3_Right", "images": ["Image/P10_D3_P3_High_T2_3_Right/1.png", "Image/P10_D3_P3_High_T2_3_Right/2.png", "Image/P10_D3_P3_High_T2_3_Right/3.png", "Image/P10_D3_P3_High_T2_3_Right/4.png", "Image/P10_D3_P3_High_T2_3_Right/5.png", "Image/P10_D3_P3_High_T2_3_Right/6.png", "Image/P10_D3_P3_High_T2_3_Right/7.png", "Image/P10_D3_P3_High_T2_3_Right/8.png", "Image/P10_D3_P3_High_T2_3_Right/9.png", "Image/P10_D3_P3_High_T2_3_Right/10.png", "Image/P10_D3_P3_High_T2_3_Right/11.png", "Image/P10_D3_P3_High_T2_3_Right/12.png", "Image/P10_D3_P3_High_T2_3_Right/13.png", "Image/P10_D3_P3_High_T2_3_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 929.93139 seconds to 949.37629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P1_Low_T2_18_Mid", "images": ["Image/P27_D1_P1_Low_T2_18_Mid/1.png", "Image/P27_D1_P1_Low_T2_18_Mid/2.png", "Image/P27_D1_P1_Low_T2_18_Mid/3.png", "Image/P27_D1_P1_Low_T2_18_Mid/4.png", "Image/P27_D1_P1_Low_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 977.3125 seconds to 989.0332 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P2_High_T1_23_Right", "images": ["Image/P26_D3_P2_High_T1_23_Right/1.png", "Image/P26_D3_P2_High_T1_23_Right/2.png", "Image/P26_D3_P2_High_T1_23_Right/3.png", "Image/P26_D3_P2_High_T1_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 903.8956 seconds to 913.3459 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P1_High_T1_19_Mid", "images": ["Image/P26_D2_P1_High_T1_19_Mid/1.png", "Image/P26_D2_P1_High_T1_19_Mid/2.png", "Image/P26_D2_P1_High_T1_19_Mid/3.png", "Image/P26_D2_P1_High_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move the white base to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 1256.9915 seconds to 1268.39086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P5_Medium_T1_23_Right", "images": ["Image/P8_D3_P5_Medium_T1_23_Right/1.png", "Image/P8_D3_P5_Medium_T1_23_Right/2.png", "Image/P8_D3_P5_Medium_T1_23_Right/3.png", "Image/P8_D3_P5_Medium_T1_23_Right/4.png", "Image/P8_D3_P5_Medium_T1_23_Right/5.png", "Image/P8_D3_P5_Medium_T1_23_Right/6.png", "Image/P8_D3_P5_Medium_T1_23_Right/7.png", "Image/P8_D3_P5_Medium_T1_23_Right/8.png", "Image/P8_D3_P5_Medium_T1_23_Right/9.png", "Image/P8_D3_P5_Medium_T1_23_Right/10.png", "Image/P8_D3_P5_Medium_T1_23_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1614.9356 seconds to 1631.2086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P2_Low_T3_20_Right", "images": ["Image/P9_D1_P2_Low_T3_20_Right/1.png", "Image/P9_D1_P2_Low_T3_20_Right/2.png", "Image/P9_D1_P2_Low_T3_20_Right/3.png", "Image/P9_D1_P2_Low_T3_20_Right/4.png", "Image/P9_D1_P2_Low_T3_20_Right/5.png", "Image/P9_D1_P2_Low_T3_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1582.7448 seconds to 1595.2421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P5_Medium_T1_17_Left", "images": ["Image/P5_D1_P5_Medium_T1_17_Left/1.png", "Image/P5_D1_P5_Medium_T1_17_Left/2.png", "Image/P5_D1_P5_Medium_T1_17_Left/3.png", "Image/P5_D1_P5_Medium_T1_17_Left/4.png", "Image/P5_D1_P5_Medium_T1_17_Left/5.png", "Image/P5_D1_P5_Medium_T1_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup from here to here. Finished.\"\nThe frames roughly correspond to the speech interval from 1539.7182 seconds to 1551.2395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P8_Low_T1_23_Right", "images": ["Image/P15_D2_P8_Low_T1_23_Right/1.png", "Image/P15_D2_P8_Low_T1_23_Right/2.png", "Image/P15_D2_P8_Low_T1_23_Right/3.png", "Image/P15_D2_P8_Low_T1_23_Right/4.png", "Image/P15_D2_P8_Low_T1_23_Right/5.png", "Image/P15_D2_P8_Low_T1_23_Right/6.png", "Image/P15_D2_P8_Low_T1_23_Right/7.png", "Image/P15_D2_P8_Low_T1_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the floor into the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2922.3689 seconds to 2935.313 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P6_Low_T1_16_Right", "images": ["Image/P11_D1_P6_Low_T1_16_Right/1.png", "Image/P11_D1_P6_Low_T1_16_Right/2.png", "Image/P11_D1_P6_Low_T1_16_Right/3.png", "Image/P11_D1_P6_Low_T1_16_Right/4.png", "Image/P11_D1_P6_Low_T1_16_Right/5.png", "Image/P11_D1_P6_Low_T1_16_Right/6.png", "Image/P11_D1_P6_Low_T1_16_Right/7.png", "Image/P11_D1_P6_Low_T1_16_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the bottle to the left side of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2442.1125999999995 seconds to 2455.3502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P2_Medium_T2_7_Right", "images": ["Image/P28_D2_P2_Medium_T2_7_Right/1.png", "Image/P28_D2_P2_Medium_T2_7_Right/2.png", "Image/P28_D2_P2_Medium_T2_7_Right/3.png", "Image/P28_D2_P2_Medium_T2_7_Right/4.png", "Image/P28_D2_P2_Medium_T2_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 975.6202 seconds to 986.825 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P5_Low_T3_4_Right", "images": ["Image/P27_D1_P5_Low_T3_4_Right/1.png", "Image/P27_D1_P5_Low_T3_4_Right/2.png", "Image/P27_D1_P5_Low_T3_4_Right/3.png", "Image/P27_D1_P5_Low_T3_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1127.3971 seconds to 1137.4879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P1_Medium_T3_19_Right", "images": ["Image/P4_D2_P1_Medium_T3_19_Right/1.png", "Image/P4_D2_P1_Medium_T3_19_Right/2.png", "Image/P4_D2_P1_Medium_T3_19_Right/3.png", "Image/P4_D2_P1_Medium_T3_19_Right/4.png", "Image/P4_D2_P1_Medium_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the right bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 1719.5070999999998 seconds to 1732.6601999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P9_High_T2_5_Right", "images": ["Image/P18_D1_P9_High_T2_5_Right/1.png", "Image/P18_D1_P9_High_T2_5_Right/2.png", "Image/P18_D1_P9_High_T2_5_Right/3.png", "Image/P18_D1_P9_High_T2_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 637.41601 seconds to 647.34027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P6_Medium_T2_8_Left", "images": ["Image/P13_D3_P6_Medium_T2_8_Left/1.png", "Image/P13_D3_P6_Medium_T2_8_Left/2.png", "Image/P13_D3_P6_Medium_T2_8_Left/3.png", "Image/P13_D3_P6_Medium_T2_8_Left/4.png", "Image/P13_D3_P6_Medium_T2_8_Left/5.png", "Image/P13_D3_P6_Medium_T2_8_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the left green cup over here\"\nThe frames roughly correspond to the speech interval from 1343.48517 seconds to 1354.53607 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P7_Low_T3_17_Mid", "images": ["Image/P28_D3_P7_Low_T3_17_Mid/1.png", "Image/P28_D3_P7_Low_T3_17_Mid/2.png", "Image/P28_D3_P7_Low_T3_17_Mid/3.png", "Image/P28_D3_P7_Low_T3_17_Mid/4.png", "Image/P28_D3_P7_Low_T3_17_Mid/5.png", "Image/P28_D3_P7_Low_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase under the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 61.23020000000001 seconds to 74.78632000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P7_High_T3_12_Right", "images": ["Image/P21_D1_P7_High_T3_12_Right/1.png", "Image/P21_D1_P7_High_T3_12_Right/2.png", "Image/P21_D1_P7_High_T3_12_Right/3.png", "Image/P21_D1_P7_High_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 852.4165399999999 seconds to 864.0946099999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P6_Medium_T2_20_Mid", "images": ["Image/P24_D1_P6_Medium_T2_20_Mid/1.png", "Image/P24_D1_P6_Medium_T2_20_Mid/2.png", "Image/P24_D1_P6_Medium_T2_20_Mid/3.png", "Image/P24_D1_P6_Medium_T2_20_Mid/4.png", "Image/P24_D1_P6_Medium_T2_20_Mid/5.png", "Image/P24_D1_P6_Medium_T2_20_Mid/6.png", "Image/P24_D1_P6_Medium_T2_20_Mid/7.png", "Image/P24_D1_P6_Medium_T2_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this bottle? And finish.\"\nThe frames roughly correspond to the speech interval from 1273.7819 seconds to 1286.8515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P2_Low_T3_2_Right", "images": ["Image/P4_D2_P2_Low_T3_2_Right/1.png", "Image/P4_D2_P2_Low_T3_2_Right/2.png", "Image/P4_D2_P2_Low_T3_2_Right/3.png", "Image/P4_D2_P2_Low_T3_2_Right/4.png", "Image/P4_D2_P2_Low_T3_2_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1929.4213999999995 seconds to 1941.0548 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P4_High_T1_18_Mid", "images": ["Image/P10_D2_P4_High_T1_18_Mid/1.png", "Image/P10_D2_P4_High_T1_18_Mid/2.png", "Image/P10_D2_P4_High_T1_18_Mid/3.png", "Image/P10_D2_P4_High_T1_18_Mid/4.png", "Image/P10_D2_P4_High_T1_18_Mid/5.png", "Image/P10_D2_P4_High_T1_18_Mid/6.png", "Image/P10_D2_P4_High_T1_18_Mid/7.png", "Image/P10_D2_P4_High_T1_18_Mid/8.png", "Image/P10_D2_P4_High_T1_18_Mid/9.png", "Image/P10_D2_P4_High_T1_18_Mid/10.png", "Image/P10_D2_P4_High_T1_18_Mid/11.png", "Image/P10_D2_P4_High_T1_18_Mid/12.png", "Image/P10_D2_P4_High_T1_18_Mid/13.png", "Image/P10_D2_P4_High_T1_18_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1346.12838 seconds to 1367.70298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P7_Low_T3_26_Mid", "images": ["Image/P1_D1_P7_Low_T3_26_Mid/1.png", "Image/P1_D1_P7_Low_T3_26_Mid/2.png", "Image/P1_D1_P7_Low_T3_26_Mid/3.png", "Image/P1_D1_P7_Low_T3_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 141.64181 seconds to 151.42804999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P5_Low_T3_5_Right", "images": ["Image/P3_D3_P5_Low_T3_5_Right/1.png", "Image/P3_D3_P5_Low_T3_5_Right/2.png", "Image/P3_D3_P5_Low_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 923.0888 seconds to 932.0819 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P3_Low_T1_13_Right", "images": ["Image/P21_D3_P3_Low_T1_13_Right/1.png", "Image/P21_D3_P3_Low_T1_13_Right/2.png", "Image/P21_D3_P3_Low_T1_13_Right/3.png", "Image/P21_D3_P3_Low_T1_13_Right/4.png", "Image/P21_D3_P3_Low_T1_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 643.0211999999999 seconds to 654.8387 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P1_Medium_T2_21_Mid", "images": ["Image/P8_D2_P1_Medium_T2_21_Mid/1.png", "Image/P8_D2_P1_Medium_T2_21_Mid/2.png", "Image/P8_D2_P1_Medium_T2_21_Mid/3.png", "Image/P8_D2_P1_Medium_T2_21_Mid/4.png", "Image/P8_D2_P1_Medium_T2_21_Mid/5.png", "Image/P8_D2_P1_Medium_T2_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the white glass in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2298.963 seconds to 2310.6688000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P5_Medium_T2_3_Mid", "images": ["Image/P1_D2_P5_Medium_T2_3_Mid/1.png", "Image/P1_D2_P5_Medium_T2_3_Mid/2.png", "Image/P1_D2_P5_Medium_T2_3_Mid/3.png", "Image/P1_D2_P5_Medium_T2_3_Mid/4.png", "Image/P1_D2_P5_Medium_T2_3_Mid/5.png", "Image/P1_D2_P5_Medium_T2_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water to the middle bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 862.9969000000001 seconds to 874.4954 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P9_High_T2_6_Mid", "images": ["Image/P26_D1_P9_High_T2_6_Mid/1.png", "Image/P26_D1_P9_High_T2_6_Mid/2.png", "Image/P26_D1_P9_High_T2_6_Mid/3.png", "Image/P26_D1_P9_High_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish\"\nThe frames roughly correspond to the speech interval from 132.221 seconds to 142.38289999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P4_High_T1_6_Right", "images": ["Image/P20_D1_P4_High_T1_6_Right/1.png", "Image/P20_D1_P4_High_T1_6_Right/2.png", "Image/P20_D1_P4_High_T1_6_Right/3.png", "Image/P20_D1_P4_High_T1_6_Right/4.png", "Image/P20_D1_P4_High_T1_6_Right/5.png", "Image/P20_D1_P4_High_T1_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant over here? Thanks. Finish.\"\nThe frames roughly correspond to the speech interval from 212.0009 seconds to 223.8045 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P2_Medium_T2_25_Mid", "images": ["Image/P1_D2_P2_Medium_T2_25_Mid/1.png", "Image/P1_D2_P2_Medium_T2_25_Mid/2.png", "Image/P1_D2_P2_Medium_T2_25_Mid/3.png", "Image/P1_D2_P2_Medium_T2_25_Mid/4.png", "Image/P1_D2_P2_Medium_T2_25_Mid/5.png", "Image/P1_D2_P2_Medium_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water in the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1023.9762 seconds to 1035.6003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P2_Medium_T2_5_Right", "images": ["Image/P3_D3_P2_Medium_T2_5_Right/1.png", "Image/P3_D3_P2_Medium_T2_5_Right/2.png", "Image/P3_D3_P2_Medium_T2_5_Right/3.png", "Image/P3_D3_P2_Medium_T2_5_Right/4.png", "Image/P3_D3_P2_Medium_T2_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 690.0482999999999 seconds to 701.3251 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P8_High_T3_10_Mid", "images": ["Image/P11_D3_P8_High_T3_10_Mid/1.png", "Image/P11_D3_P8_High_T3_10_Mid/2.png", "Image/P11_D3_P8_High_T3_10_Mid/3.png", "Image/P11_D3_P8_High_T3_10_Mid/4.png", "Image/P11_D3_P8_High_T3_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the red bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 1975.6964 seconds to 1993.3044 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P9_Low_T3_5_Right", "images": ["Image/P4_D2_P9_Low_T3_5_Right/1.png", "Image/P4_D2_P9_Low_T3_5_Right/2.png", "Image/P4_D2_P9_Low_T3_5_Right/3.png", "Image/P4_D2_P9_Low_T3_5_Right/4.png", "Image/P4_D2_P9_Low_T3_5_Right/5.png", "Image/P4_D2_P9_Low_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 1649.2834999999998 seconds to 1663.3947999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P9_Low_T1_5_Mid", "images": ["Image/P25_D1_P9_Low_T1_5_Mid/1.png", "Image/P25_D1_P9_Low_T1_5_Mid/2.png", "Image/P25_D1_P9_Low_T1_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 423.0776 seconds to 431.821 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P8_Medium_T1_12_Right", "images": ["Image/P11_D3_P8_Medium_T1_12_Right/1.png", "Image/P11_D3_P8_Medium_T1_12_Right/2.png", "Image/P11_D3_P8_Medium_T1_12_Right/3.png", "Image/P11_D3_P8_Medium_T1_12_Right/4.png", "Image/P11_D3_P8_Medium_T1_12_Right/5.png", "Image/P11_D3_P8_Medium_T1_12_Right/6.png", "Image/P11_D3_P8_Medium_T1_12_Right/7.png", "Image/P11_D3_P8_Medium_T1_12_Right/8.png", "Image/P11_D3_P8_Medium_T1_12_Right/9.png", "Image/P11_D3_P8_Medium_T1_12_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first red bottle from the right side to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1809.2549 seconds to 1825.3594 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P9_Low_T2_6_Left", "images": ["Image/P11_D2_P9_Low_T2_6_Left/1.png", "Image/P11_D2_P9_Low_T2_6_Left/2.png", "Image/P11_D2_P9_Low_T2_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant. Please finish.\"\nThe frames roughly correspond to the speech interval from 595.774 seconds to 605.3833 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P1_High_T1_7_Mid", "images": ["Image/P20_D1_P1_High_T1_7_Mid/1.png", "Image/P20_D1_P1_High_T1_7_Mid/2.png", "Image/P20_D1_P1_High_T1_7_Mid/3.png", "Image/P20_D1_P1_High_T1_7_Mid/4.png", "Image/P20_D1_P1_High_T1_7_Mid/5.png", "Image/P20_D1_P1_High_T1_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move this club\"\nThe frames roughly correspond to the speech interval from 731.5748000000001 seconds to 746.8314 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P9_Low_T3_22_Mid", "images": ["Image/P3_D2_P9_Low_T3_22_Mid/1.png", "Image/P3_D2_P9_Low_T3_22_Mid/2.png", "Image/P3_D2_P9_Low_T3_22_Mid/3.png", "Image/P3_D2_P9_Low_T3_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean this item? Finished.\"\nThe frames roughly correspond to the speech interval from 2457.0262 seconds to 2466.1021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P8_Medium_T2_17_Mid", "images": ["Image/P16_D1_P8_Medium_T2_17_Mid/1.png", "Image/P16_D1_P8_Medium_T2_17_Mid/2.png", "Image/P16_D1_P8_Medium_T2_17_Mid/3.png", "Image/P16_D1_P8_Medium_T2_17_Mid/4.png", "Image/P16_D1_P8_Medium_T2_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish\"\nThe frames roughly correspond to the speech interval from 1432.5671 seconds to 1443.7444 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P2_Low_T2_18_Left", "images": ["Image/P17_D2_P2_Low_T2_18_Left/1.png", "Image/P17_D2_P2_Low_T2_18_Left/2.png", "Image/P17_D2_P2_Low_T2_18_Left/3.png", "Image/P17_D2_P2_Low_T2_18_Left/4.png", "Image/P17_D2_P2_Low_T2_18_Left/5.png", "Image/P17_D2_P2_Low_T2_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that brown cup right in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 585.0517 seconds to 597.2007 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P8_Low_T1_4_Left", "images": ["Image/P6_D2_P8_Low_T1_4_Left/1.png", "Image/P6_D2_P8_Low_T1_4_Left/2.png", "Image/P6_D2_P8_Low_T1_4_Left/3.png", "Image/P6_D2_P8_Low_T1_4_Left/4.png", "Image/P6_D2_P8_Low_T1_4_Left/5.png", "Image/P6_D2_P8_Low_T1_4_Left/6.png", "Image/P6_D2_P8_Low_T1_4_Left/7.png", "Image/P6_D2_P8_Low_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1229.6978 seconds to 1251.0238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P7_High_T2_21_Left", "images": ["Image/P25_D2_P7_High_T2_21_Left/1.png", "Image/P25_D2_P7_High_T2_21_Left/2.png", "Image/P25_D2_P7_High_T2_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 789.6613 seconds to 798.0972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P9_High_T2_11_Mid", "images": ["Image/P16_D2_P9_High_T2_11_Mid/1.png", "Image/P16_D2_P9_High_T2_11_Mid/2.png", "Image/P16_D2_P9_High_T2_11_Mid/3.png", "Image/P16_D2_P9_High_T2_11_Mid/4.png", "Image/P16_D2_P9_High_T2_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 684.46225 seconds to 697.4905000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P7_Medium_T1_5_Mid", "images": ["Image/P10_D2_P7_Medium_T1_5_Mid/1.png", "Image/P10_D2_P7_Medium_T1_5_Mid/2.png", "Image/P10_D2_P7_Medium_T1_5_Mid/3.png", "Image/P10_D2_P7_Medium_T1_5_Mid/4.png", "Image/P10_D2_P7_Medium_T1_5_Mid/5.png", "Image/P10_D2_P7_Medium_T1_5_Mid/6.png", "Image/P10_D2_P7_Medium_T1_5_Mid/7.png", "Image/P10_D2_P7_Medium_T1_5_Mid/8.png", "Image/P10_D2_P7_Medium_T1_5_Mid/9.png", "Image/P10_D2_P7_Medium_T1_5_Mid/10.png", "Image/P10_D2_P7_Medium_T1_5_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1671.54778 seconds to 1688.0565800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P6_High_T1_15_Right", "images": ["Image/P13_D2_P6_High_T1_15_Right/1.png", "Image/P13_D2_P6_High_T1_15_Right/2.png", "Image/P13_D2_P6_High_T1_15_Right/3.png", "Image/P13_D2_P6_High_T1_15_Right/4.png", "Image/P13_D2_P6_High_T1_15_Right/5.png", "Image/P13_D2_P6_High_T1_15_Right/6.png", "Image/P13_D2_P6_High_T1_15_Right/7.png", "Image/P13_D2_P6_High_T1_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from over there to the middle shelf right here\"\nThe frames roughly correspond to the speech interval from 938.65496 seconds to 951.64106 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P4_Medium_T1_18_Right", "images": ["Image/P22_D2_P4_Medium_T1_18_Right/1.png", "Image/P22_D2_P4_Medium_T1_18_Right/2.png", "Image/P22_D2_P4_Medium_T1_18_Right/3.png", "Image/P22_D2_P4_Medium_T1_18_Right/4.png", "Image/P22_D2_P4_Medium_T1_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put that cup over there. Finish.\"\nThe frames roughly correspond to the speech interval from 701.2136999999999 seconds to 718.1393999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P1_Low_T1_5_Right", "images": ["Image/P17_D1_P1_Low_T1_5_Right/1.png", "Image/P17_D1_P1_Low_T1_5_Right/2.png", "Image/P17_D1_P1_Low_T1_5_Right/3.png", "Image/P17_D1_P1_Low_T1_5_Right/4.png", "Image/P17_D1_P1_Low_T1_5_Right/5.png", "Image/P17_D1_P1_Low_T1_5_Right/6.png", "Image/P17_D1_P1_Low_T1_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green potted plant to this basket right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1901.0811 seconds to 1914.2068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P8_High_T1_20_Left", "images": ["Image/P17_D3_P8_High_T1_20_Left/1.png", "Image/P17_D3_P8_High_T1_20_Left/2.png", "Image/P17_D3_P8_High_T1_20_Left/3.png", "Image/P17_D3_P8_High_T1_20_Left/4.png", "Image/P17_D3_P8_High_T1_20_Left/5.png", "Image/P17_D3_P8_High_T1_20_Left/6.png", "Image/P17_D3_P8_High_T1_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white vase on the floor to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1300.8419 seconds to 1313.7581 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P8_High_T1_27_Mid", "images": ["Image/P19_D1_P8_High_T1_27_Mid/1.png", "Image/P19_D1_P8_High_T1_27_Mid/2.png", "Image/P19_D1_P8_High_T1_27_Mid/3.png", "Image/P19_D1_P8_High_T1_27_Mid/4.png", "Image/P19_D1_P8_High_T1_27_Mid/5.png", "Image/P19_D1_P8_High_T1_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please move this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 628.4669 seconds to 641.3561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P6_Low_T2_13_Left", "images": ["Image/P22_D3_P6_Low_T2_13_Left/1.png", "Image/P22_D3_P6_Low_T2_13_Left/2.png", "Image/P22_D3_P6_Low_T2_13_Left/3.png", "Image/P22_D3_P6_Low_T2_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water that plant. Finish\"\nThe frames roughly correspond to the speech interval from 176.1687 seconds to 185.3143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P6_Low_T3_25_Left", "images": ["Image/P12_D1_P6_Low_T3_25_Left/1.png", "Image/P12_D1_P6_Low_T3_25_Left/2.png", "Image/P12_D1_P6_Low_T3_25_Left/3.png", "Image/P12_D1_P6_Low_T3_25_Left/4.png", "Image/P12_D1_P6_Low_T3_25_Left/5.png", "Image/P12_D1_P6_Low_T3_25_Left/6.png", "Image/P12_D1_P6_Low_T3_25_Left/7.png", "Image/P12_D1_P6_Low_T3_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1641.8581 seconds to 1656.6698 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P9_High_T3_5_Left", "images": ["Image/P9_D3_P9_High_T3_5_Left/1.png", "Image/P9_D3_P9_High_T3_5_Left/2.png", "Image/P9_D3_P9_High_T3_5_Left/3.png", "Image/P9_D3_P9_High_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you help me to clean the plants? Finish\"\nThe frames roughly correspond to the speech interval from 195.14874 seconds to 205.63158 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P6_Medium_T2_24_Left", "images": ["Image/P1_D2_P6_Medium_T2_24_Left/1.png", "Image/P1_D2_P6_Medium_T2_24_Left/2.png", "Image/P1_D2_P6_Medium_T2_24_Left/3.png", "Image/P1_D2_P6_Medium_T2_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1091.5116 seconds to 1101.3542 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P5_Low_T2_15_Left", "images": ["Image/P2_D2_P5_Low_T2_15_Left/1.png", "Image/P2_D2_P5_Low_T2_15_Left/2.png", "Image/P2_D2_P5_Low_T2_15_Left/3.png", "Image/P2_D2_P5_Low_T2_15_Left/4.png", "Image/P2_D2_P5_Low_T2_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please water that plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2756.9431999999997 seconds to 2767.6057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P1_High_T3_25_Right", "images": ["Image/P28_D2_P1_High_T3_25_Right/1.png", "Image/P28_D2_P1_High_T3_25_Right/2.png", "Image/P28_D2_P1_High_T3_25_Right/3.png", "Image/P28_D2_P1_High_T3_25_Right/4.png", "Image/P28_D2_P1_High_T3_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that white vase over there. Finish\"\nThe frames roughly correspond to the speech interval from 907.511 seconds to 918.9945 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P7_Low_T1_7_Right", "images": ["Image/P29_D3_P7_Low_T1_7_Right/1.png", "Image/P29_D3_P7_Low_T1_7_Right/2.png", "Image/P29_D3_P7_Low_T1_7_Right/3.png", "Image/P29_D3_P7_Low_T1_7_Right/4.png", "Image/P29_D3_P7_Low_T1_7_Right/5.png", "Image/P29_D3_P7_Low_T1_7_Right/6.png", "Image/P29_D3_P7_Low_T1_7_Right/7.png", "Image/P29_D3_P7_Low_T1_7_Right/8.png", "Image/P29_D3_P7_Low_T1_7_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 1315.21258 seconds to 1331.08156 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P5_Medium_T2_11_Left", "images": ["Image/P5_D3_P5_Medium_T2_11_Left/1.png", "Image/P5_D3_P5_Medium_T2_11_Left/2.png", "Image/P5_D3_P5_Medium_T2_11_Left/3.png", "Image/P5_D3_P5_Medium_T2_11_Left/4.png", "Image/P5_D3_P5_Medium_T2_11_Left/5.png", "Image/P5_D3_P5_Medium_T2_11_Left/6.png", "Image/P5_D3_P5_Medium_T2_11_Left/7.png", "Image/P5_D3_P5_Medium_T2_11_Left/8.png", "Image/P5_D3_P5_Medium_T2_11_Left/9.png", "Image/P5_D3_P5_Medium_T2_11_Left/10.png", "Image/P5_D3_P5_Medium_T2_11_Left/11.png", "Image/P5_D3_P5_Medium_T2_11_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle On-the-shelf and the lemon bottle. Finished.\"\nThe frames roughly correspond to the speech interval from 2779.1391999999996 seconds to 2796.5654 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P9_Medium_T2_24_Right", "images": ["Image/P9_D1_P9_Medium_T2_24_Right/1.png", "Image/P9_D1_P9_Medium_T2_24_Right/2.png", "Image/P9_D1_P9_Medium_T2_24_Right/3.png", "Image/P9_D1_P9_Medium_T2_24_Right/4.png", "Image/P9_D1_P9_Medium_T2_24_Right/5.png", "Image/P9_D1_P9_Medium_T2_24_Right/6.png", "Image/P9_D1_P9_Medium_T2_24_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1607.3930999999998 seconds to 1620.4863 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P1_Low_T2_21_Left", "images": ["Image/P13_D3_P1_Low_T2_21_Left/1.png", "Image/P13_D3_P1_Low_T2_21_Left/2.png", "Image/P13_D3_P1_Low_T2_21_Left/3.png", "Image/P13_D3_P1_Low_T2_21_Left/4.png", "Image/P13_D3_P1_Low_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the white vase up there\"\nThe frames roughly correspond to the speech interval from 1446.3786 seconds to 1456.8196 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P4_Low_T3_3_Right", "images": ["Image/P14_D1_P4_Low_T3_3_Right/1.png", "Image/P14_D1_P4_Low_T3_3_Right/2.png", "Image/P14_D1_P4_Low_T3_3_Right/3.png", "Image/P14_D1_P4_Low_T3_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that bottle up. Finished.\"\nThe frames roughly correspond to the speech interval from 535.1519000000001 seconds to 544.297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P4_Medium_T3_14_Left", "images": ["Image/P12_D1_P4_Medium_T3_14_Left/1.png", "Image/P12_D1_P4_Medium_T3_14_Left/2.png", "Image/P12_D1_P4_Medium_T3_14_Left/3.png", "Image/P12_D1_P4_Medium_T3_14_Left/4.png", "Image/P12_D1_P4_Medium_T3_14_Left/5.png", "Image/P12_D1_P4_Medium_T3_14_Left/6.png", "Image/P12_D1_P4_Medium_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1783.6613 seconds to 1795.9369000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P2_Low_T3_26_Right", "images": ["Image/P21_D3_P2_Low_T3_26_Right/1.png", "Image/P21_D3_P2_Low_T3_26_Right/2.png", "Image/P21_D3_P2_Low_T3_26_Right/3.png", "Image/P21_D3_P2_Low_T3_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 478.7082 seconds to 488.3404 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P1_High_T3_20_Mid", "images": ["Image/P1_D2_P1_High_T3_20_Mid/1.png", "Image/P1_D2_P1_High_T3_20_Mid/2.png", "Image/P1_D2_P1_High_T3_20_Mid/3.png", "Image/P1_D2_P1_High_T3_20_Mid/4.png", "Image/P1_D2_P1_High_T3_20_Mid/5.png", "Image/P1_D2_P1_High_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 752.7892 seconds to 763.7176000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P3_Low_T1_13_Right", "images": ["Image/P7_D1_P3_Low_T1_13_Right/1.png", "Image/P7_D1_P3_Low_T1_13_Right/2.png", "Image/P7_D1_P3_Low_T1_13_Right/3.png", "Image/P7_D1_P3_Low_T1_13_Right/4.png", "Image/P7_D1_P3_Low_T1_13_Right/5.png", "Image/P7_D1_P3_Low_T1_13_Right/6.png", "Image/P7_D1_P3_Low_T1_13_Right/7.png", "Image/P7_D1_P3_Low_T1_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 664.6854 seconds to 680.721 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P3_High_T2_9_Mid", "images": ["Image/P12_D1_P3_High_T2_9_Mid/1.png", "Image/P12_D1_P3_High_T2_9_Mid/2.png", "Image/P12_D1_P3_High_T2_9_Mid/3.png", "Image/P12_D1_P3_High_T2_9_Mid/4.png", "Image/P12_D1_P3_High_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1574.39 seconds to 1587.4738 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P4_High_T2_27_Right", "images": ["Image/P28_D3_P4_High_T2_27_Right/1.png", "Image/P28_D3_P4_High_T2_27_Right/2.png", "Image/P28_D3_P4_High_T2_27_Right/3.png", "Image/P28_D3_P4_High_T2_27_Right/4.png", "Image/P28_D3_P4_High_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish\"\nThe frames roughly correspond to the speech interval from 526.0137 seconds to 537.2768000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P1_Medium_T2_8_Left", "images": ["Image/P23_D1_P1_Medium_T2_8_Left/1.png", "Image/P23_D1_P1_Medium_T2_8_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1601.5503 seconds to 1609.8053 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_Medium_T3_3_Left", "images": ["Image/P8_D3_P3_Medium_T3_3_Left/1.png", "Image/P8_D3_P3_Medium_T3_3_Left/2.png", "Image/P8_D3_P3_Medium_T3_3_Left/3.png", "Image/P8_D3_P3_Medium_T3_3_Left/4.png", "Image/P8_D3_P3_Medium_T3_3_Left/5.png", "Image/P8_D3_P3_Medium_T3_3_Left/6.png", "Image/P8_D3_P3_Medium_T3_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean that green glass that is on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1804.3438 seconds to 1817.1949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P6_Low_T3_25_Mid", "images": ["Image/P22_D2_P6_Low_T3_25_Mid/1.png", "Image/P22_D2_P6_Low_T3_25_Mid/2.png", "Image/P22_D2_P6_Low_T3_25_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 916.4013 seconds to 927.1325 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P4_Medium_T1_25_Left", "images": ["Image/P23_D2_P4_Medium_T1_25_Left/1.png", "Image/P23_D2_P4_Medium_T1_25_Left/2.png", "Image/P23_D2_P4_Medium_T1_25_Left/3.png", "Image/P23_D2_P4_Medium_T1_25_Left/4.png", "Image/P23_D2_P4_Medium_T1_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the one on the left over there. Finish.\"\nThe frames roughly correspond to the speech interval from 324.73303999999996 seconds to 335.9104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P7_High_T3_2_Mid", "images": ["Image/P9_D1_P7_High_T3_2_Mid/1.png", "Image/P9_D1_P7_High_T3_2_Mid/2.png", "Image/P9_D1_P7_High_T3_2_Mid/3.png", "Image/P9_D1_P7_High_T3_2_Mid/4.png", "Image/P9_D1_P7_High_T3_2_Mid/5.png", "Image/P9_D1_P7_High_T3_2_Mid/6.png", "Image/P9_D1_P7_High_T3_2_Mid/7.png", "Image/P9_D1_P7_High_T3_2_Mid/8.png", "Image/P9_D1_P7_High_T3_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the green bottle in this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1433.6582 seconds to 1449.2238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P6_Low_T3_16_Mid", "images": ["Image/P15_D3_P6_Low_T3_16_Mid/1.png", "Image/P15_D3_P6_Low_T3_16_Mid/2.png", "Image/P15_D3_P6_Low_T3_16_Mid/3.png", "Image/P15_D3_P6_Low_T3_16_Mid/4.png", "Image/P15_D3_P6_Low_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 197.32180000000005 seconds to 209.22240000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P3_Low_T3_18_Mid", "images": ["Image/P19_D1_P3_Low_T3_18_Mid/1.png", "Image/P19_D1_P3_Low_T3_18_Mid/2.png", "Image/P19_D1_P3_Low_T3_18_Mid/3.png", "Image/P19_D1_P3_Low_T3_18_Mid/4.png", "Image/P19_D1_P3_Low_T3_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase here. Finish.\"\nThe frames roughly correspond to the speech interval from 758.2565999999999 seconds to 769.5174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P2_High_T2_3_Mid", "images": ["Image/P2_D1_P2_High_T2_3_Mid/1.png", "Image/P2_D1_P2_High_T2_3_Mid/2.png", "Image/P2_D1_P2_High_T2_3_Mid/3.png", "Image/P2_D1_P2_High_T2_3_Mid/4.png", "Image/P2_D1_P2_High_T2_3_Mid/5.png", "Image/P2_D1_P2_High_T2_3_Mid/6.png", "Image/P2_D1_P2_High_T2_3_Mid/7.png", "Image/P2_D1_P2_High_T2_3_Mid/8.png", "Image/P2_D1_P2_High_T2_3_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 787.9176 seconds to 805.9699 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P3_Medium_T1_5_Right", "images": ["Image/P17_D3_P3_Medium_T1_5_Right/1.png", "Image/P17_D3_P3_Medium_T1_5_Right/2.png", "Image/P17_D3_P3_Medium_T1_5_Right/3.png", "Image/P17_D3_P3_Medium_T1_5_Right/4.png", "Image/P17_D3_P3_Medium_T1_5_Right/5.png", "Image/P17_D3_P3_Medium_T1_5_Right/6.png", "Image/P17_D3_P3_Medium_T1_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant on the top shelf on the right down to the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 904.3615 seconds to 917.8076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P9_Low_T1_22_Left", "images": ["Image/P10_D3_P9_Low_T1_22_Left/1.png", "Image/P10_D3_P9_Low_T1_22_Left/2.png", "Image/P10_D3_P9_Low_T1_22_Left/3.png", "Image/P10_D3_P9_Low_T1_22_Left/4.png", "Image/P10_D3_P9_Low_T1_22_Left/5.png", "Image/P10_D3_P9_Low_T1_22_Left/6.png", "Image/P10_D3_P9_Low_T1_22_Left/7.png", "Image/P10_D3_P9_Low_T1_22_Left/8.png", "Image/P10_D3_P9_Low_T1_22_Left/9.png", "Image/P10_D3_P9_Low_T1_22_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 591.64919 seconds to 607.01599 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P8_High_T2_5_Left", "images": ["Image/P12_D3_P8_High_T2_5_Left/1.png", "Image/P12_D3_P8_High_T2_5_Left/2.png", "Image/P12_D3_P8_High_T2_5_Left/3.png", "Image/P12_D3_P8_High_T2_5_Left/4.png", "Image/P12_D3_P8_High_T2_5_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2693.16469 seconds to 2703.4223899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P6_Low_T3_20_Mid", "images": ["Image/P26_D2_P6_Low_T3_20_Mid/1.png", "Image/P26_D2_P6_Low_T3_20_Mid/2.png", "Image/P26_D2_P6_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this? Finish.\"\nThe frames roughly correspond to the speech interval from 1546.7891 seconds to 1555.8363 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P8_Low_T2_6_Right", "images": ["Image/P4_D2_P8_Low_T2_6_Right/1.png", "Image/P4_D2_P8_Low_T2_6_Right/2.png", "Image/P4_D2_P8_Low_T2_6_Right/3.png", "Image/P4_D2_P8_Low_T2_6_Right/4.png", "Image/P4_D2_P8_Low_T2_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1740.1816 seconds to 1753.6929999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P6_Medium_T1_3_Mid", "images": ["Image/P10_D1_P6_Medium_T1_3_Mid/1.png", "Image/P10_D1_P6_Medium_T1_3_Mid/2.png", "Image/P10_D1_P6_Medium_T1_3_Mid/3.png", "Image/P10_D1_P6_Medium_T1_3_Mid/4.png", "Image/P10_D1_P6_Medium_T1_3_Mid/5.png", "Image/P10_D1_P6_Medium_T1_3_Mid/6.png", "Image/P10_D1_P6_Medium_T1_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2359.0712 seconds to 2373.9788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P2_Medium_T2_2_Right", "images": ["Image/P29_D2_P2_Medium_T2_2_Right/1.png", "Image/P29_D2_P2_Medium_T2_2_Right/2.png", "Image/P29_D2_P2_Medium_T2_2_Right/3.png", "Image/P29_D2_P2_Medium_T2_2_Right/4.png", "Image/P29_D2_P2_Medium_T2_2_Right/5.png", "Image/P29_D2_P2_Medium_T2_2_Right/6.png", "Image/P29_D2_P2_Medium_T2_2_Right/7.png", "Image/P29_D2_P2_Medium_T2_2_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 111.38022 seconds to 125.48083 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P7_Low_T3_2_Left", "images": ["Image/P24_D1_P7_Low_T3_2_Left/1.png", "Image/P24_D1_P7_Low_T3_2_Left/2.png", "Image/P24_D1_P7_Low_T3_2_Left/3.png", "Image/P24_D1_P7_Low_T3_2_Left/4.png", "Image/P24_D1_P7_Low_T3_2_Left/5.png", "Image/P24_D1_P7_Low_T3_2_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1031.0055 seconds to 1042.7256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P1_High_T1_7_Mid", "images": ["Image/P22_D1_P1_High_T1_7_Mid/1.png", "Image/P22_D1_P1_High_T1_7_Mid/2.png", "Image/P22_D1_P1_High_T1_7_Mid/3.png", "Image/P22_D1_P1_High_T1_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Put the cup in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 1409.53175 seconds to 1419.31678 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P7_High_T1_26_Mid", "images": ["Image/P8_D2_P7_High_T1_26_Mid/1.png", "Image/P8_D2_P7_High_T1_26_Mid/2.png", "Image/P8_D2_P7_High_T1_26_Mid/3.png", "Image/P8_D2_P7_High_T1_26_Mid/4.png", "Image/P8_D2_P7_High_T1_26_Mid/5.png", "Image/P8_D2_P7_High_T1_26_Mid/6.png", "Image/P8_D2_P7_High_T1_26_Mid/7.png", "Image/P8_D2_P7_High_T1_26_Mid/8.png", "Image/P8_D2_P7_High_T1_26_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you move this white cup on this shelf on the top\"\nThe frames roughly correspond to the speech interval from 2491.2716 seconds to 2506.0768000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P1_High_T2_10_Left", "images": ["Image/P5_D1_P1_High_T2_10_Left/1.png", "Image/P5_D1_P1_High_T2_10_Left/2.png", "Image/P5_D1_P1_High_T2_10_Left/3.png", "Image/P5_D1_P1_High_T2_10_Left/4.png", "Image/P5_D1_P1_High_T2_10_Left/5.png", "Image/P5_D1_P1_High_T2_10_Left/6.png", "Image/P5_D1_P1_High_T2_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle next to the white cup. Finished.\"\nThe frames roughly correspond to the speech interval from 1581.8121 seconds to 1594.3523 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P9_Medium_T3_25_Mid", "images": ["Image/P22_D3_P9_Medium_T3_25_Mid/1.png", "Image/P22_D3_P9_Medium_T3_25_Mid/2.png", "Image/P22_D3_P9_Medium_T3_25_Mid/3.png", "Image/P22_D3_P9_Medium_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 370.0316 seconds to 379.5518 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P2_Medium_T3_9_Left", "images": ["Image/P18_D2_P2_Medium_T3_9_Left/1.png", "Image/P18_D2_P2_Medium_T3_9_Left/2.png", "Image/P18_D2_P2_Medium_T3_9_Left/3.png", "Image/P18_D2_P2_Medium_T3_9_Left/4.png", "Image/P18_D2_P2_Medium_T3_9_Left/5.png", "Image/P18_D2_P2_Medium_T3_9_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that first cup that's separate from the first two. Finish.\"\nThe frames roughly correspond to the speech interval from 297.5847 seconds to 310.2778 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P2_Low_T3_25_Right", "images": ["Image/P26_D3_P2_Low_T3_25_Right/1.png", "Image/P26_D3_P2_Low_T3_25_Right/2.png", "Image/P26_D3_P2_Low_T3_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase up there? Finish.\"\nThe frames roughly correspond to the speech interval from 1015.9375 seconds to 1026.0143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P6_High_T3_8_Left", "images": ["Image/P2_D3_P6_High_T3_8_Left/1.png", "Image/P2_D3_P6_High_T3_8_Left/2.png", "Image/P2_D3_P6_High_T3_8_Left/3.png", "Image/P2_D3_P6_High_T3_8_Left/4.png", "Image/P2_D3_P6_High_T3_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that green mug for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1880.5966000000003 seconds to 1894.3483 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P8_Low_T1_27_Right", "images": ["Image/P19_D3_P8_Low_T1_27_Right/1.png", "Image/P19_D3_P8_Low_T1_27_Right/2.png", "Image/P19_D3_P8_Low_T1_27_Right/3.png", "Image/P19_D3_P8_Low_T1_27_Right/4.png", "Image/P19_D3_P8_Low_T1_27_Right/5.png", "Image/P19_D3_P8_Low_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this glass up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2164.8601999999996 seconds to 2178.723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P1_Medium_T1_20_Mid", "images": ["Image/P20_D2_P1_Medium_T1_20_Mid/1.png", "Image/P20_D2_P1_Medium_T1_20_Mid/2.png", "Image/P20_D2_P1_Medium_T1_20_Mid/3.png", "Image/P20_D2_P1_Medium_T1_20_Mid/4.png", "Image/P20_D2_P1_Medium_T1_20_Mid/5.png", "Image/P20_D2_P1_Medium_T1_20_Mid/6.png", "Image/P20_D2_P1_Medium_T1_20_Mid/7.png", "Image/P20_D2_P1_Medium_T1_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. He moves that middle bottle to the bottom of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3232.1205000000004 seconds to 3250.5489 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P4_Low_T1_14_Left", "images": ["Image/P25_D3_P4_Low_T1_14_Left/1.png", "Image/P25_D3_P4_Low_T1_14_Left/2.png", "Image/P25_D3_P4_Low_T1_14_Left/3.png", "Image/P25_D3_P4_Low_T1_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1917.4924 seconds to 1926.1922 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P3_Medium_T2_4_Right", "images": ["Image/P30_D3_P3_Medium_T2_4_Right/1.png", "Image/P30_D3_P3_Medium_T2_4_Right/2.png", "Image/P30_D3_P3_Medium_T2_4_Right/3.png", "Image/P30_D3_P3_Medium_T2_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant closest to me. Finish.\"\nThe frames roughly correspond to the speech interval from 1285.3352 seconds to 1296.8177 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P2_Low_T1_4_Mid", "images": ["Image/P13_D3_P2_Low_T1_4_Mid/1.png", "Image/P13_D3_P2_Low_T1_4_Mid/2.png", "Image/P13_D3_P2_Low_T1_4_Mid/3.png", "Image/P13_D3_P2_Low_T1_4_Mid/4.png", "Image/P13_D3_P2_Low_T1_4_Mid/5.png", "Image/P13_D3_P2_Low_T1_4_Mid/6.png", "Image/P13_D3_P2_Low_T1_4_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from the top shelf to the floor down here\"\nThe frames roughly correspond to the speech interval from 1810.0521 seconds to 1822.2441 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P4_High_T3_21_Mid", "images": ["Image/P9_D3_P4_High_T3_21_Mid/1.png", "Image/P9_D3_P4_High_T3_21_Mid/2.png", "Image/P9_D3_P4_High_T3_21_Mid/3.png", "Image/P9_D3_P4_High_T3_21_Mid/4.png", "Image/P9_D3_P4_High_T3_21_Mid/5.png", "Image/P9_D3_P4_High_T3_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the bottle on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 840.0231 seconds to 852.3541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P4_Low_T3_23_Left", "images": ["Image/P4_D3_P4_Low_T3_23_Left/1.png", "Image/P4_D3_P4_Low_T3_23_Left/2.png", "Image/P4_D3_P4_Low_T3_23_Left/3.png", "Image/P4_D3_P4_Low_T3_23_Left/4.png", "Image/P4_D3_P4_Low_T3_23_Left/5.png", "Image/P4_D3_P4_Low_T3_23_Left/6.png", "Image/P4_D3_P4_Low_T3_23_Left/7.png", "Image/P4_D3_P4_Low_T3_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plants on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 650.4205 seconds to 665.6235999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P8_High_T2_8_Left", "images": ["Image/P15_D3_P8_High_T2_8_Left/1.png", "Image/P15_D3_P8_High_T2_8_Left/2.png", "Image/P15_D3_P8_High_T2_8_Left/3.png", "Image/P15_D3_P8_High_T2_8_Left/4.png", "Image/P15_D3_P8_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 255.96560000000005 seconds to 267.1435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P2_Low_T3_4_Mid", "images": ["Image/P11_D1_P2_Low_T3_4_Mid/1.png", "Image/P11_D1_P2_Low_T3_4_Mid/2.png", "Image/P11_D1_P2_Low_T3_4_Mid/3.png", "Image/P11_D1_P2_Low_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the plant in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2668.9824999999996 seconds to 2678.2256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P5_High_T3_9_Mid", "images": ["Image/P6_D1_P5_High_T3_9_Mid/1.png", "Image/P6_D1_P5_High_T3_9_Mid/2.png", "Image/P6_D1_P5_High_T3_9_Mid/3.png", "Image/P6_D1_P5_High_T3_9_Mid/4.png", "Image/P6_D1_P5_High_T3_9_Mid/5.png", "Image/P6_D1_P5_High_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1992.346 seconds to 2005.9578 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P4_Low_T2_27_Mid", "images": ["Image/P9_D1_P4_Low_T2_27_Mid/1.png", "Image/P9_D1_P4_Low_T2_27_Mid/2.png", "Image/P9_D1_P4_Low_T2_27_Mid/3.png", "Image/P9_D1_P4_Low_T2_27_Mid/4.png", "Image/P9_D1_P4_Low_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1915.048 seconds to 1926.4065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P3_High_T3_20_Right", "images": ["Image/P16_D2_P3_High_T3_20_Right/1.png", "Image/P16_D2_P3_High_T3_20_Right/2.png", "Image/P16_D2_P3_High_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish\"\nThe frames roughly correspond to the speech interval from 832.7230000000001 seconds to 842.6607000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P9_High_T3_3_Right", "images": ["Image/P3_D1_P9_High_T3_3_Right/1.png", "Image/P3_D1_P9_High_T3_3_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Would you clean this item?\"\nThe frames roughly correspond to the speech interval from 1447.1265 seconds to 1456.1064 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P1_Low_T3_17_Left", "images": ["Image/P23_D3_P1_Low_T3_17_Left/1.png", "Image/P23_D3_P1_Low_T3_17_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 666.12228 seconds to 674.37552 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P4_High_T3_5_Right", "images": ["Image/P23_D1_P4_High_T3_5_Right/1.png", "Image/P23_D1_P4_High_T3_5_Right/2.png", "Image/P23_D1_P4_High_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Not clean at the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1557.32592 seconds to 1566.0404 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P3_Medium_T2_11_Mid", "images": ["Image/P14_D1_P3_Medium_T2_11_Mid/1.png", "Image/P14_D1_P3_Medium_T2_11_Mid/2.png", "Image/P14_D1_P3_Medium_T2_11_Mid/3.png", "Image/P14_D1_P3_Medium_T2_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Fill that bottle up\"\nThe frames roughly correspond to the speech interval from 254.6097 seconds to 264.4924 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P5_High_T1_5_Right", "images": ["Image/P29_D3_P5_High_T1_5_Right/1.png", "Image/P29_D3_P5_High_T1_5_Right/2.png", "Image/P29_D3_P5_High_T1_5_Right/3.png", "Image/P29_D3_P5_High_T1_5_Right/4.png", "Image/P29_D3_P5_High_T1_5_Right/5.png", "Image/P29_D3_P5_High_T1_5_Right/6.png", "Image/P29_D3_P5_High_T1_5_Right/7.png", "Image/P29_D3_P5_High_T1_5_Right/8.png", "Image/P29_D3_P5_High_T1_5_Right/9.png", "Image/P29_D3_P5_High_T1_5_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1739.9511999999995 seconds to 1755.0679999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P9_High_T1_1_Mid", "images": ["Image/P4_D2_P9_High_T1_1_Mid/1.png", "Image/P4_D2_P9_High_T1_1_Mid/2.png", "Image/P4_D2_P9_High_T1_1_Mid/3.png", "Image/P4_D2_P9_High_T1_1_Mid/4.png", "Image/P4_D2_P9_High_T1_1_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green bottle to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1417.09929 seconds to 1429.05264 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P3_Low_T1_9_Right", "images": ["Image/P26_D3_P3_Low_T1_9_Right/1.png", "Image/P26_D3_P3_Low_T1_9_Right/2.png", "Image/P26_D3_P3_Low_T1_9_Right/3.png", "Image/P26_D3_P3_Low_T1_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move the green vase to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 865.1279000000001 seconds to 875.4409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P5_Low_T3_24_Mid", "images": ["Image/P20_D3_P5_Low_T3_24_Mid/1.png", "Image/P20_D3_P5_Low_T3_24_Mid/2.png", "Image/P20_D3_P5_Low_T3_24_Mid/3.png", "Image/P20_D3_P5_Low_T3_24_Mid/4.png", "Image/P20_D3_P5_Low_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey robot\"\nThe frames roughly correspond to the speech interval from 1539.6298 seconds to 1550.069 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P3_High_T2_18_Mid", "images": ["Image/P7_D2_P3_High_T2_18_Mid/1.png", "Image/P7_D2_P3_High_T2_18_Mid/2.png", "Image/P7_D2_P3_High_T2_18_Mid/3.png", "Image/P7_D2_P3_High_T2_18_Mid/4.png", "Image/P7_D2_P3_High_T2_18_Mid/5.png", "Image/P7_D2_P3_High_T2_18_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1347.0764 seconds to 1361.7756000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P2_Low_T2_25_Right", "images": ["Image/P7_D1_P2_Low_T2_25_Right/1.png", "Image/P7_D1_P2_Low_T2_25_Right/2.png", "Image/P7_D1_P2_Low_T2_25_Right/3.png", "Image/P7_D1_P2_Low_T2_25_Right/4.png", "Image/P7_D1_P2_Low_T2_25_Right/5.png", "Image/P7_D1_P2_Low_T2_25_Right/6.png", "Image/P7_D1_P2_Low_T2_25_Right/7.png", "Image/P7_D1_P2_Low_T2_25_Right/8.png", "Image/P7_D1_P2_Low_T2_25_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 332.1658 seconds to 348.4239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P4_Medium_T3_6_Left", "images": ["Image/P21_D2_P4_Medium_T3_6_Left/1.png", "Image/P21_D2_P4_Medium_T3_6_Left/2.png", "Image/P21_D2_P4_Medium_T3_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1408.11887 seconds to 1418.1973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P7_Medium_T2_19_Left", "images": ["Image/P25_D3_P7_Medium_T2_19_Left/1.png", "Image/P25_D3_P7_Medium_T2_19_Left/2.png", "Image/P25_D3_P7_Medium_T2_19_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 1821.152 seconds to 1829.477 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P7_High_T1_16_Mid", "images": ["Image/P1_D1_P7_High_T1_16_Mid/1.png", "Image/P1_D1_P7_High_T1_16_Mid/2.png", "Image/P1_D1_P7_High_T1_16_Mid/3.png", "Image/P1_D1_P7_High_T1_16_Mid/4.png", "Image/P1_D1_P7_High_T1_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 504.9224 seconds to 515.1696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P5_Low_T3_5_Mid", "images": ["Image/P13_D3_P5_Low_T3_5_Mid/1.png", "Image/P13_D3_P5_Low_T3_5_Mid/2.png", "Image/P13_D3_P5_Low_T3_5_Mid/3.png", "Image/P13_D3_P5_Low_T3_5_Mid/4.png", "Image/P13_D3_P5_Low_T3_5_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean the plant over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1556.4739 seconds to 1567.246 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P3_Medium_T2_25_Right", "images": ["Image/P5_D2_P3_Medium_T2_25_Right/1.png", "Image/P5_D2_P3_Medium_T2_25_Right/2.png", "Image/P5_D2_P3_Medium_T2_25_Right/3.png", "Image/P5_D2_P3_Medium_T2_25_Right/4.png", "Image/P5_D2_P3_Medium_T2_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Pour water into the cup\"\nThe frames roughly correspond to the speech interval from 647.2535 seconds to 660.6975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P5_Low_T2_14_Left", "images": ["Image/P9_D1_P5_Low_T2_14_Left/1.png", "Image/P9_D1_P5_Low_T2_14_Left/2.png", "Image/P9_D1_P5_Low_T2_14_Left/3.png", "Image/P9_D1_P5_Low_T2_14_Left/4.png", "Image/P9_D1_P5_Low_T2_14_Left/5.png", "Image/P9_D1_P5_Low_T2_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1332.7587999999998 seconds to 1345.4654999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P8_Low_T1_27_Right", "images": ["Image/P8_D2_P8_Low_T1_27_Right/1.png", "Image/P8_D2_P8_Low_T1_27_Right/2.png", "Image/P8_D2_P8_Low_T1_27_Right/3.png", "Image/P8_D2_P8_Low_T1_27_Right/4.png", "Image/P8_D2_P8_Low_T1_27_Right/5.png", "Image/P8_D2_P8_Low_T1_27_Right/6.png", "Image/P8_D2_P8_Low_T1_27_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Could you move that white glass on the top of this shelf? Just on it\"\nThe frames roughly correspond to the speech interval from 2155.865 seconds to 2168.2824 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P5_High_T3_8_Mid", "images": ["Image/P16_D3_P5_High_T3_8_Mid/1.png", "Image/P16_D3_P5_High_T3_8_Mid/2.png", "Image/P16_D3_P5_High_T3_8_Mid/3.png", "Image/P16_D3_P5_High_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 215.0002 seconds to 224.563 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P5_Medium_T2_3_Mid", "images": ["Image/P26_D1_P5_Medium_T2_3_Mid/1.png", "Image/P26_D1_P5_Medium_T2_3_Mid/2.png", "Image/P26_D1_P5_Medium_T2_3_Mid/3.png", "Image/P26_D1_P5_Medium_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into the middle canister? Finish.\"\nThe frames roughly correspond to the speech interval from 388.515 seconds to 399.5250999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P8_High_T1_10_Mid", "images": ["Image/P16_D2_P8_High_T1_10_Mid/1.png", "Image/P16_D2_P8_High_T1_10_Mid/2.png", "Image/P16_D2_P8_High_T1_10_Mid/3.png", "Image/P16_D2_P8_High_T1_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 990.5187 seconds to 999.5792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P5_High_T3_6_Left", "images": ["Image/P15_D2_P5_High_T3_6_Left/1.png", "Image/P15_D2_P5_High_T3_6_Left/2.png", "Image/P15_D2_P5_High_T3_6_Left/3.png", "Image/P15_D2_P5_High_T3_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2696.2412 seconds to 2705.5792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P5_Medium_T1_16_Right", "images": ["Image/P2_D2_P5_Medium_T1_16_Right/1.png", "Image/P2_D2_P5_Medium_T1_16_Right/2.png", "Image/P2_D2_P5_Medium_T1_16_Right/3.png", "Image/P2_D2_P5_Medium_T1_16_Right/4.png", "Image/P2_D2_P5_Medium_T1_16_Right/5.png", "Image/P2_D2_P5_Medium_T1_16_Right/6.png", "Image/P2_D2_P5_Medium_T1_16_Right/7.png", "Image/P2_D2_P5_Medium_T1_16_Right/8.png", "Image/P2_D2_P5_Medium_T1_16_Right/9.png", "Image/P2_D2_P5_Medium_T1_16_Right/10.png", "Image/P2_D2_P5_Medium_T1_16_Right/11.png", "Image/P2_D2_P5_Medium_T1_16_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2661.3556 seconds to 2678.1416999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P5_High_T1_15_Left", "images": ["Image/P27_D3_P5_High_T1_15_Left/1.png", "Image/P27_D3_P5_High_T1_15_Left/2.png", "Image/P27_D3_P5_High_T1_15_Left/3.png", "Image/P27_D3_P5_High_T1_15_Left/4.png", "Image/P27_D3_P5_High_T1_15_Left/5.png", "Image/P27_D3_P5_High_T1_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that plant and put it on the middle shelf over there? Finish\"\nThe frames roughly correspond to the speech interval from 249.4354 seconds to 261.5599 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P8_Low_T2_9_Mid", "images": ["Image/P1_D3_P8_Low_T2_9_Mid/1.png", "Image/P1_D3_P8_Low_T2_9_Mid/2.png", "Image/P1_D3_P8_Low_T2_9_Mid/3.png", "Image/P1_D3_P8_Low_T2_9_Mid/4.png", "Image/P1_D3_P8_Low_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the green cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1678.9210999999998 seconds to 1689.8608999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P2_High_T3_9_Mid", "images": ["Image/P27_D1_P2_High_T3_9_Mid/1.png", "Image/P27_D1_P2_High_T3_9_Mid/2.png", "Image/P27_D1_P2_High_T3_9_Mid/3.png", "Image/P27_D1_P2_High_T3_9_Mid/4.png", "Image/P27_D1_P2_High_T3_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this glass? Finish\"\nThe frames roughly correspond to the speech interval from 862.2921000000001 seconds to 874.2493000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P4_Low_T2_7_Left", "images": ["Image/P21_D1_P4_Low_T2_7_Left/1.png", "Image/P21_D1_P4_Low_T2_7_Left/2.png", "Image/P21_D1_P4_Low_T2_7_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1067.4926 seconds to 1077.1810999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P7_High_T2_14_Mid", "images": ["Image/P3_D2_P7_High_T2_14_Mid/1.png", "Image/P3_D2_P7_High_T2_14_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2251.7192 seconds to 2262.0181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P2_High_T3_6_Mid", "images": ["Image/P22_D3_P2_High_T3_6_Mid/1.png", "Image/P22_D3_P2_High_T3_6_Mid/2.png", "Image/P22_D3_P2_High_T3_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Finish.\"\nThe frames roughly correspond to the speech interval from 437.8912 seconds to 446.3683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P5_High_T1_14_Right", "images": ["Image/P15_D3_P5_High_T1_14_Right/1.png", "Image/P15_D3_P5_High_T1_14_Right/2.png", "Image/P15_D3_P5_High_T1_14_Right/3.png", "Image/P15_D3_P5_High_T1_14_Right/4.png", "Image/P15_D3_P5_High_T1_14_Right/5.png", "Image/P15_D3_P5_High_T1_14_Right/6.png", "Image/P15_D3_P5_High_T1_14_Right/7.png", "Image/P15_D3_P5_High_T1_14_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant to the brown shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 460.2268000000001 seconds to 474.2157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P4_Medium_T1_23_Mid", "images": ["Image/P6_D3_P4_Medium_T1_23_Mid/1.png", "Image/P6_D3_P4_Medium_T1_23_Mid/2.png", "Image/P6_D3_P4_Medium_T1_23_Mid/3.png", "Image/P6_D3_P4_Medium_T1_23_Mid/4.png", "Image/P6_D3_P4_Medium_T1_23_Mid/5.png", "Image/P6_D3_P4_Medium_T1_23_Mid/6.png", "Image/P6_D3_P4_Medium_T1_23_Mid/7.png", "Image/P6_D3_P4_Medium_T1_23_Mid/8.png", "Image/P6_D3_P4_Medium_T1_23_Mid/9.png", "Image/P6_D3_P4_Medium_T1_23_Mid/10.png", "Image/P6_D3_P4_Medium_T1_23_Mid/11.png", "Image/P6_D3_P4_Medium_T1_23_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Such. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3382.8465 seconds to 3403.5022 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P5_Low_T1_24_Left", "images": ["Image/P3_D2_P5_Low_T1_24_Left/1.png", "Image/P3_D2_P5_Low_T1_24_Left/2.png", "Image/P3_D2_P5_Low_T1_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean this item? Finished.\"\nThe frames roughly correspond to the speech interval from 2473.1482 seconds to 2482.8789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P9_Medium_T2_1_Mid", "images": ["Image/P24_D1_P9_Medium_T2_1_Mid/1.png", "Image/P24_D1_P9_Medium_T2_1_Mid/2.png", "Image/P24_D1_P9_Medium_T2_1_Mid/3.png", "Image/P24_D1_P9_Medium_T2_1_Mid/4.png", "Image/P24_D1_P9_Medium_T2_1_Mid/5.png", "Image/P24_D1_P9_Medium_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in the middle bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 903.6796 seconds to 916.1513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P5_Medium_T2_6_Left", "images": ["Image/P3_D3_P5_Medium_T2_6_Left/1.png", "Image/P3_D3_P5_Medium_T2_6_Left/2.png", "Image/P3_D3_P5_Medium_T2_6_Left/3.png", "Image/P3_D3_P5_Medium_T2_6_Left/4.png", "Image/P3_D3_P5_Medium_T2_6_Left/5.png", "Image/P3_D3_P5_Medium_T2_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the first item? Finish.\"\nThe frames roughly correspond to the speech interval from 588.40762 seconds to 602.8289599999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P2_High_T3_13_Mid", "images": ["Image/P19_D2_P2_High_T3_13_Mid/1.png", "Image/P19_D2_P2_High_T3_13_Mid/2.png", "Image/P19_D2_P2_High_T3_13_Mid/3.png", "Image/P19_D2_P2_High_T3_13_Mid/4.png", "Image/P19_D2_P2_High_T3_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1866.3782 seconds to 1879.6846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P8_Medium_T1_3_Left", "images": ["Image/P21_D3_P8_Medium_T1_3_Left/1.png", "Image/P21_D3_P8_Medium_T1_3_Left/2.png", "Image/P21_D3_P8_Medium_T1_3_Left/3.png", "Image/P21_D3_P8_Medium_T1_3_Left/4.png", "Image/P21_D3_P8_Medium_T1_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 214.64807 seconds to 225.90822999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P2_Medium_T3_21_Left", "images": ["Image/P7_D2_P2_Medium_T3_21_Left/1.png", "Image/P7_D2_P2_Medium_T3_21_Left/2.png", "Image/P7_D2_P2_Medium_T3_21_Left/3.png", "Image/P7_D2_P2_Medium_T3_21_Left/4.png", "Image/P7_D2_P2_Medium_T3_21_Left/5.png", "Image/P7_D2_P2_Medium_T3_21_Left/6.png", "Image/P7_D2_P2_Medium_T3_21_Left/7.png", "Image/P7_D2_P2_Medium_T3_21_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 919.21951 seconds to 934.62851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P4_Low_T3_16_Mid", "images": ["Image/P4_D2_P4_Low_T3_16_Mid/1.png", "Image/P4_D2_P4_Low_T3_16_Mid/2.png", "Image/P4_D2_P4_Low_T3_16_Mid/3.png", "Image/P4_D2_P4_Low_T3_16_Mid/4.png", "Image/P4_D2_P4_Low_T3_16_Mid/5.png", "Image/P4_D2_P4_Low_T3_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1967.9933 seconds to 1981.717 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P8_Low_T3_9_Mid", "images": ["Image/P8_D3_P8_Low_T3_9_Mid/1.png", "Image/P8_D3_P8_Low_T3_9_Mid/2.png", "Image/P8_D3_P8_Low_T3_9_Mid/3.png", "Image/P8_D3_P8_Low_T3_9_Mid/4.png", "Image/P8_D3_P8_Low_T3_9_Mid/5.png", "Image/P8_D3_P8_Low_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hey\"\nThe frames roughly correspond to the speech interval from 1764.2412 seconds to 1776.0989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P3_High_T3_19_Left", "images": ["Image/P15_D1_P3_High_T3_19_Left/1.png", "Image/P15_D1_P3_High_T3_19_Left/2.png", "Image/P15_D1_P3_High_T3_19_Left/3.png", "Image/P15_D1_P3_High_T3_19_Left/4.png", "Image/P15_D1_P3_High_T3_19_Left/5.png", "Image/P15_D1_P3_High_T3_19_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white container on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1574.1297699999998 seconds to 1586.85207 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P2_High_T2_18_Right", "images": ["Image/P24_D2_P2_High_T2_18_Right/1.png", "Image/P24_D2_P2_High_T2_18_Right/2.png", "Image/P24_D2_P2_High_T2_18_Right/3.png", "Image/P24_D2_P2_High_T2_18_Right/4.png", "Image/P24_D2_P2_High_T2_18_Right/5.png", "Image/P24_D2_P2_High_T2_18_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 531.0788 seconds to 542.715 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P5_Low_T1_2_Left", "images": ["Image/P15_D2_P5_Low_T1_2_Left/1.png", "Image/P15_D2_P5_Low_T1_2_Left/2.png", "Image/P15_D2_P5_Low_T1_2_Left/3.png", "Image/P15_D2_P5_Low_T1_2_Left/4.png", "Image/P15_D2_P5_Low_T1_2_Left/5.png", "Image/P15_D2_P5_Low_T1_2_Left/6.png", "Image/P15_D2_P5_Low_T1_2_Left/7.png", "Image/P15_D2_P5_Low_T1_2_Left/8.png", "Image/P15_D2_P5_Low_T1_2_Left/9.png", "Image/P15_D2_P5_Low_T1_2_Left/10.png", "Image/P15_D2_P5_Low_T1_2_Left/11.png", "Image/P15_D2_P5_Low_T1_2_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green pot from the middle shelf to the right shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3221.9102 seconds to 3238.9879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P9_Low_T2_17_Left", "images": ["Image/P11_D1_P9_Low_T2_17_Left/1.png", "Image/P11_D1_P9_Low_T2_17_Left/2.png", "Image/P11_D1_P9_Low_T2_17_Left/3.png", "Image/P11_D1_P9_Low_T2_17_Left/4.png", "Image/P11_D1_P9_Low_T2_17_Left/5.png", "Image/P11_D1_P9_Low_T2_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a glass which is in the basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2566.4602999999997 seconds to 2578.7519999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P2_Low_T2_18_Right", "images": ["Image/P11_D3_P2_Low_T2_18_Right/1.png", "Image/P11_D3_P2_Low_T2_18_Right/2.png", "Image/P11_D3_P2_Low_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the right glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1858.3107 seconds to 1866.981 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P7_Low_T2_9_Mid", "images": ["Image/P2_D2_P7_Low_T2_9_Mid/1.png", "Image/P2_D2_P7_Low_T2_9_Mid/2.png", "Image/P2_D2_P7_Low_T2_9_Mid/3.png", "Image/P2_D2_P7_Low_T2_9_Mid/4.png", "Image/P2_D2_P7_Low_T2_9_Mid/5.png", "Image/P2_D2_P7_Low_T2_9_Mid/6.png", "Image/P2_D2_P7_Low_T2_9_Mid/7.png", "Image/P2_D2_P7_Low_T2_9_Mid/8.png", "Image/P2_D2_P7_Low_T2_9_Mid/9.png", "Image/P2_D2_P7_Low_T2_9_Mid/10.png", "Image/P2_D2_P7_Low_T2_9_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please add some water into the green cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2896.4482 seconds to 2912.4988999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P5_Medium_T1_6_Right", "images": ["Image/P28_D1_P5_Medium_T1_6_Right/1.png", "Image/P28_D1_P5_Medium_T1_6_Right/2.png", "Image/P28_D1_P5_Medium_T1_6_Right/3.png", "Image/P28_D1_P5_Medium_T1_6_Right/4.png", "Image/P28_D1_P5_Medium_T1_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1625.0322 seconds to 1635.1662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P8_Low_T1_7_Left", "images": ["Image/P28_D3_P8_Low_T1_7_Left/1.png", "Image/P28_D3_P8_Low_T1_7_Left/2.png", "Image/P28_D3_P8_Low_T1_7_Left/3.png", "Image/P28_D3_P8_Low_T1_7_Left/4.png", "Image/P28_D3_P8_Low_T1_7_Left/5.png", "Image/P28_D3_P8_Low_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 415.4357 seconds to 426.6124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P9_Medium_T1_4_Right", "images": ["Image/P23_D3_P9_Medium_T1_4_Right/1.png", "Image/P23_D3_P9_Medium_T1_4_Right/2.png", "Image/P23_D3_P9_Medium_T1_4_Right/3.png", "Image/P23_D3_P9_Medium_T1_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 750.4487 seconds to 759.9274 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P7_High_T2_6_Left", "images": ["Image/P10_D2_P7_High_T2_6_Left/1.png", "Image/P10_D2_P7_High_T2_6_Left/2.png", "Image/P10_D2_P7_High_T2_6_Left/3.png", "Image/P10_D2_P7_High_T2_6_Left/4.png", "Image/P10_D2_P7_High_T2_6_Left/5.png", "Image/P10_D2_P7_High_T2_6_Left/6.png", "Image/P10_D2_P7_High_T2_6_Left/7.png", "Image/P10_D2_P7_High_T2_6_Left/8.png", "Image/P10_D2_P7_High_T2_6_Left/9.png", "Image/P10_D2_P7_High_T2_6_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1561.5247800000002 seconds to 1578.6741800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P3_Low_T1_12_Left", "images": ["Image/P14_D1_P3_Low_T1_12_Left/1.png", "Image/P14_D1_P3_Low_T1_12_Left/2.png", "Image/P14_D1_P3_Low_T1_12_Left/3.png", "Image/P14_D1_P3_Low_T1_12_Left/4.png", "Image/P14_D1_P3_Low_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that to the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 233.4096 seconds to 244.3232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P9_Medium_T1_12_Left", "images": ["Image/P18_D2_P9_Medium_T1_12_Left/1.png", "Image/P18_D2_P9_Medium_T1_12_Left/2.png", "Image/P18_D2_P9_Medium_T1_12_Left/3.png", "Image/P18_D2_P9_Medium_T1_12_Left/4.png", "Image/P18_D2_P9_Medium_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move those vases to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 38.534280000000024 seconds to 50.11516000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P8_High_T1_12_Right", "images": ["Image/P12_D2_P8_High_T1_12_Right/1.png", "Image/P12_D2_P8_High_T1_12_Right/2.png", "Image/P12_D2_P8_High_T1_12_Right/3.png", "Image/P12_D2_P8_High_T1_12_Right/4.png", "Image/P12_D2_P8_High_T1_12_Right/5.png", "Image/P12_D2_P8_High_T1_12_Right/6.png", "Image/P12_D2_P8_High_T1_12_Right/7.png", "Image/P12_D2_P8_High_T1_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the orange vase on the bottom up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 539.66716 seconds to 553.60003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P7_Low_T3_3_Right", "images": ["Image/P21_D1_P7_Low_T3_3_Right/1.png", "Image/P21_D1_P7_Low_T3_3_Right/2.png", "Image/P21_D1_P7_Low_T3_3_Right/3.png", "Image/P21_D1_P7_Low_T3_3_Right/4.png", "Image/P21_D1_P7_Low_T3_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1025.3438999999998 seconds to 1035.5318 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P6_High_T3_26_Left", "images": ["Image/P15_D2_P6_High_T3_26_Left/1.png", "Image/P15_D2_P6_High_T3_26_Left/2.png", "Image/P15_D2_P6_High_T3_26_Left/3.png", "Image/P15_D2_P6_High_T3_26_Left/4.png", "Image/P15_D2_P6_High_T3_26_Left/5.png", "Image/P15_D2_P6_High_T3_26_Left/6.png", "Image/P15_D2_P6_High_T3_26_Left/7.png", "Image/P15_D2_P6_High_T3_26_Left/8.png", "Image/P15_D2_P6_High_T3_26_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2615.61634 seconds to 2631.3155 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P4_Medium_T2_1_Mid", "images": ["Image/P14_D2_P4_Medium_T2_1_Mid/1.png", "Image/P14_D2_P4_Medium_T2_1_Mid/2.png", "Image/P14_D2_P4_Medium_T2_1_Mid/3.png", "Image/P14_D2_P4_Medium_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water that middle bottle\"\nThe frames roughly correspond to the speech interval from 1492.0574399999998 seconds to 1501.1748999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P5_Medium_T3_24_Right", "images": ["Image/P10_D2_P5_Medium_T3_24_Right/1.png", "Image/P10_D2_P5_Medium_T3_24_Right/2.png", "Image/P10_D2_P5_Medium_T3_24_Right/3.png", "Image/P10_D2_P5_Medium_T3_24_Right/4.png", "Image/P10_D2_P5_Medium_T3_24_Right/5.png", "Image/P10_D2_P5_Medium_T3_24_Right/6.png", "Image/P10_D2_P5_Medium_T3_24_Right/7.png", "Image/P10_D2_P5_Medium_T3_24_Right/8.png", "Image/P10_D2_P5_Medium_T3_24_Right/9.png", "Image/P10_D2_P5_Medium_T3_24_Right/10.png", "Image/P10_D2_P5_Medium_T3_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1235.82758 seconds to 1255.7175800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P2_High_T1_8_Right", "images": ["Image/P26_D1_P2_High_T1_8_Right/1.png", "Image/P26_D1_P2_High_T1_8_Right/2.png", "Image/P26_D1_P2_High_T1_8_Right/3.png", "Image/P26_D1_P2_High_T1_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green cup on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 513.7804 seconds to 524.3456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P3_Low_T3_10_Left", "images": ["Image/P20_D1_P3_Low_T3_10_Left/1.png", "Image/P20_D1_P3_Low_T3_10_Left/2.png", "Image/P20_D1_P3_Low_T3_10_Left/3.png", "Image/P20_D1_P3_Low_T3_10_Left/4.png", "Image/P20_D1_P3_Low_T3_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Thanks. Finish.\"\nThe frames roughly correspond to the speech interval from 158.81282 seconds to 171.39625999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P2_Low_T1_10_Left", "images": ["Image/P12_D1_P2_Low_T1_10_Left/1.png", "Image/P12_D1_P2_Low_T1_10_Left/2.png", "Image/P12_D1_P2_Low_T1_10_Left/3.png", "Image/P12_D1_P2_Low_T1_10_Left/4.png", "Image/P12_D1_P2_Low_T1_10_Left/5.png", "Image/P12_D1_P2_Low_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1455.8489 seconds to 1467.2746 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P4_High_T3_8_Left", "images": ["Image/P18_D2_P4_High_T3_8_Left/1.png", "Image/P18_D2_P4_High_T3_8_Left/2.png", "Image/P18_D2_P4_High_T3_8_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 408.6634 seconds to 417.9351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P1_Low_T1_3_Right", "images": ["Image/P20_D3_P1_Low_T1_3_Right/1.png", "Image/P20_D3_P1_Low_T1_3_Right/2.png", "Image/P20_D3_P1_Low_T1_3_Right/3.png", "Image/P20_D3_P1_Low_T1_3_Right/4.png", "Image/P20_D3_P1_Low_T1_3_Right/5.png", "Image/P20_D3_P1_Low_T1_3_Right/6.png", "Image/P20_D3_P1_Low_T1_3_Right/7.png", "Image/P20_D3_P1_Low_T1_3_Right/8.png", "Image/P20_D3_P1_Low_T1_3_Right/9.png", "Image/P20_D3_P1_Low_T1_3_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green bottle to the bottom of that table? Finished.\"\nThe frames roughly correspond to the speech interval from 1791.3584 seconds to 1808.6045 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P7_Low_T2_5_Left", "images": ["Image/P11_D3_P7_Low_T2_5_Left/1.png", "Image/P11_D3_P7_Low_T2_5_Left/2.png", "Image/P11_D3_P7_Low_T2_5_Left/3.png", "Image/P11_D3_P7_Low_T2_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant under the table. Finish.\"\nThe frames roughly correspond to the speech interval from 1578.4504 seconds to 1587.6786 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P5_High_T2_11_Left", "images": ["Image/P30_D3_P5_High_T2_11_Left/1.png", "Image/P30_D3_P5_High_T2_11_Left/2.png", "Image/P30_D3_P5_High_T2_11_Left/3.png", "Image/P30_D3_P5_High_T2_11_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1437.3964999999998 seconds to 1448.3366999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P7_Low_T2_27_Right", "images": ["Image/P16_D3_P7_Low_T2_27_Right/1.png", "Image/P16_D3_P7_Low_T2_27_Right/2.png", "Image/P16_D3_P7_Low_T2_27_Right/3.png", "Image/P16_D3_P7_Low_T2_27_Right/4.png", "Image/P16_D3_P7_Low_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water into that cup? Finish\"\nThe frames roughly correspond to the speech interval from 111.1577 seconds to 121.4021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P1_Low_T2_6_Left", "images": ["Image/P13_D2_P1_Low_T2_6_Left/1.png", "Image/P13_D2_P1_Low_T2_6_Left/2.png", "Image/P13_D2_P1_Low_T2_6_Left/3.png", "Image/P13_D2_P1_Low_T2_6_Left/4.png", "Image/P13_D2_P1_Low_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the plant over there\"\nThe frames roughly correspond to the speech interval from 1078.15976 seconds to 1088.04266 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P7_High_T1_21_Right", "images": ["Image/P26_D3_P7_High_T1_21_Right/1.png", "Image/P26_D3_P7_High_T1_21_Right/2.png", "Image/P26_D3_P7_High_T1_21_Right/3.png", "Image/P26_D3_P7_High_T1_21_Right/4.png", "Image/P26_D3_P7_High_T1_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the white face to the top shelf of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 650.1909400000001 seconds to 661.5206900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P5_Medium_T2_11_Left", "images": ["Image/P27_D3_P5_Medium_T2_11_Left/1.png", "Image/P27_D3_P5_Medium_T2_11_Left/2.png", "Image/P27_D3_P5_Medium_T2_11_Left/3.png", "Image/P27_D3_P5_Medium_T2_11_Left/4.png", "Image/P27_D3_P5_Medium_T2_11_Left/5.png", "Image/P27_D3_P5_Medium_T2_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in the left red glass? Finish\"\nThe frames roughly correspond to the speech interval from 623.3806 seconds to 634.6422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P8_Medium_T2_23_Left", "images": ["Image/P1_D2_P8_Medium_T2_23_Left/1.png", "Image/P1_D2_P8_Medium_T2_23_Left/2.png", "Image/P1_D2_P8_Medium_T2_23_Left/3.png", "Image/P1_D2_P8_Medium_T2_23_Left/4.png", "Image/P1_D2_P8_Medium_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 806.7927000000001 seconds to 817.8329000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P6_Low_T3_20_Left", "images": ["Image/P24_D2_P6_Low_T3_20_Left/1.png", "Image/P24_D2_P6_Low_T3_20_Left/2.png", "Image/P24_D2_P6_Low_T3_20_Left/3.png", "Image/P24_D2_P6_Low_T3_20_Left/4.png", "Image/P24_D2_P6_Low_T3_20_Left/5.png", "Image/P24_D2_P6_Low_T3_20_Left/6.png", "Image/P24_D2_P6_Low_T3_20_Left/7.png", "Image/P24_D2_P6_Low_T3_20_Left/8.png", "Image/P24_D2_P6_Low_T3_20_Left/9.png", "Image/P24_D2_P6_Low_T3_20_Left/10.png", "Image/P24_D2_P6_Low_T3_20_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 124.83929999999998 seconds to 138.3412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P2_High_T1_20_Mid", "images": ["Image/P22_D1_P2_High_T1_20_Mid/1.png", "Image/P22_D1_P2_High_T1_20_Mid/2.png", "Image/P22_D1_P2_High_T1_20_Mid/3.png", "Image/P22_D1_P2_High_T1_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase to the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 1478.5878 seconds to 1488.6233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P8_High_T2_5_Right", "images": ["Image/P10_D2_P8_High_T2_5_Right/1.png", "Image/P10_D2_P8_High_T2_5_Right/2.png", "Image/P10_D2_P8_High_T2_5_Right/3.png", "Image/P10_D2_P8_High_T2_5_Right/4.png", "Image/P10_D2_P8_High_T2_5_Right/5.png", "Image/P10_D2_P8_High_T2_5_Right/6.png", "Image/P10_D2_P8_High_T2_5_Right/7.png", "Image/P10_D2_P8_High_T2_5_Right/8.png", "Image/P10_D2_P8_High_T2_5_Right/9.png", "Image/P10_D2_P8_High_T2_5_Right/10.png", "Image/P10_D2_P8_High_T2_5_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1696.22748 seconds to 1712.90398 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P7_Medium_T1_26_Left", "images": ["Image/P22_D1_P7_Medium_T1_26_Left/1.png", "Image/P22_D1_P7_Medium_T1_26_Left/2.png", "Image/P22_D1_P7_Medium_T1_26_Left/3.png", "Image/P22_D1_P7_Medium_T1_26_Left/4.png", "Image/P22_D1_P7_Medium_T1_26_Left/5.png", "Image/P22_D1_P7_Medium_T1_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that cup on the left up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1390.6698 seconds to 1401.4983 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P1_Medium_T2_7_Mid", "images": ["Image/P12_D3_P1_Medium_T2_7_Mid/1.png", "Image/P12_D3_P1_Medium_T2_7_Mid/2.png", "Image/P12_D3_P1_Medium_T2_7_Mid/3.png", "Image/P12_D3_P1_Medium_T2_7_Mid/4.png", "Image/P12_D3_P1_Medium_T2_7_Mid/5.png", "Image/P12_D3_P1_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2550.63719 seconds to 2563.15179 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P1_High_T1_14_Mid", "images": ["Image/P16_D3_P1_High_T1_14_Mid/1.png", "Image/P16_D3_P1_High_T1_14_Mid/2.png", "Image/P16_D3_P1_High_T1_14_Mid/3.png", "Image/P16_D3_P1_High_T1_14_Mid/4.png", "Image/P16_D3_P1_High_T1_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase down to the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 353.76789999999994 seconds to 363.6082 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P6_Low_T3_26_Mid", "images": ["Image/P29_D1_P6_Low_T3_26_Mid/1.png", "Image/P29_D1_P6_Low_T3_26_Mid/2.png", "Image/P29_D1_P6_Low_T3_26_Mid/3.png", "Image/P29_D1_P6_Low_T3_26_Mid/4.png", "Image/P29_D1_P6_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2670.8880999999997 seconds to 2681.6607999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P5_Low_T1_15_Right", "images": ["Image/P20_D1_P5_Low_T1_15_Right/1.png", "Image/P20_D1_P5_Low_T1_15_Right/2.png", "Image/P20_D1_P5_Low_T1_15_Right/3.png", "Image/P20_D1_P5_Low_T1_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 778.0118 seconds to 790.0098 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P5_High_T3_19_Left", "images": ["Image/P10_D1_P5_High_T3_19_Left/1.png", "Image/P10_D1_P5_High_T3_19_Left/2.png", "Image/P10_D1_P5_High_T3_19_Left/3.png", "Image/P10_D1_P5_High_T3_19_Left/4.png", "Image/P10_D1_P5_High_T3_19_Left/5.png", "Image/P10_D1_P5_High_T3_19_Left/6.png", "Image/P10_D1_P5_High_T3_19_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2076.90052 seconds to 2091.79374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P8_Low_T2_19_Right", "images": ["Image/P25_D1_P8_Low_T2_19_Right/1.png", "Image/P25_D1_P8_Low_T2_19_Right/2.png", "Image/P25_D1_P8_Low_T2_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 179.425 seconds to 187.7205 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P5_Medium_T1_24_Mid", "images": ["Image/P12_D1_P5_Medium_T1_24_Mid/1.png", "Image/P12_D1_P5_Medium_T1_24_Mid/2.png", "Image/P12_D1_P5_Medium_T1_24_Mid/3.png", "Image/P12_D1_P5_Medium_T1_24_Mid/4.png", "Image/P12_D1_P5_Medium_T1_24_Mid/5.png", "Image/P12_D1_P5_Medium_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1474.1949 seconds to 1485.5358 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P8_Medium_T3_8_Right", "images": ["Image/P26_D2_P8_Medium_T3_8_Right/1.png", "Image/P26_D2_P8_Medium_T3_8_Right/2.png", "Image/P26_D2_P8_Medium_T3_8_Right/3.png", "Image/P26_D2_P8_Medium_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1528.7489 seconds to 1539.6063 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P9_Medium_T2_15_Left", "images": ["Image/P25_D2_P9_Medium_T2_15_Left/1.png", "Image/P25_D2_P9_Medium_T2_15_Left/2.png", "Image/P25_D2_P9_Medium_T2_15_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the third plant. Finish.\"\nThe frames roughly correspond to the speech interval from 807.1451 seconds to 815.7206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P2_High_T3_21_Left", "images": ["Image/P21_D1_P2_High_T3_21_Left/1.png", "Image/P21_D1_P2_High_T3_21_Left/2.png", "Image/P21_D1_P2_High_T3_21_Left/3.png", "Image/P21_D1_P2_High_T3_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 971.5151 seconds to 986.4792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P5_Low_T1_25_Right", "images": ["Image/P6_D1_P5_Low_T1_25_Right/1.png", "Image/P6_D1_P5_Low_T1_25_Right/2.png", "Image/P6_D1_P5_Low_T1_25_Right/3.png", "Image/P6_D1_P5_Low_T1_25_Right/4.png", "Image/P6_D1_P5_Low_T1_25_Right/5.png", "Image/P6_D1_P5_Low_T1_25_Right/6.png", "Image/P6_D1_P5_Low_T1_25_Right/7.png", "Image/P6_D1_P5_Low_T1_25_Right/8.png", "Image/P6_D1_P5_Low_T1_25_Right/9.png", "Image/P6_D1_P5_Low_T1_25_Right/10.png", "Image/P6_D1_P5_Low_T1_25_Right/11.png", "Image/P6_D1_P5_Low_T1_25_Right/12.png", "Image/P6_D1_P5_Low_T1_25_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2282.0621 seconds to 2300.5039 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P7_High_T3_4_Left", "images": ["Image/P9_D2_P7_High_T3_4_Left/1.png", "Image/P9_D2_P7_High_T3_4_Left/2.png", "Image/P9_D2_P7_High_T3_4_Left/3.png", "Image/P9_D2_P7_High_T3_4_Left/4.png", "Image/P9_D2_P7_High_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the plant on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 2709.38909 seconds to 2720.77628 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P4_High_T2_27_Left", "images": ["Image/P30_D1_P4_High_T2_27_Left/1.png", "Image/P30_D1_P4_High_T2_27_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 780.3138 seconds to 788.4972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P9_Low_T3_27_Right", "images": ["Image/P22_D2_P9_Low_T3_27_Right/1.png", "Image/P22_D2_P9_Low_T3_27_Right/2.png", "Image/P22_D2_P9_Low_T3_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 899.2104999999999 seconds to 908.2451 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P9_Low_T2_3_Right", "images": ["Image/P24_D2_P9_Low_T2_3_Right/1.png", "Image/P24_D2_P9_Low_T2_3_Right/2.png", "Image/P24_D2_P9_Low_T2_3_Right/3.png", "Image/P24_D2_P9_Low_T2_3_Right/4.png", "Image/P24_D2_P9_Low_T2_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 661.7446 seconds to 672.8374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P3_Low_T3_10_Mid", "images": ["Image/P21_D1_P3_Low_T3_10_Mid/1.png", "Image/P21_D1_P3_Low_T3_10_Mid/2.png", "Image/P21_D1_P3_Low_T3_10_Mid/3.png", "Image/P21_D1_P3_Low_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1176.4025 seconds to 1187.3015 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P6_Medium_T1_5_Left", "images": ["Image/P15_D3_P6_Medium_T1_5_Left/1.png", "Image/P15_D3_P6_Medium_T1_5_Left/2.png", "Image/P15_D3_P6_Medium_T1_5_Left/3.png", "Image/P15_D3_P6_Medium_T1_5_Left/4.png", "Image/P15_D3_P6_Medium_T1_5_Left/5.png", "Image/P15_D3_P6_Medium_T1_5_Left/6.png", "Image/P15_D3_P6_Medium_T1_5_Left/7.png", "Image/P15_D3_P6_Medium_T1_5_Left/8.png", "Image/P15_D3_P6_Medium_T1_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant at the left side into the table disk. Finish.\"\nThe frames roughly correspond to the speech interval from 348.2687 seconds to 363.0371 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P5_Low_T3_26_Left", "images": ["Image/P12_D3_P5_Low_T3_26_Left/1.png", "Image/P12_D3_P5_Low_T3_26_Left/2.png", "Image/P12_D3_P5_Low_T3_26_Left/3.png", "Image/P12_D3_P5_Low_T3_26_Left/4.png", "Image/P12_D3_P5_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 2751.31929 seconds to 2761.6768899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P7_High_T3_24_Mid", "images": ["Image/P20_D1_P7_High_T3_24_Mid/1.png", "Image/P20_D1_P7_High_T3_24_Mid/2.png", "Image/P20_D1_P7_High_T3_24_Mid/3.png", "Image/P20_D1_P7_High_T3_24_Mid/4.png", "Image/P20_D1_P7_High_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean a plant in a basket. Finish.\"\nThe frames roughly correspond to the speech interval from 489.9668 seconds to 500.2396 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P8_Medium_T3_20_Mid", "images": ["Image/P26_D3_P8_Medium_T3_20_Mid/1.png", "Image/P26_D3_P8_Medium_T3_20_Mid/2.png", "Image/P26_D3_P8_Medium_T3_20_Mid/3.png", "Image/P26_D3_P8_Medium_T3_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the middle base on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 694.16425 seconds to 705.1319900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P6_High_T2_19_Right", "images": ["Image/P2_D2_P6_High_T2_19_Right/1.png", "Image/P2_D2_P6_High_T2_19_Right/2.png", "Image/P2_D2_P6_High_T2_19_Right/3.png", "Image/P2_D2_P6_High_T2_19_Right/4.png", "Image/P2_D2_P6_High_T2_19_Right/5.png", "Image/P2_D2_P6_High_T2_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2686.4678 seconds to 2699.6202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P6_High_T2_13_Left", "images": ["Image/P9_D1_P6_High_T2_13_Left/1.png", "Image/P9_D1_P6_High_T2_13_Left/2.png", "Image/P9_D1_P6_High_T2_13_Left/3.png", "Image/P9_D1_P6_High_T2_13_Left/4.png", "Image/P9_D1_P6_High_T2_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1461.9725 seconds to 1474.0805 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P4_High_T2_16_Right", "images": ["Image/P18_D3_P4_High_T2_16_Right/1.png", "Image/P18_D3_P4_High_T2_16_Right/2.png", "Image/P18_D3_P4_High_T2_16_Right/3.png", "Image/P18_D3_P4_High_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1193.42435 seconds to 1202.7508400000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P3_Medium_T2_22_Right", "images": ["Image/P12_D1_P3_Medium_T2_22_Right/1.png", "Image/P12_D1_P3_Medium_T2_22_Right/2.png", "Image/P12_D1_P3_Medium_T2_22_Right/3.png", "Image/P12_D1_P3_Medium_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1414.5744 seconds to 1424.9579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P5_Medium_T2_5_Left", "images": ["Image/P30_D1_P5_Medium_T2_5_Left/1.png", "Image/P30_D1_P5_Medium_T2_5_Left/2.png", "Image/P30_D1_P5_Medium_T2_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 700.35783 seconds to 709.5999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P2_High_T3_22_Left", "images": ["Image/P10_D1_P2_High_T3_22_Left/1.png", "Image/P10_D1_P2_High_T3_22_Left/2.png", "Image/P10_D1_P2_High_T3_22_Left/3.png", "Image/P10_D1_P2_High_T3_22_Left/4.png", "Image/P10_D1_P2_High_T3_22_Left/5.png", "Image/P10_D1_P2_High_T3_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2381.1195 seconds to 2393.7024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P3_High_T2_18_Right", "images": ["Image/P23_D3_P3_High_T2_18_Right/1.png", "Image/P23_D3_P3_High_T2_18_Right/2.png", "Image/P23_D3_P3_High_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 816.4014 seconds to 825.4206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P5_High_T3_1_Left", "images": ["Image/P12_D3_P5_High_T3_1_Left/1.png", "Image/P12_D3_P5_High_T3_1_Left/2.png", "Image/P12_D3_P5_High_T3_1_Left/3.png", "Image/P12_D3_P5_High_T3_1_Left/4.png", "Image/P12_D3_P5_High_T3_1_Left/5.png", "Image/P12_D3_P5_High_T3_1_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2318.17549 seconds to 2333.48739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P7_Low_T1_15_Right", "images": ["Image/P4_D3_P7_Low_T1_15_Right/1.png", "Image/P4_D3_P7_Low_T1_15_Right/2.png", "Image/P4_D3_P7_Low_T1_15_Right/3.png", "Image/P4_D3_P7_Low_T1_15_Right/4.png", "Image/P4_D3_P7_Low_T1_15_Right/5.png", "Image/P4_D3_P7_Low_T1_15_Right/6.png", "Image/P4_D3_P7_Low_T1_15_Right/7.png", "Image/P4_D3_P7_Low_T1_15_Right/8.png", "Image/P4_D3_P7_Low_T1_15_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plants on the ground to the top of the shelter on the desk? Finished.\"\nThe frames roughly correspond to the speech interval from 495.0236999999999 seconds to 512.7303999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P8_Medium_T2_18_Left", "images": ["Image/P27_D2_P8_Medium_T2_18_Left/1.png", "Image/P27_D2_P8_Medium_T2_18_Left/2.png", "Image/P27_D2_P8_Medium_T2_18_Left/3.png", "Image/P27_D2_P8_Medium_T2_18_Left/4.png", "Image/P27_D2_P8_Medium_T2_18_Left/5.png", "Image/P27_D2_P8_Medium_T2_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You put water in that first red cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1448.2132 seconds to 1459.501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P4_Medium_T1_27_Right", "images": ["Image/P16_D1_P4_Medium_T1_27_Right/1.png", "Image/P16_D1_P4_Medium_T1_27_Right/2.png", "Image/P16_D1_P4_Medium_T1_27_Right/3.png", "Image/P16_D1_P4_Medium_T1_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this mug over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1218.50631 seconds to 1228.09652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P1_High_T1_24_Mid", "images": ["Image/P27_D1_P1_High_T1_24_Mid/1.png", "Image/P27_D1_P1_High_T1_24_Mid/2.png", "Image/P27_D1_P1_High_T1_24_Mid/3.png", "Image/P27_D1_P1_High_T1_24_Mid/4.png", "Image/P27_D1_P1_High_T1_24_Mid/5.png", "Image/P27_D1_P1_High_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant into the center of that basket? Finish.\"\nThe frames roughly correspond to the speech interval from 1054.4147 seconds to 1066.2333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P1_Medium_T2_21_Mid", "images": ["Image/P22_D1_P1_Medium_T2_21_Mid/1.png", "Image/P22_D1_P1_Medium_T2_21_Mid/2.png", "Image/P22_D1_P1_Medium_T2_21_Mid/3.png", "Image/P22_D1_P1_Medium_T2_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 1799.4146 seconds to 1808.6148 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P5_Low_T1_13_Left", "images": ["Image/P26_D3_P5_Low_T1_13_Left/1.png", "Image/P26_D3_P5_Low_T1_13_Left/2.png", "Image/P26_D3_P5_Low_T1_13_Left/3.png", "Image/P26_D3_P5_Low_T1_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that plant to that shelf there? Finish.\"\nThe frames roughly correspond to the speech interval from 1035.2275 seconds to 1044.8752 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P6_Medium_T3_10_Left", "images": ["Image/P29_D3_P6_Medium_T3_10_Left/1.png", "Image/P29_D3_P6_Medium_T3_10_Left/2.png", "Image/P29_D3_P6_Medium_T3_10_Left/3.png", "Image/P29_D3_P6_Medium_T3_10_Left/4.png", "Image/P29_D3_P6_Medium_T3_10_Left/5.png", "Image/P29_D3_P6_Medium_T3_10_Left/6.png", "Image/P29_D3_P6_Medium_T3_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1439.3371999999997 seconds to 1451.5993999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P3_Low_T1_24_Left", "images": ["Image/P2_D3_P3_Low_T1_24_Left/1.png", "Image/P2_D3_P3_Low_T1_24_Left/2.png", "Image/P2_D3_P3_Low_T1_24_Left/3.png", "Image/P2_D3_P3_Low_T1_24_Left/4.png", "Image/P2_D3_P3_Low_T1_24_Left/5.png", "Image/P2_D3_P3_Low_T1_24_Left/6.png", "Image/P2_D3_P3_Low_T1_24_Left/7.png", "Image/P2_D3_P3_Low_T1_24_Left/8.png", "Image/P2_D3_P3_Low_T1_24_Left/9.png", "Image/P2_D3_P3_Low_T1_24_Left/10.png", "Image/P2_D3_P3_Low_T1_24_Left/11.png", "Image/P2_D3_P3_Low_T1_24_Left/12.png", "Image/P2_D3_P3_Low_T1_24_Left/13.png", "Image/P2_D3_P3_Low_T1_24_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 1957.2408000000005 seconds to 1978.4522 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P6_Medium_T1_16_Mid", "images": ["Image/P17_D2_P6_Medium_T1_16_Mid/1.png", "Image/P17_D2_P6_Medium_T1_16_Mid/2.png", "Image/P17_D2_P6_Medium_T1_16_Mid/3.png", "Image/P17_D2_P6_Medium_T1_16_Mid/4.png", "Image/P17_D2_P6_Medium_T1_16_Mid/5.png", "Image/P17_D2_P6_Medium_T1_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that mug in the center to the shelf over here? Finish.\"\nThe frames roughly correspond to the speech interval from 239.16849999999997 seconds to 252.3084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P3_Low_T3_22_Left", "images": ["Image/P25_D2_P3_Low_T3_22_Left/1.png", "Image/P25_D2_P3_Low_T3_22_Left/2.png", "Image/P25_D2_P3_Low_T3_22_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1002.6639 seconds to 1019.1024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P7_Low_T2_23_Right", "images": ["Image/P16_D2_P7_Low_T2_23_Right/1.png", "Image/P16_D2_P7_Low_T2_23_Right/2.png", "Image/P16_D2_P7_Low_T2_23_Right/3.png", "Image/P16_D2_P7_Low_T2_23_Right/4.png", "Image/P16_D2_P7_Low_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 885.1592 seconds to 898.2859000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P2_Low_T1_5_Mid", "images": ["Image/P26_D2_P2_Low_T1_5_Mid/1.png", "Image/P26_D2_P2_Low_T1_5_Mid/2.png", "Image/P26_D2_P2_Low_T1_5_Mid/3.png", "Image/P26_D2_P2_Low_T1_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this plant to the floor here? Finish.\"\nThe frames roughly correspond to the speech interval from 1593.5743 seconds to 1604.8217 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P6_High_T1_17_Right", "images": ["Image/P10_D3_P6_High_T1_17_Right/1.png", "Image/P10_D3_P6_High_T1_17_Right/2.png", "Image/P10_D3_P6_High_T1_17_Right/3.png", "Image/P10_D3_P6_High_T1_17_Right/4.png", "Image/P10_D3_P6_High_T1_17_Right/5.png", "Image/P10_D3_P6_High_T1_17_Right/6.png", "Image/P10_D3_P6_High_T1_17_Right/7.png", "Image/P10_D3_P6_High_T1_17_Right/8.png", "Image/P10_D3_P6_High_T1_17_Right/9.png", "Image/P10_D3_P6_High_T1_17_Right/10.png", "Image/P10_D3_P6_High_T1_17_Right/11.png", "Image/P10_D3_P6_High_T1_17_Right/12.png", "Image/P10_D3_P6_High_T1_17_Right/13.png", "Image/P10_D3_P6_High_T1_17_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 491.03649 seconds to 512.12389 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P7_Medium_T3_10_Left", "images": ["Image/P21_D2_P7_Medium_T3_10_Left/1.png", "Image/P21_D2_P7_Medium_T3_10_Left/2.png", "Image/P21_D2_P7_Medium_T3_10_Left/3.png", "Image/P21_D2_P7_Medium_T3_10_Left/4.png", "Image/P21_D2_P7_Medium_T3_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 1495.6334000000002 seconds to 1506.3374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P8_High_T3_9_Right", "images": ["Image/P1_D2_P8_High_T3_9_Right/1.png", "Image/P1_D2_P8_High_T3_9_Right/2.png", "Image/P1_D2_P8_High_T3_9_Right/3.png", "Image/P1_D2_P8_High_T3_9_Right/4.png", "Image/P1_D2_P8_High_T3_9_Right/5.png", "Image/P1_D2_P8_High_T3_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the left cup? Finish.\"\nThe frames roughly correspond to the speech interval from 880.0772000000001 seconds to 891.4515000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P6_Medium_T3_6_Right", "images": ["Image/P20_D3_P6_Medium_T3_6_Right/1.png", "Image/P20_D3_P6_Medium_T3_6_Right/2.png", "Image/P20_D3_P6_Medium_T3_6_Right/3.png", "Image/P20_D3_P6_Medium_T3_6_Right/4.png", "Image/P20_D3_P6_Medium_T3_6_Right/5.png", "Image/P20_D3_P6_Medium_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move? Um\"\nThe frames roughly correspond to the speech interval from 1163.23567 seconds to 1180.38438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P3_Low_T2_2_Mid", "images": ["Image/P19_D3_P3_Low_T2_2_Mid/1.png", "Image/P19_D3_P3_Low_T2_2_Mid/2.png", "Image/P19_D3_P3_Low_T2_2_Mid/3.png", "Image/P19_D3_P3_Low_T2_2_Mid/4.png", "Image/P19_D3_P3_Low_T2_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that container right there.\"\nThe frames roughly correspond to the speech interval from 2546.7805 seconds to 2561.2006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P8_Medium_T2_20_Right", "images": ["Image/P23_D1_P8_Medium_T2_20_Right/1.png", "Image/P23_D1_P8_Medium_T2_20_Right/2.png", "Image/P23_D1_P8_Medium_T2_20_Right/3.png", "Image/P23_D1_P8_Medium_T2_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the one on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 1617.1961 seconds to 1626.6341 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P2_High_T3_13_Right", "images": ["Image/P12_D2_P2_High_T3_13_Right/1.png", "Image/P12_D2_P2_High_T3_13_Right/2.png", "Image/P12_D2_P2_High_T3_13_Right/3.png", "Image/P12_D2_P2_High_T3_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 637.2597 seconds to 648.1994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P4_Medium_T2_13_Mid", "images": ["Image/P27_D3_P4_Medium_T2_13_Mid/1.png", "Image/P27_D3_P4_Medium_T2_13_Mid/2.png", "Image/P27_D3_P4_Medium_T2_13_Mid/3.png", "Image/P27_D3_P4_Medium_T2_13_Mid/4.png", "Image/P27_D3_P4_Medium_T2_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant in the middle? Finish\"\nThe frames roughly correspond to the speech interval from 547.4479 seconds to 558.0408 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P5_Low_T3_13_Mid", "images": ["Image/P6_D2_P5_Low_T3_13_Mid/1.png", "Image/P6_D2_P5_Low_T3_13_Mid/2.png", "Image/P6_D2_P5_Low_T3_13_Mid/3.png", "Image/P6_D2_P5_Low_T3_13_Mid/4.png", "Image/P6_D2_P5_Low_T3_13_Mid/5.png", "Image/P6_D2_P5_Low_T3_13_Mid/6.png", "Image/P6_D2_P5_Low_T3_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 1060.1826 seconds to 1075.4541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P9_High_T2_27_Right", "images": ["Image/P29_D1_P9_High_T2_27_Right/1.png", "Image/P29_D1_P9_High_T2_27_Right/2.png", "Image/P29_D1_P9_High_T2_27_Right/3.png", "Image/P29_D1_P9_High_T2_27_Right/4.png", "Image/P29_D1_P9_High_T2_27_Right/5.png", "Image/P29_D1_P9_High_T2_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2861.2742 seconds to 2873.2684999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P9_Low_T2_7_Mid", "images": ["Image/P19_D3_P9_Low_T2_7_Mid/1.png", "Image/P19_D3_P9_Low_T2_7_Mid/2.png", "Image/P19_D3_P9_Low_T2_7_Mid/3.png", "Image/P19_D3_P9_Low_T2_7_Mid/4.png", "Image/P19_D3_P9_Low_T2_7_Mid/5.png", "Image/P19_D3_P9_Low_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 2406.7758 seconds to 2420.3871 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P8_High_T3_1_Mid", "images": ["Image/P15_D2_P8_High_T3_1_Mid/1.png", "Image/P15_D2_P8_High_T3_1_Mid/2.png", "Image/P15_D2_P8_High_T3_1_Mid/3.png", "Image/P15_D2_P8_High_T3_1_Mid/4.png", "Image/P15_D2_P8_High_T3_1_Mid/5.png", "Image/P15_D2_P8_High_T3_1_Mid/6.png", "Image/P15_D2_P8_High_T3_1_Mid/7.png", "Image/P15_D2_P8_High_T3_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the green container in the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3125.7242 seconds to 3139.0297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P4_High_T1_4_Right", "images": ["Image/P7_D1_P4_High_T1_4_Right/1.png", "Image/P7_D1_P4_High_T1_4_Right/2.png", "Image/P7_D1_P4_High_T1_4_Right/3.png", "Image/P7_D1_P4_High_T1_4_Right/4.png", "Image/P7_D1_P4_High_T1_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Freddy\"\nThe frames roughly correspond to the speech interval from 465.673 seconds to 487.8038 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P2_Medium_T2_4_Mid", "images": ["Image/P22_D1_P2_Medium_T2_4_Mid/1.png", "Image/P22_D1_P2_Medium_T2_4_Mid/2.png", "Image/P22_D1_P2_Medium_T2_4_Mid/3.png", "Image/P22_D1_P2_Medium_T2_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this one. Finish.\"\nThe frames roughly correspond to the speech interval from 1567.2299 seconds to 1576.4723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P3_Low_T3_18_Mid", "images": ["Image/P29_D1_P3_Low_T3_18_Mid/1.png", "Image/P29_D1_P3_Low_T3_18_Mid/2.png", "Image/P29_D1_P3_Low_T3_18_Mid/3.png", "Image/P29_D1_P3_Low_T3_18_Mid/4.png", "Image/P29_D1_P3_Low_T3_18_Mid/5.png", "Image/P29_D1_P3_Low_T3_18_Mid/6.png", "Image/P29_D1_P3_Low_T3_18_Mid/7.png", "Image/P29_D1_P3_Low_T3_18_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2564.0809 seconds to 2578.8621 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P2_Medium_T1_5_Mid", "images": ["Image/P20_D1_P2_Medium_T1_5_Mid/1.png", "Image/P20_D1_P2_Medium_T1_5_Mid/2.png", "Image/P20_D1_P2_Medium_T1_5_Mid/3.png", "Image/P20_D1_P2_Medium_T1_5_Mid/4.png", "Image/P20_D1_P2_Medium_T1_5_Mid/5.png", "Image/P20_D1_P2_Medium_T1_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move this middle plant into the basket\"\nThe frames roughly correspond to the speech interval from 515.2867 seconds to 528.8861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P4_High_T3_4_Mid", "images": ["Image/P13_D1_P4_High_T3_4_Mid/1.png", "Image/P13_D1_P4_High_T3_4_Mid/2.png", "Image/P13_D1_P4_High_T3_4_Mid/3.png", "Image/P13_D1_P4_High_T3_4_Mid/4.png", "Image/P13_D1_P4_High_T3_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean this plant here please? Finish.\"\nThe frames roughly correspond to the speech interval from 677.0426 seconds to 687.8026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P6_Medium_T3_8_Right", "images": ["Image/P27_D3_P6_Medium_T3_8_Right/1.png", "Image/P27_D3_P6_Medium_T3_8_Right/2.png", "Image/P27_D3_P6_Medium_T3_8_Right/3.png", "Image/P27_D3_P6_Medium_T3_8_Right/4.png", "Image/P27_D3_P6_Medium_T3_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that third glass over there? Finish.\"\nThe frames roughly correspond to the speech interval from 163.25841000000003 seconds to 173.92108000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P9_Low_T2_19_Left", "images": ["Image/P6_D3_P9_Low_T2_19_Left/1.png", "Image/P6_D3_P9_Low_T2_19_Left/2.png", "Image/P6_D3_P9_Low_T2_19_Left/3.png", "Image/P6_D3_P9_Low_T2_19_Left/4.png", "Image/P6_D3_P9_Low_T2_19_Left/5.png", "Image/P6_D3_P9_Low_T2_19_Left/6.png", "Image/P6_D3_P9_Low_T2_19_Left/7.png", "Image/P6_D3_P9_Low_T2_19_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4589.584699999999 seconds to 4603.2432 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P9_High_T3_11_Left", "images": ["Image/P25_D1_P9_High_T3_11_Left/1.png", "Image/P25_D1_P9_High_T3_11_Left/2.png", "Image/P25_D1_P9_High_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this. Finish.\"\nThe frames roughly correspond to the speech interval from 364.1975 seconds to 372.3119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P4_Medium_T2_27_Left", "images": ["Image/P7_D1_P4_Medium_T2_27_Left/1.png", "Image/P7_D1_P4_Medium_T2_27_Left/2.png", "Image/P7_D1_P4_Medium_T2_27_Left/3.png", "Image/P7_D1_P4_Medium_T2_27_Left/4.png", "Image/P7_D1_P4_Medium_T2_27_Left/5.png", "Image/P7_D1_P4_Medium_T2_27_Left/6.png", "Image/P7_D1_P4_Medium_T2_27_Left/7.png", "Image/P7_D1_P4_Medium_T2_27_Left/8.png", "Image/P7_D1_P4_Medium_T2_27_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 306.8465 seconds to 324.6349 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P5_High_T2_7_Left", "images": ["Image/P11_D2_P5_High_T2_7_Left/1.png", "Image/P11_D2_P5_High_T2_7_Left/2.png", "Image/P11_D2_P5_High_T2_7_Left/3.png", "Image/P11_D2_P5_High_T2_7_Left/4.png", "Image/P11_D2_P5_High_T2_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Pour the water in a green glass\"\nThe frames roughly correspond to the speech interval from 786.5188 seconds to 798.0447 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P5_High_T1_25_Left", "images": ["Image/P13_D2_P5_High_T1_25_Left/1.png", "Image/P13_D2_P5_High_T1_25_Left/2.png", "Image/P13_D2_P5_High_T1_25_Left/3.png", "Image/P13_D2_P5_High_T1_25_Left/4.png", "Image/P13_D2_P5_High_T1_25_Left/5.png", "Image/P13_D2_P5_High_T1_25_Left/6.png", "Image/P13_D2_P5_High_T1_25_Left/7.png", "Image/P13_D2_P5_High_T1_25_Left/8.png", "Image/P13_D2_P5_High_T1_25_Left/9.png", "Image/P13_D2_P5_High_T1_25_Left/10.png", "Image/P13_D2_P5_High_T1_25_Left/11.png", "Image/P13_D2_P5_High_T1_25_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup in front of me over to this table over here\"\nThe frames roughly correspond to the speech interval from 1114.49166 seconds to 1131.75126 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P5_Medium_T1_3_Right", "images": ["Image/P24_D1_P5_Medium_T1_3_Right/1.png", "Image/P24_D1_P5_Medium_T1_3_Right/2.png", "Image/P24_D1_P5_Medium_T1_3_Right/3.png", "Image/P24_D1_P5_Medium_T1_3_Right/4.png", "Image/P24_D1_P5_Medium_T1_3_Right/5.png", "Image/P24_D1_P5_Medium_T1_3_Right/6.png", "Image/P24_D1_P5_Medium_T1_3_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you please move this bottle to this area\"\nThe frames roughly correspond to the speech interval from 1252.5407 seconds to 1265.1927 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P5_Low_T2_26_Left", "images": ["Image/P26_D2_P5_Low_T2_26_Left/1.png", "Image/P26_D2_P5_Low_T2_26_Left/2.png", "Image/P26_D2_P5_Low_T2_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water in there? Finish.\"\nThe frames roughly correspond to the speech interval from 1611.0565 seconds to 1621.357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P8_High_T1_9_Left", "images": ["Image/P3_D1_P8_High_T1_9_Left/1.png", "Image/P3_D1_P8_High_T1_9_Left/2.png", "Image/P3_D1_P8_High_T1_9_Left/3.png", "Image/P3_D1_P8_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1464.4036999999998 seconds to 1474.5801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P7_Medium_T2_5_Left", "images": ["Image/P7_D2_P7_Medium_T2_5_Left/1.png", "Image/P7_D2_P7_Medium_T2_5_Left/2.png", "Image/P7_D2_P7_Medium_T2_5_Left/3.png", "Image/P7_D2_P7_Medium_T2_5_Left/4.png", "Image/P7_D2_P7_Medium_T2_5_Left/5.png", "Image/P7_D2_P7_Medium_T2_5_Left/6.png", "Image/P7_D2_P7_Medium_T2_5_Left/7.png", "Image/P7_D2_P7_Medium_T2_5_Left/8.png", "Image/P7_D2_P7_Medium_T2_5_Left/9.png", "Image/P7_D2_P7_Medium_T2_5_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 991.2407000000002 seconds to 1008.0000000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P6_Low_T2_14_Mid", "images": ["Image/P17_D2_P6_Low_T2_14_Mid/1.png", "Image/P17_D2_P6_Low_T2_14_Mid/2.png", "Image/P17_D2_P6_Low_T2_14_Mid/3.png", "Image/P17_D2_P6_Low_T2_14_Mid/4.png", "Image/P17_D2_P6_Low_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that plant right there? Finish.\"\nThe frames roughly correspond to the speech interval from 332.66769999999997 seconds to 345.4166 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P6_High_T2_17_Mid", "images": ["Image/P3_D2_P6_High_T2_17_Mid/1.png", "Image/P3_D2_P6_High_T2_17_Mid/2.png", "Image/P3_D2_P6_High_T2_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2536.4251 seconds to 2546.2926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P5_High_T3_18_Right", "images": ["Image/P11_D3_P5_High_T3_18_Right/1.png", "Image/P11_D3_P5_High_T3_18_Right/2.png", "Image/P11_D3_P5_High_T3_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the right glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1725.5826 seconds to 1734.0333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P5_Low_T3_14_Left", "images": ["Image/P18_D1_P5_Low_T3_14_Left/1.png", "Image/P18_D1_P5_Low_T3_14_Left/2.png", "Image/P18_D1_P5_Low_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 848.6773000000001 seconds to 858.2817 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P6_Low_T1_21_Right", "images": ["Image/P5_D1_P6_Low_T1_21_Right/1.png", "Image/P5_D1_P6_Low_T1_21_Right/2.png", "Image/P5_D1_P6_Low_T1_21_Right/3.png", "Image/P5_D1_P6_Low_T1_21_Right/4.png", "Image/P5_D1_P6_Low_T1_21_Right/5.png", "Image/P5_D1_P6_Low_T1_21_Right/6.png", "Image/P5_D1_P6_Low_T1_21_Right/7.png", "Image/P5_D1_P6_Low_T1_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move the bottle to my right hand side\"\nThe frames roughly correspond to the speech interval from 1602.1903 seconds to 1615.3831 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P3_High_T2_12_Right", "images": ["Image/P6_D1_P3_High_T2_12_Right/1.png", "Image/P6_D1_P3_High_T2_12_Right/2.png", "Image/P6_D1_P3_High_T2_12_Right/3.png", "Image/P6_D1_P3_High_T2_12_Right/4.png", "Image/P6_D1_P3_High_T2_12_Right/5.png", "Image/P6_D1_P3_High_T2_12_Right/6.png", "Image/P6_D1_P3_High_T2_12_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2150.5609000000004 seconds to 2163.4194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P9_Medium_T3_10_Mid", "images": ["Image/P8_D1_P9_Medium_T3_10_Mid/1.png", "Image/P8_D1_P9_Medium_T3_10_Mid/2.png", "Image/P8_D1_P9_Medium_T3_10_Mid/3.png", "Image/P8_D1_P9_Medium_T3_10_Mid/4.png", "Image/P8_D1_P9_Medium_T3_10_Mid/5.png", "Image/P8_D1_P9_Medium_T3_10_Mid/6.png", "Image/P8_D1_P9_Medium_T3_10_Mid/7.png", "Image/P8_D1_P9_Medium_T3_10_Mid/8.png", "Image/P8_D1_P9_Medium_T3_10_Mid/9.png", "Image/P8_D1_P9_Medium_T3_10_Mid/10.png", "Image/P8_D1_P9_Medium_T3_10_Mid/11.png", "Image/P8_D1_P9_Medium_T3_10_Mid/12.png", "Image/P8_D1_P9_Medium_T3_10_Mid/13.png", "Image/P8_D1_P9_Medium_T3_10_Mid/14.png", "Image/P8_D1_P9_Medium_T3_10_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. So you can see that there are three brown glass or three brown cap bottles on the right side. So could you clean the middle one? Finish.\"\nThe frames roughly correspond to the speech interval from 581.2963 seconds to 603.3865 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P9_High_T2_24_Right", "images": ["Image/P5_D3_P9_High_T2_24_Right/1.png", "Image/P5_D3_P9_High_T2_24_Right/2.png", "Image/P5_D3_P9_High_T2_24_Right/3.png", "Image/P5_D3_P9_High_T2_24_Right/4.png", "Image/P5_D3_P9_High_T2_24_Right/5.png", "Image/P5_D3_P9_High_T2_24_Right/6.png", "Image/P5_D3_P9_High_T2_24_Right/7.png", "Image/P5_D3_P9_High_T2_24_Right/8.png", "Image/P5_D3_P9_High_T2_24_Right/9.png", "Image/P5_D3_P9_High_T2_24_Right/10.png", "Image/P5_D3_P9_High_T2_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" water the plant\"\nThe frames roughly correspond to the speech interval from 2409.7743 seconds to 2426.9082 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P5_Medium_T2_24_Mid", "images": ["Image/P20_D2_P5_Medium_T2_24_Mid/1.png", "Image/P20_D2_P5_Medium_T2_24_Mid/2.png", "Image/P20_D2_P5_Medium_T2_24_Mid/3.png", "Image/P20_D2_P5_Medium_T2_24_Mid/4.png", "Image/P20_D2_P5_Medium_T2_24_Mid/5.png", "Image/P20_D2_P5_Medium_T2_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3150.7731000000003 seconds to 3162.5478000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P2_Low_T1_13_Right", "images": ["Image/P28_D2_P2_Low_T1_13_Right/1.png", "Image/P28_D2_P2_Low_T1_13_Right/2.png", "Image/P28_D2_P2_Low_T1_13_Right/3.png", "Image/P28_D2_P2_Low_T1_13_Right/4.png", "Image/P28_D2_P2_Low_T1_13_Right/5.png", "Image/P28_D2_P2_Low_T1_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to the bottom shelf. Finish\"\nThe frames roughly correspond to the speech interval from 889.3166 seconds to 900.6613 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P6_High_T3_21_Mid", "images": ["Image/P18_D2_P6_High_T3_21_Mid/1.png", "Image/P18_D2_P6_High_T3_21_Mid/2.png", "Image/P18_D2_P6_High_T3_21_Mid/3.png", "Image/P18_D2_P6_High_T3_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the white vase. Finish.\"\nThe frames roughly correspond to the speech interval from 353.97310000000004 seconds to 364.0362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P3_Low_T1_16_Left", "images": ["Image/P1_D3_P3_Low_T1_16_Left/1.png", "Image/P1_D3_P3_Low_T1_16_Left/2.png", "Image/P1_D3_P3_Low_T1_16_Left/3.png", "Image/P1_D3_P3_Low_T1_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1537.5704999999998 seconds to 1547.7331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P8_High_T3_22_Mid", "images": ["Image/P1_D1_P8_High_T3_22_Mid/1.png", "Image/P1_D1_P8_High_T3_22_Mid/2.png", "Image/P1_D1_P8_High_T3_22_Mid/3.png", "Image/P1_D1_P8_High_T3_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 411.7569 seconds to 420.5963 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P5_Low_T2_21_Right", "images": ["Image/P11_D1_P5_Low_T2_21_Right/1.png", "Image/P11_D1_P5_Low_T2_21_Right/2.png", "Image/P11_D1_P5_Low_T2_21_Right/3.png", "Image/P11_D1_P5_Low_T2_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 2851.8758 seconds to 2861.7578 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P4_Low_T1_27_Mid", "images": ["Image/P14_D3_P4_Low_T1_27_Mid/1.png", "Image/P14_D3_P4_Low_T1_27_Mid/2.png", "Image/P14_D3_P4_Low_T1_27_Mid/3.png", "Image/P14_D3_P4_Low_T1_27_Mid/4.png", "Image/P14_D3_P4_Low_T1_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that over to that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1062.60482 seconds to 1072.2134 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P3_High_T2_26_Left", "images": ["Image/P20_D3_P3_High_T2_26_Left/1.png", "Image/P20_D3_P3_High_T2_26_Left/2.png", "Image/P20_D3_P3_High_T2_26_Left/3.png", "Image/P20_D3_P3_High_T2_26_Left/4.png", "Image/P20_D3_P3_High_T2_26_Left/5.png", "Image/P20_D3_P3_High_T2_26_Left/6.png", "Image/P20_D3_P3_High_T2_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Finished. Hey\"\nThe frames roughly correspond to the speech interval from 1195.38995 seconds to 1209.2544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P1_Low_T2_18_Right", "images": ["Image/P24_D3_P1_Low_T2_18_Right/1.png", "Image/P24_D3_P1_Low_T2_18_Right/2.png", "Image/P24_D3_P1_Low_T2_18_Right/3.png", "Image/P24_D3_P1_Low_T2_18_Right/4.png", "Image/P24_D3_P1_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1831.658 seconds to 1841.7769 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P6_High_T2_13_Left", "images": ["Image/P15_D1_P6_High_T2_13_Left/1.png", "Image/P15_D1_P6_High_T2_13_Left/2.png", "Image/P15_D1_P6_High_T2_13_Left/3.png", "Image/P15_D1_P6_High_T2_13_Left/4.png", "Image/P15_D1_P6_High_T2_13_Left/5.png", "Image/P15_D1_P6_High_T2_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1696.21987 seconds to 1708.56547 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P8_Medium_T2_9_Left", "images": ["Image/P24_D1_P8_Medium_T2_9_Left/1.png", "Image/P24_D1_P8_Medium_T2_9_Left/2.png", "Image/P24_D1_P8_Medium_T2_9_Left/3.png", "Image/P24_D1_P8_Medium_T2_9_Left/4.png", "Image/P24_D1_P8_Medium_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1295.9978 seconds to 1306.6323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P8_Medium_T1_15_Left", "images": ["Image/P7_D1_P8_Medium_T1_15_Left/1.png", "Image/P7_D1_P8_Medium_T1_15_Left/2.png", "Image/P7_D1_P8_Medium_T1_15_Left/3.png", "Image/P7_D1_P8_Medium_T1_15_Left/4.png", "Image/P7_D1_P8_Medium_T1_15_Left/5.png", "Image/P7_D1_P8_Medium_T1_15_Left/6.png", "Image/P7_D1_P8_Medium_T1_15_Left/7.png", "Image/P7_D1_P8_Medium_T1_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 496.4903 seconds to 511.8165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P4_High_T1_24_Right", "images": ["Image/P6_D2_P4_High_T1_24_Right/1.png", "Image/P6_D2_P4_High_T1_24_Right/2.png", "Image/P6_D2_P4_High_T1_24_Right/3.png", "Image/P6_D2_P4_High_T1_24_Right/4.png", "Image/P6_D2_P4_High_T1_24_Right/5.png", "Image/P6_D2_P4_High_T1_24_Right/6.png", "Image/P6_D2_P4_High_T1_24_Right/7.png", "Image/P6_D2_P4_High_T1_24_Right/8.png", "Image/P6_D2_P4_High_T1_24_Right/9.png", "Image/P6_D2_P4_High_T1_24_Right/10.png", "Image/P6_D2_P4_High_T1_24_Right/11.png", "Image/P6_D2_P4_High_T1_24_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday\"\nThe frames roughly correspond to the speech interval from 648.5504 seconds to 668.7928999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P3_Low_T2_13_Left", "images": ["Image/P20_D2_P3_Low_T2_13_Left/1.png", "Image/P20_D2_P3_Low_T2_13_Left/2.png", "Image/P20_D2_P3_Low_T2_13_Left/3.png", "Image/P20_D2_P3_Low_T2_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 3257.8583000000003 seconds to 3267.3666000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P7_Medium_T3_16_Left", "images": ["Image/P25_D2_P7_Medium_T3_16_Left/1.png", "Image/P25_D2_P7_Medium_T3_16_Left/2.png", "Image/P25_D2_P7_Medium_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Lean the third cup. Finish.\"\nThe frames roughly correspond to the speech interval from 855.1543 seconds to 863.2564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P1_High_T3_16_Left", "images": ["Image/P4_D3_P1_High_T3_16_Left/1.png", "Image/P4_D3_P1_High_T3_16_Left/2.png", "Image/P4_D3_P1_High_T3_16_Left/3.png", "Image/P4_D3_P1_High_T3_16_Left/4.png", "Image/P4_D3_P1_High_T3_16_Left/5.png", "Image/P4_D3_P1_High_T3_16_Left/6.png", "Image/P4_D3_P1_High_T3_16_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left red bottle on the shoulder? Finish.\"\nThe frames roughly correspond to the speech interval from 521.8384999999998 seconds to 537.3738999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P8_High_T2_4_Left", "images": ["Image/P30_D1_P8_High_T2_4_Left/1.png", "Image/P30_D1_P8_High_T2_4_Left/2.png", "Image/P30_D1_P8_High_T2_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 940.2396 seconds to 951.2077 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P9_Low_T1_20_Mid", "images": ["Image/P5_D3_P9_Low_T1_20_Mid/1.png", "Image/P5_D3_P9_Low_T1_20_Mid/2.png", "Image/P5_D3_P9_Low_T1_20_Mid/3.png", "Image/P5_D3_P9_Low_T1_20_Mid/4.png", "Image/P5_D3_P9_Low_T1_20_Mid/5.png", "Image/P5_D3_P9_Low_T1_20_Mid/6.png", "Image/P5_D3_P9_Low_T1_20_Mid/7.png", "Image/P5_D3_P9_Low_T1_20_Mid/8.png", "Image/P5_D3_P9_Low_T1_20_Mid/9.png", "Image/P5_D3_P9_Low_T1_20_Mid/10.png", "Image/P5_D3_P9_Low_T1_20_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottle From the bottom level of the shell to the top level of the shell in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2836.5306 seconds to 2854.1241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P2_High_T1_6_Right", "images": ["Image/P23_D1_P2_High_T1_6_Right/1.png", "Image/P23_D1_P2_High_T1_6_Right/2.png", "Image/P23_D1_P2_High_T1_6_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1585.8765 seconds to 1595.3549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P2_Medium_T2_12_Mid", "images": ["Image/P5_D2_P2_Medium_T2_12_Mid/1.png", "Image/P5_D2_P2_Medium_T2_12_Mid/2.png", "Image/P5_D2_P2_Medium_T2_12_Mid/3.png", "Image/P5_D2_P2_Medium_T2_12_Mid/4.png", "Image/P5_D2_P2_Medium_T2_12_Mid/5.png", "Image/P5_D2_P2_Medium_T2_12_Mid/6.png", "Image/P5_D2_P2_Medium_T2_12_Mid/7.png", "Image/P5_D2_P2_Medium_T2_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. There are three wells in front of me. Pour water into the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 870.9526000000001 seconds to 884.8154999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P7_Low_T1_11_Right", "images": ["Image/P11_D1_P7_Low_T1_11_Right/1.png", "Image/P11_D1_P7_Low_T1_11_Right/2.png", "Image/P11_D1_P7_Low_T1_11_Right/3.png", "Image/P11_D1_P7_Low_T1_11_Right/4.png", "Image/P11_D1_P7_Low_T1_11_Right/5.png", "Image/P11_D1_P7_Low_T1_11_Right/6.png", "Image/P11_D1_P7_Low_T1_11_Right/7.png", "Image/P11_D1_P7_Low_T1_11_Right/8.png", "Image/P11_D1_P7_Low_T1_11_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the red bottle in the basket to the top of the shelf on the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 2911.3677999999995 seconds to 2925.7745999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P5_Medium_T2_5_Left", "images": ["Image/P28_D2_P5_Medium_T2_5_Left/1.png", "Image/P28_D2_P5_Medium_T2_5_Left/2.png", "Image/P28_D2_P5_Medium_T2_5_Left/3.png", "Image/P28_D2_P5_Medium_T2_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1181.3426 seconds to 1190.6006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P5_High_T3_20_Right", "images": ["Image/P23_D1_P5_High_T3_20_Right/1.png", "Image/P23_D1_P5_High_T3_20_Right/2.png", "Image/P23_D1_P5_High_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1831.3681 seconds to 1840.3994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P2_High_T2_1_Mid", "images": ["Image/P28_D1_P2_High_T2_1_Mid/1.png", "Image/P28_D1_P2_High_T2_1_Mid/2.png", "Image/P28_D1_P2_High_T2_1_Mid/3.png", "Image/P28_D1_P2_High_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1786.0022 seconds to 1799.4906 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P1_Medium_T3_23_Right", "images": ["Image/P11_D1_P1_Medium_T3_23_Right/1.png", "Image/P11_D1_P1_Medium_T3_23_Right/2.png", "Image/P11_D1_P1_Medium_T3_23_Right/3.png", "Image/P11_D1_P1_Medium_T3_23_Right/4.png", "Image/P11_D1_P1_Medium_T3_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first plant from the right side. Finish.\"\nThe frames roughly correspond to the speech interval from 2932.5274 seconds to 2943.7879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P1_Low_T3_8_Mid", "images": ["Image/P4_D1_P1_Low_T3_8_Mid/3.png", "Image/P4_D1_P1_Low_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2086.24246 seconds to 2100.23024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P9_Medium_T3_27_Left", "images": ["Image/P21_D1_P9_Medium_T3_27_Left/1.png", "Image/P21_D1_P9_Medium_T3_27_Left/2.png", "Image/P21_D1_P9_Medium_T3_27_Left/3.png", "Image/P21_D1_P9_Medium_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 896.5005699999999 seconds to 906.36965 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P7_Low_T1_13_Mid", "images": ["Image/P3_D2_P7_Low_T1_13_Mid/1.png", "Image/P3_D2_P7_Low_T1_13_Mid/2.png", "Image/P3_D2_P7_Low_T1_13_Mid/3.png", "Image/P3_D2_P7_Low_T1_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2387.9469 seconds to 2398.8042 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P6_Medium_T3_15_Right", "images": ["Image/P4_D1_P6_Medium_T3_15_Right/1.png", "Image/P4_D1_P6_Medium_T3_15_Right/2.png", "Image/P4_D1_P6_Medium_T3_15_Right/3.png", "Image/P4_D1_P6_Medium_T3_15_Right/4.png", "Image/P4_D1_P6_Medium_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start and clean this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2205.3234 seconds to 2218.4060999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P9_Low_T2_6_Mid", "images": ["Image/P7_D1_P9_Low_T2_6_Mid/1.png", "Image/P7_D1_P9_Low_T2_6_Mid/2.png", "Image/P7_D1_P9_Low_T2_6_Mid/3.png", "Image/P7_D1_P9_Low_T2_6_Mid/4.png", "Image/P7_D1_P9_Low_T2_6_Mid/5.png", "Image/P7_D1_P9_Low_T2_6_Mid/6.png", "Image/P7_D1_P9_Low_T2_6_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Done. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 259.337 seconds to 274.37 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P6_Low_T3_27_Right", "images": ["Image/P5_D2_P6_Low_T3_27_Right/1.png", "Image/P5_D2_P6_Low_T3_27_Right/2.png", "Image/P5_D2_P6_Low_T3_27_Right/3.png", "Image/P5_D2_P6_Low_T3_27_Right/4.png", "Image/P5_D2_P6_Low_T3_27_Right/5.png", "Image/P5_D2_P6_Low_T3_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup to the rest of you. Finish.\"\nThe frames roughly correspond to the speech interval from 757.8649 seconds to 771.1287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P7_High_T2_26_Mid", "images": ["Image/P18_D2_P7_High_T2_26_Mid/1.png", "Image/P18_D2_P7_High_T2_26_Mid/2.png", "Image/P18_D2_P7_High_T2_26_Mid/3.png", "Image/P18_D2_P7_High_T2_26_Mid/4.png", "Image/P18_D2_P7_High_T2_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour the water out of the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 218.1135 seconds to 229.9181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P7_Medium_T1_23_Left", "images": ["Image/P30_D3_P7_Medium_T1_23_Left/1.png", "Image/P30_D3_P7_Medium_T1_23_Left/2.png", "Image/P30_D3_P7_Medium_T1_23_Left/3.png", "Image/P30_D3_P7_Medium_T1_23_Left/4.png", "Image/P30_D3_P7_Medium_T1_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1348.6559 seconds to 1361.964 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P6_Medium_T1_23_Mid", "images": ["Image/P16_D1_P6_Medium_T1_23_Mid/1.png", "Image/P16_D1_P6_Medium_T1_23_Mid/2.png", "Image/P16_D1_P6_Medium_T1_23_Mid/3.png", "Image/P16_D1_P6_Medium_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you please move this plant over here? Finish\"\nThe frames roughly correspond to the speech interval from 1609.2248 seconds to 1618.953 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P3_Medium_T2_27_Left", "images": ["Image/P27_D2_P3_Medium_T2_27_Left/1.png", "Image/P27_D2_P3_Medium_T2_27_Left/2.png", "Image/P27_D2_P3_Medium_T2_27_Left/3.png", "Image/P27_D2_P3_Medium_T2_27_Left/4.png", "Image/P27_D2_P3_Medium_T2_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in that first cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1351.2044999999998 seconds to 1362.172 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P8_High_T3_19_Mid", "images": ["Image/P6_D3_P8_High_T3_19_Mid/1.png", "Image/P6_D3_P8_High_T3_19_Mid/2.png", "Image/P6_D3_P8_High_T3_19_Mid/3.png", "Image/P6_D3_P8_High_T3_19_Mid/4.png", "Image/P6_D3_P8_High_T3_19_Mid/5.png", "Image/P6_D3_P8_High_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4518.2629 seconds to 4531.683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P6_High_T1_11_Left", "images": ["Image/P8_D1_P6_High_T1_11_Left/1.png", "Image/P8_D1_P6_High_T1_11_Left/2.png", "Image/P8_D1_P6_High_T1_11_Left/3.png", "Image/P8_D1_P6_High_T1_11_Left/4.png", "Image/P8_D1_P6_High_T1_11_Left/5.png", "Image/P8_D1_P6_High_T1_11_Left/6.png", "Image/P8_D1_P6_High_T1_11_Left/7.png", "Image/P8_D1_P6_High_T1_11_Left/8.png", "Image/P8_D1_P6_High_T1_11_Left/9.png", "Image/P8_D1_P6_High_T1_11_Left/10.png", "Image/P8_D1_P6_High_T1_11_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. So\"\nThe frames roughly correspond to the speech interval from 639.0062999999999 seconds to 657.1000999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P5_Low_T1_19_Mid", "images": ["Image/P22_D2_P5_Low_T1_19_Mid/1.png", "Image/P22_D2_P5_Low_T1_19_Mid/2.png", "Image/P22_D2_P5_Low_T1_19_Mid/3.png", "Image/P22_D2_P5_Low_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put the vase right there. Finish.\"\nThe frames roughly correspond to the speech interval from 934.1348 seconds to 943.8103 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P3_High_T2_19_Mid", "images": ["Image/P21_D3_P3_High_T2_19_Mid/1.png", "Image/P21_D3_P3_High_T2_19_Mid/2.png", "Image/P21_D3_P3_High_T2_19_Mid/3.png", "Image/P21_D3_P3_High_T2_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 527.8471999999999 seconds to 539.1354 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P3_High_T3_2_Right", "images": ["Image/P10_D1_P3_High_T3_2_Right/1.png", "Image/P10_D1_P3_High_T3_2_Right/2.png", "Image/P10_D1_P3_High_T3_2_Right/3.png", "Image/P10_D1_P3_High_T3_2_Right/4.png", "Image/P10_D1_P3_High_T3_2_Right/5.png", "Image/P10_D1_P3_High_T3_2_Right/6.png", "Image/P10_D1_P3_High_T3_2_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2338.8032 seconds to 2350.8719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P9_Low_T3_17_Left", "images": ["Image/P12_D1_P9_Low_T3_17_Left/1.png", "Image/P12_D1_P9_Low_T3_17_Left/2.png", "Image/P12_D1_P9_Low_T3_17_Left/3.png", "Image/P12_D1_P9_Low_T3_17_Left/4.png", "Image/P12_D1_P9_Low_T3_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1437.4588 seconds to 1448.9562 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P5_High_T2_20_Mid", "images": ["Image/P14_D1_P5_High_T2_20_Mid/1.png", "Image/P14_D1_P5_High_T2_20_Mid/2.png", "Image/P14_D1_P5_High_T2_20_Mid/3.png", "Image/P14_D1_P5_High_T2_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 481.0875 seconds to 490.5241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P2_Medium_T1_17_Right", "images": ["Image/P13_D1_P2_Medium_T1_17_Right/1.png", "Image/P13_D1_P2_Medium_T1_17_Right/2.png", "Image/P13_D1_P2_Medium_T1_17_Right/3.png", "Image/P13_D1_P2_Medium_T1_17_Right/4.png", "Image/P13_D1_P2_Medium_T1_17_Right/5.png", "Image/P13_D1_P2_Medium_T1_17_Right/6.png", "Image/P13_D1_P2_Medium_T1_17_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the right round cup down to here\"\nThe frames roughly correspond to the speech interval from 656.9565 seconds to 669.1218 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P3_High_T1_10_Left", "images": ["Image/P24_D3_P3_High_T1_10_Left/1.png", "Image/P24_D3_P3_High_T1_10_Left/2.png", "Image/P24_D3_P3_High_T1_10_Left/3.png", "Image/P24_D3_P3_High_T1_10_Left/4.png", "Image/P24_D3_P3_High_T1_10_Left/5.png", "Image/P24_D3_P3_High_T1_10_Left/6.png", "Image/P24_D3_P3_High_T1_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle down to this area? Finish.\"\nThe frames roughly correspond to the speech interval from 2043.781 seconds to 2056.085 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P6_Medium_T3_23_Right", "images": ["Image/P9_D3_P6_Medium_T3_23_Right/1.png", "Image/P9_D3_P6_Medium_T3_23_Right/2.png", "Image/P9_D3_P6_Medium_T3_23_Right/3.png", "Image/P9_D3_P6_Medium_T3_23_Right/4.png", "Image/P9_D3_P6_Medium_T3_23_Right/5.png", "Image/P9_D3_P6_Medium_T3_23_Right/6.png", "Image/P9_D3_P6_Medium_T3_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right plant on the second layer? Finish.\"\nThe frames roughly correspond to the speech interval from 1037.7387 seconds to 1051.624 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P8_High_T2_27_Right", "images": ["Image/P23_D3_P8_High_T2_27_Right/1.png", "Image/P23_D3_P8_High_T2_27_Right/2.png", "Image/P23_D3_P8_High_T2_27_Right/3.png", "Image/P23_D3_P8_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 628.25926 seconds to 639.70055 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P2_Medium_T1_26_Mid", "images": ["Image/P28_D3_P2_Medium_T1_26_Mid/1.png", "Image/P28_D3_P2_Medium_T1_26_Mid/2.png", "Image/P28_D3_P2_Medium_T1_26_Mid/3.png", "Image/P28_D3_P2_Medium_T1_26_Mid/4.png", "Image/P28_D3_P2_Medium_T1_26_Mid/5.png", "Image/P28_D3_P2_Medium_T1_26_Mid/6.png", "Image/P28_D3_P2_Medium_T1_26_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move the middle white vase to the floor underneath the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 335.40930000000003 seconds to 348.3549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P9_Medium_T1_7_Left", "images": ["Image/P18_D1_P9_Medium_T1_7_Left/1.png", "Image/P18_D1_P9_Medium_T1_7_Left/2.png", "Image/P18_D1_P9_Medium_T1_7_Left/3.png", "Image/P18_D1_P9_Medium_T1_7_Left/4.png", "Image/P18_D1_P9_Medium_T1_7_Left/5.png", "Image/P18_D1_P9_Medium_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this cup on the left up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 746.2133 seconds to 761.0504000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P3_High_T1_6_Left", "images": ["Image/P30_D3_P3_High_T1_6_Left/1.png", "Image/P30_D3_P3_High_T1_6_Left/2.png", "Image/P30_D3_P3_High_T1_6_Left/3.png", "Image/P30_D3_P3_High_T1_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1545.4709 seconds to 1554.6987 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P3_Low_T1_27_Left", "images": ["Image/P15_D1_P3_Low_T1_27_Left/1.png", "Image/P15_D1_P3_Low_T1_27_Left/2.png", "Image/P15_D1_P3_Low_T1_27_Left/3.png", "Image/P15_D1_P3_Low_T1_27_Left/4.png", "Image/P15_D1_P3_Low_T1_27_Left/5.png", "Image/P15_D1_P3_Low_T1_27_Left/6.png", "Image/P15_D1_P3_Low_T1_27_Left/7.png", "Image/P15_D1_P3_Low_T1_27_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white cup from the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 1612.82647 seconds to 1626.67707 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P6_Medium_T1_1_Right", "images": ["Image/P11_D3_P6_Medium_T1_1_Right/1.png", "Image/P11_D3_P6_Medium_T1_1_Right/2.png", "Image/P11_D3_P6_Medium_T1_1_Right/3.png", "Image/P11_D3_P6_Medium_T1_1_Right/4.png", "Image/P11_D3_P6_Medium_T1_1_Right/5.png", "Image/P11_D3_P6_Medium_T1_1_Right/6.png", "Image/P11_D3_P6_Medium_T1_1_Right/7.png", "Image/P11_D3_P6_Medium_T1_1_Right/8.png", "Image/P11_D3_P6_Medium_T1_1_Right/9.png", "Image/P11_D3_P6_Medium_T1_1_Right/10.png", "Image/P11_D3_P6_Medium_T1_1_Right/11.png", "Image/P11_D3_P6_Medium_T1_1_Right/12.png", "Image/P11_D3_P6_Medium_T1_1_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first green bottle from the right side to the desk on my left side. Finish.\"\nThe frames roughly correspond to the speech interval from 1698.2999 seconds to 1717.0491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P9_Low_T1_16_Left", "images": ["Image/P20_D2_P9_Low_T1_16_Left/1.png", "Image/P20_D2_P9_Low_T1_16_Left/2.png", "Image/P20_D2_P9_Low_T1_16_Left/3.png", "Image/P20_D2_P9_Low_T1_16_Left/4.png", "Image/P20_D2_P9_Low_T1_16_Left/5.png", "Image/P20_D2_P9_Low_T1_16_Left/6.png", "Image/P20_D2_P9_Low_T1_16_Left/7.png", "Image/P20_D2_P9_Low_T1_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that cup to the top of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 2854.793 seconds to 2868.5734 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P4_High_T2_21_Left", "images": ["Image/P3_D3_P4_High_T2_21_Left/1.png", "Image/P3_D3_P4_High_T2_21_Left/2.png", "Image/P3_D3_P4_High_T2_21_Left/3.png", "Image/P3_D3_P4_High_T2_21_Left/4.png", "Image/P3_D3_P4_High_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1088.2873 seconds to 1098.7837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P4_Low_T3_19_Left", "images": ["Image/P27_D2_P4_Low_T3_19_Left/1.png", "Image/P27_D2_P4_Low_T3_19_Left/2.png", "Image/P27_D2_P4_Low_T3_19_Left/3.png", "Image/P27_D2_P4_Low_T3_19_Left/4.png", "Image/P27_D2_P4_Low_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that vase? Finish\"\nThe frames roughly correspond to the speech interval from 1532.8168999999998 seconds to 1545.8598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P8_Low_T3_7_Right", "images": ["Image/P18_D2_P8_Low_T3_7_Right/1.png", "Image/P18_D2_P8_Low_T3_7_Right/2.png", "Image/P18_D2_P8_Low_T3_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that green cup. Finish.\"\nThe frames roughly correspond to the speech interval from 479.7387 seconds to 488.7036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P2_High_T1_2_Left", "images": ["Image/P14_D3_P2_High_T1_2_Left/1.png", "Image/P14_D3_P2_High_T1_2_Left/2.png", "Image/P14_D3_P2_High_T1_2_Left/3.png", "Image/P14_D3_P2_High_T1_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1372.13536 seconds to 1381.38436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P2_Low_T3_14_Left", "images": ["Image/P2_D3_P2_Low_T3_14_Left/1.png", "Image/P2_D3_P2_Low_T3_14_Left/2.png", "Image/P2_D3_P2_Low_T3_14_Left/3.png", "Image/P2_D3_P2_Low_T3_14_Left/4.png", "Image/P2_D3_P2_Low_T3_14_Left/5.png", "Image/P2_D3_P2_Low_T3_14_Left/6.png", "Image/P2_D3_P2_Low_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 2032.4773 seconds to 2046.5199 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P2_Medium_T3_1_Right", "images": ["Image/P17_D1_P2_Medium_T3_1_Right/1.png", "Image/P17_D1_P2_Medium_T3_1_Right/2.png", "Image/P17_D1_P2_Medium_T3_1_Right/3.png", "Image/P17_D1_P2_Medium_T3_1_Right/4.png", "Image/P17_D1_P2_Medium_T3_1_Right/5.png", "Image/P17_D1_P2_Medium_T3_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this green vase right here? Finish\"\nThe frames roughly correspond to the speech interval from 1746.3331 seconds to 1758.0395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P6_Low_T2_17_Mid", "images": ["Image/P3_D3_P6_Low_T2_17_Mid/1.png", "Image/P3_D3_P6_Low_T2_17_Mid/2.png", "Image/P3_D3_P6_Low_T2_17_Mid/3.png", "Image/P3_D3_P6_Low_T2_17_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you give some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 868.4024999999999 seconds to 878.6770999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P3_High_T3_11_Left", "images": ["Image/P26_D1_P3_High_T3_11_Left/1.png", "Image/P26_D1_P3_High_T3_11_Left/2.png", "Image/P26_D1_P3_High_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this vase? Finish.\"\nThe frames roughly correspond to the speech interval from 42.522829999999985 seconds to 54.27024999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P8_High_T2_10_Left", "images": ["Image/P13_D2_P8_High_T2_10_Left/1.png", "Image/P13_D2_P8_High_T2_10_Left/2.png", "Image/P13_D2_P8_High_T2_10_Left/3.png", "Image/P13_D2_P8_High_T2_10_Left/4.png", "Image/P13_D2_P8_High_T2_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the brown bottle down here\"\nThe frames roughly correspond to the speech interval from 1284.82986 seconds to 1295.05966 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P8_High_T2_9_Mid", "images": ["Image/P8_D1_P8_High_T2_9_Mid/1.png", "Image/P8_D1_P8_High_T2_9_Mid/2.png", "Image/P8_D1_P8_High_T2_9_Mid/3.png", "Image/P8_D1_P8_High_T2_9_Mid/4.png", "Image/P8_D1_P8_High_T2_9_Mid/5.png", "Image/P8_D1_P8_High_T2_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 863.6338999999999 seconds to 876.9366999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P6_Medium_T2_26_Left", "images": ["Image/P25_D3_P6_Medium_T2_26_Left/1.png", "Image/P25_D3_P6_Medium_T2_26_Left/2.png", "Image/P25_D3_P6_Medium_T2_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1901.3268 seconds to 1909.9867 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P3_Medium_T3_5_Mid", "images": ["Image/P5_D3_P3_Medium_T3_5_Mid/1.png", "Image/P5_D3_P3_Medium_T3_5_Mid/2.png", "Image/P5_D3_P3_Medium_T3_5_Mid/3.png", "Image/P5_D3_P3_Medium_T3_5_Mid/4.png", "Image/P5_D3_P3_Medium_T3_5_Mid/5.png", "Image/P5_D3_P3_Medium_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle plant on the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 2264.9927 seconds to 2276.6007999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_Medium_T1_8_Left", "images": ["Image/P20_D2_P4_Medium_T1_8_Left/1.png", "Image/P20_D2_P4_Medium_T1_8_Left/2.png", "Image/P20_D2_P4_Medium_T1_8_Left/3.png", "Image/P20_D2_P4_Medium_T1_8_Left/4.png", "Image/P20_D2_P4_Medium_T1_8_Left/5.png", "Image/P20_D2_P4_Medium_T1_8_Left/6.png", "Image/P20_D2_P4_Medium_T1_8_Left/7.png", "Image/P20_D2_P4_Medium_T1_8_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that left cup to that table. Finish\"\nThe frames roughly correspond to the speech interval from 3337.7871000000005 seconds to 3351.3163000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P3_Low_T2_1_Mid", "images": ["Image/P14_D3_P3_Low_T2_1_Mid/1.png", "Image/P14_D3_P3_Low_T2_1_Mid/2.png", "Image/P14_D3_P3_Low_T2_1_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1131.78656 seconds to 1140.31326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P1_Medium_T1_5_Left", "images": ["Image/P29_D1_P1_Medium_T1_5_Left/1.png", "Image/P29_D1_P1_Medium_T1_5_Left/2.png", "Image/P29_D1_P1_Medium_T1_5_Left/3.png", "Image/P29_D1_P1_Medium_T1_5_Left/4.png", "Image/P29_D1_P1_Medium_T1_5_Left/5.png", "Image/P29_D1_P1_Medium_T1_5_Left/6.png", "Image/P29_D1_P1_Medium_T1_5_Left/7.png", "Image/P29_D1_P1_Medium_T1_5_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Hey Friday\"\nThe frames roughly correspond to the speech interval from 2608.9302 seconds to 2624.8399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P7_High_T2_23_Right", "images": ["Image/P4_D3_P7_High_T2_23_Right/1.png", "Image/P4_D3_P7_High_T2_23_Right/2.png", "Image/P4_D3_P7_High_T2_23_Right/3.png", "Image/P4_D3_P7_High_T2_23_Right/4.png", "Image/P4_D3_P7_High_T2_23_Right/5.png", "Image/P4_D3_P7_High_T2_23_Right/6.png", "Image/P4_D3_P7_High_T2_23_Right/7.png", "Image/P4_D3_P7_High_T2_23_Right/8.png", "Image/P4_D3_P7_High_T2_23_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start? Can you water the right plant on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 601.5553 seconds to 616.9938999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P9_Low_T3_17_Mid", "images": ["Image/P17_D1_P9_Low_T3_17_Mid/1.png", "Image/P17_D1_P9_Low_T3_17_Mid/2.png", "Image/P17_D1_P9_Low_T3_17_Mid/3.png", "Image/P17_D1_P9_Low_T3_17_Mid/4.png", "Image/P17_D1_P9_Low_T3_17_Mid/5.png", "Image/P17_D1_P9_Low_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the brown mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1839.6378 seconds to 1850.7736 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P3_High_T1_7_Right", "images": ["Image/P28_D1_P3_High_T1_7_Right/1.png", "Image/P28_D1_P3_High_T1_7_Right/2.png", "Image/P28_D1_P3_High_T1_7_Right/3.png", "Image/P28_D1_P3_High_T1_7_Right/4.png", "Image/P28_D1_P3_High_T1_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1528.4694 seconds to 1538.8546 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P7_High_T2_22_Left", "images": ["Image/P21_D2_P7_High_T2_22_Left/1.png", "Image/P21_D2_P7_High_T2_22_Left/2.png", "Image/P21_D2_P7_High_T2_22_Left/3.png", "Image/P21_D2_P7_High_T2_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1742.3352 seconds to 1752.303 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P5_High_T1_2_Mid", "images": ["Image/P20_D3_P5_High_T1_2_Mid/1.png", "Image/P20_D3_P5_High_T1_2_Mid/2.png", "Image/P20_D3_P5_High_T1_2_Mid/3.png", "Image/P20_D3_P5_High_T1_2_Mid/4.png", "Image/P20_D3_P5_High_T1_2_Mid/5.png", "Image/P20_D3_P5_High_T1_2_Mid/6.png", "Image/P20_D3_P5_High_T1_2_Mid/7.png", "Image/P20_D3_P5_High_T1_2_Mid/8.png", "Image/P20_D3_P5_High_T1_2_Mid/9.png", "Image/P20_D3_P5_High_T1_2_Mid/10.png", "Image/P20_D3_P5_High_T1_2_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Okay\"\nThe frames roughly correspond to the speech interval from 1467.2044 seconds to 1484.7283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P3_Medium_T3_8_Mid", "images": ["Image/P11_D2_P3_Medium_T3_8_Mid/1.png", "Image/P11_D2_P3_Medium_T3_8_Mid/2.png", "Image/P11_D2_P3_Medium_T3_8_Mid/3.png", "Image/P11_D2_P3_Medium_T3_8_Mid/4.png", "Image/P11_D2_P3_Medium_T3_8_Mid/5.png", "Image/P11_D2_P3_Medium_T3_8_Mid/6.png", "Image/P11_D2_P3_Medium_T3_8_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the middle of the green glass. Please finish.\"\nThe frames roughly correspond to the speech interval from 537.6598 seconds to 551.1152 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P8_Low_T3_20_Mid", "images": ["Image/P27_D3_P8_Low_T3_20_Mid/1.png", "Image/P27_D3_P8_Low_T3_20_Mid/2.png", "Image/P27_D3_P8_Low_T3_20_Mid/3.png", "Image/P27_D3_P8_Low_T3_20_Mid/4.png", "Image/P27_D3_P8_Low_T3_20_Mid/5.png", "Image/P27_D3_P8_Low_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that white vase at the bottom? Finish.\"\nThe frames roughly correspond to the speech interval from 143.22851000000003 seconds to 154.83660000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P9_Medium_T2_6_Left", "images": ["Image/P8_D3_P9_Medium_T2_6_Left/1.png", "Image/P8_D3_P9_Medium_T2_6_Left/2.png", "Image/P8_D3_P9_Medium_T2_6_Left/3.png", "Image/P8_D3_P9_Medium_T2_6_Left/41.png", "Image/P8_D3_P9_Medium_T2_6_Left/42.png", "Image/P8_D3_P9_Medium_T2_6_Left/43.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you water the planet that is on the leftmost? Finish.\"\nThe frames roughly correspond to the speech interval from 1057.22237 seconds to 1118.53581 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P4_Low_T1_5_Left", "images": ["Image/P18_D3_P4_Low_T1_5_Left/1.png", "Image/P18_D3_P4_Low_T1_5_Left/2.png", "Image/P18_D3_P4_Low_T1_5_Left/3.png", "Image/P18_D3_P4_Low_T1_5_Left/4.png", "Image/P18_D3_P4_Low_T1_5_Left/5.png", "Image/P18_D3_P4_Low_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1159.47388 seconds to 1171.93235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P9_Low_T1_3_Right", "images": ["Image/P3_D3_P9_Low_T1_3_Right/1.png", "Image/P3_D3_P9_Low_T1_3_Right/2.png", "Image/P3_D3_P9_Low_T1_3_Right/3.png", "Image/P3_D3_P9_Low_T1_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you move the item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 886.9884999999999 seconds to 896.9839 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P5_High_T3_25_Right", "images": ["Image/P20_D1_P5_High_T3_25_Right/1.png", "Image/P20_D1_P5_High_T3_25_Right/2.png", "Image/P20_D1_P5_High_T3_25_Right/3.png", "Image/P20_D1_P5_High_T3_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the white cup on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 313.7267 seconds to 324.3477 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P5_Medium_T2_26_Left", "images": ["Image/P15_D2_P5_Medium_T2_26_Left/1.png", "Image/P15_D2_P5_Medium_T2_26_Left/2.png", "Image/P15_D2_P5_Medium_T2_26_Left/3.png", "Image/P15_D2_P5_Medium_T2_26_Left/4.png", "Image/P15_D2_P5_Medium_T2_26_Left/5.png", "Image/P15_D2_P5_Medium_T2_26_Left/6.png", "Image/P15_D2_P5_Medium_T2_26_Left/7.png", "Image/P15_D2_P5_Medium_T2_26_Left/8.png", "Image/P15_D2_P5_Medium_T2_26_Left/9.png", "Image/P15_D2_P5_Medium_T2_26_Left/10.png", "Image/P15_D2_P5_Medium_T2_26_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2855.2314 seconds to 2871.2405 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P3_Low_T2_20_Mid", "images": ["Image/P12_D2_P3_Low_T2_20_Mid/1.png", "Image/P12_D2_P3_Low_T2_20_Mid/2.png", "Image/P12_D2_P3_Low_T2_20_Mid/3.png", "Image/P12_D2_P3_Low_T2_20_Mid/4.png", "Image/P12_D2_P3_Low_T2_20_Mid/5.png", "Image/P12_D2_P3_Low_T2_20_Mid/6.png", "Image/P12_D2_P3_Low_T2_20_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the white vase that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 477.24570000000006 seconds to 489.7732999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P4_Low_T2_10_Right", "images": ["Image/P14_D2_P4_Low_T2_10_Right/1.png", "Image/P14_D2_P4_Low_T2_10_Right/2.png", "Image/P14_D2_P4_Low_T2_10_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1839.2990999999995 seconds to 1847.2969 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P8_Low_T3_20_Left", "images": ["Image/P21_D3_P8_Low_T3_20_Left/1.png", "Image/P21_D3_P8_Low_T3_20_Left/2.png", "Image/P21_D3_P8_Low_T3_20_Left/3.png", "Image/P21_D3_P8_Low_T3_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 618.8137999999999 seconds to 634.6692 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P7_High_T3_2_Right", "images": ["Image/P8_D1_P7_High_T3_2_Right/1.png", "Image/P8_D1_P7_High_T3_2_Right/2.png", "Image/P8_D1_P7_High_T3_2_Right/3.png", "Image/P8_D1_P7_High_T3_2_Right/4.png", "Image/P8_D1_P7_High_T3_2_Right/5.png", "Image/P8_D1_P7_High_T3_2_Right/6.png", "Image/P8_D1_P7_High_T3_2_Right/7.png", "Image/P8_D1_P7_High_T3_2_Right/8.png", "Image/P8_D1_P7_High_T3_2_Right/9.png", "Image/P8_D1_P7_High_T3_2_Right/10.png", "Image/P8_D1_P7_High_T3_2_Right/11.png", "Image/P8_D1_P7_High_T3_2_Right/12.png", "Image/P8_D1_P7_High_T3_2_Right/13.png", "Image/P8_D1_P7_High_T3_2_Right/14.png", "Image/P8_D1_P7_High_T3_2_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 352.7647999999999 seconds to 373.5514999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P2_Low_T2_18_Right", "images": ["Image/P23_D2_P2_Low_T2_18_Right/1.png", "Image/P23_D2_P2_Low_T2_18_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 486.1618 seconds to 495.4879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P5_Medium_T2_17_Right", "images": ["Image/P30_D3_P5_Medium_T2_17_Right/1.png", "Image/P30_D3_P5_Medium_T2_17_Right/2.png", "Image/P30_D3_P5_Medium_T2_17_Right/3.png", "Image/P30_D3_P5_Medium_T2_17_Right/4.png", "Image/P30_D3_P5_Medium_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1454.7823 seconds to 1468.2835 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P6_Low_T2_26_Left", "images": ["Image/P9_D2_P6_Low_T2_26_Left/1.png", "Image/P9_D2_P6_Low_T2_26_Left/2.png", "Image/P9_D2_P6_Low_T2_26_Left/3.png", "Image/P9_D2_P6_Low_T2_26_Left/4.png", "Image/P9_D2_P6_Low_T2_26_Left/5.png", "Image/P9_D2_P6_Low_T2_26_Left/6.png", "Image/P9_D2_P6_Low_T2_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in that cup on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2776.5862 seconds to 2789.4443 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P6_High_T1_27_Right", "images": ["Image/P21_D3_P6_High_T1_27_Right/1.png", "Image/P21_D3_P6_High_T1_27_Right/2.png", "Image/P21_D3_P6_High_T1_27_Right/3.png", "Image/P21_D3_P6_High_T1_27_Right/4.png", "Image/P21_D3_P6_High_T1_27_Right/5.png", "Image/P21_D3_P6_High_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 546.3743 seconds to 559.5424 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P5_Medium_T1_22_Right", "images": ["Image/P14_D1_P5_Medium_T1_22_Right/1.png", "Image/P14_D1_P5_Medium_T1_22_Right/2.png", "Image/P14_D1_P5_Medium_T1_22_Right/3.png", "Image/P14_D1_P5_Medium_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move this plant right here\"\nThe frames roughly correspond to the speech interval from 569.0747 seconds to 578.484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P5_Medium_T2_11_Left", "images": ["Image/P17_D1_P5_Medium_T2_11_Left/1.png", "Image/P17_D1_P5_Medium_T2_11_Left/2.png", "Image/P17_D1_P5_Medium_T2_11_Left/3.png", "Image/P17_D1_P5_Medium_T2_11_Left/4.png", "Image/P17_D1_P5_Medium_T2_11_Left/5.png", "Image/P17_D1_P5_Medium_T2_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this brown vase right here? Finish\"\nThe frames roughly correspond to the speech interval from 1625.0217000000002 seconds to 1637.4926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P5_High_T1_12_Mid", "images": ["Image/P27_D2_P5_High_T1_12_Mid/1.png", "Image/P27_D2_P5_High_T1_12_Mid/2.png", "Image/P27_D2_P5_High_T1_12_Mid/3.png", "Image/P27_D2_P5_High_T1_12_Mid/4.png", "Image/P27_D2_P5_High_T1_12_Mid/5.png", "Image/P27_D2_P5_High_T1_12_Mid/6.png", "Image/P27_D2_P5_High_T1_12_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that red vase right there on that cabinet? Finish\"\nThe frames roughly correspond to the speech interval from 1429.1558 seconds to 1442.0582 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P1_Low_T3_1_Right", "images": ["Image/P11_D2_P1_Low_T3_1_Right/1.png", "Image/P11_D2_P1_Low_T3_1_Right/2.png", "Image/P11_D2_P1_Low_T3_1_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the glass finish.\"\nThe frames roughly correspond to the speech interval from 1012.4395 seconds to 1021.1315 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P3_Medium_T3_16_Mid", "images": ["Image/P27_D3_P3_Medium_T3_16_Mid/1.png", "Image/P27_D3_P3_Medium_T3_16_Mid/2.png", "Image/P27_D3_P3_Medium_T3_16_Mid/3.png", "Image/P27_D3_P3_Medium_T3_16_Mid/4.png", "Image/P27_D3_P3_Medium_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the middle cup over there? Finish\"\nThe frames roughly correspond to the speech interval from 392.2553 seconds to 406.3423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P6_High_T2_10_Mid", "images": ["Image/P6_D1_P6_High_T2_10_Mid/1.png", "Image/P6_D1_P6_High_T2_10_Mid/2.png", "Image/P6_D1_P6_High_T2_10_Mid/3.png", "Image/P6_D1_P6_High_T2_10_Mid/4.png", "Image/P6_D1_P6_High_T2_10_Mid/5.png", "Image/P6_D1_P6_High_T2_10_Mid/6.png", "Image/P6_D1_P6_High_T2_10_Mid/7.png", "Image/P6_D1_P6_High_T2_10_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1971.6065000000003 seconds to 1984.7577 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P1_Low_T2_12_Left", "images": ["Image/P15_D1_P1_Low_T2_12_Left/1.png", "Image/P15_D1_P1_Low_T2_12_Left/2.png", "Image/P15_D1_P1_Low_T2_12_Left/3.png", "Image/P15_D1_P1_Low_T2_12_Left/4.png", "Image/P15_D1_P1_Low_T2_12_Left/5.png", "Image/P15_D1_P1_Low_T2_12_Left/6.png", "Image/P15_D1_P1_Low_T2_12_Left/7.png", "Image/P15_D1_P1_Low_T2_12_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the brown jar at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1977.73847 seconds to 1991.25287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P5_Medium_T3_27_Mid", "images": ["Image/P5_D2_P5_Medium_T3_27_Mid/1.png", "Image/P5_D2_P5_Medium_T3_27_Mid/2.png", "Image/P5_D2_P5_Medium_T3_27_Mid/3.png", "Image/P5_D2_P5_Medium_T3_27_Mid/4.png", "Image/P5_D2_P5_Medium_T3_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup that's in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 777.452 seconds to 793.4874 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P6_Low_T3_4_Right", "images": ["Image/P16_D3_P6_Low_T3_4_Right/1.png", "Image/P16_D3_P6_Low_T3_4_Right/2.png", "Image/P16_D3_P6_Low_T3_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 433.5432 seconds to 442.3126999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P9_Medium_T2_13_Right", "images": ["Image/P13_D3_P9_Medium_T2_13_Right/1.png", "Image/P13_D3_P9_Medium_T2_13_Right/2.png", "Image/P13_D3_P9_Medium_T2_13_Right/3.png", "Image/P13_D3_P9_Medium_T2_13_Right/4.png", "Image/P13_D3_P9_Medium_T2_13_Right/5.png", "Image/P13_D3_P9_Medium_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1575.2776 seconds to 1586.5668999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P6_Medium_T1_21_Mid", "images": ["Image/P24_D2_P6_Medium_T1_21_Mid/1.png", "Image/P24_D2_P6_Medium_T1_21_Mid/2.png", "Image/P24_D2_P6_Medium_T1_21_Mid/3.png", "Image/P24_D2_P6_Medium_T1_21_Mid/4.png", "Image/P24_D2_P6_Medium_T1_21_Mid/5.png", "Image/P24_D2_P6_Medium_T1_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one here please? Finish.\"\nThe frames roughly correspond to the speech interval from 345.0526 seconds to 358.05279999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P3_High_T2_21_Right", "images": ["Image/P7_D3_P3_High_T2_21_Right/1.png", "Image/P7_D3_P3_High_T2_21_Right/2.png", "Image/P7_D3_P3_High_T2_21_Right/3.png", "Image/P7_D3_P3_High_T2_21_Right/4.png", "Image/P7_D3_P3_High_T2_21_Right/5.png", "Image/P7_D3_P3_High_T2_21_Right/6.png", "Image/P7_D3_P3_High_T2_21_Right/7.png", "Image/P7_D3_P3_High_T2_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1633.22261 seconds to 1648.1443800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P3_Low_T2_4_Mid", "images": ["Image/P22_D3_P3_Low_T2_4_Mid/1.png", "Image/P22_D3_P3_Low_T2_4_Mid/2.png", "Image/P22_D3_P3_Low_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 405.1932000000001 seconds to 413.725 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P9_High_T1_20_Mid", "images": ["Image/P11_D3_P9_High_T1_20_Mid/1.png", "Image/P11_D3_P9_High_T1_20_Mid/2.png", "Image/P11_D3_P9_High_T1_20_Mid/3.png", "Image/P11_D3_P9_High_T1_20_Mid/4.png", "Image/P11_D3_P9_High_T1_20_Mid/5.png", "Image/P11_D3_P9_High_T1_20_Mid/6.png", "Image/P11_D3_P9_High_T1_20_Mid/7.png", "Image/P11_D3_P9_High_T1_20_Mid/8.png", "Image/P11_D3_P9_High_T1_20_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white bottle from the bottom of the shelf to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1554.507 seconds to 1569.8331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P6_Medium_T3_6_Mid", "images": ["Image/P25_D2_P6_Medium_T3_6_Mid/1.png", "Image/P25_D2_P6_Medium_T3_6_Mid/2.png", "Image/P25_D2_P6_Medium_T3_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the second plant. Finish.\"\nThe frames roughly correspond to the speech interval from 886.2945 seconds to 894.6738 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P1_High_T2_6_Right", "images": ["Image/P7_D1_P1_High_T2_6_Right/1.png", "Image/P7_D1_P1_High_T2_6_Right/2.png", "Image/P7_D1_P1_High_T2_6_Right/3.png", "Image/P7_D1_P1_High_T2_6_Right/4.png", "Image/P7_D1_P1_High_T2_6_Right/5.png", "Image/P7_D1_P1_High_T2_6_Right/6.png", "Image/P7_D1_P1_High_T2_6_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 402.5869 seconds to 417.3843 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P1_Medium_T2_21_Left", "images": ["Image/P2_D3_P1_Medium_T2_21_Left/1.png", "Image/P2_D3_P1_Medium_T2_21_Left/2.png", "Image/P2_D3_P1_Medium_T2_21_Left/3.png", "Image/P2_D3_P1_Medium_T2_21_Left/4.png", "Image/P2_D3_P1_Medium_T2_21_Left/5.png", "Image/P2_D3_P1_Medium_T2_21_Left/6.png", "Image/P2_D3_P1_Medium_T2_21_Left/7.png", "Image/P2_D3_P1_Medium_T2_21_Left/8.png", "Image/P2_D3_P1_Medium_T2_21_Left/9.png", "Image/P2_D3_P1_Medium_T2_21_Left/10.png", "Image/P2_D3_P1_Medium_T2_21_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the wide vessel on the very left side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2249.0575000000003 seconds to 2270.5079000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P3_Medium_T1_20_Right", "images": ["Image/P25_D2_P3_Medium_T1_20_Right/1.png", "Image/P25_D2_P3_Medium_T1_20_Right/2.png", "Image/P25_D2_P3_Medium_T1_20_Right/3.png", "Image/P25_D2_P3_Medium_T1_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1043.0737 seconds to 1052.1218 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P6_High_T3_10_Left", "images": ["Image/P29_D1_P6_High_T3_10_Left/1.png", "Image/P29_D1_P6_High_T3_10_Left/2.png", "Image/P29_D1_P6_High_T3_10_Left/3.png", "Image/P29_D1_P6_High_T3_10_Left/4.png", "Image/P29_D1_P6_High_T3_10_Left/5.png", "Image/P29_D1_P6_High_T3_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2970.3594 seconds to 2981.4113 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P3_Medium_T2_4_Mid", "images": ["Image/P3_D3_P3_Medium_T2_4_Mid/1.png", "Image/P3_D3_P3_Medium_T2_4_Mid/2.png", "Image/P3_D3_P3_Medium_T2_4_Mid/3.png", "Image/P3_D3_P3_Medium_T2_4_Mid/4.png", "Image/P3_D3_P3_Medium_T2_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 670.8774 seconds to 681.8191999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P4_High_T2_2_Mid", "images": ["Image/P1_D1_P4_High_T2_2_Mid/1.png", "Image/P1_D1_P4_High_T2_2_Mid/2.png", "Image/P1_D1_P4_High_T2_2_Mid/3.png", "Image/P1_D1_P4_High_T2_2_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 489.1194 seconds to 498.5736 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P8_Medium_T3_21_Left", "images": ["Image/P19_D2_P8_Medium_T3_21_Left/1.png", "Image/P19_D2_P8_Medium_T3_21_Left/2.png", "Image/P19_D2_P8_Medium_T3_21_Left/3.png", "Image/P19_D2_P8_Medium_T3_21_Left/4.png", "Image/P19_D2_P8_Medium_T3_21_Left/5.png", "Image/P19_D2_P8_Medium_T3_21_Left/6.png", "Image/P19_D2_P8_Medium_T3_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this item down here. Okay.\"\nThe frames roughly correspond to the speech interval from 1404.1948599999998 seconds to 1419.4219999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P1_Low_T1_18_Right", "images": ["Image/P7_D1_P1_Low_T1_18_Right/1.png", "Image/P7_D1_P1_Low_T1_18_Right/2.png", "Image/P7_D1_P1_Low_T1_18_Right/3.png", "Image/P7_D1_P1_Low_T1_18_Right/4.png", "Image/P7_D1_P1_Low_T1_18_Right/5.png", "Image/P7_D1_P1_Low_T1_18_Right/6.png", "Image/P7_D1_P1_Low_T1_18_Right/7.png", "Image/P7_D1_P1_Low_T1_18_Right/8.png", "Image/P7_D1_P1_Low_T1_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 761.1082 seconds to 777.7559 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P3_Medium_T2_27_Mid", "images": ["Image/P1_D1_P3_Medium_T2_27_Mid/1.png", "Image/P1_D1_P3_Medium_T2_27_Mid/2.png", "Image/P1_D1_P3_Medium_T2_27_Mid/3.png", "Image/P1_D1_P3_Medium_T2_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water to this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 291.34479999999996 seconds to 301.0902 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P4_Medium_T3_9_Left", "images": ["Image/P20_D3_P4_Medium_T3_9_Left/1.png", "Image/P20_D3_P4_Medium_T3_9_Left/2.png", "Image/P20_D3_P4_Medium_T3_9_Left/3.png", "Image/P20_D3_P4_Medium_T3_9_Left/4.png", "Image/P20_D3_P4_Medium_T3_9_Left/5.png", "Image/P20_D3_P4_Medium_T3_9_Left/6.png", "Image/P20_D3_P4_Medium_T3_9_Left/7.png", "Image/P20_D3_P4_Medium_T3_9_Left/8.png", "Image/P20_D3_P4_Medium_T3_9_Left/9.png", "Image/P20_D3_P4_Medium_T3_9_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the... Cup on the left over there. Finished.\"\nThe frames roughly correspond to the speech interval from 1561.4425 seconds to 1578.4662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P9_High_T1_17_Left", "images": ["Image/P28_D3_P9_High_T1_17_Left/1.png", "Image/P28_D3_P9_High_T1_17_Left/2.png", "Image/P28_D3_P9_High_T1_17_Left/3.png", "Image/P28_D3_P9_High_T1_17_Left/4.png", "Image/P28_D3_P9_High_T1_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move the orange vase to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 194.7212 seconds to 207.1515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P7_Low_T3_13_Mid", "images": ["Image/P14_D1_P7_Low_T3_13_Mid/1.png", "Image/P14_D1_P7_Low_T3_13_Mid/2.png", "Image/P14_D1_P7_Low_T3_13_Mid/3.png", "Image/P14_D1_P7_Low_T3_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 291.3438 seconds to 301.0473 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P9_Low_T3_27_Left", "images": ["Image/P26_D1_P9_Low_T3_27_Left/1.png", "Image/P26_D1_P9_Low_T3_27_Left/2.png", "Image/P26_D1_P9_Low_T3_27_Left/3.png", "Image/P26_D1_P9_Low_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that cup on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 494.6681 seconds to 505.0254 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P4_High_T3_17_Right", "images": ["Image/P6_D3_P4_High_T3_17_Right/1.png", "Image/P6_D3_P4_High_T3_17_Right/2.png", "Image/P6_D3_P4_High_T3_17_Right/3.png", "Image/P6_D3_P4_High_T3_17_Right/4.png", "Image/P6_D3_P4_High_T3_17_Right/5.png", "Image/P6_D3_P4_High_T3_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Freddy\"\nThe frames roughly correspond to the speech interval from 4543.3075 seconds to 4577.9735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P9_Medium_T1_8_Left", "images": ["Image/P29_D1_P9_Medium_T1_8_Left/1.png", "Image/P29_D1_P9_Medium_T1_8_Left/2.png", "Image/P29_D1_P9_Medium_T1_8_Left/3.png", "Image/P29_D1_P9_Medium_T1_8_Left/4.png", "Image/P29_D1_P9_Medium_T1_8_Left/5.png", "Image/P29_D1_P9_Medium_T1_8_Left/6.png", "Image/P29_D1_P9_Medium_T1_8_Left/7.png", "Image/P29_D1_P9_Medium_T1_8_Left/8.png", "Image/P29_D1_P9_Medium_T1_8_Left/9.png", "Image/P29_D1_P9_Medium_T1_8_Left/10.png", "Image/P29_D1_P9_Medium_T1_8_Left/11.png", "Image/P29_D1_P9_Medium_T1_8_Left/12.png", "Image/P29_D1_P9_Medium_T1_8_Left/13.png", "Image/P29_D1_P9_Medium_T1_8_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2542.5739 seconds to 2556.5352999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P3_Medium_T2_2_Left", "images": ["Image/P1_D2_P3_Medium_T2_2_Left/1.png", "Image/P1_D2_P3_Medium_T2_2_Left/2.png", "Image/P1_D2_P3_Medium_T2_2_Left/3.png", "Image/P1_D2_P3_Medium_T2_2_Left/4.png", "Image/P1_D2_P3_Medium_T2_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water to the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 826.1307 seconds to 837.3358000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P5_Low_T1_11_Right", "images": ["Image/P16_D3_P5_Low_T1_11_Right/1.png", "Image/P16_D3_P5_Low_T1_11_Right/2.png", "Image/P16_D3_P5_Low_T1_11_Right/3.png", "Image/P16_D3_P5_Low_T1_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 265.293 seconds to 274.8414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P7_Low_T2_8_Right", "images": ["Image/P24_D2_P7_Low_T2_8_Right/1.png", "Image/P24_D2_P7_Low_T2_8_Right/2.png", "Image/P24_D2_P7_Low_T2_8_Right/3.png", "Image/P24_D2_P7_Low_T2_8_Right/4.png", "Image/P24_D2_P7_Low_T2_8_Right/5.png", "Image/P24_D2_P7_Low_T2_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you pour water in this cup\"\nThe frames roughly correspond to the speech interval from 256.89739999999995 seconds to 268.479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P8_High_T2_23_Left", "images": ["Image/P18_D2_P8_High_T2_23_Left/1.png", "Image/P18_D2_P8_High_T2_23_Left/2.png", "Image/P18_D2_P8_High_T2_23_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 318.89390000000003 seconds to 327.4705 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P4_Low_T1_24_Mid", "images": ["Image/P24_D3_P4_Low_T1_24_Mid/1.png", "Image/P24_D3_P4_Low_T1_24_Mid/2.png", "Image/P24_D3_P4_Low_T1_24_Mid/3.png", "Image/P24_D3_P4_Low_T1_24_Mid/4.png", "Image/P24_D3_P4_Low_T1_24_Mid/5.png", "Image/P24_D3_P4_Low_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shell? Finish.\"\nThe frames roughly correspond to the speech interval from 1850.1977 seconds to 1861.0967 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P6_Medium_T2_1_Left", "images": ["Image/P26_D1_P6_Medium_T2_1_Left/1.png", "Image/P26_D1_P6_Medium_T2_1_Left/2.png", "Image/P26_D1_P6_Medium_T2_1_Left/3.png", "Image/P26_D1_P6_Medium_T2_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water in this leftmost jug? Finish.\"\nThe frames roughly correspond to the speech interval from 235.5353 seconds to 247.0173 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P4_Medium_T1_20_Right", "images": ["Image/P19_D1_P4_Medium_T1_20_Right/1.png", "Image/P19_D1_P4_Medium_T1_20_Right/2.png", "Image/P19_D1_P4_Medium_T1_20_Right/3.png", "Image/P19_D1_P4_Medium_T1_20_Right/4.png", "Image/P19_D1_P4_Medium_T1_20_Right/5.png", "Image/P19_D1_P4_Medium_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1252.6050999999998 seconds to 1268.5425999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P4_Medium_T1_24_Left", "images": ["Image/P11_D1_P4_Medium_T1_24_Left/1.png", "Image/P11_D1_P4_Medium_T1_24_Left/2.png", "Image/P11_D1_P4_Medium_T1_24_Left/3.png", "Image/P11_D1_P4_Medium_T1_24_Left/4.png", "Image/P11_D1_P4_Medium_T1_24_Left/5.png", "Image/P11_D1_P4_Medium_T1_24_Left/6.png", "Image/P11_D1_P4_Medium_T1_24_Left/7.png", "Image/P11_D1_P4_Medium_T1_24_Left/8.png", "Image/P11_D1_P4_Medium_T1_24_Left/9.png", "Image/P11_D1_P4_Medium_T1_24_Left/10.png", "Image/P11_D1_P4_Medium_T1_24_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first plant from the left side to the middle of the shelf close to the wall. Finish.\"\nThe frames roughly correspond to the speech interval from 2620.0401999999995 seconds to 2636.784799999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P8_High_T1_6_Left", "images": ["Image/P9_D3_P8_High_T1_6_Left/1.png", "Image/P9_D3_P8_High_T1_6_Left/2.png", "Image/P9_D3_P8_High_T1_6_Left/3.png", "Image/P9_D3_P8_High_T1_6_Left/4.png", "Image/P9_D3_P8_High_T1_6_Left/5.png", "Image/P9_D3_P8_High_T1_6_Left/6.png", "Image/P9_D3_P8_High_T1_6_Left/7.png", "Image/P9_D3_P8_High_T1_6_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the floor to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 712.7528 seconds to 727.4305 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P3_High_T1_6_Mid", "images": ["Image/P18_D2_P3_High_T1_6_Mid/1.png", "Image/P18_D2_P3_High_T1_6_Mid/2.png", "Image/P18_D2_P3_High_T1_6_Mid/3.png", "Image/P18_D2_P3_High_T1_6_Mid/4.png", "Image/P18_D2_P3_High_T1_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green plant to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 80.80873000000003 seconds to 92.54184000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P2_Low_T3_8_Mid", "images": ["Image/P29_D1_P2_Low_T3_8_Mid/1.png", "Image/P29_D1_P2_Low_T3_8_Mid/2.png", "Image/P29_D1_P2_Low_T3_8_Mid/3.png", "Image/P29_D1_P2_Low_T3_8_Mid/4.png", "Image/P29_D1_P2_Low_T3_8_Mid/5.png", "Image/P29_D1_P2_Low_T3_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 2900.7218 seconds to 2912.6063 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P7_Medium_T2_22_Left", "images": ["Image/P13_D1_P7_Medium_T2_22_Left/1.png", "Image/P13_D1_P7_Medium_T2_22_Left/2.png", "Image/P13_D1_P7_Medium_T2_22_Left/3.png", "Image/P13_D1_P7_Medium_T2_22_Left/4.png", "Image/P13_D1_P7_Medium_T2_22_Left/5.png", "Image/P13_D1_P7_Medium_T2_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the far left plant down there\"\nThe frames roughly correspond to the speech interval from 243.605 seconds to 255.3527 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P3_High_T3_24_Left", "images": ["Image/P30_D2_P3_High_T3_24_Left/1.png", "Image/P30_D2_P3_High_T3_24_Left/2.png", "Image/P30_D2_P3_High_T3_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 504.8243 seconds to 513.0921 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P4_High_T2_2_Left", "images": ["Image/P22_D3_P4_High_T2_2_Left/1.png", "Image/P22_D3_P4_High_T2_2_Left/2.png", "Image/P22_D3_P4_High_T2_2_Left/3.png", "Image/P22_D3_P4_High_T2_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water in the green jug. Finish.\"\nThe frames roughly correspond to the speech interval from 211.3037 seconds to 222.6616 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P8_High_T3_25_Mid", "images": ["Image/P27_D1_P8_High_T3_25_Mid/1.png", "Image/P27_D1_P8_High_T3_25_Mid/2.png", "Image/P27_D1_P8_High_T3_25_Mid/3.png", "Image/P27_D1_P8_High_T3_25_Mid/4.png", "Image/P27_D1_P8_High_T3_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1035.8604 seconds to 1046.6756 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P3_Low_T2_4_Mid", "images": ["Image/P18_D2_P3_Low_T2_4_Mid/1.png", "Image/P18_D2_P3_Low_T2_4_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 193.4902 seconds to 203.12150000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P4_High_T3_10_Left", "images": ["Image/P4_D1_P4_High_T3_10_Left/1.png", "Image/P4_D1_P4_High_T3_10_Left/2.png", "Image/P4_D1_P4_High_T3_10_Left/3.png", "Image/P4_D1_P4_High_T3_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2412.4651 seconds to 2424.2239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P8_Medium_T3_24_Right", "images": ["Image/P25_D3_P8_Medium_T3_24_Right/1.png", "Image/P25_D3_P8_Medium_T3_24_Right/2.png", "Image/P25_D3_P8_Medium_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Clean the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2024.0501 seconds to 2032.6124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P7_Medium_T3_23_Mid", "images": ["Image/P27_D1_P7_Medium_T3_23_Mid/1.png", "Image/P27_D1_P7_Medium_T3_23_Mid/2.png", "Image/P27_D1_P7_Medium_T3_23_Mid/3.png", "Image/P27_D1_P7_Medium_T3_23_Mid/4.png", "Image/P27_D1_P7_Medium_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the plant in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 957.7842 seconds to 970.0731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P1_High_T2_19_Right", "images": ["Image/P12_D1_P1_High_T2_19_Right/1.png", "Image/P12_D1_P1_High_T2_19_Right/2.png", "Image/P12_D1_P1_High_T2_19_Right/3.png", "Image/P12_D1_P1_High_T2_19_Right/4.png", "Image/P12_D1_P1_High_T2_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1392.9576 seconds to 1407.1694 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P3_Low_T2_9_Left", "images": ["Image/P17_D2_P3_Low_T2_9_Left/1.png", "Image/P17_D2_P3_Low_T2_9_Left/2.png", "Image/P17_D2_P3_Low_T2_9_Left/3.png", "Image/P17_D2_P3_Low_T2_9_Left/4.png", "Image/P17_D2_P3_Low_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that green cup? Finish.\"\nThe frames roughly correspond to the speech interval from 486.4967 seconds to 501.8362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P8_Low_T3_16_Mid", "images": ["Image/P7_D1_P8_Low_T3_16_Mid/1.png", "Image/P7_D1_P8_Low_T3_16_Mid/2.png", "Image/P7_D1_P8_Low_T3_16_Mid/3.png", "Image/P7_D1_P8_Low_T3_16_Mid/4.png", "Image/P7_D1_P8_Low_T3_16_Mid/5.png", "Image/P7_D1_P8_Low_T3_16_Mid/6.png", "Image/P7_D1_P8_Low_T3_16_Mid/7.png", "Image/P7_D1_P8_Low_T3_16_Mid/8.png", "Image/P7_D1_P8_Low_T3_16_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 124.45337 seconds to 141.2689 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P7_High_T1_6_Left", "images": ["Image/P12_D1_P7_High_T1_6_Left/1.png", "Image/P12_D1_P7_High_T1_6_Left/2.png", "Image/P12_D1_P7_High_T1_6_Left/3.png", "Image/P12_D1_P7_High_T1_6_Left/4.png", "Image/P12_D1_P7_High_T1_6_Left/5.png", "Image/P12_D1_P7_High_T1_6_Left/6.png", "Image/P12_D1_P7_High_T1_6_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1620.0049 seconds to 1634.7867 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P9_High_T2_4_Mid", "images": ["Image/P3_D3_P9_High_T2_4_Mid/1.png", "Image/P3_D3_P9_High_T2_4_Mid/2.png", "Image/P3_D3_P9_High_T2_4_Mid/3.png", "Image/P3_D3_P9_High_T2_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 956.2652 seconds to 966.9415 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P7_High_T3_19_Left", "images": ["Image/P30_D1_P7_High_T3_19_Left/1.png", "Image/P30_D1_P7_High_T3_19_Left/2.png", "Image/P30_D1_P7_High_T3_19_Left/3.png", "Image/P30_D1_P7_High_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1025.8194 seconds to 1034.3242 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P5_Low_T1_14_Left", "images": ["Image/P15_D1_P5_Low_T1_14_Left/1.png", "Image/P15_D1_P5_Low_T1_14_Left/2.png", "Image/P15_D1_P5_Low_T1_14_Left/3.png", "Image/P15_D1_P5_Low_T1_14_Left/4.png", "Image/P15_D1_P5_Low_T1_14_Left/5.png", "Image/P15_D1_P5_Low_T1_14_Left/6.png", "Image/P15_D1_P5_Low_T1_14_Left/7.png", "Image/P15_D1_P5_Low_T1_14_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant At the right side of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2112.98247 seconds to 2126.67917 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P1_High_T2_23_Right", "images": ["Image/P27_D2_P1_High_T2_23_Right/1.png", "Image/P27_D2_P1_High_T2_23_Right/2.png", "Image/P27_D2_P1_High_T2_23_Right/3.png", "Image/P27_D2_P1_High_T2_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1314.4418 seconds to 1323.9487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P2_High_T2_8_Left", "images": ["Image/P6_D2_P2_High_T2_8_Left/1.png", "Image/P6_D2_P2_High_T2_8_Left/2.png", "Image/P6_D2_P2_High_T2_8_Left/3.png", "Image/P6_D2_P2_High_T2_8_Left/4.png", "Image/P6_D2_P2_High_T2_8_Left/5.png", "Image/P6_D2_P2_High_T2_8_Left/6.png", "Image/P6_D2_P2_High_T2_8_Left/7.png", "Image/P6_D2_P2_High_T2_8_Left/8.png", "Image/P6_D2_P2_High_T2_8_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 746.2962 seconds to 762.068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P5_Medium_T1_18_Left", "images": ["Image/P28_D3_P5_Medium_T1_18_Left/1.png", "Image/P28_D3_P5_Medium_T1_18_Left/2.png", "Image/P28_D3_P5_Medium_T1_18_Left/3.png", "Image/P28_D3_P5_Medium_T1_18_Left/4.png", "Image/P28_D3_P5_Medium_T1_18_Left/5.png", "Image/P28_D3_P5_Medium_T1_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 356.581 seconds to 367.9537 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P5_Low_T2_13_Right", "images": ["Image/P13_D1_P5_Low_T2_13_Right/1.png", "Image/P13_D1_P5_Low_T2_13_Right/2.png", "Image/P13_D1_P5_Low_T2_13_Right/3.png", "Image/P13_D1_P5_Low_T2_13_Right/4.png", "Image/P13_D1_P5_Low_T2_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you please pour water into the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 335.7407 seconds to 346.8484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P7_Low_T3_17_Right", "images": ["Image/P5_D1_P7_Low_T3_17_Right/1.png", "Image/P5_D1_P7_Low_T3_17_Right/2.png", "Image/P5_D1_P7_Low_T3_17_Right/3.png", "Image/P5_D1_P7_Low_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup in the bucket. Finished.\"\nThe frames roughly correspond to the speech interval from 1876.7601 seconds to 1886.308 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P6_Medium_T3_27_Left", "images": ["Image/P13_D1_P6_Medium_T3_27_Left/1.png", "Image/P13_D1_P6_Medium_T3_27_Left/2.png", "Image/P13_D1_P6_Medium_T3_27_Left/3.png", "Image/P13_D1_P6_Medium_T3_27_Left/4.png", "Image/P13_D1_P6_Medium_T3_27_Left/5.png", "Image/P13_D1_P6_Medium_T3_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday\"\nThe frames roughly correspond to the speech interval from 530.7873 seconds to 542.0313 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P8_Low_T1_17_Left", "images": ["Image/P24_D1_P8_Low_T1_17_Left/1.png", "Image/P24_D1_P8_Low_T1_17_Left/2.png", "Image/P24_D1_P8_Low_T1_17_Left/3.png", "Image/P24_D1_P8_Low_T1_17_Left/4.png", "Image/P24_D1_P8_Low_T1_17_Left/5.png", "Image/P24_D1_P8_Low_T1_17_Left/6.png", "Image/P24_D1_P8_Low_T1_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you please move this cup over here\"\nThe frames roughly correspond to the speech interval from 1177.9992 seconds to 1190.1233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P7_Low_T3_11_Right", "images": ["Image/P18_D2_P7_Low_T3_11_Right/1.png", "Image/P18_D2_P7_Low_T3_11_Right/2.png", "Image/P18_D2_P7_Low_T3_11_Right/3.png", "Image/P18_D2_P7_Low_T3_11_Right/4.png", "Image/P18_D2_P7_Low_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that orange bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 462.5755 seconds to 473.3913 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P7_Medium_T3_26_Right", "images": ["Image/P1_D1_P7_Medium_T3_26_Right/1.png", "Image/P1_D1_P7_Medium_T3_26_Right/2.png", "Image/P1_D1_P7_Medium_T3_26_Right/3.png", "Image/P1_D1_P7_Medium_T3_26_Right/4.png", "Image/P1_D1_P7_Medium_T3_26_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 521.4066 seconds to 531.3183 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P4_High_T3_1_Left", "images": ["Image/P22_D2_P4_High_T3_1_Left/1.png", "Image/P22_D2_P4_High_T3_1_Left/2.png", "Image/P22_D2_P4_High_T3_1_Left/3.png", "Image/P22_D2_P4_High_T3_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 740.8393999999998 seconds to 750.1414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P8_Medium_T1_8_Right", "images": ["Image/P30_D1_P8_Medium_T1_8_Right/1.png", "Image/P30_D1_P8_Medium_T1_8_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Move that cup up here. Finish.\"\nThe frames roughly correspond to the speech interval from 976.6251 seconds to 984.9782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P9_Medium_T1_25_Left", "images": ["Image/P10_D2_P9_Medium_T1_25_Left/1.png", "Image/P10_D2_P9_Medium_T1_25_Left/2.png", "Image/P10_D2_P9_Medium_T1_25_Left/3.png", "Image/P10_D2_P9_Medium_T1_25_Left/4.png", "Image/P10_D2_P9_Medium_T1_25_Left/5.png", "Image/P10_D2_P9_Medium_T1_25_Left/6.png", "Image/P10_D2_P9_Medium_T1_25_Left/7.png", "Image/P10_D2_P9_Medium_T1_25_Left/8.png", "Image/P10_D2_P9_Medium_T1_25_Left/9.png", "Image/P10_D2_P9_Medium_T1_25_Left/10.png", "Image/P10_D2_P9_Medium_T1_25_Left/11.png", "Image/P10_D2_P9_Medium_T1_25_Left/12.png", "Image/P10_D2_P9_Medium_T1_25_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1741.5097800000003 seconds to 1759.36798 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P3_Low_T3_26_Left", "images": ["Image/P10_D1_P3_Low_T3_26_Left/1.png", "Image/P10_D1_P3_Low_T3_26_Left/2.png", "Image/P10_D1_P3_Low_T3_26_Left/3.png", "Image/P10_D1_P3_Low_T3_26_Left/4.png", "Image/P10_D1_P3_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2591.9653 seconds to 2602.5297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P4_High_T1_14_Mid", "images": ["Image/P26_D3_P4_High_T1_14_Mid/1.png", "Image/P26_D3_P4_High_T1_14_Mid/2.png", "Image/P26_D3_P4_High_T1_14_Mid/3.png", "Image/P26_D3_P4_High_T1_14_Mid/4.png", "Image/P26_D3_P4_High_T1_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move the plant to that shelf on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 713.8048100000001 seconds to 725.3995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P9_High_T3_23_Left", "images": ["Image/P30_D3_P9_High_T3_23_Left/1.png", "Image/P30_D3_P9_High_T3_23_Left/2.png", "Image/P30_D3_P9_High_T3_23_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1268.6031 seconds to 1276.9277 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P8_Medium_T2_7_Mid", "images": ["Image/P27_D1_P8_Medium_T2_7_Mid/1.png", "Image/P27_D1_P8_Medium_T2_7_Mid/2.png", "Image/P27_D1_P8_Medium_T2_7_Mid/3.png", "Image/P27_D1_P8_Medium_T2_7_Mid/4.png", "Image/P27_D1_P8_Medium_T2_7_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1179.888 seconds to 1190.0911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P7_Low_T1_12_Mid", "images": ["Image/P21_D2_P7_Low_T1_12_Mid/1.png", "Image/P21_D2_P7_Low_T1_12_Mid/2.png", "Image/P21_D2_P7_Low_T1_12_Mid/3.png", "Image/P21_D2_P7_Low_T1_12_Mid/4.png", "Image/P21_D2_P7_Low_T1_12_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1568.1287000000002 seconds to 1579.4738000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P2_Medium_T1_16_Left", "images": ["Image/P8_D3_P2_Medium_T1_16_Left/1.png", "Image/P8_D3_P2_Medium_T1_16_Left/2.png", "Image/P8_D3_P2_Medium_T1_16_Left/3.png", "Image/P8_D3_P2_Medium_T1_16_Left/4.png", "Image/P8_D3_P2_Medium_T1_16_Left/5.png", "Image/P8_D3_P2_Medium_T1_16_Left/6.png", "Image/P8_D3_P2_Medium_T1_16_Left/7.png", "Image/P8_D3_P2_Medium_T1_16_Left/8.png", "Image/P8_D3_P2_Medium_T1_16_Left/9.png", "Image/P8_D3_P2_Medium_T1_16_Left/10.png", "Image/P8_D3_P2_Medium_T1_16_Left/11.png", "Image/P8_D3_P2_Medium_T1_16_Left/12.png", "Image/P8_D3_P2_Medium_T1_16_Left/13.png", "Image/P8_D3_P2_Medium_T1_16_Left/14.png", "Image/P8_D3_P2_Medium_T1_16_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Hi. Could you move the leftmost brown glass under the wooden shelf? You can see the black strip there. Just move it on there. Finish.\"\nThe frames roughly correspond to the speech interval from 1659.5358 seconds to 1680.1236 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P7_Low_T2_26_Right", "images": ["Image/P6_D1_P7_Low_T2_26_Right/1.png", "Image/P6_D1_P7_Low_T2_26_Right/2.png", "Image/P6_D1_P7_Low_T2_26_Right/3.png", "Image/P6_D1_P7_Low_T2_26_Right/4.png", "Image/P6_D1_P7_Low_T2_26_Right/5.png", "Image/P6_D1_P7_Low_T2_26_Right/6.png", "Image/P6_D1_P7_Low_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2531.2293 seconds to 2546.6774 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P8_High_T2_13_Right", "images": ["Image/P21_D2_P8_High_T2_13_Right/1.png", "Image/P21_D2_P8_High_T2_13_Right/2.png", "Image/P21_D2_P8_High_T2_13_Right/3.png", "Image/P21_D2_P8_High_T2_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1813.593 seconds to 1823.7941 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P4_Medium_T1_24_Right", "images": ["Image/P21_D3_P4_Medium_T1_24_Right/1.png", "Image/P21_D3_P4_Medium_T1_24_Right/2.png", "Image/P21_D3_P4_Medium_T1_24_Right/3.png", "Image/P21_D3_P4_Medium_T1_24_Right/4.png", "Image/P21_D3_P4_Medium_T1_24_Right/5.png", "Image/P21_D3_P4_Medium_T1_24_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 677.7245 seconds to 689.9024999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P8_Low_T3_7_Left", "images": ["Image/P17_D1_P8_Low_T3_7_Left/1.png", "Image/P17_D1_P8_Low_T3_7_Left/2.png", "Image/P17_D1_P8_Low_T3_7_Left/3.png", "Image/P17_D1_P8_Low_T3_7_Left/4.png", "Image/P17_D1_P8_Low_T3_7_Left/5.png", "Image/P17_D1_P8_Low_T3_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1924.7586 seconds to 1936.4664 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P6_Low_T1_25_Left", "images": ["Image/P19_D2_P6_Low_T1_25_Left/1.png", "Image/P19_D2_P6_Low_T1_25_Left/2.png", "Image/P19_D2_P6_Low_T1_25_Left/3.png", "Image/P19_D2_P6_Low_T1_25_Left/4.png", "Image/P19_D2_P6_Low_T1_25_Left/5.png", "Image/P19_D2_P6_Low_T1_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this glass over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1707.5376 seconds to 1722.0413 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P1_Medium_T2_16_Mid", "images": ["Image/P7_D1_P1_Medium_T2_16_Mid/1.png", "Image/P7_D1_P1_Medium_T2_16_Mid/2.png", "Image/P7_D1_P1_Medium_T2_16_Mid/3.png", "Image/P7_D1_P1_Medium_T2_16_Mid/4.png", "Image/P7_D1_P1_Medium_T2_16_Mid/5.png", "Image/P7_D1_P1_Medium_T2_16_Mid/6.png", "Image/P7_D1_P1_Medium_T2_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 714.4489 seconds to 729.1882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P8_Medium_T3_16_Right", "images": ["Image/P1_D1_P8_Medium_T3_16_Right/1.png", "Image/P1_D1_P8_Medium_T3_16_Right/2.png", "Image/P1_D1_P8_Medium_T3_16_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 274.45730000000003 seconds to 284.04859999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P2_Medium_T2_7_Mid", "images": ["Image/P8_D2_P2_Medium_T2_7_Mid/1.png", "Image/P8_D2_P2_Medium_T2_7_Mid/2.png", "Image/P8_D2_P2_Medium_T2_7_Mid/3.png", "Image/P8_D2_P2_Medium_T2_7_Mid/4.png", "Image/P8_D2_P2_Medium_T2_7_Mid/5.png", "Image/P8_D2_P2_Medium_T2_7_Mid/6.png", "Image/P8_D2_P2_Medium_T2_7_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the green glass in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2277.0136 seconds to 2291.7949000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P3_Low_T1_23_Right", "images": ["Image/P27_D1_P3_Low_T1_23_Right/1.png", "Image/P27_D1_P3_Low_T1_23_Right/2.png", "Image/P27_D1_P3_Low_T1_23_Right/3.png", "Image/P27_D1_P3_Low_T1_23_Right/4.png", "Image/P27_D1_P3_Low_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant into that basket? Finish\"\nThe frames roughly correspond to the speech interval from 705.8738800000001 seconds to 716.8292100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P7_Low_T1_9_Mid", "images": ["Image/P18_D1_P7_Low_T1_9_Mid/1.png", "Image/P18_D1_P7_Low_T1_9_Mid/2.png", "Image/P18_D1_P7_Low_T1_9_Mid/3.png", "Image/P18_D1_P7_Low_T1_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this green cup to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 790.6453 seconds to 800.7513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P6_Low_T1_1_Left", "images": ["Image/P23_D2_P6_Low_T1_1_Left/1.png", "Image/P23_D2_P6_Low_T1_1_Left/2.png", "Image/P23_D2_P6_Low_T1_1_Left/3.png", "Image/P23_D2_P6_Low_T1_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 120.84685000000002 seconds to 133.47090000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P8_High_T3_14_Left", "images": ["Image/P23_D1_P8_High_T3_14_Left/1.png", "Image/P23_D1_P8_High_T3_14_Left/2.png", "Image/P23_D1_P8_High_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1735.0843 seconds to 1744.1312 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P8_Low_T1_26_Right", "images": ["Image/P13_D2_P8_Low_T1_26_Right/1.png", "Image/P13_D2_P8_Low_T1_26_Right/2.png", "Image/P13_D2_P8_Low_T1_26_Right/3.png", "Image/P13_D2_P8_Low_T1_26_Right/4.png", "Image/P13_D2_P8_Low_T1_26_Right/5.png", "Image/P13_D2_P8_Low_T1_26_Right/6.png", "Image/P13_D2_P8_Low_T1_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup from the bottom all the way up to the top shelf here\"\nThe frames roughly correspond to the speech interval from 1246.45126 seconds to 1258.77136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P9_High_T3_17_Left", "images": ["Image/P8_D3_P9_High_T3_17_Left/1.png", "Image/P8_D3_P9_High_T3_17_Left/2.png", "Image/P8_D3_P9_High_T3_17_Left/3.png", "Image/P8_D3_P9_High_T3_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean that brown glass? Finish.\"\nThe frames roughly correspond to the speech interval from 1824.1387 seconds to 1834.147 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P8_Low_T2_17_Right", "images": ["Image/P19_D1_P8_Low_T2_17_Right/1.png", "Image/P19_D1_P8_Low_T2_17_Right/2.png", "Image/P19_D1_P8_Low_T2_17_Right/3.png", "Image/P19_D1_P8_Low_T2_17_Right/4.png", "Image/P19_D1_P8_Low_T2_17_Right/5.png", "Image/P19_D1_P8_Low_T2_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this container. Finish.\"\nThe frames roughly correspond to the speech interval from 657.9487 seconds to 672.2167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P5_High_T3_1_Mid", "images": ["Image/P24_D2_P5_High_T3_1_Mid/1.png", "Image/P24_D2_P5_High_T3_1_Mid/2.png", "Image/P24_D2_P5_High_T3_1_Mid/3.png", "Image/P24_D2_P5_High_T3_1_Mid/4.png", "Image/P24_D2_P5_High_T3_1_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 725.6359 seconds to 736.1168 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P5_High_T2_14_Left", "images": ["Image/P30_D1_P5_High_T2_14_Left/1.png", "Image/P30_D1_P5_High_T2_14_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Water this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 683.7094800000001 seconds to 691.1143400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P1_Low_T1_5_Right", "images": ["Image/P19_D1_P1_Low_T1_5_Right/1.png", "Image/P19_D1_P1_Low_T1_5_Right/2.png", "Image/P19_D1_P1_Low_T1_5_Right/3.png", "Image/P19_D1_P1_Low_T1_5_Right/4.png", "Image/P19_D1_P1_Low_T1_5_Right/5.png", "Image/P19_D1_P1_Low_T1_5_Right/6.png", "Image/P19_D1_P1_Low_T1_5_Right/7.png", "Image/P19_D1_P1_Low_T1_5_Right/8.png", "Image/P19_D1_P1_Low_T1_5_Right/9.png", "Image/P19_D1_P1_Low_T1_5_Right/10.png", "Image/P19_D1_P1_Low_T1_5_Right/11.png", "Image/P19_D1_P1_Low_T1_5_Right/12.png", "Image/P19_D1_P1_Low_T1_5_Right/13.png", "Image/P19_D1_P1_Low_T1_5_Right/14.png", "Image/P19_D1_P1_Low_T1_5_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this flowerpot to this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 726.8826 seconds to 744.5029999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P2_Low_T1_6_Right", "images": ["Image/P18_D1_P2_Low_T1_6_Right/1.png", "Image/P18_D1_P2_Low_T1_6_Right/2.png", "Image/P18_D1_P2_Low_T1_6_Right/3.png", "Image/P18_D1_P2_Low_T1_6_Right/4.png", "Image/P18_D1_P2_Low_T1_6_Right/5.png", "Image/P18_D1_P2_Low_T1_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1062.4603 seconds to 1073.8322 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P7_Medium_T3_19_Mid", "images": ["Image/P2_D2_P7_Medium_T3_19_Mid/1.png", "Image/P2_D2_P7_Medium_T3_19_Mid/2.png", "Image/P2_D2_P7_Medium_T3_19_Mid/3.png", "Image/P2_D2_P7_Medium_T3_19_Mid/4.png", "Image/P2_D2_P7_Medium_T3_19_Mid/5.png", "Image/P2_D2_P7_Medium_T3_19_Mid/6.png", "Image/P2_D2_P7_Medium_T3_19_Mid/7.png", "Image/P2_D2_P7_Medium_T3_19_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you please clean the white bottle in the very middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2841.4924 seconds to 2857.3741999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P2_Medium_T1_3_Mid", "images": ["Image/P22_D3_P2_Medium_T1_3_Mid/1.png", "Image/P22_D3_P2_Medium_T1_3_Mid/2.png", "Image/P22_D3_P2_Medium_T1_3_Mid/3.png", "Image/P22_D3_P2_Medium_T1_3_Mid/4.png", "Image/P22_D3_P2_Medium_T1_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move the middle vase to the floor there. Finish\"\nThe frames roughly correspond to the speech interval from 159.312 seconds to 169.77890000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P9_Medium_T3_14_Left", "images": ["Image/P30_D2_P9_Medium_T3_14_Left/1.png", "Image/P30_D2_P9_Medium_T3_14_Left/2.png", "Image/P30_D2_P9_Medium_T3_14_Left/3.png", "Image/P30_D2_P9_Medium_T3_14_Left/4.png", "Image/P30_D2_P9_Medium_T3_14_Left/5.png", "Image/P30_D2_P9_Medium_T3_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move or clean the plant furthest away from you. Finish.\"\nThe frames roughly correspond to the speech interval from 375.9117 seconds to 388.1333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
