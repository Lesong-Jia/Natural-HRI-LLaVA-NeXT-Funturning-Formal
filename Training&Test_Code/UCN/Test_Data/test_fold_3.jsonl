{"id": "P17_D2_P5_Medium_T2_4_Right", "images": ["Image/P17_D2_P5_Medium_T2_4_Right/1.png", "Image/P17_D2_P5_Medium_T2_4_Right/2.png", "Image/P17_D2_P5_Medium_T2_4_Right/3.png", "Image/P17_D2_P5_Medium_T2_4_Right/4.png", "Image/P17_D2_P5_Medium_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 630.3877 seconds to 644.0145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_Low_T1_14_Left", "images": ["Image/P8_D3_P3_Low_T1_14_Left/1.png", "Image/P8_D3_P3_Low_T1_14_Left/2.png", "Image/P8_D3_P3_Low_T1_14_Left/3.png", "Image/P8_D3_P3_Low_T1_14_Left/4.png", "Image/P8_D3_P3_Low_T1_14_Left/5.png", "Image/P8_D3_P3_Low_T1_14_Left/6.png", "Image/P8_D3_P3_Low_T1_14_Left/7.png", "Image/P8_D3_P3_Low_T1_14_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Could you move that planet just at the bottom of the shelf\"\nThe frames roughly correspond to the speech interval from 1783.0472 seconds to 1796.5472 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P3_High_T2_9_Left", "images": ["Image/P29_D1_P3_High_T2_9_Left/1.png", "Image/P29_D1_P3_High_T2_9_Left/2.png", "Image/P29_D1_P3_High_T2_9_Left/3.png", "Image/P29_D1_P3_High_T2_9_Left/4.png", "Image/P29_D1_P3_High_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2481.8284299999996 seconds to 2492.4763999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P9_Medium_T2_23_Right", "images": ["Image/P15_D2_P9_Medium_T2_23_Right/1.png", "Image/P15_D2_P9_Medium_T2_23_Right/2.png", "Image/P15_D2_P9_Medium_T2_23_Right/3.png", "Image/P15_D2_P9_Medium_T2_23_Right/4.png", "Image/P15_D2_P9_Medium_T2_23_Right/5.png", "Image/P15_D2_P9_Medium_T2_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2742.6761 seconds to 2755.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P3_Medium_T3_26_Right", "images": ["Image/P7_D1_P3_Medium_T3_26_Right/1.png", "Image/P7_D1_P3_Medium_T3_26_Right/2.png", "Image/P7_D1_P3_Medium_T3_26_Right/3.png", "Image/P7_D1_P3_Medium_T3_26_Right/4.png", "Image/P7_D1_P3_Medium_T3_26_Right/5.png", "Image/P7_D1_P3_Medium_T3_26_Right/6.png", "Image/P7_D1_P3_Medium_T3_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 186.0076 seconds to 200.3719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P3_Low_T2_2_Left", "images": ["Image/P20_D3_P3_Low_T2_2_Left/1.png", "Image/P20_D3_P3_Low_T2_2_Left/2.png", "Image/P20_D3_P3_Low_T2_2_Left/3.png", "Image/P20_D3_P3_Low_T2_2_Left/4.png", "Image/P20_D3_P3_Low_T2_2_Left/5.png", "Image/P20_D3_P3_Low_T2_2_Left/6.png", "Image/P20_D3_P3_Low_T2_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that blue-green bottle over there. Yeah.\"\nThe frames roughly correspond to the speech interval from 1280.0793 seconds to 1295.1122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P6_Low_T1_2_Mid", "images": ["Image/P2_D2_P6_Low_T1_2_Mid/1.png", "Image/P2_D2_P6_Low_T1_2_Mid/2.png", "Image/P2_D2_P6_Low_T1_2_Mid/3.png", "Image/P2_D2_P6_Low_T1_2_Mid/4.png", "Image/P2_D2_P6_Low_T1_2_Mid/5.png", "Image/P2_D2_P6_Low_T1_2_Mid/6.png", "Image/P2_D2_P6_Low_T1_2_Mid/7.png", "Image/P2_D2_P6_Low_T1_2_Mid/8.png", "Image/P2_D2_P6_Low_T1_2_Mid/9.png", "Image/P2_D2_P6_Low_T1_2_Mid/10.png", "Image/P2_D2_P6_Low_T1_2_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that green bottle? To that shelf. Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2445.10858 seconds to 2461.10229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P7_High_T2_8_Left", "images": ["Image/P19_D1_P7_High_T2_8_Left/1.png", "Image/P19_D1_P7_High_T2_8_Left/2.png", "Image/P19_D1_P7_High_T2_8_Left/3.png", "Image/P19_D1_P7_High_T2_8_Left/4.png", "Image/P19_D1_P7_High_T2_8_Left/5.png", "Image/P19_D1_P7_High_T2_8_Left/6.png", "Image/P19_D1_P7_High_T2_8_Left/7.png", "Image/P19_D1_P7_High_T2_8_Left/8.png", "Image/P19_D1_P7_High_T2_8_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour from this container and do that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 689.5612 seconds to 709.8978999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P7_High_T3_25_Mid", "images": ["Image/P18_D1_P7_High_T3_25_Mid/1.png", "Image/P18_D1_P7_High_T3_25_Mid/2.png", "Image/P18_D1_P7_High_T3_25_Mid/3.png", "Image/P18_D1_P7_High_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the cook. Finish.\"\nThe frames roughly correspond to the speech interval from 658.57348 seconds to 668.76206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P7_Medium_T3_20_Left", "images": ["Image/P22_D3_P7_Medium_T3_20_Left/1.png", "Image/P22_D3_P7_Medium_T3_20_Left/2.png", "Image/P22_D3_P7_Medium_T3_20_Left/3.png", "Image/P22_D3_P7_Medium_T3_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 248.0374 seconds to 258.6874 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P3_Medium_T2_12_Mid", "images": ["Image/P30_D1_P3_Medium_T2_12_Mid/1.png", "Image/P30_D1_P3_Medium_T2_12_Mid/2.png", "Image/P30_D1_P3_Medium_T2_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 764.2927000000001 seconds to 773.0332000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P3_Low_T3_12_Right", "images": ["Image/P11_D1_P3_Low_T3_12_Right/1.png", "Image/P11_D1_P3_Low_T3_12_Right/2.png", "Image/P11_D1_P3_Low_T3_12_Right/3.png", "Image/P11_D1_P3_Low_T3_12_Right/4.png", "Image/P11_D1_P3_Low_T3_12_Right/5.png", "Image/P11_D1_P3_Low_T3_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the red bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2890.8374999999996 seconds to 2902.5703999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P3_Medium_T1_4_Right", "images": ["Image/P8_D1_P3_Medium_T1_4_Right/1.png", "Image/P8_D1_P3_Medium_T1_4_Right/2.png", "Image/P8_D1_P3_Medium_T1_4_Right/3.png", "Image/P8_D1_P3_Medium_T1_4_Right/4.png", "Image/P8_D1_P3_Medium_T1_4_Right/5.png", "Image/P8_D1_P3_Medium_T1_4_Right/6.png", "Image/P8_D1_P3_Medium_T1_4_Right/7.png", "Image/P8_D1_P3_Medium_T1_4_Right/8.png", "Image/P8_D1_P3_Medium_T1_4_Right/9.png", "Image/P8_D1_P3_Medium_T1_4_Right/10.png", "Image/P8_D1_P3_Medium_T1_4_Right/11.png", "Image/P8_D1_P3_Medium_T1_4_Right/12.png", "Image/P8_D1_P3_Medium_T1_4_Right/13.png", "Image/P8_D1_P3_Medium_T1_4_Right/14.png", "Image/P8_D1_P3_Medium_T1_4_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"um star hi could you move the the rightmost planet to the uh wooden basket below that is on the floor just place place it there thank you finished\"\nThe frames roughly correspond to the speech interval from 231.7862999999999 seconds to 256.1280999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P9_High_T3_19_Right", "images": ["Image/P7_D3_P9_High_T3_19_Right/1.png", "Image/P7_D3_P9_High_T3_19_Right/2.png", "Image/P7_D3_P9_High_T3_19_Right/3.png", "Image/P7_D3_P9_High_T3_19_Right/4.png", "Image/P7_D3_P9_High_T3_19_Right/5.png", "Image/P7_D3_P9_High_T3_19_Right/6.png", "Image/P7_D3_P9_High_T3_19_Right/7.png", "Image/P7_D3_P9_High_T3_19_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2207.9766 seconds to 2223.7887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P5_Low_T2_1_Right", "images": ["Image/P24_D2_P5_Low_T2_1_Right/1.png", "Image/P24_D2_P5_Low_T2_1_Right/2.png", "Image/P24_D2_P5_Low_T2_1_Right/3.png", "Image/P24_D2_P5_Low_T2_1_Right/4.png", "Image/P24_D2_P5_Low_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you pour water in this bottle\"\nThe frames roughly correspond to the speech interval from 554.5321 seconds to 565.2517 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P7_High_T3_18_Right", "images": ["Image/P16_D1_P7_High_T3_18_Right/1.png", "Image/P16_D1_P7_High_T3_18_Right/2.png", "Image/P16_D1_P7_High_T3_18_Right/3.png", "Image/P16_D1_P7_High_T3_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1173.48933 seconds to 1185.40486 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P3_Low_T2_26_Right", "images": ["Image/P8_D1_P3_Low_T2_26_Right/1.png", "Image/P8_D1_P3_Low_T2_26_Right/2.png", "Image/P8_D1_P3_Low_T2_26_Right/3.png", "Image/P8_D1_P3_Low_T2_26_Right/4.png", "Image/P8_D1_P3_Low_T2_26_Right/5.png", "Image/P8_D1_P3_Low_T2_26_Right/6.png", "Image/P8_D1_P3_Low_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. There's a white cap here. Could you pour some water to it? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 208.5261999999999 seconds to 221.74419999999992 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P5_Medium_T1_19_Right", "images": ["Image/P13_D3_P5_Medium_T1_19_Right/1.png", "Image/P13_D3_P5_Medium_T1_19_Right/2.png", "Image/P13_D3_P5_Medium_T1_19_Right/3.png", "Image/P13_D3_P5_Medium_T1_19_Right/4.png", "Image/P13_D3_P5_Medium_T1_19_Right/5.png", "Image/P13_D3_P5_Medium_T1_19_Right/6.png", "Image/P13_D3_P5_Medium_T1_19_Right/7.png", "Image/P13_D3_P5_Medium_T1_19_Right/8.png", "Image/P13_D3_P5_Medium_T1_19_Right/9.png", "Image/P13_D3_P5_Medium_T1_19_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move the right white bottle from there over to that shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1465.8264 seconds to 1481.054 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P2_Low_T2_17_Mid", "images": ["Image/P29_D2_P2_Low_T2_17_Mid/1.png", "Image/P29_D2_P2_Low_T2_17_Mid/2.png", "Image/P29_D2_P2_Low_T2_17_Mid/3.png", "Image/P29_D2_P2_Low_T2_17_Mid/4.png", "Image/P29_D2_P2_Low_T2_17_Mid/5.png", "Image/P29_D2_P2_Low_T2_17_Mid/6.png", "Image/P29_D2_P2_Low_T2_17_Mid/7.png", "Image/P29_D2_P2_Low_T2_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 274.47929999999997 seconds to 288.0369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P8_Medium_T1_1_Left", "images": ["Image/P28_D2_P8_Medium_T1_1_Left/1.png", "Image/P28_D2_P8_Medium_T1_1_Left/2.png", "Image/P28_D2_P8_Medium_T1_1_Left/3.png", "Image/P28_D2_P8_Medium_T1_1_Left/4.png", "Image/P28_D2_P8_Medium_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that green vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1112.872 seconds to 1123.0748 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P2_High_T3_7_Left", "images": ["Image/P15_D3_P2_High_T3_7_Left/1.png", "Image/P15_D3_P2_High_T3_7_Left/2.png", "Image/P15_D3_P2_High_T3_7_Left/3.png", "Image/P15_D3_P2_High_T3_7_Left/4.png", "Image/P15_D3_P2_High_T3_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 487.9137 seconds to 502.2641 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P5_Low_T3_5_Left", "images": ["Image/P1_D3_P5_Low_T3_5_Left/1.png", "Image/P1_D3_P5_Low_T3_5_Left/2.png", "Image/P1_D3_P5_Low_T3_5_Left/3.png", "Image/P1_D3_P5_Low_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1554.583 seconds to 1565.4144 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P2_High_T3_10_Right", "images": ["Image/P18_D1_P2_High_T3_10_Right/1.png", "Image/P18_D1_P2_High_T3_10_Right/2.png", "Image/P18_D1_P2_High_T3_10_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1031.4458 seconds to 1039.492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P9_High_T2_9_Mid", "images": ["Image/P10_D2_P9_High_T2_9_Mid/1.png", "Image/P10_D2_P9_High_T2_9_Mid/2.png", "Image/P10_D2_P9_High_T2_9_Mid/3.png", "Image/P10_D2_P9_High_T2_9_Mid/4.png", "Image/P10_D2_P9_High_T2_9_Mid/5.png", "Image/P10_D2_P9_High_T2_9_Mid/6.png", "Image/P10_D2_P9_High_T2_9_Mid/7.png", "Image/P10_D2_P9_High_T2_9_Mid/8.png", "Image/P10_D2_P9_High_T2_9_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1622.38238 seconds to 1637.80678 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P2_Medium_T2_14_Mid", "images": ["Image/P15_D2_P2_Medium_T2_14_Mid/1.png", "Image/P15_D2_P2_Medium_T2_14_Mid/2.png", "Image/P15_D2_P2_Medium_T2_14_Mid/3.png", "Image/P15_D2_P2_Medium_T2_14_Mid/4.png", "Image/P15_D2_P2_Medium_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2878.8255 seconds to 2889.1967999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P2_Low_T3_9_Left", "images": ["Image/P14_D3_P2_Low_T3_9_Left/1.png", "Image/P14_D3_P2_Low_T3_9_Left/2.png", "Image/P14_D3_P2_Low_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1210.24356 seconds to 1218.8257600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P7_High_T2_8_Left", "images": ["Image/P21_D3_P7_High_T2_8_Left/1.png", "Image/P21_D3_P7_High_T2_8_Left/2.png", "Image/P21_D3_P7_High_T2_8_Left/3.png", "Image/P21_D3_P7_High_T2_8_Left/4.png", "Image/P21_D3_P7_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill the green mug with water. Finish.\"\nThe frames roughly correspond to the speech interval from 496.5118 seconds to 520.3546 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P9_Low_T1_17_Mid", "images": ["Image/P21_D1_P9_Low_T1_17_Mid/1.png", "Image/P21_D1_P9_Low_T1_17_Mid/2.png", "Image/P21_D1_P9_Low_T1_17_Mid/3.png", "Image/P21_D1_P9_Low_T1_17_Mid/4.png", "Image/P21_D1_P9_Low_T1_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 914.1925 seconds to 926.331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P9_Low_T3_24_Mid", "images": ["Image/P1_D2_P9_Low_T3_24_Mid/1.png", "Image/P1_D2_P9_Low_T3_24_Mid/2.png", "Image/P1_D2_P9_Low_T3_24_Mid/3.png", "Image/P1_D2_P9_Low_T3_24_Mid/4.png", "Image/P1_D2_P9_Low_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 1159.6738 seconds to 1170.1288 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P8_Medium_T3_8_Right", "images": ["Image/P8_D2_P8_Medium_T3_8_Right/1.png", "Image/P8_D2_P8_Medium_T3_8_Right/2.png", "Image/P8_D2_P8_Medium_T3_8_Right/3.png", "Image/P8_D2_P8_Medium_T3_8_Right/4.png", "Image/P8_D2_P8_Medium_T3_8_Right/5.png", "Image/P8_D2_P8_Medium_T3_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the right green cups. Finish.\"\nThe frames roughly correspond to the speech interval from 2512.8133000000003 seconds to 2525.7994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P9_Low_T3_6_Left", "images": ["Image/P27_D1_P9_Low_T3_6_Left/1.png", "Image/P27_D1_P9_Low_T3_6_Left/2.png", "Image/P27_D1_P9_Low_T3_6_Left/3.png", "Image/P27_D1_P9_Low_T3_6_Left/4.png", "Image/P27_D1_P9_Low_T3_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 938.9351 seconds to 949.6818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P7_Low_T2_12_Mid", "images": ["Image/P7_D2_P7_Low_T2_12_Mid/1.png", "Image/P7_D2_P7_Low_T2_12_Mid/2.png", "Image/P7_D2_P7_Low_T2_12_Mid/3.png", "Image/P7_D2_P7_Low_T2_12_Mid/4.png", "Image/P7_D2_P7_Low_T2_12_Mid/5.png", "Image/P7_D2_P7_Low_T2_12_Mid/6.png", "Image/P7_D2_P7_Low_T2_12_Mid/7.png", "Image/P7_D2_P7_Low_T2_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. If ready\"\nThe frames roughly correspond to the speech interval from 1207.3196000000005 seconds to 1222.7981000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P3_Medium_T2_21_Mid", "images": ["Image/P13_D1_P3_Medium_T2_21_Mid/1.png", "Image/P13_D1_P3_Medium_T2_21_Mid/2.png", "Image/P13_D1_P3_Medium_T2_21_Mid/3.png", "Image/P13_D1_P3_Medium_T2_21_Mid/4.png", "Image/P13_D1_P3_Medium_T2_21_Mid/5.png", "Image/P13_D1_P3_Medium_T2_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the middle white vase\"\nThe frames roughly correspond to the speech interval from 615.1138000000001 seconds to 627.1122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P1_Medium_T1_25_Mid", "images": ["Image/P8_D1_P1_Medium_T1_25_Mid/1.png", "Image/P8_D1_P1_Medium_T1_25_Mid/2.png", "Image/P8_D1_P1_Medium_T1_25_Mid/3.png", "Image/P8_D1_P1_Medium_T1_25_Mid/4.png", "Image/P8_D1_P1_Medium_T1_25_Mid/5.png", "Image/P8_D1_P1_Medium_T1_25_Mid/6.png", "Image/P8_D1_P1_Medium_T1_25_Mid/7.png", "Image/P8_D1_P1_Medium_T1_25_Mid/8.png", "Image/P8_D1_P1_Medium_T1_25_Mid/9.png", "Image/P8_D1_P1_Medium_T1_25_Mid/10.png", "Image/P8_D1_P1_Medium_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi\"\nThe frames roughly correspond to the speech interval from 321.7143999999999 seconds to 340.2104999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P4_Low_T1_4_Mid", "images": ["Image/P12_D2_P4_Low_T1_4_Mid/1.png", "Image/P12_D2_P4_Low_T1_4_Mid/2.png", "Image/P12_D2_P4_Low_T1_4_Mid/3.png", "Image/P12_D2_P4_Low_T1_4_Mid/4.png", "Image/P12_D2_P4_Low_T1_4_Mid/5.png", "Image/P12_D2_P4_Low_T1_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this plant over to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 428.31590000000006 seconds to 441.2621 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P9_Medium_T2_26_Mid", "images": ["Image/P20_D3_P9_Medium_T2_26_Mid/1.png", "Image/P20_D3_P9_Medium_T2_26_Mid/2.png", "Image/P20_D3_P9_Medium_T2_26_Mid/3.png", "Image/P20_D3_P9_Medium_T2_26_Mid/4.png", "Image/P20_D3_P9_Medium_T2_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you add some water into the middle cup over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1670.2545 seconds to 1682.406 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P6_High_T3_4_Right", "images": ["Image/P5_D3_P6_High_T3_4_Right/1.png", "Image/P5_D3_P6_High_T3_4_Right/2.png", "Image/P5_D3_P6_High_T3_4_Right/3.png", "Image/P5_D3_P6_High_T3_4_Right/4.png", "Image/P5_D3_P6_High_T3_4_Right/5.png", "Image/P5_D3_P6_High_T3_4_Right/6.png", "Image/P5_D3_P6_High_T3_4_Right/7.png", "Image/P5_D3_P6_High_T3_4_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plants on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2696.8714 seconds to 2710.4012 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P8_High_T1_26_Left", "images": ["Image/P13_D3_P8_High_T1_26_Left/1.png", "Image/P13_D3_P8_High_T1_26_Left/2.png", "Image/P13_D3_P8_High_T1_26_Left/3.png", "Image/P13_D3_P8_High_T1_26_Left/4.png", "Image/P13_D3_P8_High_T1_26_Left/5.png", "Image/P13_D3_P8_High_T1_26_Left/6.png", "Image/P13_D3_P8_High_T1_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup from the bottom to the top shelf\"\nThe frames roughly correspond to the speech interval from 1384.35296 seconds to 1396.64369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P4_Low_T2_18_Right", "images": ["Image/P1_D1_P4_Low_T2_18_Right/1.png", "Image/P1_D1_P4_Low_T2_18_Right/2.png", "Image/P1_D1_P4_Low_T2_18_Right/3.png", "Image/P1_D1_P4_Low_T2_18_Right/4.png", "Image/P1_D1_P4_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 379.609 seconds to 389.5372 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P5_Medium_T2_21_Left", "images": ["Image/P10_D1_P5_Medium_T2_21_Left/1.png", "Image/P10_D1_P5_Medium_T2_21_Left/2.png", "Image/P10_D1_P5_Medium_T2_21_Left/3.png", "Image/P10_D1_P5_Medium_T2_21_Left/4.png", "Image/P10_D1_P5_Medium_T2_21_Left/5.png", "Image/P10_D1_P5_Medium_T2_21_Left/6.png", "Image/P10_D1_P5_Medium_T2_21_Left/7.png", "Image/P10_D1_P5_Medium_T2_21_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2293.3403 seconds to 2307.3719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P2_Medium_T2_12_Left", "images": ["Image/P9_D2_P2_Medium_T2_12_Left/1.png", "Image/P9_D2_P2_Medium_T2_12_Left/2.png", "Image/P9_D2_P2_Medium_T2_12_Left/3.png", "Image/P9_D2_P2_Medium_T2_12_Left/4.png", "Image/P9_D2_P2_Medium_T2_12_Left/5.png", "Image/P9_D2_P2_Medium_T2_12_Left/6.png", "Image/P9_D2_P2_Medium_T2_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2728.87207 seconds to 2742.00753 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P2_Low_T1_19_Mid", "images": ["Image/P3_D2_P2_Low_T1_19_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you give some water to this bottle? Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2232.99407 seconds to 2243.48965 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P8_High_T2_23_Left", "images": ["Image/P7_D1_P8_High_T2_23_Left/1.png", "Image/P7_D1_P8_High_T2_23_Left/2.png", "Image/P7_D1_P8_High_T2_23_Left/3.png", "Image/P7_D1_P8_High_T2_23_Left/4.png", "Image/P7_D1_P8_High_T2_23_Left/5.png", "Image/P7_D1_P8_High_T2_23_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 619.2086 seconds to 634.1438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P4_Medium_T3_22_Mid", "images": ["Image/P10_D2_P4_Medium_T3_22_Mid/1.png", "Image/P10_D2_P4_Medium_T3_22_Mid/2.png", "Image/P10_D2_P4_Medium_T3_22_Mid/3.png", "Image/P10_D2_P4_Medium_T3_22_Mid/4.png", "Image/P10_D2_P4_Medium_T3_22_Mid/5.png", "Image/P10_D2_P4_Medium_T3_22_Mid/6.png", "Image/P10_D2_P4_Medium_T3_22_Mid/7.png", "Image/P10_D2_P4_Medium_T3_22_Mid/8.png", "Image/P10_D2_P4_Medium_T3_22_Mid/9.png", "Image/P10_D2_P4_Medium_T3_22_Mid/10.png", "Image/P10_D2_P4_Medium_T3_22_Mid/11.png", "Image/P10_D2_P4_Medium_T3_22_Mid/12.png", "Image/P10_D2_P4_Medium_T3_22_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1480.3579800000002 seconds to 1499.27508 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P5_Medium_T2_7_Mid", "images": ["Image/P11_D3_P5_Medium_T2_7_Mid/1.png", "Image/P11_D3_P5_Medium_T2_7_Mid/2.png", "Image/P11_D3_P5_Medium_T2_7_Mid/3.png", "Image/P11_D3_P5_Medium_T2_7_Mid/4.png", "Image/P11_D3_P5_Medium_T2_7_Mid/5.png", "Image/P11_D3_P5_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour the water in the second green glass on the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 1473.2584 seconds to 1485.326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P1_Medium_T2_1_Left", "images": ["Image/P19_D1_P1_Medium_T2_1_Left/1.png", "Image/P19_D1_P1_Medium_T2_1_Left/2.png", "Image/P19_D1_P1_Medium_T2_1_Left/3.png", "Image/P19_D1_P1_Medium_T2_1_Left/4.png", "Image/P19_D1_P1_Medium_T2_1_Left/5.png", "Image/P19_D1_P1_Medium_T2_1_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour from this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1154.8176 seconds to 1166.8723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P7_Low_T3_11_Left", "images": ["Image/P12_D1_P7_Low_T3_11_Left/1.png", "Image/P12_D1_P7_Low_T3_11_Left/2.png", "Image/P12_D1_P7_Low_T3_11_Left/3.png", "Image/P12_D1_P7_Low_T3_11_Left/4.png", "Image/P12_D1_P7_Low_T3_11_Left/5.png", "Image/P12_D1_P7_Low_T3_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown base that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 1237.47311 seconds to 1252.89449 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P8_Low_T1_1_Right", "images": ["Image/P27_D1_P8_Low_T1_1_Right/1.png", "Image/P27_D1_P8_Low_T1_1_Right/2.png", "Image/P27_D1_P8_Low_T1_1_Right/3.png", "Image/P27_D1_P8_Low_T1_1_Right/4.png", "Image/P27_D1_P8_Low_T1_1_Right/5.png", "Image/P27_D1_P8_Low_T1_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take that glass and put it in the center of this cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 995.325 seconds to 1008.5754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P1_Low_T3_24_Left", "images": ["Image/P7_D3_P1_Low_T3_24_Left/1.png", "Image/P7_D3_P1_Low_T3_24_Left/2.png", "Image/P7_D3_P1_Low_T3_24_Left/3.png", "Image/P7_D3_P1_Low_T3_24_Left/4.png", "Image/P7_D3_P1_Low_T3_24_Left/5.png", "Image/P7_D3_P1_Low_T3_24_Left/6.png", "Image/P7_D3_P1_Low_T3_24_Left/7.png", "Image/P7_D3_P1_Low_T3_24_Left/8.png", "Image/P7_D3_P1_Low_T3_24_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2054.5086 seconds to 2070.1409000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P3_Low_T1_23_Left", "images": ["Image/P11_D2_P3_Low_T1_23_Left/1.png", "Image/P11_D2_P3_Low_T1_23_Left/2.png", "Image/P11_D2_P3_Low_T1_23_Left/3.png", "Image/P11_D2_P3_Low_T1_23_Left/4.png", "Image/P11_D2_P3_Low_T1_23_Left/5.png", "Image/P11_D2_P3_Low_T1_23_Left/6.png", "Image/P11_D2_P3_Low_T1_23_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please move the plant to the bottom of the shelf on the right side\"\nThe frames roughly correspond to the speech interval from 488.9466 seconds to 503.35899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P7_Medium_T2_12_Right", "images": ["Image/P2_D1_P7_Medium_T2_12_Right/1.png", "Image/P2_D1_P7_Medium_T2_12_Right/2.png", "Image/P2_D1_P7_Medium_T2_12_Right/3.png", "Image/P2_D1_P7_Medium_T2_12_Right/4.png", "Image/P2_D1_P7_Medium_T2_12_Right/5.png", "Image/P2_D1_P7_Medium_T2_12_Right/6.png", "Image/P2_D1_P7_Medium_T2_12_Right/7.png", "Image/P2_D1_P7_Medium_T2_12_Right/8.png", "Image/P2_D1_P7_Medium_T2_12_Right/9.png", "Image/P2_D1_P7_Medium_T2_12_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello. Could you please clean the brown vessel on the very right\"\nThe frames roughly correspond to the speech interval from 409.18549999999993 seconds to 426.9453999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P9_High_T2_16_Left", "images": ["Image/P30_D1_P9_High_T2_16_Left/1.png", "Image/P30_D1_P9_High_T2_16_Left/2.png", "Image/P30_D1_P9_High_T2_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1044.7231000000002 seconds to 1053.0463 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P9_Medium_T2_5_Right", "images": ["Image/P24_D2_P9_Medium_T2_5_Right/1.png", "Image/P24_D2_P9_Medium_T2_5_Right/2.png", "Image/P24_D2_P9_Medium_T2_5_Right/3.png", "Image/P24_D2_P9_Medium_T2_5_Right/4.png", "Image/P24_D2_P9_Medium_T2_5_Right/5.png", "Image/P24_D2_P9_Medium_T2_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 640.8649 seconds to 651.8056 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P6_Low_T2_8_Left", "images": ["Image/P16_D1_P6_Low_T2_8_Left/1.png", "Image/P16_D1_P6_Low_T2_8_Left/2.png", "Image/P16_D1_P6_Low_T2_8_Left/3.png", "Image/P16_D1_P6_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into this mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1235.25206 seconds to 1245.32938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P7_Low_T1_9_Mid", "images": ["Image/P25_D2_P7_Low_T1_9_Mid/1.png", "Image/P25_D2_P7_Low_T1_9_Mid/2.png", "Image/P25_D2_P7_Low_T1_9_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 692.94548 seconds to 701.5064 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P5_Low_T1_9_Mid", "images": ["Image/P19_D1_P5_Low_T1_9_Mid/1.png", "Image/P19_D1_P5_Low_T1_9_Mid/2.png", "Image/P19_D1_P5_Low_T1_9_Mid/3.png", "Image/P19_D1_P5_Low_T1_9_Mid/4.png", "Image/P19_D1_P5_Low_T1_9_Mid/5.png", "Image/P19_D1_P5_Low_T1_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 830.2922 seconds to 847.3984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P5_High_T2_15_Right", "images": ["Image/P6_D3_P5_High_T2_15_Right/1.png", "Image/P6_D3_P5_High_T2_15_Right/2.png", "Image/P6_D3_P5_High_T2_15_Right/3.png", "Image/P6_D3_P5_High_T2_15_Right/4.png", "Image/P6_D3_P5_High_T2_15_Right/5.png", "Image/P6_D3_P5_High_T2_15_Right/6.png", "Image/P6_D3_P5_High_T2_15_Right/7.png", "Image/P6_D3_P5_High_T2_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4685.2288 seconds to 4703.8004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P9_Medium_T2_23_Left", "images": ["Image/P18_D3_P9_Medium_T2_23_Left/1.png", "Image/P18_D3_P9_Medium_T2_23_Left/2.png", "Image/P18_D3_P9_Medium_T2_23_Left/3.png", "Image/P18_D3_P9_Medium_T2_23_Left/4.png", "Image/P18_D3_P9_Medium_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1582.8332 seconds to 1593.5106 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P9_Low_T3_16_Left", "images": ["Image/P21_D2_P9_Low_T3_16_Left/1.png", "Image/P21_D2_P9_Low_T3_16_Left/2.png", "Image/P21_D2_P9_Low_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1478.7922 seconds to 1488.938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P1_Low_T2_5_Mid", "images": ["Image/P24_D1_P1_Low_T2_5_Mid/1.png", "Image/P24_D1_P1_Low_T2_5_Mid/2.png", "Image/P24_D1_P1_Low_T2_5_Mid/3.png", "Image/P24_D1_P1_Low_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Can you please pour water in this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1461.3118 seconds to 1471.6812 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P3_Medium_T1_8_Mid", "images": ["Image/P2_D1_P3_Medium_T1_8_Mid/1.png", "Image/P2_D1_P3_Medium_T1_8_Mid/2.png", "Image/P2_D1_P3_Medium_T1_8_Mid/3.png", "Image/P2_D1_P3_Medium_T1_8_Mid/4.png", "Image/P2_D1_P3_Medium_T1_8_Mid/5.png", "Image/P2_D1_P3_Medium_T1_8_Mid/6.png", "Image/P2_D1_P3_Medium_T1_8_Mid/7.png", "Image/P2_D1_P3_Medium_T1_8_Mid/8.png", "Image/P2_D1_P3_Medium_T1_8_Mid/9.png", "Image/P2_D1_P3_Medium_T1_8_Mid/10.png", "Image/P2_D1_P3_Medium_T1_8_Mid/11.png", "Image/P2_D1_P3_Medium_T1_8_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"start. Start. Hello\"\nThe frames roughly correspond to the speech interval from 699.6238999999999 seconds to 713.4861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P8_Medium_T1_10_Mid", "images": ["Image/P27_D3_P8_Medium_T1_10_Mid/1.png", "Image/P27_D3_P8_Medium_T1_10_Mid/2.png", "Image/P27_D3_P8_Medium_T1_10_Mid/3.png", "Image/P27_D3_P8_Medium_T1_10_Mid/4.png", "Image/P27_D3_P8_Medium_T1_10_Mid/5.png", "Image/P27_D3_P8_Medium_T1_10_Mid/6.png", "Image/P27_D3_P8_Medium_T1_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you get that middle red cup and put it on top of the cabinet? Finished.\"\nThe frames roughly correspond to the speech interval from 122.32064000000004 seconds to 135.12613000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P2_Low_T2_18_Right", "images": ["Image/P8_D3_P2_Low_T2_18_Right/1.png", "Image/P8_D3_P2_Low_T2_18_Right/2.png", "Image/P8_D3_P2_Low_T2_18_Right/3.png", "Image/P8_D3_P2_Low_T2_18_Right/4.png", "Image/P8_D3_P2_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1523.32849 seconds to 1534.70158 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P5_High_T3_19_Mid", "images": ["Image/P22_D1_P5_High_T3_19_Mid/1.png", "Image/P22_D1_P5_High_T3_19_Mid/2.png", "Image/P22_D1_P5_High_T3_19_Mid/3.png", "Image/P22_D1_P5_High_T3_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1295.52529 seconds to 1305.04589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P5_High_T1_17_Left", "images": ["Image/P11_D1_P5_High_T1_17_Left/1.png", "Image/P11_D1_P5_High_T1_17_Left/2.png", "Image/P11_D1_P5_High_T1_17_Left/3.png", "Image/P11_D1_P5_High_T1_17_Left/4.png", "Image/P11_D1_P5_High_T1_17_Left/5.png", "Image/P11_D1_P5_High_T1_17_Left/6.png", "Image/P11_D1_P5_High_T1_17_Left/7.png", "Image/P11_D1_P5_High_T1_17_Left/8.png", "Image/P11_D1_P5_High_T1_17_Left/9.png", "Image/P11_D1_P5_High_T1_17_Left/10.png", "Image/P11_D1_P5_High_T1_17_Left/11.png", "Image/P11_D1_P5_High_T1_17_Left/12.png", "Image/P11_D1_P5_High_T1_17_Left/13.png", "Image/P11_D1_P5_High_T1_17_Left/14.png", "Image/P11_D1_P5_High_T1_17_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please move the breath Bottle to the right side of the shelf\"\nThe frames roughly correspond to the speech interval from 2586.9228999999996 seconds to 2610.4185 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P7_Low_T3_10_Left", "images": ["Image/P8_D2_P7_Low_T3_10_Left/1.png", "Image/P8_D2_P7_Low_T3_10_Left/2.png", "Image/P8_D2_P7_Low_T3_10_Left/3.png", "Image/P8_D2_P7_Low_T3_10_Left/4.png", "Image/P8_D2_P7_Low_T3_10_Left/5.png", "Image/P8_D2_P7_Low_T3_10_Left/6.png", "Image/P8_D2_P7_Low_T3_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Could you clean that brown\"\nThe frames roughly correspond to the speech interval from 2257.4119 seconds to 2270.5677 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P2_High_T2_23_Left", "images": ["Image/P22_D2_P2_High_T2_23_Left/1.png", "Image/P22_D2_P2_High_T2_23_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"What does that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 825.0726 seconds to 837.6006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P4_Medium_T3_25_Mid", "images": ["Image/P4_D3_P4_Medium_T3_25_Mid/1.png", "Image/P4_D3_P4_Medium_T3_25_Mid/2.png", "Image/P4_D3_P4_Medium_T3_25_Mid/3.png", "Image/P4_D3_P4_Medium_T3_25_Mid/4.png", "Image/P4_D3_P4_Medium_T3_25_Mid/5.png", "Image/P4_D3_P4_Medium_T3_25_Mid/6.png", "Image/P4_D3_P4_Medium_T3_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the photo in the middle on the desk? Finished.\"\nThe frames roughly correspond to the speech interval from 271.4942299999999 seconds to 286.6802599999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P1_High_T1_14_Mid", "images": ["Image/P1_D3_P1_High_T1_14_Mid/1.png", "Image/P1_D3_P1_High_T1_14_Mid/2.png", "Image/P1_D3_P1_High_T1_14_Mid/3.png", "Image/P1_D3_P1_High_T1_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1643.3775999999998 seconds to 1653.0526999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P8_High_T3_17_Mid", "images": ["Image/P26_D1_P8_High_T3_17_Mid/1.png", "Image/P26_D1_P8_High_T3_17_Mid/2.png", "Image/P26_D1_P8_High_T3_17_Mid/3.png", "Image/P26_D1_P8_High_T3_17_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this face in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 278.841 seconds to 295.0004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P1_Low_T2_14_Left", "images": ["Image/P16_D3_P1_Low_T2_14_Left/1.png", "Image/P16_D3_P1_Low_T2_14_Left/2.png", "Image/P16_D3_P1_Low_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 516.3806999999999 seconds to 524.5783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P2_Medium_T3_18_Left", "images": ["Image/P20_D2_P2_Medium_T3_18_Left/1.png", "Image/P20_D2_P2_Medium_T3_18_Left/2.png", "Image/P20_D2_P2_Medium_T3_18_Left/3.png", "Image/P20_D2_P2_Medium_T3_18_Left/4.png", "Image/P20_D2_P2_Medium_T3_18_Left/5.png", "Image/P20_D2_P2_Medium_T3_18_Left/6.png", "Image/P20_D2_P2_Medium_T3_18_Left/7.png", "Image/P20_D2_P2_Medium_T3_18_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. That left cup on the shelf\"\nThe frames roughly correspond to the speech interval from 3058.5396 seconds to 3071.3849000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P6_High_T2_10_Right", "images": ["Image/P9_D3_P6_High_T2_10_Right/1.png", "Image/P9_D3_P6_High_T2_10_Right/2.png", "Image/P9_D3_P6_High_T2_10_Right/3.png", "Image/P9_D3_P6_High_T2_10_Right/4.png", "Image/P9_D3_P6_High_T2_10_Right/5.png", "Image/P9_D3_P6_High_T2_10_Right/6.png", "Image/P9_D3_P6_High_T2_10_Right/7.png", "Image/P9_D3_P6_High_T2_10_Right/8.png", "Image/P9_D3_P6_High_T2_10_Right/9.png", "Image/P9_D3_P6_High_T2_10_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to pour some water in the red bottle\"\nThe frames roughly correspond to the speech interval from 645.4862 seconds to 661.845 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P5_Medium_T3_16_Left", "images": ["Image/P4_D2_P5_Medium_T3_16_Left/1.png", "Image/P4_D2_P5_Medium_T3_16_Left/2.png", "Image/P4_D2_P5_Medium_T3_16_Left/3.png", "Image/P4_D2_P5_Medium_T3_16_Left/4.png", "Image/P4_D2_P5_Medium_T3_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left cap? Finish.\"\nThe frames roughly correspond to the speech interval from 2008.1373 seconds to 2021.9583 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P9_Medium_T1_10_Left", "images": ["Image/P6_D3_P9_Medium_T1_10_Left/1.png", "Image/P6_D3_P9_Medium_T1_10_Left/2.png", "Image/P6_D3_P9_Medium_T1_10_Left/3.png", "Image/P6_D3_P9_Medium_T1_10_Left/4.png", "Image/P6_D3_P9_Medium_T1_10_Left/5.png", "Image/P6_D3_P9_Medium_T1_10_Left/6.png", "Image/P6_D3_P9_Medium_T1_10_Left/7.png", "Image/P6_D3_P9_Medium_T1_10_Left/8.png", "Image/P6_D3_P9_Medium_T1_10_Left/9.png", "Image/P6_D3_P9_Medium_T1_10_Left/10.png", "Image/P6_D3_P9_Medium_T1_10_Left/11.png", "Image/P6_D3_P9_Medium_T1_10_Left/12.png", "Image/P6_D3_P9_Medium_T1_10_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3654.9518 seconds to 3675.1629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P3_Low_T3_8_Right", "images": ["Image/P13_D2_P3_Low_T3_8_Right/1.png", "Image/P13_D2_P3_Low_T3_8_Right/2.png", "Image/P13_D2_P3_Low_T3_8_Right/3.png", "Image/P13_D2_P3_Low_T3_8_Right/4.png", "Image/P13_D2_P3_Low_T3_8_Right/5.png", "Image/P13_D2_P3_Low_T3_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the green glass over here\"\nThe frames roughly correspond to the speech interval from 859.92346 seconds to 870.47396 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P2_High_T3_7_Right", "images": ["Image/P19_D1_P2_High_T3_7_Right/1.png", "Image/P19_D1_P2_High_T3_7_Right/2.png", "Image/P19_D1_P2_High_T3_7_Right/3.png", "Image/P19_D1_P2_High_T3_7_Right/4.png", "Image/P19_D1_P2_High_T3_7_Right/5.png", "Image/P19_D1_P2_High_T3_7_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 777.8556 seconds to 790.6474000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P1_Low_T2_8_Mid", "images": ["Image/P10_D2_P1_Low_T2_8_Mid/1.png", "Image/P10_D2_P1_Low_T2_8_Mid/2.png", "Image/P10_D2_P1_Low_T2_8_Mid/3.png", "Image/P10_D2_P1_Low_T2_8_Mid/4.png", "Image/P10_D2_P1_Low_T2_8_Mid/5.png", "Image/P10_D2_P1_Low_T2_8_Mid/6.png", "Image/P10_D2_P1_Low_T2_8_Mid/7.png", "Image/P10_D2_P1_Low_T2_8_Mid/8.png", "Image/P10_D2_P1_Low_T2_8_Mid/9.png", "Image/P10_D2_P1_Low_T2_8_Mid/10.png", "Image/P10_D2_P1_Low_T2_8_Mid/11.png", "Image/P10_D2_P1_Low_T2_8_Mid/12.png", "Image/P10_D2_P1_Low_T2_8_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1209.2945800000002 seconds to 1228.0183800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P2_Low_T1_20_Mid", "images": ["Image/P1_D2_P2_Low_T1_20_Mid/1.png", "Image/P1_D2_P2_Low_T1_20_Mid/2.png", "Image/P1_D2_P2_Low_T1_20_Mid/3.png", "Image/P1_D2_P2_Low_T1_20_Mid/4.png", "Image/P1_D2_P2_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 986.0394 seconds to 996.7434 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P4_High_T1_14_Right", "images": ["Image/P27_D3_P4_High_T1_14_Right/1.png", "Image/P27_D3_P4_High_T1_14_Right/2.png", "Image/P27_D3_P4_High_T1_14_Right/3.png", "Image/P27_D3_P4_High_T1_14_Right/4.png", "Image/P27_D3_P4_High_T1_14_Right/5.png", "Image/P27_D3_P4_High_T1_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant to the middle shelf right there? Finish\"\nThe frames roughly correspond to the speech interval from 433.2917000000001 seconds to 445.1362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P1_High_T1_17_Mid", "images": ["Image/P24_D3_P1_High_T1_17_Mid/1.png", "Image/P24_D3_P1_High_T1_17_Mid/2.png", "Image/P24_D3_P1_High_T1_17_Mid/3.png", "Image/P24_D3_P1_High_T1_17_Mid/4.png", "Image/P24_D3_P1_High_T1_17_Mid/5.png", "Image/P24_D3_P1_High_T1_17_Mid/6.png", "Image/P24_D3_P1_High_T1_17_Mid/7.png", "Image/P24_D3_P1_High_T1_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this hotel to this area? Finish.\"\nThe frames roughly correspond to the speech interval from 1713.2946 seconds to 1726.7829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P3_High_T1_12_Mid", "images": ["Image/P23_D2_P3_High_T1_12_Mid/1.png", "Image/P23_D2_P3_High_T1_12_Mid/2.png", "Image/P23_D2_P3_High_T1_12_Mid/3.png", "Image/P23_D2_P3_High_T1_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 194.1759 seconds to 203.7667 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P6_Medium_T3_25_Right", "images": ["Image/P12_D2_P6_Medium_T3_25_Right/1.png", "Image/P12_D2_P6_Medium_T3_25_Right/2.png", "Image/P12_D2_P6_Medium_T3_25_Right/3.png", "Image/P12_D2_P6_Medium_T3_25_Right/4.png", "Image/P12_D2_P6_Medium_T3_25_Right/5.png", "Image/P12_D2_P6_Medium_T3_25_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 618.69094 seconds to 630.5635 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P3_High_T2_24_Right", "images": ["Image/P13_D3_P3_High_T2_24_Right/1.png", "Image/P13_D3_P3_High_T2_24_Right/2.png", "Image/P13_D3_P3_High_T2_24_Right/3.png", "Image/P13_D3_P3_High_T2_24_Right/4.png", "Image/P13_D3_P3_High_T2_24_Right/5.png", "Image/P13_D3_P3_High_T2_24_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the plant over here\"\nThe frames roughly correspond to the speech interval from 1830.0965 seconds to 1840.9665 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P1_High_T1_11_Right", "images": ["Image/P23_D3_P1_High_T1_11_Right/1.png", "Image/P23_D3_P1_High_T1_11_Right/2.png", "Image/P23_D3_P1_High_T1_11_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the red one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 998.8775 seconds to 1007.6743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P5_Medium_T2_26_Mid", "images": ["Image/P19_D1_P5_Medium_T2_26_Mid/1.png", "Image/P19_D1_P5_Medium_T2_26_Mid/2.png", "Image/P19_D1_P5_Medium_T2_26_Mid/3.png", "Image/P19_D1_P5_Medium_T2_26_Mid/4.png", "Image/P19_D1_P5_Medium_T2_26_Mid/5.png", "Image/P19_D1_P5_Medium_T2_26_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour from this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 1102.2703 seconds to 1116.62 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P7_Medium_T3_10_Left", "images": ["Image/P10_D1_P7_Medium_T3_10_Left/1.png", "Image/P10_D1_P7_Medium_T3_10_Left/2.png", "Image/P10_D1_P7_Medium_T3_10_Left/3.png", "Image/P10_D1_P7_Medium_T3_10_Left/4.png", "Image/P10_D1_P7_Medium_T3_10_Left/5.png", "Image/P10_D1_P7_Medium_T3_10_Left/6.png", "Image/P10_D1_P7_Medium_T3_10_Left/7.png", "Image/P10_D1_P7_Medium_T3_10_Left/8.png", "Image/P10_D1_P7_Medium_T3_10_Left/9.png", "Image/P10_D1_P7_Medium_T3_10_Left/10.png", "Image/P10_D1_P7_Medium_T3_10_Left/11.png", "Image/P10_D1_P7_Medium_T3_10_Left/12.png", "Image/P10_D1_P7_Medium_T3_10_Left/13.png", "Image/P10_D1_P7_Medium_T3_10_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1903.13049 seconds to 1926.60057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P4_Low_T2_10_Left", "images": ["Image/P6_D3_P4_Low_T2_10_Left/1.png", "Image/P6_D3_P4_Low_T2_10_Left/2.png", "Image/P6_D3_P4_Low_T2_10_Left/3.png", "Image/P6_D3_P4_Low_T2_10_Left/4.png", "Image/P6_D3_P4_Low_T2_10_Left/5.png", "Image/P6_D3_P4_Low_T2_10_Left/6.png", "Image/P6_D3_P4_Low_T2_10_Left/7.png", "Image/P6_D3_P4_Low_T2_10_Left/8.png", "Image/P6_D3_P4_Low_T2_10_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4721.1064 seconds to 4738.2851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P3_Medium_T3_24_Left", "images": ["Image/P18_D1_P3_Medium_T3_24_Left/1.png", "Image/P18_D1_P3_Medium_T3_24_Left/2.png", "Image/P18_D1_P3_Medium_T3_24_Left/3.png", "Image/P18_D1_P3_Medium_T3_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up this plant on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 831.8625000000001 seconds to 841.9544000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P8_High_T3_20_Left", "images": ["Image/P25_D2_P8_High_T3_20_Left/1.png", "Image/P25_D2_P8_High_T3_20_Left/2.png", "Image/P25_D2_P8_High_T3_20_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 939.3144 seconds to 947.7079 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P3_Low_T1_19_Right", "images": ["Image/P18_D1_P3_Low_T1_19_Right/1.png", "Image/P18_D1_P3_Low_T1_19_Right/2.png", "Image/P18_D1_P3_Low_T1_19_Right/3.png", "Image/P18_D1_P3_Low_T1_19_Right/4.png", "Image/P18_D1_P3_Low_T1_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down here to the bin in the back. Finish.\"\nThe frames roughly correspond to the speech interval from 611.2328 seconds to 623.14805 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P4_Medium_T3_8_Right", "images": ["Image/P15_D3_P4_Medium_T3_8_Right/1.png", "Image/P15_D3_P4_Medium_T3_8_Right/2.png", "Image/P15_D3_P4_Medium_T3_8_Right/3.png", "Image/P15_D3_P4_Medium_T3_8_Right/4.png", "Image/P15_D3_P4_Medium_T3_8_Right/5.png", "Image/P15_D3_P4_Medium_T3_8_Right/6.png", "Image/P15_D3_P4_Medium_T3_8_Right/7.png", "Image/P15_D3_P4_Medium_T3_8_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the cup at the right side on the table desk. Finish.\"\nThe frames roughly correspond to the speech interval from 377.2928 seconds to 391.8927000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P5_High_T2_16_Right", "images": ["Image/P2_D3_P5_High_T2_16_Right/1.png", "Image/P2_D3_P5_High_T2_16_Right/2.png", "Image/P2_D3_P5_High_T2_16_Right/3.png", "Image/P2_D3_P5_High_T2_16_Right/4.png", "Image/P2_D3_P5_High_T2_16_Right/5.png", "Image/P2_D3_P5_High_T2_16_Right/6.png", "Image/P2_D3_P5_High_T2_16_Right/7.png", "Image/P2_D3_P5_High_T2_16_Right/8.png", "Image/P2_D3_P5_High_T2_16_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that brown cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1833.3128 seconds to 1847.8412000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P3_Low_T3_5_Right", "images": ["Image/P22_D1_P3_Low_T3_5_Right/1.png", "Image/P22_D1_P3_Low_T3_5_Right/2.png", "Image/P22_D1_P3_Low_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 1460.35333 seconds to 1471.25338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P8_High_T1_24_Right", "images": ["Image/P14_D3_P8_High_T1_24_Right/1.png", "Image/P14_D3_P8_High_T1_24_Right/2.png", "Image/P14_D3_P8_High_T1_24_Right/3.png", "Image/P14_D3_P8_High_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start and move the plant up to the top. Finish.\"\nThe frames roughly correspond to the speech interval from 1420.50186 seconds to 1429.54206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P2_High_T1_20_Mid", "images": ["Image/P27_D2_P2_High_T1_20_Mid/1.png", "Image/P27_D2_P2_High_T1_20_Mid/2.png", "Image/P27_D2_P2_High_T1_20_Mid/3.png", "Image/P27_D2_P2_High_T1_20_Mid/4.png", "Image/P27_D2_P2_High_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You move the white vase to the bottom of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1585.2806 seconds to 1596.0829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P7_Medium_T3_22_Mid", "images": ["Image/P1_D3_P7_Medium_T3_22_Mid/1.png", "Image/P1_D3_P7_Medium_T3_22_Mid/2.png", "Image/P1_D3_P7_Medium_T3_22_Mid/3.png", "Image/P1_D3_P7_Medium_T3_22_Mid/4.png", "Image/P1_D3_P7_Medium_T3_22_Mid/5.png", "Image/P1_D3_P7_Medium_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the middle plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1590.9055999999998 seconds to 1601.5418999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P7_Medium_T3_22_Mid", "images": ["Image/P28_D3_P7_Medium_T3_22_Mid/1.png", "Image/P28_D3_P7_Medium_T3_22_Mid/2.png", "Image/P28_D3_P7_Medium_T3_22_Mid/3.png", "Image/P28_D3_P7_Medium_T3_22_Mid/4.png", "Image/P28_D3_P7_Medium_T3_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the middle plant on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 393.5807 seconds to 408.5307 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P6_Medium_T1_3_Right", "images": ["Image/P23_D1_P6_Medium_T1_3_Right/1.png", "Image/P23_D1_P6_Medium_T1_3_Right/2.png", "Image/P23_D1_P6_Medium_T1_3_Right/3.png", "Image/P23_D1_P6_Medium_T1_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1815.8466 seconds to 1825.5341 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P7_High_T2_23_Mid", "images": ["Image/P29_D2_P7_High_T2_23_Mid/1.png", "Image/P29_D2_P7_High_T2_23_Mid/2.png", "Image/P29_D2_P7_High_T2_23_Mid/3.png", "Image/P29_D2_P7_High_T2_23_Mid/4.png", "Image/P29_D2_P7_High_T2_23_Mid/5.png", "Image/P29_D2_P7_High_T2_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 491.2397 seconds to 502.98800000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P8_High_T2_22_Mid", "images": ["Image/P11_D1_P8_High_T2_22_Mid/1.png", "Image/P11_D1_P8_High_T2_22_Mid/2.png", "Image/P11_D1_P8_High_T2_22_Mid/3.png", "Image/P11_D1_P8_High_T2_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 2352.2475099999997 seconds to 2362.3814799999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P7_High_T2_24_Right", "images": ["Image/P23_D3_P7_High_T2_24_Right/1.png", "Image/P23_D3_P7_High_T2_24_Right/2.png", "Image/P23_D3_P7_High_T2_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 978.9147 seconds to 987.4913 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P3_Medium_T3_19_Right", "images": ["Image/P24_D1_P3_Medium_T3_19_Right/1.png", "Image/P24_D1_P3_Medium_T3_19_Right/2.png", "Image/P24_D1_P3_Medium_T3_19_Right/3.png", "Image/P24_D1_P3_Medium_T3_19_Right/4.png", "Image/P24_D1_P3_Medium_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1378.6408 seconds to 1389.4290999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P3_High_T2_7_Right", "images": ["Image/P29_D2_P3_High_T2_7_Right/1.png", "Image/P29_D2_P3_High_T2_7_Right/2.png", "Image/P29_D2_P3_High_T2_7_Right/3.png", "Image/P29_D2_P3_High_T2_7_Right/4.png", "Image/P29_D2_P3_High_T2_7_Right/5.png", "Image/P29_D2_P3_High_T2_7_Right/6.png", "Image/P29_D2_P3_High_T2_7_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi Friday\"\nThe frames roughly correspond to the speech interval from 621.0014 seconds to 634.9894 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P1_Low_T1_20_Mid", "images": ["Image/P18_D1_P1_Low_T1_20_Mid/1.png", "Image/P18_D1_P1_Low_T1_20_Mid/2.png", "Image/P18_D1_P1_Low_T1_20_Mid/3.png", "Image/P18_D1_P1_Low_T1_20_Mid/4.png", "Image/P18_D1_P1_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 957.5458 seconds to 967.9987 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P1_Medium_T1_12_Mid", "images": ["Image/P25_D1_P1_Medium_T1_12_Mid/1.png", "Image/P25_D1_P1_Medium_T1_12_Mid/2.png", "Image/P25_D1_P1_Medium_T1_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 513.5020999999999 seconds to 521.9232999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P2_Low_T1_20_Right", "images": ["Image/P19_D1_P2_Low_T1_20_Right/1.png", "Image/P19_D1_P2_Low_T1_20_Right/2.png", "Image/P19_D1_P2_Low_T1_20_Right/3.png", "Image/P19_D1_P2_Low_T1_20_Right/4.png", "Image/P19_D1_P2_Low_T1_20_Right/5.png", "Image/P19_D1_P2_Low_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container. Finish.\"\nThe frames roughly correspond to the speech interval from 426.25194 seconds to 440.77027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P6_High_T1_9_Left", "images": ["Image/P28_D1_P6_High_T1_9_Left/1.png", "Image/P28_D1_P6_High_T1_9_Left/2.png", "Image/P28_D1_P6_High_T1_9_Left/3.png", "Image/P28_D1_P6_High_T1_9_Left/4.png", "Image/P28_D1_P6_High_T1_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this green cup over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1588.5343 seconds to 1599.5167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P5_Low_T3_8_Right", "images": ["Image/P22_D1_P5_Low_T3_8_Right/1.png", "Image/P22_D1_P5_Low_T3_8_Right/2.png", "Image/P22_D1_P5_Low_T3_8_Right/3.png", "Image/P22_D1_P5_Low_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this. Finish.\"\nThe frames roughly correspond to the speech interval from 1582.4718 seconds to 1593.7602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P1_Medium_T1_2_Right", "images": ["Image/P11_D3_P1_Medium_T1_2_Right/1.png", "Image/P11_D3_P1_Medium_T1_2_Right/2.png", "Image/P11_D3_P1_Medium_T1_2_Right/3.png", "Image/P11_D3_P1_Medium_T1_2_Right/4.png", "Image/P11_D3_P1_Medium_T1_2_Right/5.png", "Image/P11_D3_P1_Medium_T1_2_Right/6.png", "Image/P11_D3_P1_Medium_T1_2_Right/7.png", "Image/P11_D3_P1_Medium_T1_2_Right/8.png", "Image/P11_D3_P1_Medium_T1_2_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first green bottle from the right side to the under of the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 1949.9586 seconds to 1964.7135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P6_High_T2_27_Mid", "images": ["Image/P27_D1_P6_High_T2_27_Mid/1.png", "Image/P27_D1_P6_High_T2_27_Mid/2.png", "Image/P27_D1_P6_High_T2_27_Mid/3.png", "Image/P27_D1_P6_High_T2_27_Mid/4.png", "Image/P27_D1_P6_High_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that cup? Finish\"\nThe frames roughly correspond to the speech interval from 1159.9124000000002 seconds to 1169.6145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P9_Low_T3_21_Right", "images": ["Image/P6_D1_P9_Low_T3_21_Right/1.png", "Image/P6_D1_P9_Low_T3_21_Right/2.png", "Image/P6_D1_P9_Low_T3_21_Right/3.png", "Image/P6_D1_P9_Low_T3_21_Right/4.png", "Image/P6_D1_P9_Low_T3_21_Right/5.png", "Image/P6_D1_P9_Low_T3_21_Right/6.png", "Image/P6_D1_P9_Low_T3_21_Right/7.png", "Image/P6_D1_P9_Low_T3_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey buddy\"\nThe frames roughly correspond to the speech interval from 2414.0519000000004 seconds to 2426.9665000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P2_Low_T3_21_Left", "images": ["Image/P16_D1_P2_Low_T3_21_Left/1.png", "Image/P16_D1_P2_Low_T3_21_Left/2.png", "Image/P16_D1_P2_Low_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1484.2782 seconds to 1493.6186 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P3_High_T3_25_Left", "images": ["Image/P1_D3_P3_High_T3_25_Left/1.png", "Image/P1_D3_P3_High_T3_25_Left/2.png", "Image/P1_D3_P3_High_T3_25_Left/3.png", "Image/P1_D3_P3_High_T3_25_Left/4.png", "Image/P1_D3_P3_High_T3_25_Left/5.png", "Image/P1_D3_P3_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1751.3153999999995 seconds to 1762.6194999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P7_Medium_T2_17_Right", "images": ["Image/P10_D3_P7_Medium_T2_17_Right/1.png", "Image/P10_D3_P7_Medium_T2_17_Right/2.png", "Image/P10_D3_P7_Medium_T2_17_Right/3.png", "Image/P10_D3_P7_Medium_T2_17_Right/4.png", "Image/P10_D3_P7_Medium_T2_17_Right/5.png", "Image/P10_D3_P7_Medium_T2_17_Right/6.png", "Image/P10_D3_P7_Medium_T2_17_Right/7.png", "Image/P10_D3_P7_Medium_T2_17_Right/8.png", "Image/P10_D3_P7_Medium_T2_17_Right/9.png", "Image/P10_D3_P7_Medium_T2_17_Right/10.png", "Image/P10_D3_P7_Medium_T2_17_Right/11.png", "Image/P10_D3_P7_Medium_T2_17_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey\"\nThe frames roughly correspond to the speech interval from 620.37969 seconds to 638.72609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P1_High_T1_16_Mid", "images": ["Image/P12_D3_P1_High_T1_16_Mid/1.png", "Image/P12_D3_P1_High_T1_16_Mid/2.png", "Image/P12_D3_P1_High_T1_16_Mid/3.png", "Image/P12_D3_P1_High_T1_16_Mid/4.png", "Image/P12_D3_P1_High_T1_16_Mid/5.png", "Image/P12_D3_P1_High_T1_16_Mid/6.png", "Image/P12_D3_P1_High_T1_16_Mid/7.png", "Image/P12_D3_P1_High_T1_16_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2294.55408 seconds to 2312.21769 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P3_Medium_T1_8_Right", "images": ["Image/P12_D2_P3_Medium_T1_8_Right/1.png", "Image/P12_D2_P3_Medium_T1_8_Right/2.png", "Image/P12_D2_P3_Medium_T1_8_Right/3.png", "Image/P12_D2_P3_Medium_T1_8_Right/4.png", "Image/P12_D2_P3_Medium_T1_8_Right/5.png", "Image/P12_D2_P3_Medium_T1_8_Right/6.png", "Image/P12_D2_P3_Medium_T1_8_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the mug on the right to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 314.34090000000003 seconds to 329.66690000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P4_Low_T3_22_Left", "images": ["Image/P24_D1_P4_Low_T3_22_Left/1.png", "Image/P24_D1_P4_Low_T3_22_Left/2.png", "Image/P24_D1_P4_Low_T3_22_Left/3.png", "Image/P24_D1_P4_Low_T3_22_Left/4.png", "Image/P24_D1_P4_Low_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star\"\nThe frames roughly correspond to the speech interval from 1234.4172 seconds to 1244.7869999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P3_Medium_T1_26_Right", "images": ["Image/P6_D1_P3_Medium_T1_26_Right/1.png", "Image/P6_D1_P3_Medium_T1_26_Right/2.png", "Image/P6_D1_P3_Medium_T1_26_Right/3.png", "Image/P6_D1_P3_Medium_T1_26_Right/4.png", "Image/P6_D1_P3_Medium_T1_26_Right/5.png", "Image/P6_D1_P3_Medium_T1_26_Right/6.png", "Image/P6_D1_P3_Medium_T1_26_Right/7.png", "Image/P6_D1_P3_Medium_T1_26_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1889.14305 seconds to 1904.71984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P2_Low_T2_7_Left", "images": ["Image/P2_D2_P2_Low_T2_7_Left/1.png", "Image/P2_D2_P2_Low_T2_7_Left/2.png", "Image/P2_D2_P2_Low_T2_7_Left/3.png", "Image/P2_D2_P2_Low_T2_7_Left/4.png", "Image/P2_D2_P2_Low_T2_7_Left/5.png", "Image/P2_D2_P2_Low_T2_7_Left/6.png", "Image/P2_D2_P2_Low_T2_7_Left/7.png", "Image/P2_D2_P2_Low_T2_7_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2919.0701 seconds to 2932.6683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P4_Low_T2_11_Mid", "images": ["Image/P10_D1_P4_Low_T2_11_Mid/1.png", "Image/P10_D1_P4_Low_T2_11_Mid/2.png", "Image/P10_D1_P4_Low_T2_11_Mid/3.png", "Image/P10_D1_P4_Low_T2_11_Mid/4.png", "Image/P10_D1_P4_Low_T2_11_Mid/5.png", "Image/P10_D1_P4_Low_T2_11_Mid/6.png", "Image/P10_D1_P4_Low_T2_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2139.63685 seconds to 2152.79119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P1_Low_T1_5_Mid", "images": ["Image/P8_D3_P1_Low_T1_5_Mid/1.png", "Image/P8_D3_P1_Low_T1_5_Mid/2.png", "Image/P8_D3_P1_Low_T1_5_Mid/3.png", "Image/P8_D3_P1_Low_T1_5_Mid/4.png", "Image/P8_D3_P1_Low_T1_5_Mid/5.png", "Image/P8_D3_P1_Low_T1_5_Mid/6.png", "Image/P8_D3_P1_Low_T1_5_Mid/7.png", "Image/P8_D3_P1_Low_T1_5_Mid/8.png", "Image/P8_D3_P1_Low_T1_5_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the planet beneath the desk that's next to the green chair? That is fine. Finish.\"\nThe frames roughly correspond to the speech interval from 1868.8495 seconds to 1883.1316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P3_High_T2_24_Mid", "images": ["Image/P17_D3_P3_High_T2_24_Mid/1.png", "Image/P17_D3_P3_High_T2_24_Mid/2.png", "Image/P17_D3_P3_High_T2_24_Mid/3.png", "Image/P17_D3_P3_High_T2_24_Mid/4.png", "Image/P17_D3_P3_High_T2_24_Mid/5.png", "Image/P17_D3_P3_High_T2_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the white plant on the shelf over there? Finish.\"\nThe frames roughly correspond to the speech interval from 996.0236 seconds to 1010.598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P1_Low_T1_25_Right", "images": ["Image/P1_D2_P1_Low_T1_25_Right/1.png", "Image/P1_D2_P1_Low_T1_25_Right/2.png", "Image/P1_D2_P1_Low_T1_25_Right/3.png", "Image/P1_D2_P1_Low_T1_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1124.7013000000002 seconds to 1134.8643000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P2_High_T3_13_Left", "images": ["Image/P20_D1_P2_High_T3_13_Left/1.png", "Image/P20_D1_P2_High_T3_13_Left/2.png", "Image/P20_D1_P2_High_T3_13_Left/3.png", "Image/P20_D1_P2_High_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 364.68839 seconds to 375.50474 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P6_High_T1_22_Right", "images": ["Image/P3_D3_P6_High_T1_22_Right/1.png", "Image/P3_D3_P6_High_T1_22_Right/2.png", "Image/P3_D3_P6_High_T1_22_Right/3.png", "Image/P3_D3_P6_High_T1_22_Right/4.png", "Image/P3_D3_P6_High_T1_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you move the item to this table? Finish.\"\nThe frames roughly correspond to the speech interval from 905.9226 seconds to 916.7953 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P2_High_T2_14_Mid", "images": ["Image/P4_D3_P2_High_T2_14_Mid/1.png", "Image/P4_D3_P2_High_T2_14_Mid/2.png", "Image/P4_D3_P2_High_T2_14_Mid/3.png", "Image/P4_D3_P2_High_T2_14_Mid/4.png", "Image/P4_D3_P2_High_T2_14_Mid/5.png", "Image/P4_D3_P2_High_T2_14_Mid/6.png", "Image/P4_D3_P2_High_T2_14_Mid/7.png", "Image/P4_D3_P2_High_T2_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plants in the middle on the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 383.8718999999999 seconds to 397.6102 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P6_Medium_T2_8_Left", "images": ["Image/P18_D1_P6_Medium_T2_8_Left/1.png", "Image/P18_D1_P6_Medium_T2_8_Left/2.png", "Image/P18_D1_P6_Medium_T2_8_Left/3.png", "Image/P18_D1_P6_Medium_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 976.003 seconds to 986.2336 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P5_High_T1_16_Right", "images": ["Image/P23_D2_P5_High_T1_16_Right/1.png", "Image/P23_D2_P5_High_T1_16_Right/2.png", "Image/P23_D2_P5_High_T1_16_Right/3.png", "Image/P23_D2_P5_High_T1_16_Right/4.png", "Image/P23_D2_P5_High_T1_16_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start for the red one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 427.5019 seconds to 439.5153 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P4_Medium_T2_5_Mid", "images": ["Image/P6_D1_P4_Medium_T2_5_Mid/1.png", "Image/P6_D1_P4_Medium_T2_5_Mid/2.png", "Image/P6_D1_P4_Medium_T2_5_Mid/3.png", "Image/P6_D1_P4_Medium_T2_5_Mid/4.png", "Image/P6_D1_P4_Medium_T2_5_Mid/5.png", "Image/P6_D1_P4_Medium_T2_5_Mid/6.png", "Image/P6_D1_P4_Medium_T2_5_Mid/7.png", "Image/P6_D1_P4_Medium_T2_5_Mid/8.png", "Image/P6_D1_P4_Medium_T2_5_Mid/9.png", "Image/P6_D1_P4_Medium_T2_5_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2059.8017 seconds to 2075.7650000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P4_Low_T2_18_Right", "images": ["Image/P18_D1_P4_Low_T2_18_Right/1.png", "Image/P18_D1_P4_Low_T2_18_Right/2.png", "Image/P18_D1_P4_Low_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 866.8706 seconds to 876.7536 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P8_Medium_T1_9_Mid", "images": ["Image/P8_D3_P8_Medium_T1_9_Mid/1.png", "Image/P8_D3_P8_Medium_T1_9_Mid/2.png", "Image/P8_D3_P8_Medium_T1_9_Mid/3.png", "Image/P8_D3_P8_Medium_T1_9_Mid/4.png", "Image/P8_D3_P8_Medium_T1_9_Mid/5.png", "Image/P8_D3_P8_Medium_T1_9_Mid/6.png", "Image/P8_D3_P8_Medium_T1_9_Mid/7.png", "Image/P8_D3_P8_Medium_T1_9_Mid/8.png", "Image/P8_D3_P8_Medium_T1_9_Mid/9.png", "Image/P8_D3_P8_Medium_T1_9_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the green glass in the middle? Move it to the top floor of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1842.9176 seconds to 1861.4296 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P9_Low_T1_9_Right", "images": ["Image/P12_D2_P9_Low_T1_9_Right/1.png", "Image/P12_D2_P9_Low_T1_9_Right/2.png", "Image/P12_D2_P9_Low_T1_9_Right/3.png", "Image/P12_D2_P9_Low_T1_9_Right/4.png", "Image/P12_D2_P9_Low_T1_9_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this green mug to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 676.0548 seconds to 687.078 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P8_High_T2_15_Right", "images": ["Image/P3_D2_P8_High_T2_15_Right/1.png", "Image/P3_D2_P8_High_T2_15_Right/2.png", "Image/P3_D2_P8_High_T2_15_Right/3.png", "Image/P3_D2_P8_High_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2439.8758000000003 seconds to 2449.6872000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P7_Low_T3_19_Left", "images": ["Image/P30_D3_P7_Low_T3_19_Left/1.png", "Image/P30_D3_P7_Low_T3_19_Left/2.png", "Image/P30_D3_P7_Low_T3_19_Left/3.png", "Image/P30_D3_P7_Low_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1385.2097 seconds to 1394.5377 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P5_High_T3_24_Left", "images": ["Image/P19_D1_P5_High_T3_24_Left/1.png", "Image/P19_D1_P5_High_T3_24_Left/2.png", "Image/P19_D1_P5_High_T3_24_Left/3.png", "Image/P19_D1_P5_High_T3_24_Left/4.png", "Image/P19_D1_P5_High_T3_24_Left/5.png", "Image/P19_D1_P5_High_T3_24_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this flowerpot here. Okay.\"\nThe frames roughly correspond to the speech interval from 857.7129 seconds to 877.0337999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P5_Low_T2_14_Left", "images": ["Image/P24_D3_P5_Low_T2_14_Left/1.png", "Image/P24_D3_P5_Low_T2_14_Left/2.png", "Image/P24_D3_P5_Low_T2_14_Left/3.png", "Image/P24_D3_P5_Low_T2_14_Left/4.png", "Image/P24_D3_P5_Low_T2_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1923.9581 seconds to 1934.0501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P2_Medium_T1_10_Left", "images": ["Image/P4_D3_P2_Medium_T1_10_Left/1.png", "Image/P4_D3_P2_Medium_T1_10_Left/2.png", "Image/P4_D3_P2_Medium_T1_10_Left/3.png", "Image/P4_D3_P2_Medium_T1_10_Left/4.png", "Image/P4_D3_P2_Medium_T1_10_Left/5.png", "Image/P4_D3_P2_Medium_T1_10_Left/6.png", "Image/P4_D3_P2_Medium_T1_10_Left/7.png", "Image/P4_D3_P2_Medium_T1_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the left bottle to the ground? Finish\"\nThe frames roughly correspond to the speech interval from 792.538 seconds to 807.9314999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P3_High_T1_15_Mid", "images": ["Image/P19_D1_P3_High_T1_15_Mid/1.png", "Image/P19_D1_P3_High_T1_15_Mid/2.png", "Image/P19_D1_P3_High_T1_15_Mid/3.png", "Image/P19_D1_P3_High_T1_15_Mid/4.png", "Image/P19_D1_P3_High_T1_15_Mid/5.png", "Image/P19_D1_P3_High_T1_15_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1039.6724499999998 seconds to 1063.4887399999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P3_High_T1_8_Left", "images": ["Image/P4_D1_P3_High_T1_8_Left/1.png", "Image/P4_D1_P3_High_T1_8_Left/2.png", "Image/P4_D1_P3_High_T1_8_Left/3.png", "Image/P4_D1_P3_High_T1_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move it down to the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 2283.2614999999996 seconds to 2296.5227999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P4_High_T3_11_Mid", "images": ["Image/P12_D3_P4_High_T3_11_Mid/1.png", "Image/P12_D3_P4_High_T3_11_Mid/2.png", "Image/P12_D3_P4_High_T3_11_Mid/3.png", "Image/P12_D3_P4_High_T3_11_Mid/4.png", "Image/P12_D3_P4_High_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2339.91899 seconds to 2350.74739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P4_Low_T3_2_Right", "images": ["Image/P5_D1_P4_Low_T3_2_Right/1.png", "Image/P5_D1_P4_Low_T3_2_Right/2.png", "Image/P5_D1_P4_Low_T3_2_Right/3.png", "Image/P5_D1_P4_Low_T3_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle. Finished.\"\nThe frames roughly correspond to the speech interval from 2005.1879 seconds to 2015.2766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P8_Medium_T3_26_Right", "images": ["Image/P24_D3_P8_Medium_T3_26_Right/1.png", "Image/P24_D3_P8_Medium_T3_26_Right/2.png", "Image/P24_D3_P8_Medium_T3_26_Right/3.png", "Image/P24_D3_P8_Medium_T3_26_Right/4.png", "Image/P24_D3_P8_Medium_T3_26_Right/5.png", "Image/P24_D3_P8_Medium_T3_26_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2025.6433 seconds to 2036.5564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P7_Low_T2_1_Right", "images": ["Image/P4_D2_P7_Low_T2_1_Right/1.png", "Image/P4_D2_P7_Low_T2_1_Right/2.png", "Image/P4_D2_P7_Low_T2_1_Right/3.png", "Image/P4_D2_P7_Low_T2_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you fill in the bottle? Finish\"\nThe frames roughly correspond to the speech interval from 1806.0759 seconds to 1819.4216 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P1_Low_T3_10_Right", "images": ["Image/P22_D1_P1_Low_T3_10_Right/1.png", "Image/P22_D1_P1_Low_T3_10_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\" In that\"\nThe frames roughly correspond to the speech interval from 1784.269 seconds to 1791.9381 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P8_High_T2_14_Left", "images": ["Image/P28_D2_P8_High_T2_14_Left/1.png", "Image/P28_D2_P8_High_T2_14_Left/2.png", "Image/P28_D2_P8_High_T2_14_Left/3.png", "Image/P28_D2_P8_High_T2_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1165.7936 seconds to 1174.883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P4_Low_T1_17_Right", "images": ["Image/P8_D1_P4_Low_T1_17_Right/1.png", "Image/P8_D1_P4_Low_T1_17_Right/2.png", "Image/P8_D1_P4_Low_T1_17_Right/3.png", "Image/P8_D1_P4_Low_T1_17_Right/4.png", "Image/P8_D1_P4_Low_T1_17_Right/5.png", "Image/P8_D1_P4_Low_T1_17_Right/6.png", "Image/P8_D1_P4_Low_T1_17_Right/7.png", "Image/P8_D1_P4_Low_T1_17_Right/8.png", "Image/P8_D1_P4_Low_T1_17_Right/9.png", "Image/P8_D1_P4_Low_T1_17_Right/10.png", "Image/P8_D1_P4_Low_T1_17_Right/11.png", "Image/P8_D1_P4_Low_T1_17_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 492.4615999999999 seconds to 512.3041999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P3_High_T3_12_Right", "images": ["Image/P15_D2_P3_High_T3_12_Right/1.png", "Image/P15_D2_P3_High_T3_12_Right/2.png", "Image/P15_D2_P3_High_T3_12_Right/3.png", "Image/P15_D2_P3_High_T3_12_Right/4.png", "Image/P15_D2_P3_High_T3_12_Right/5.png", "Image/P15_D2_P3_High_T3_12_Right/6.png", "Image/P15_D2_P3_High_T3_12_Right/7.png", "Image/P15_D2_P3_High_T3_12_Right/8.png", "Image/P15_D2_P3_High_T3_12_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown. Put on the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2565.18374 seconds to 2579.6456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P9_Low_T3_19_Mid", "images": ["Image/P13_D1_P9_Low_T3_19_Mid/1.png", "Image/P13_D1_P9_Low_T3_19_Mid/2.png", "Image/P13_D1_P9_Low_T3_19_Mid/3.png", "Image/P13_D1_P9_Low_T3_19_Mid/4.png", "Image/P13_D1_P9_Low_T3_19_Mid/5.png", "Image/P13_D1_P9_Low_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 720.3335 seconds to 731.9432 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P2_High_T1_1_Mid", "images": ["Image/P29_D2_P2_High_T1_1_Mid/1.png", "Image/P29_D2_P2_High_T1_1_Mid/2.png", "Image/P29_D2_P2_High_T1_1_Mid/3.png", "Image/P29_D2_P2_High_T1_1_Mid/4.png", "Image/P29_D2_P2_High_T1_1_Mid/5.png", "Image/P29_D2_P2_High_T1_1_Mid/6.png", "Image/P29_D2_P2_High_T1_1_Mid/7.png", "Image/P29_D2_P2_High_T1_1_Mid/8.png", "Image/P29_D2_P2_High_T1_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 364.304 seconds to 378.8359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P7_High_T2_4_Mid", "images": ["Image/P20_D2_P7_High_T2_4_Mid/1.png", "Image/P20_D2_P7_High_T2_4_Mid/2.png", "Image/P20_D2_P7_High_T2_4_Mid/3.png", "Image/P20_D2_P7_High_T2_4_Mid/4.png", "Image/P20_D2_P7_High_T2_4_Mid/5.png", "Image/P20_D2_P7_High_T2_4_Mid/6.png", "Image/P20_D2_P7_High_T2_4_Mid/7.png", "Image/P20_D2_P7_High_T2_4_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3169.6330000000003 seconds to 3183.1487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P6_High_T1_17_Left", "images": ["Image/P22_D2_P6_High_T1_17_Left/1.png", "Image/P22_D2_P6_High_T1_17_Left/2.png", "Image/P22_D2_P6_High_T1_17_Left/3.png", "Image/P22_D2_P6_High_T1_17_Left/4.png", "Image/P22_D2_P6_High_T1_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move the cup to the opposite shelf. Finish\"\nThe frames roughly correspond to the speech interval from 950.9632 seconds to 962.8088 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P1_High_T2_27_Right", "images": ["Image/P18_D1_P1_High_T2_27_Right/1.png", "Image/P18_D1_P1_High_T2_27_Right/2.png", "Image/P18_D1_P1_High_T2_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 770.0443 seconds to 780.6659 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P8_Medium_T3_4_Left", "images": ["Image/P12_D1_P8_Medium_T3_4_Left/1.png", "Image/P12_D1_P8_Medium_T3_4_Left/2.png", "Image/P12_D1_P8_Medium_T3_4_Left/3.png", "Image/P12_D1_P8_Medium_T3_4_Left/4.png", "Image/P12_D1_P8_Medium_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1528.7314 seconds to 1542.7773 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P1_High_T3_21_Right", "images": ["Image/P14_D1_P1_High_T3_21_Right/1.png", "Image/P14_D1_P1_High_T3_21_Right/2.png", "Image/P14_D1_P1_High_T3_21_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 463.4512 seconds to 472.2194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P3_High_T3_5_Right", "images": ["Image/P13_D2_P3_High_T3_5_Right/1.png", "Image/P13_D2_P3_High_T3_5_Right/2.png", "Image/P13_D2_P3_High_T3_5_Right/3.png", "Image/P13_D2_P3_High_T3_5_Right/4.png", "Image/P13_D2_P3_High_T3_5_Right/5.png", "Image/P13_D2_P3_High_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the plant over here\"\nThe frames roughly correspond to the speech interval from 1209.92616 seconds to 1220.54646 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P3_High_T2_21_Mid", "images": ["Image/P23_D1_P3_High_T2_21_Mid/1.png", "Image/P23_D1_P3_High_T2_21_Mid/2.png", "Image/P23_D1_P3_High_T2_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Just pour water here. Finish.\"\nThe frames roughly correspond to the speech interval from 1632.5231 seconds to 1641.2911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P2_Low_T3_15_Left", "images": ["Image/P28_D1_P2_Low_T3_15_Left/1.png", "Image/P28_D1_P2_Low_T3_15_Left/2.png", "Image/P28_D1_P2_Low_T3_15_Left/3.png", "Image/P28_D1_P2_Low_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1421.6493 seconds to 1430.7937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P4_High_T1_23_Right", "images": ["Image/P24_D1_P4_High_T1_23_Right/1.png", "Image/P24_D1_P4_High_T1_23_Right/2.png", "Image/P24_D1_P4_High_T1_23_Right/3.png", "Image/P24_D1_P4_High_T1_23_Right/4.png", "Image/P24_D1_P4_High_T1_23_Right/5.png", "Image/P24_D1_P4_High_T1_23_Right/6.png", "Image/P24_D1_P4_High_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this planet over here please? Finish.\"\nThe frames roughly correspond to the speech interval from 1009.9036 seconds to 1022.3464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P4_High_T2_12_Right", "images": ["Image/P11_D3_P4_High_T2_12_Right/1.png", "Image/P11_D3_P4_High_T2_12_Right/2.png", "Image/P11_D3_P4_High_T2_12_Right/3.png", "Image/P11_D3_P4_High_T2_12_Right/4.png", "Image/P11_D3_P4_High_T2_12_Right/5.png", "Image/P11_D3_P4_High_T2_12_Right/6.png", "Image/P11_D3_P4_High_T2_12_Right/7.png", "Image/P11_D3_P4_High_T2_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the red bottle on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 1594.0261 seconds to 1608.0712 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P8_Low_T1_22_Right", "images": ["Image/P12_D1_P8_Low_T1_22_Right/1.png", "Image/P12_D1_P8_Low_T1_22_Right/2.png", "Image/P12_D1_P8_Low_T1_22_Right/3.png", "Image/P12_D1_P8_Low_T1_22_Right/4.png", "Image/P12_D1_P8_Low_T1_22_Right/5.png", "Image/P12_D1_P8_Low_T1_22_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1510.7615 seconds to 1522.3567 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P6_Low_T2_3_Mid", "images": ["Image/P18_D1_P6_Low_T2_3_Mid/1.png", "Image/P18_D1_P6_Low_T2_3_Mid/2.png", "Image/P18_D1_P6_Low_T2_3_Mid/3.png", "Image/P18_D1_P6_Low_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 720.349 seconds to 733.9908 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P7_High_T2_8_Mid", "images": ["Image/P1_D3_P7_High_T2_8_Mid/1.png", "Image/P1_D3_P7_High_T2_8_Mid/2.png", "Image/P1_D3_P7_High_T2_8_Mid/3.png", "Image/P1_D3_P7_High_T2_8_Mid/4.png", "Image/P1_D3_P7_High_T2_8_Mid/5.png", "Image/P1_D3_P7_High_T2_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1607.9327999999998 seconds to 1620.0432999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P7_Low_T2_6_Right", "images": ["Image/P26_D1_P7_Low_T2_6_Right/1.png", "Image/P26_D1_P7_Low_T2_6_Right/2.png", "Image/P26_D1_P7_Low_T2_6_Right/3.png", "Image/P26_D1_P7_Low_T2_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 150.27659999999997 seconds to 160.2145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P6_High_T2_22_Mid", "images": ["Image/P16_D1_P6_High_T2_22_Mid/1.png", "Image/P16_D1_P6_High_T2_22_Mid/2.png", "Image/P16_D1_P6_High_T2_22_Mid/3.png", "Image/P16_D1_P6_High_T2_22_Mid/4.png", "Image/P16_D1_P6_High_T2_22_Mid/5.png", "Image/P16_D1_P6_High_T2_22_Mid/6.png", "Image/P16_D1_P6_High_T2_22_Mid/7.png", "Image/P16_D1_P6_High_T2_22_Mid/8.png", "Image/P16_D1_P6_High_T2_22_Mid/9.png", "Image/P16_D1_P6_High_T2_22_Mid/10.png", "Image/P16_D1_P6_High_T2_22_Mid/11.png", "Image/P16_D1_P6_High_T2_22_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please water this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1648.1562 seconds to 1666.3922 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P3_Medium_T1_1_Left", "images": ["Image/P10_D3_P3_Medium_T1_1_Left/1.png", "Image/P10_D3_P3_Medium_T1_1_Left/2.png", "Image/P10_D3_P3_Medium_T1_1_Left/3.png", "Image/P10_D3_P3_Medium_T1_1_Left/4.png", "Image/P10_D3_P3_Medium_T1_1_Left/5.png", "Image/P10_D3_P3_Medium_T1_1_Left/6.png", "Image/P10_D3_P3_Medium_T1_1_Left/7.png", "Image/P10_D3_P3_Medium_T1_1_Left/8.png", "Image/P10_D3_P3_Medium_T1_1_Left/9.png", "Image/P10_D3_P3_Medium_T1_1_Left/10.png", "Image/P10_D3_P3_Medium_T1_1_Left/11.png", "Image/P10_D3_P3_Medium_T1_1_Left/12.png", "Image/P10_D3_P3_Medium_T1_1_Left/13.png", "Image/P10_D3_P3_Medium_T1_1_Left/14.png", "Image/P10_D3_P3_Medium_T1_1_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 130.95145 seconds to 157.09174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P7_Medium_T3_20_Mid", "images": ["Image/P11_D3_P7_Medium_T3_20_Mid/1.png", "Image/P11_D3_P7_Medium_T3_20_Mid/2.png", "Image/P11_D3_P7_Medium_T3_20_Mid/3.png", "Image/P11_D3_P7_Medium_T3_20_Mid/4.png", "Image/P11_D3_P7_Medium_T3_20_Mid/5.png", "Image/P11_D3_P7_Medium_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white glass in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 1930.9699 seconds to 1942.8181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P8_Medium_T2_12_Left", "images": ["Image/P6_D3_P8_Medium_T2_12_Left/1.png", "Image/P6_D3_P8_Medium_T2_12_Left/2.png", "Image/P6_D3_P8_Medium_T2_12_Left/3.png", "Image/P6_D3_P8_Medium_T2_12_Left/4.png", "Image/P6_D3_P8_Medium_T2_12_Left/5.png", "Image/P6_D3_P8_Medium_T2_12_Left/6.png", "Image/P6_D3_P8_Medium_T2_12_Left/7.png", "Image/P6_D3_P8_Medium_T2_12_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 3303.52956 seconds to 3319.04908 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P9_Low_T3_15_Right", "images": ["Image/P15_D1_P9_Low_T3_15_Right/1.png", "Image/P15_D1_P9_Low_T3_15_Right/2.png", "Image/P15_D1_P9_Low_T3_15_Right/3.png", "Image/P15_D1_P9_Low_T3_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1856.48097 seconds to 1865.73727 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P3_Low_T2_13_Right", "images": ["Image/P30_D3_P3_Low_T2_13_Right/1.png", "Image/P30_D3_P3_Low_T2_13_Right/2.png", "Image/P30_D3_P3_Low_T2_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1561.951 seconds to 1569.8021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P2_Low_T1_27_Mid", "images": ["Image/P3_D1_P2_Low_T1_27_Mid/1.png", "Image/P3_D1_P2_Low_T1_27_Mid/2.png", "Image/P3_D1_P2_Low_T1_27_Mid/3.png", "Image/P3_D1_P2_Low_T1_27_Mid/4.png", "Image/P3_D1_P2_Low_T1_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1304.4687 seconds to 1316.1211 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P5_High_T1_3_Mid", "images": ["Image/P18_D2_P5_High_T1_3_Mid/1.png", "Image/P18_D2_P5_High_T1_3_Mid/2.png", "Image/P18_D2_P5_High_T1_3_Mid/3.png", "Image/P18_D2_P5_High_T1_3_Mid/4.png", "Image/P18_D2_P5_High_T1_3_Mid/5.png", "Image/P18_D2_P5_High_T1_3_Mid/6.png", "Image/P18_D2_P5_High_T1_3_Mid/7.png", "Image/P18_D2_P5_High_T1_3_Mid/8.png", "Image/P18_D2_P5_High_T1_3_Mid/9.png", "Image/P18_D2_P5_High_T1_3_Mid/10.png", "Image/P18_D2_P5_High_T1_3_Mid/11.png", "Image/P18_D2_P5_High_T1_3_Mid/12.png", "Image/P18_D2_P5_High_T1_3_Mid/13.png", "Image/P18_D2_P5_High_T1_3_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green vase to the top of the table. Finish.\"\nThe frames roughly correspond to the speech interval from 165.46930000000003 seconds to 184.4414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P5_High_T2_11_Left", "images": ["Image/P14_D2_P5_High_T2_11_Left/1.png", "Image/P14_D2_P5_High_T2_11_Left/2.png", "Image/P14_D2_P5_High_T2_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Fill up that bottle\"\nThe frames roughly correspond to the speech interval from 1897.2055 seconds to 1905.7754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P1_High_T2_7_Right", "images": ["Image/P16_D2_P1_High_T2_7_Right/1.png", "Image/P16_D2_P1_High_T2_7_Right/2.png", "Image/P16_D2_P1_High_T2_7_Right/3.png", "Image/P16_D2_P1_High_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 723.7436 seconds to 733.0563000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P9_High_T2_8_Mid", "images": ["Image/P20_D2_P9_High_T2_8_Mid/1.png", "Image/P20_D2_P9_High_T2_8_Mid/2.png", "Image/P20_D2_P9_High_T2_8_Mid/3.png", "Image/P20_D2_P9_High_T2_8_Mid/4.png", "Image/P20_D2_P9_High_T2_8_Mid/5.png", "Image/P20_D2_P9_High_T2_8_Mid/6.png", "Image/P20_D2_P9_High_T2_8_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into the green cup next to the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2937.226 seconds to 2950.2003000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P8_Low_T1_13_Left", "images": ["Image/P10_D3_P8_Low_T1_13_Left/1.png", "Image/P10_D3_P8_Low_T1_13_Left/2.png", "Image/P10_D3_P8_Low_T1_13_Left/3.png", "Image/P10_D3_P8_Low_T1_13_Left/4.png", "Image/P10_D3_P8_Low_T1_13_Left/5.png", "Image/P10_D3_P8_Low_T1_13_Left/6.png", "Image/P10_D3_P8_Low_T1_13_Left/7.png", "Image/P10_D3_P8_Low_T1_13_Left/8.png", "Image/P10_D3_P8_Low_T1_13_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 959.17709 seconds to 972.98589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P3_Low_T3_21_Right", "images": ["Image/P1_D1_P3_Low_T3_21_Right/1.png", "Image/P1_D1_P3_Low_T3_21_Right/2.png", "Image/P1_D1_P3_Low_T3_21_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 396.9572 seconds to 405.6861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P4_Medium_T2_14_Left", "images": ["Image/P29_D1_P4_Medium_T2_14_Left/1.png", "Image/P29_D1_P4_Medium_T2_14_Left/2.png", "Image/P29_D1_P4_Medium_T2_14_Left/3.png", "Image/P29_D1_P4_Medium_T2_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2735.2821 seconds to 2747.2374999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P6_Low_T2_24_Left", "images": ["Image/P19_D1_P6_Low_T2_24_Left/1.png", "Image/P19_D1_P6_Low_T2_24_Left/2.png", "Image/P19_D1_P6_Low_T2_24_Left/3.png", "Image/P19_D1_P6_Low_T2_24_Left/4.png", "Image/P19_D1_P6_Low_T2_24_Left/5.png", "Image/P19_D1_P6_Low_T2_24_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 920.3395 seconds to 938.2104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P3_Low_T2_1_Right", "images": ["Image/P1_D2_P3_Low_T2_1_Right/1.png", "Image/P1_D2_P3_Low_T2_1_Right/2.png", "Image/P1_D2_P3_Low_T2_1_Right/3.png", "Image/P1_D2_P3_Low_T2_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1004.8466 seconds to 1015.6367 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P3_Medium_T3_20_Left", "images": ["Image/P16_D2_P3_Medium_T3_20_Left/1.png", "Image/P16_D2_P3_Medium_T3_20_Left/2.png", "Image/P16_D2_P3_Medium_T3_20_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1044.3609 seconds to 1052.9638 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P8_Low_T2_5_Left", "images": ["Image/P30_D1_P8_Low_T2_5_Left/1.png", "Image/P30_D1_P8_Low_T2_5_Left/2.png", "Image/P30_D1_P8_Low_T2_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 843.9983000000001 seconds to 852.4455 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P9_Low_T2_22_Right", "images": ["Image/P18_D2_P9_Low_T2_22_Right/1.png", "Image/P18_D2_P9_Low_T2_22_Right/2.png", "Image/P18_D2_P9_Low_T2_22_Right/3.png", "Image/P18_D2_P9_Low_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water that bottom plant. Finish.\"\nThe frames roughly correspond to the speech interval from 442.7396 seconds to 454.4321 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P6_Medium_T3_15_Left", "images": ["Image/P18_D3_P6_Medium_T3_15_Left/1.png", "Image/P18_D3_P6_Medium_T3_15_Left/2.png", "Image/P18_D3_P6_Medium_T3_15_Left/3.png", "Image/P18_D3_P6_Medium_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1406.538 seconds to 1419.4696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P3_Low_T3_7_Mid", "images": ["Image/P15_D3_P3_Low_T3_7_Mid/1.png", "Image/P15_D3_P3_Low_T3_7_Mid/2.png", "Image/P15_D3_P3_Low_T3_7_Mid/3.png", "Image/P15_D3_P3_Low_T3_7_Mid/4.png", "Image/P15_D3_P3_Low_T3_7_Mid/5.png", "Image/P15_D3_P3_Low_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the cup at the top of the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 431.1483 seconds to 442.4086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P4_Medium_T1_7_Left", "images": ["Image/P5_D1_P4_Medium_T1_7_Left/1.png", "Image/P5_D1_P4_Medium_T1_7_Left/2.png", "Image/P5_D1_P4_Medium_T1_7_Left/3.png", "Image/P5_D1_P4_Medium_T1_7_Left/4.png", "Image/P5_D1_P4_Medium_T1_7_Left/5.png", "Image/P5_D1_P4_Medium_T1_7_Left/6.png", "Image/P5_D1_P4_Medium_T1_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the curve over here to here\"\nThe frames roughly correspond to the speech interval from 1982.6646 seconds to 1997.5001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P4_High_T3_5_Mid", "images": ["Image/P20_D3_P4_High_T3_5_Mid/1.png", "Image/P20_D3_P4_High_T3_5_Mid/2.png", "Image/P20_D3_P4_High_T3_5_Mid/3.png", "Image/P20_D3_P4_High_T3_5_Mid/4.png", "Image/P20_D3_P4_High_T3_5_Mid/5.png", "Image/P20_D3_P4_High_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant in the middle of the table? Finish.\"\nThe frames roughly correspond to the speech interval from 1347.1731 seconds to 1363.2092 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P4_High_T3_25_Left", "images": ["Image/P9_D1_P4_High_T3_25_Left/1.png", "Image/P9_D1_P4_High_T3_25_Left/2.png", "Image/P9_D1_P4_High_T3_25_Left/3.png", "Image/P9_D1_P4_High_T3_25_Left/4.png", "Image/P9_D1_P4_High_T3_25_Left/5.png", "Image/P9_D1_P4_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Would you help me to clean this white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1358.3673 seconds to 1370.9331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P6_Medium_T3_25_Mid", "images": ["Image/P3_D2_P6_Medium_T3_25_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you move the item to this space? Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2371.9391 seconds to 2381.165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P1_High_T3_23_Right", "images": ["Image/P16_D1_P1_High_T3_23_Right/1.png", "Image/P16_D1_P1_High_T3_23_Right/2.png", "Image/P16_D1_P1_High_T3_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1396.862 seconds to 1407.3436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P1_Medium_T3_15_Mid", "images": ["Image/P23_D3_P1_Medium_T3_15_Mid/1.png", "Image/P23_D3_P1_Medium_T3_15_Mid/2.png", "Image/P23_D3_P1_Medium_T3_15_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 867.8211000000001 seconds to 876.9104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P9_High_T3_25_Right", "images": ["Image/P24_D3_P9_High_T3_25_Right/1.png", "Image/P24_D3_P9_High_T3_25_Right/2.png", "Image/P24_D3_P9_High_T3_25_Right/3.png", "Image/P24_D3_P9_High_T3_25_Right/4.png", "Image/P24_D3_P9_High_T3_25_Right/5.png", "Image/P24_D3_P9_High_T3_25_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1756.6134 seconds to 1767.5254 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P7_High_T1_6_Left", "images": ["Image/P19_D3_P7_High_T1_6_Left/1.png", "Image/P19_D3_P7_High_T1_6_Left/2.png", "Image/P19_D3_P7_High_T1_6_Left/3.png", "Image/P19_D3_P7_High_T1_6_Left/4.png", "Image/P19_D3_P7_High_T1_6_Left/5.png", "Image/P19_D3_P7_High_T1_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that flowerpot up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2449.3273999999997 seconds to 2462.2875 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P4_High_T3_21_Right", "images": ["Image/P16_D1_P4_High_T3_21_Right/1.png", "Image/P16_D1_P4_High_T3_21_Right/2.png", "Image/P16_D1_P4_High_T3_21_Right/3.png", "Image/P16_D1_P4_High_T3_21_Right/4.png", "Image/P16_D1_P4_High_T3_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1273.5311 seconds to 1285.5022 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P6_Low_T3_25_Mid", "images": ["Image/P4_D3_P6_Low_T3_25_Mid/1.png", "Image/P4_D3_P6_Low_T3_25_Mid/2.png", "Image/P4_D3_P6_Low_T3_25_Mid/3.png", "Image/P4_D3_P6_Low_T3_25_Mid/4.png", "Image/P4_D3_P6_Low_T3_25_Mid/5.png", "Image/P4_D3_P6_Low_T3_25_Mid/6.png", "Image/P4_D3_P6_Low_T3_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the white bottle in the middle of that shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 442.1481999999999 seconds to 456.0259999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P3_Medium_T1_1_Left", "images": ["Image/P30_D2_P3_Medium_T1_1_Left/1.png", "Image/P30_D2_P3_Medium_T1_1_Left/2.png", "Image/P30_D2_P3_Medium_T1_1_Left/3.png", "Image/P30_D2_P3_Medium_T1_1_Left/4.png", "Image/P30_D2_P3_Medium_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that base down there. Finish.\"\nThe frames roughly correspond to the speech interval from 302.3319 seconds to 315.76329999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P3_High_T1_13_Right", "images": ["Image/P25_D3_P3_High_T1_13_Right/1.png", "Image/P25_D3_P3_High_T1_13_Right/2.png", "Image/P25_D3_P3_High_T1_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down there. Finish.\"\nThe frames roughly correspond to the speech interval from 2071.1621 seconds to 2080.7823 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P7_Low_T1_16_Mid", "images": ["Image/P13_D1_P7_Low_T1_16_Mid/1.png", "Image/P13_D1_P7_Low_T1_16_Mid/2.png", "Image/P13_D1_P7_Low_T1_16_Mid/3.png", "Image/P13_D1_P7_Low_T1_16_Mid/4.png", "Image/P13_D1_P7_Low_T1_16_Mid/5.png", "Image/P13_D1_P7_Low_T1_16_Mid/6.png", "Image/P13_D1_P7_Low_T1_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you please put the brown cup up here please? Finish.\"\nThe frames roughly correspond to the speech interval from 444.776 seconds to 456.9684 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P4_Medium_T3_16_Left", "images": ["Image/P22_D3_P4_Medium_T3_16_Left/1.png", "Image/P22_D3_P4_Medium_T3_16_Left/2.png", "Image/P22_D3_P4_Medium_T3_16_Left/3.png", "Image/P22_D3_P4_Medium_T3_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 143.3322 seconds to 153.53502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P1_Medium_T1_12_Mid", "images": ["Image/P27_D2_P1_Medium_T1_12_Mid/1.png", "Image/P27_D2_P1_Medium_T1_12_Mid/2.png", "Image/P27_D2_P1_Medium_T1_12_Mid/3.png", "Image/P27_D2_P1_Medium_T1_12_Mid/4.png", "Image/P27_D2_P1_Medium_T1_12_Mid/5.png", "Image/P27_D2_P1_Medium_T1_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the middle vase to the bottom of that shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1552.2082 seconds to 1564.192 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P8_Low_T2_8_Left", "images": ["Image/P28_D1_P8_Low_T2_8_Left/1.png", "Image/P28_D1_P8_Low_T2_8_Left/2.png", "Image/P28_D1_P8_Low_T2_8_Left/3.png", "Image/P28_D1_P8_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1385.51204 seconds to 1394.978 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P1_Medium_T1_1_Mid", "images": ["Image/P5_D1_P1_Medium_T1_1_Mid/1.png", "Image/P5_D1_P1_Medium_T1_1_Mid/2.png", "Image/P5_D1_P1_Medium_T1_1_Mid/3.png", "Image/P5_D1_P1_Medium_T1_1_Mid/4.png", "Image/P5_D1_P1_Medium_T1_1_Mid/5.png", "Image/P5_D1_P1_Medium_T1_1_Mid/6.png", "Image/P5_D1_P1_Medium_T1_1_Mid/7.png", "Image/P5_D1_P1_Medium_T1_1_Mid/8.png", "Image/P5_D1_P1_Medium_T1_1_Mid/9.png", "Image/P5_D1_P1_Medium_T1_1_Mid/10.png", "Image/P5_D1_P1_Medium_T1_1_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the middle bottle into the bucket to my left. Finished.\"\nThe frames roughly correspond to the speech interval from 2045.3733 seconds to 2061.2817999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P4_Low_T2_6_Mid", "images": ["Image/P15_D1_P4_Low_T2_6_Mid/1.png", "Image/P15_D1_P4_Low_T2_6_Mid/2.png", "Image/P15_D1_P4_Low_T2_6_Mid/3.png", "Image/P15_D1_P4_Low_T2_6_Mid/4.png", "Image/P15_D1_P4_Low_T2_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2088.1073699999997 seconds to 2099.24387 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P8_Low_T2_3_Mid", "images": ["Image/P6_D1_P8_Low_T2_3_Mid/1.png", "Image/P6_D1_P8_Low_T2_3_Mid/2.png", "Image/P6_D1_P8_Low_T2_3_Mid/3.png", "Image/P6_D1_P8_Low_T2_3_Mid/4.png", "Image/P6_D1_P8_Low_T2_3_Mid/5.png", "Image/P6_D1_P8_Low_T2_3_Mid/6.png", "Image/P6_D1_P8_Low_T2_3_Mid/7.png", "Image/P6_D1_P8_Low_T2_3_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2087.0303000000004 seconds to 2102.0906000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P1_High_T2_4_Right", "images": ["Image/P15_D1_P1_High_T2_4_Right/1.png", "Image/P15_D1_P1_High_T2_4_Right/2.png", "Image/P15_D1_P1_High_T2_4_Right/3.png", "Image/P15_D1_P1_High_T2_4_Right/4.png", "Image/P15_D1_P1_High_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2031.65007 seconds to 2042.90887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P2_Medium_T2_14_Mid", "images": ["Image/P2_D2_P2_Medium_T2_14_Mid/1.png", "Image/P2_D2_P2_Medium_T2_14_Mid/2.png", "Image/P2_D2_P2_Medium_T2_14_Mid/3.png", "Image/P2_D2_P2_Medium_T2_14_Mid/4.png", "Image/P2_D2_P2_Medium_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please water that plant in the middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2775.5407 seconds to 2788.0672999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P1_Low_T1_9_Left", "images": ["Image/P15_D3_P1_Low_T1_9_Left/1.png", "Image/P15_D3_P1_Low_T1_9_Left/2.png", "Image/P15_D3_P1_Low_T1_9_Left/3.png", "Image/P15_D3_P1_Low_T1_9_Left/4.png", "Image/P15_D3_P1_Low_T1_9_Left/5.png", "Image/P15_D3_P1_Low_T1_9_Left/6.png", "Image/P15_D3_P1_Low_T1_9_Left/7.png", "Image/P15_D3_P1_Low_T1_9_Left/8.png", "Image/P15_D3_P1_Low_T1_9_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 137.77320000000003 seconds to 153.7797 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P9_Medium_T3_17_Right", "images": ["Image/P12_D3_P9_Medium_T3_17_Right/1.png", "Image/P12_D3_P9_Medium_T3_17_Right/2.png", "Image/P12_D3_P9_Medium_T3_17_Right/3.png", "Image/P12_D3_P9_Medium_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2436.38429 seconds to 2448.11839 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_High_T2_22_Mid", "images": ["Image/P20_D2_P4_High_T2_22_Mid/1.png", "Image/P20_D2_P4_High_T2_22_Mid/2.png", "Image/P20_D2_P4_High_T2_22_Mid/3.png", "Image/P20_D2_P4_High_T2_22_Mid/4.png", "Image/P20_D2_P4_High_T2_22_Mid/5.png", "Image/P20_D2_P4_High_T2_22_Mid/6.png", "Image/P20_D2_P4_High_T2_22_Mid/7.png", "Image/P20_D2_P4_High_T2_22_Mid/8.png", "Image/P20_D2_P4_High_T2_22_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that plant on the table next to those thingy. Finish.\"\nThe frames roughly correspond to the speech interval from 2987.7698 seconds to 3003.1088000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P5_Medium_T2_2_Mid", "images": ["Image/P15_D3_P5_Medium_T2_2_Mid/1.png", "Image/P15_D3_P5_Medium_T2_2_Mid/2.png", "Image/P15_D3_P5_Medium_T2_2_Mid/3.png", "Image/P15_D3_P5_Medium_T2_2_Mid/4.png", "Image/P15_D3_P5_Medium_T2_2_Mid/5.png", "Image/P15_D3_P5_Medium_T2_2_Mid/6.png", "Image/P15_D3_P5_Medium_T2_2_Mid/7.png", "Image/P15_D3_P5_Medium_T2_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the middle pot in the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 670.8897 seconds to 684.3507 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P9_High_T3_15_Right", "images": ["Image/P4_D1_P9_High_T3_15_Right/1.png", "Image/P4_D1_P9_High_T3_15_Right/2.png", "Image/P4_D1_P9_High_T3_15_Right/3.png", "Image/P4_D1_P9_High_T3_15_Right/4.png", "Image/P4_D1_P9_High_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 2115.55098 seconds to 2131.09601 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P2_Low_T2_25_Mid", "images": ["Image/P11_D2_P2_Low_T2_25_Mid/1.png", "Image/P11_D2_P2_Low_T2_25_Mid/2.png", "Image/P11_D2_P2_Low_T2_25_Mid/3.png", "Image/P11_D2_P2_Low_T2_25_Mid/4.png", "Image/P11_D2_P2_Low_T2_25_Mid/5.png", "Image/P11_D2_P2_Low_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour the water in a white glass\"\nThe frames roughly correspond to the speech interval from 807.4186 seconds to 819.8605 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P9_Low_T2_4_Left", "images": ["Image/P8_D2_P9_Low_T2_4_Left/1.png", "Image/P8_D2_P9_Low_T2_4_Left/2.png", "Image/P8_D2_P9_Low_T2_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"water that planet. Finish\"\nThe frames roughly correspond to the speech interval from 2549.2007000000003 seconds to 2559.3634 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P7_Low_T1_11_Left", "images": ["Image/P13_D2_P7_Low_T1_11_Left/1.png", "Image/P13_D2_P7_Low_T1_11_Left/2.png", "Image/P13_D2_P7_Low_T1_11_Left/3.png", "Image/P13_D2_P7_Low_T1_11_Left/4.png", "Image/P13_D2_P7_Low_T1_11_Left/5.png", "Image/P13_D2_P7_Low_T1_11_Left/6.png", "Image/P13_D2_P7_Low_T1_11_Left/7.png", "Image/P13_D2_P7_Low_T1_11_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1001.69796 seconds to 1014.71286 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P7_Low_T1_1_Right", "images": ["Image/P13_D3_P7_Low_T1_1_Right/1.png", "Image/P13_D3_P7_Low_T1_1_Right/2.png", "Image/P13_D3_P7_Low_T1_1_Right/3.png", "Image/P13_D3_P7_Low_T1_1_Right/4.png", "Image/P13_D3_P7_Low_T1_1_Right/5.png", "Image/P13_D3_P7_Low_T1_1_Right/6.png", "Image/P13_D3_P7_Low_T1_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the green bottle from the floor to the top shelf\"\nThe frames roughly correspond to the speech interval from 1426.64116 seconds to 1438.7232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P3_Low_T3_26_Left", "images": ["Image/P27_D2_P3_Low_T3_26_Left/1.png", "Image/P27_D2_P3_Low_T3_26_Left/2.png", "Image/P27_D2_P3_Low_T3_26_Left/3.png", "Image/P27_D2_P3_Low_T3_26_Left/4.png", "Image/P27_D2_P3_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1500.0794999999998 seconds to 1510.0157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P1_High_T1_2_Left", "images": ["Image/P24_D1_P1_High_T1_2_Left/1.png", "Image/P24_D1_P1_High_T1_2_Left/2.png", "Image/P24_D1_P1_High_T1_2_Left/3.png", "Image/P24_D1_P1_High_T1_2_Left/4.png", "Image/P24_D1_P1_High_T1_2_Left/5.png", "Image/P24_D1_P1_High_T1_2_Left/6.png", "Image/P24_D1_P1_High_T1_2_Left/7.png", "Image/P24_D1_P1_High_T1_2_Left/8.png", "Image/P24_D1_P1_High_T1_2_Left/9.png", "Image/P24_D1_P1_High_T1_2_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you please move this bottle over there\"\nThe frames roughly correspond to the speech interval from 1097.0412 seconds to 1112.0328 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P7_Low_T1_11_Right", "images": ["Image/P9_D3_P7_Low_T1_11_Right/1.png", "Image/P9_D3_P7_Low_T1_11_Right/2.png", "Image/P9_D3_P7_Low_T1_11_Right/3.png", "Image/P9_D3_P7_Low_T1_11_Right/4.png", "Image/P9_D3_P7_Low_T1_11_Right/5.png", "Image/P9_D3_P7_Low_T1_11_Right/6.png", "Image/P9_D3_P7_Low_T1_11_Right/7.png", "Image/P9_D3_P7_Low_T1_11_Right/8.png", "Image/P9_D3_P7_Low_T1_11_Right/9.png", "Image/P9_D3_P7_Low_T1_11_Right/10.png", "Image/P9_D3_P7_Low_T1_11_Right/11.png", "Image/P9_D3_P7_Low_T1_11_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the red bottle from the floor to the top layer of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1004.8549 seconds to 1024.9202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P2_Low_T3_25_Left", "images": ["Image/P15_D1_P2_Low_T3_25_Left/1.png", "Image/P15_D1_P2_Low_T3_25_Left/2.png", "Image/P15_D1_P2_Low_T3_25_Left/3.png", "Image/P15_D1_P2_Low_T3_25_Left/4.png", "Image/P15_D1_P2_Low_T3_25_Left/5.png", "Image/P15_D1_P2_Low_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1467.0160099999998 seconds to 1479.98743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P1_Low_T2_3_Mid", "images": ["Image/P12_D3_P1_Low_T2_3_Mid/1.png", "Image/P12_D3_P1_Low_T2_3_Mid/2.png", "Image/P12_D3_P1_Low_T2_3_Mid/3.png", "Image/P12_D3_P1_Low_T2_3_Mid/4.png", "Image/P12_D3_P1_Low_T2_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2532.11109 seconds to 2543.37139 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P6_Medium_T3_17_Right", "images": ["Image/P2_D1_P6_Medium_T3_17_Right/1.png", "Image/P2_D1_P6_Medium_T3_17_Right/2.png", "Image/P2_D1_P6_Medium_T3_17_Right/3.png", "Image/P2_D1_P6_Medium_T3_17_Right/4.png", "Image/P2_D1_P6_Medium_T3_17_Right/5.png", "Image/P2_D1_P6_Medium_T3_17_Right/6.png", "Image/P2_D1_P6_Medium_T3_17_Right/7.png", "Image/P2_D1_P6_Medium_T3_17_Right/8.png", "Image/P2_D1_P6_Medium_T3_17_Right/9.png", "Image/P2_D1_P6_Medium_T3_17_Right/10.png", "Image/P2_D1_P6_Medium_T3_17_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 488.3615 seconds to 506.3167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P8_Low_T2_16_Left", "images": ["Image/P18_D1_P8_Low_T2_16_Left/1.png", "Image/P18_D1_P8_Low_T2_16_Left/2.png", "Image/P18_D1_P8_Low_T2_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1098.0952 seconds to 1106.1129 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P5_Medium_T2_18_Mid", "images": ["Image/P18_D2_P5_Medium_T2_18_Mid/1.png", "Image/P18_D2_P5_Medium_T2_18_Mid/2.png", "Image/P18_D2_P5_Medium_T2_18_Mid/3.png", "Image/P18_D2_P5_Medium_T2_18_Mid/4.png", "Image/P18_D2_P5_Medium_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour water into the middle cup. Finish.\"\nThe frames roughly correspond to the speech interval from 424.4485 seconds to 434.9167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P7_High_T2_5_Mid", "images": ["Image/P16_D2_P7_High_T2_5_Mid/1.png", "Image/P16_D2_P7_High_T2_5_Mid/2.png", "Image/P16_D2_P7_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 974.0649 seconds to 982.7926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P6_Medium_T2_23_Left", "images": ["Image/P10_D2_P6_Medium_T2_23_Left/1.png", "Image/P10_D2_P6_Medium_T2_23_Left/2.png", "Image/P10_D2_P6_Medium_T2_23_Left/3.png", "Image/P10_D2_P6_Medium_T2_23_Left/4.png", "Image/P10_D2_P6_Medium_T2_23_Left/5.png", "Image/P10_D2_P6_Medium_T2_23_Left/6.png", "Image/P10_D2_P6_Medium_T2_23_Left/7.png", "Image/P10_D2_P6_Medium_T2_23_Left/8.png", "Image/P10_D2_P6_Medium_T2_23_Left/9.png", "Image/P10_D2_P6_Medium_T2_23_Left/10.png", "Image/P10_D2_P6_Medium_T2_23_Left/11.png", "Image/P10_D2_P6_Medium_T2_23_Left/12.png", "Image/P10_D2_P6_Medium_T2_23_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1532.51618 seconds to 1551.0434800000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P8_Low_T2_11_Mid", "images": ["Image/P17_D3_P8_Low_T2_11_Mid/1.png", "Image/P17_D3_P8_Low_T2_11_Mid/2.png", "Image/P17_D3_P8_Low_T2_11_Mid/3.png", "Image/P17_D3_P8_Low_T2_11_Mid/4.png", "Image/P17_D3_P8_Low_T2_11_Mid/5.png", "Image/P17_D3_P8_Low_T2_11_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that brown vase on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 831.4213500000001 seconds to 845.5496 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P1_High_T1_24_Left", "images": ["Image/P3_D2_P1_High_T1_24_Left/1.png", "Image/P3_D2_P1_High_T1_24_Left/2.png", "Image/P3_D2_P1_High_T1_24_Left/3.png", "Image/P3_D2_P1_High_T1_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 2322.9151 seconds to 2333.8571 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P4_Medium_T3_7_Left", "images": ["Image/P24_D2_P4_Medium_T3_7_Left/1.png", "Image/P24_D2_P4_Medium_T3_7_Left/2.png", "Image/P24_D2_P4_Medium_T3_7_Left/3.png", "Image/P24_D2_P4_Medium_T3_7_Left/4.png", "Image/P24_D2_P4_Medium_T3_7_Left/5.png", "Image/P24_D2_P4_Medium_T3_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 102.49911 seconds to 114.30138999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P7_Low_T3_5_Left", "images": ["Image/P16_D1_P7_Low_T3_5_Left/1.png", "Image/P16_D1_P7_Low_T3_5_Left/2.png", "Image/P16_D1_P7_Low_T3_5_Left/3.png", "Image/P16_D1_P7_Low_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1299.0193 seconds to 1308.234 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P7_Medium_T2_7_Mid", "images": ["Image/P13_D2_P7_Medium_T2_7_Mid/1.png", "Image/P13_D2_P7_Medium_T2_7_Mid/2.png", "Image/P13_D2_P7_Medium_T2_7_Mid/3.png", "Image/P13_D2_P7_Medium_T2_7_Mid/4.png", "Image/P13_D2_P7_Medium_T2_7_Mid/5.png", "Image/P13_D2_P7_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you fill up the middle green glass with water\"\nThe frames roughly correspond to the speech interval from 840.8384900000001 seconds to 852.1985500000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P4_High_T3_13_Left", "images": ["Image/P2_D3_P4_High_T3_13_Left/1.png", "Image/P2_D3_P4_High_T3_13_Left/2.png", "Image/P2_D3_P4_High_T3_13_Left/3.png", "Image/P2_D3_P4_High_T3_13_Left/4.png", "Image/P2_D3_P4_High_T3_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1780.9569 seconds to 1796.3927000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P7_High_T3_2_Left", "images": ["Image/P23_D2_P7_High_T3_2_Left/1.png", "Image/P23_D2_P7_High_T3_2_Left/2.png", "Image/P23_D2_P7_High_T3_2_Left/3.png", "Image/P23_D2_P7_High_T3_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the thing on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 288.72092 seconds to 299.0084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P9_Low_T3_24_Right", "images": ["Image/P11_D3_P9_Low_T3_24_Right/1.png", "Image/P11_D3_P9_Low_T3_24_Right/2.png", "Image/P11_D3_P9_Low_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2003.3274 seconds to 2011.9852 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P5_Medium_T2_22_Mid", "images": ["Image/P2_D1_P5_Medium_T2_22_Mid/1.png", "Image/P2_D1_P5_Medium_T2_22_Mid/2.png", "Image/P2_D1_P5_Medium_T2_22_Mid/3.png", "Image/P2_D1_P5_Medium_T2_22_Mid/4.png", "Image/P2_D1_P5_Medium_T2_22_Mid/5.png", "Image/P2_D1_P5_Medium_T2_22_Mid/6.png", "Image/P2_D1_P5_Medium_T2_22_Mid/7.png", "Image/P2_D1_P5_Medium_T2_22_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Hello. Could you please water the plant on the very right side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 650.7243 seconds to 665.8384 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P3_Medium_T3_18_Right", "images": ["Image/P11_D3_P3_Medium_T3_18_Right/1.png", "Image/P11_D3_P3_Medium_T3_18_Right/2.png", "Image/P11_D3_P3_Medium_T3_18_Right/3.png", "Image/P11_D3_P3_Medium_T3_18_Right/4.png", "Image/P11_D3_P3_Medium_T3_18_Right/5.png", "Image/P11_D3_P3_Medium_T3_18_Right/6.png", "Image/P11_D3_P3_Medium_T3_18_Right/7.png", "Image/P11_D3_P3_Medium_T3_18_Right/8.png", "Image/P11_D3_P3_Medium_T3_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first glass from the right side on the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1618.9003 seconds to 1633.9885 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P9_Medium_T3_2_Left", "images": ["Image/P26_D3_P9_Medium_T3_2_Left/1.png", "Image/P26_D3_P9_Medium_T3_2_Left/2.png", "Image/P26_D3_P9_Medium_T3_2_Left/3.png", "Image/P26_D3_P9_Medium_T3_2_Left/4.png", "Image/P26_D3_P9_Medium_T3_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the leftmost face down there? Finish.\"\nThe frames roughly correspond to the speech interval from 671.51556 seconds to 686.22879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P7_Medium_T1_19_Mid", "images": ["Image/P16_D2_P7_Medium_T1_19_Mid/1.png", "Image/P16_D2_P7_Medium_T1_19_Mid/2.png", "Image/P16_D2_P7_Medium_T1_19_Mid/3.png", "Image/P16_D2_P7_Medium_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 761.2017000000001 seconds to 770.9187000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P5_High_T2_6_Left", "images": ["Image/P29_D1_P5_High_T2_6_Left/1.png", "Image/P29_D1_P5_High_T2_6_Left/2.png", "Image/P29_D1_P5_High_T2_6_Left/3.png", "Image/P29_D1_P5_High_T2_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hey Friday\"\nThe frames roughly correspond to the speech interval from 2631.4952 seconds to 2641.2508 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P2_Medium_T2_27_Mid", "images": ["Image/P4_D1_P2_Medium_T2_27_Mid/1.png", "Image/P4_D1_P2_Medium_T2_27_Mid/2.png", "Image/P4_D1_P2_Medium_T2_27_Mid/3.png", "Image/P4_D1_P2_Medium_T2_27_Mid/4.png", "Image/P4_D1_P2_Medium_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the middle bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2346.7362 seconds to 2359.861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P5_Low_T3_23_Mid", "images": ["Image/P5_D1_P5_Low_T3_23_Mid/1.png", "Image/P5_D1_P5_Low_T3_23_Mid/2.png", "Image/P5_D1_P5_Low_T3_23_Mid/3.png", "Image/P5_D1_P5_Low_T3_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1857.5929 seconds to 1868.0726 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P5_Low_T2_2_Mid", "images": ["Image/P21_D2_P5_Low_T2_2_Mid/1.png", "Image/P21_D2_P5_Low_T2_2_Mid/2.png", "Image/P21_D2_P5_Low_T2_2_Mid/3.png", "Image/P21_D2_P5_Low_T2_2_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1724.6729 seconds to 1735.3074 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P8_Medium_T2_22_Left", "images": ["Image/P29_D2_P8_Medium_T2_22_Left/1.png", "Image/P29_D2_P8_Medium_T2_22_Left/2.png", "Image/P29_D2_P8_Medium_T2_22_Left/3.png", "Image/P29_D2_P8_Medium_T2_22_Left/4.png", "Image/P29_D2_P8_Medium_T2_22_Left/5.png", "Image/P29_D2_P8_Medium_T2_22_Left/6.png", "Image/P29_D2_P8_Medium_T2_22_Left/7.png", "Image/P29_D2_P8_Medium_T2_22_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 444.4964 seconds to 458.2076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P7_Low_T3_26_Left", "images": ["Image/P17_D2_P7_Low_T3_26_Left/1.png", "Image/P17_D2_P7_Low_T3_26_Left/2.png", "Image/P17_D2_P7_Low_T3_26_Left/3.png", "Image/P17_D2_P7_Low_T3_26_Left/4.png", "Image/P17_D2_P7_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that white mug down there? Finish.\"\nThe frames roughly correspond to the speech interval from 402.5454 seconds to 414.7528999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P3_Low_T3_21_Right", "images": ["Image/P18_D3_P3_Low_T3_21_Right/1.png", "Image/P18_D3_P3_Low_T3_21_Right/2.png", "Image/P18_D3_P3_Low_T3_21_Right/3.png", "Image/P18_D3_P3_Low_T3_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1319.3857 seconds to 1328.8932 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P5_Low_T3_6_Right", "images": ["Image/P8_D3_P5_Low_T3_6_Right/1.png", "Image/P8_D3_P5_Low_T3_6_Right/2.png", "Image/P8_D3_P5_Low_T3_6_Right/3.png", "Image/P8_D3_P5_Low_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you clean that planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1748.957 seconds to 1758.4501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P1_High_T3_21_Right", "images": ["Image/P6_D3_P1_High_T3_21_Right/1.png", "Image/P6_D3_P1_High_T3_21_Right/2.png", "Image/P6_D3_P1_High_T3_21_Right/3.png", "Image/P6_D3_P1_High_T3_21_Right/4.png", "Image/P6_D3_P1_High_T3_21_Right/5.png", "Image/P6_D3_P1_High_T3_21_Right/6.png", "Image/P6_D3_P1_High_T3_21_Right/7.png", "Image/P6_D3_P1_High_T3_21_Right/8.png", "Image/P6_D3_P1_High_T3_21_Right/9.png", "Image/P6_D3_P1_High_T3_21_Right/10.png", "Image/P6_D3_P1_High_T3_21_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3516.1292 seconds to 3532.7766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P7_Medium_T1_10_Mid", "images": ["Image/P24_D3_P7_Medium_T1_10_Mid/1.png", "Image/P24_D3_P7_Medium_T1_10_Mid/2.png", "Image/P24_D3_P7_Medium_T1_10_Mid/3.png", "Image/P24_D3_P7_Medium_T1_10_Mid/4.png", "Image/P24_D3_P7_Medium_T1_10_Mid/5.png", "Image/P24_D3_P7_Medium_T1_10_Mid/6.png", "Image/P24_D3_P7_Medium_T1_10_Mid/7.png", "Image/P24_D3_P7_Medium_T1_10_Mid/8.png", "Image/P24_D3_P7_Medium_T1_10_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1868.8931 seconds to 1883.3687 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P6_Low_T3_2_Left", "images": ["Image/P8_D3_P6_Low_T3_2_Left/1.png", "Image/P8_D3_P6_Low_T3_2_Left/2.png", "Image/P8_D3_P6_Low_T3_2_Left/3.png", "Image/P8_D3_P6_Low_T3_2_Left/4.png", "Image/P8_D3_P6_Low_T3_2_Left/5.png", "Image/P8_D3_P6_Low_T3_2_Left/6.png", "Image/P8_D3_P6_Low_T3_2_Left/7.png", "Image/P8_D3_P6_Low_T3_2_Left/8.png", "Image/P8_D3_P6_Low_T3_2_Left/9.png", "Image/P8_D3_P6_Low_T3_2_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 995.84375 seconds to 1006.18841 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P1_Medium_T2_22_Mid", "images": ["Image/P17_D1_P1_Medium_T2_22_Mid/1.png", "Image/P17_D1_P1_Medium_T2_22_Mid/2.png", "Image/P17_D1_P1_Medium_T2_22_Mid/3.png", "Image/P17_D1_P1_Medium_T2_22_Mid/4.png", "Image/P17_D1_P1_Medium_T2_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1583.8461000000002 seconds to 1595.9425 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P9_High_T2_3_Left", "images": ["Image/P6_D2_P9_High_T2_3_Left/1.png", "Image/P6_D2_P9_High_T2_3_Left/2.png", "Image/P6_D2_P9_High_T2_3_Left/3.png", "Image/P6_D2_P9_High_T2_3_Left/4.png", "Image/P6_D2_P9_High_T2_3_Left/5.png", "Image/P6_D2_P9_High_T2_3_Left/6.png", "Image/P6_D2_P9_High_T2_3_Left/7.png", "Image/P6_D2_P9_High_T2_3_Left/8.png", "Image/P6_D2_P9_High_T2_3_Left/9.png", "Image/P6_D2_P9_High_T2_3_Left/10.png", "Image/P6_D2_P9_High_T2_3_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1086.9128 seconds to 1104.1053 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P2_Medium_T3_18_Mid", "images": ["Image/P26_D1_P2_Medium_T3_18_Mid/1.png", "Image/P26_D1_P2_Medium_T3_18_Mid/2.png", "Image/P26_D1_P2_Medium_T3_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the middle vase? Finish.\"\nThe frames roughly correspond to the speech interval from 188.5424 seconds to 202.1557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P3_High_T3_16_Right", "images": ["Image/P25_D2_P3_High_T3_16_Right/1.png", "Image/P25_D2_P3_High_T3_16_Right/2.png", "Image/P25_D2_P3_High_T3_16_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 756.2682 seconds to 764.759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P2_Low_T1_25_Mid", "images": ["Image/P5_D2_P2_Low_T1_25_Mid/1.png", "Image/P5_D2_P2_Low_T1_25_Mid/2.png", "Image/P5_D2_P2_Low_T1_25_Mid/3.png", "Image/P5_D2_P2_Low_T1_25_Mid/4.png", "Image/P5_D2_P2_Low_T1_25_Mid/5.png", "Image/P5_D2_P2_Low_T1_25_Mid/6.png", "Image/P5_D2_P2_Low_T1_25_Mid/7.png", "Image/P5_D2_P2_Low_T1_25_Mid/8.png", "Image/P5_D2_P2_Low_T1_25_Mid/9.png", "Image/P5_D2_P2_Low_T1_25_Mid/10.png", "Image/P5_D2_P2_Low_T1_25_Mid/11.png", "Image/P5_D2_P2_Low_T1_25_Mid/12.png", "Image/P5_D2_P2_Low_T1_25_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the cup in front of me to the shelf\"\nThe frames roughly correspond to the speech interval from 319.5582 seconds to 338.20869999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P1_Low_T1_10_Mid", "images": ["Image/P19_D2_P1_Low_T1_10_Mid/1.png", "Image/P19_D2_P1_Low_T1_10_Mid/2.png", "Image/P19_D2_P1_Low_T1_10_Mid/3.png", "Image/P19_D2_P1_Low_T1_10_Mid/4.png", "Image/P19_D2_P1_Low_T1_10_Mid/5.png", "Image/P19_D2_P1_Low_T1_10_Mid/6.png", "Image/P19_D2_P1_Low_T1_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container down here.\"\nThe frames roughly correspond to the speech interval from 1729.9913 seconds to 1747.2225999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P9_Low_T1_15_Mid", "images": ["Image/P12_D3_P9_Low_T1_15_Mid/1.png", "Image/P12_D3_P9_Low_T1_15_Mid/2.png", "Image/P12_D3_P9_Low_T1_15_Mid/3.png", "Image/P12_D3_P9_Low_T1_15_Mid/4.png", "Image/P12_D3_P9_Low_T1_15_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2474.26009 seconds to 2486.8283899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P5_High_T1_6_Right", "images": ["Image/P1_D2_P5_High_T1_6_Right/1.png", "Image/P1_D2_P5_High_T1_6_Right/2.png", "Image/P1_D2_P5_High_T1_6_Right/3.png", "Image/P1_D2_P5_High_T1_6_Right/4.png", "Image/P1_D2_P5_High_T1_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 771.02713 seconds to 781.31479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P5_High_T1_23_Right", "images": ["Image/P3_D3_P5_High_T1_23_Right/1.png", "Image/P3_D3_P5_High_T1_23_Right/2.png", "Image/P3_D3_P5_High_T1_23_Right/3.png", "Image/P3_D3_P5_High_T1_23_Right/4.png", "Image/P3_D3_P5_High_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 829.0887 seconds to 840.1686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P7_Medium_T1_15_Right", "images": ["Image/P21_D1_P7_Medium_T1_15_Right/1.png", "Image/P21_D1_P7_Medium_T1_15_Right/2.png", "Image/P21_D1_P7_Medium_T1_15_Right/3.png", "Image/P21_D1_P7_Medium_T1_15_Right/4.png", "Image/P21_D1_P7_Medium_T1_15_Right/5.png", "Image/P21_D1_P7_Medium_T1_15_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1138.346 seconds to 1151.1799 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P5_Medium_T2_5_Mid", "images": ["Image/P11_D1_P5_Medium_T2_5_Mid/1.png", "Image/P11_D1_P5_Medium_T2_5_Mid/2.png", "Image/P11_D1_P5_Medium_T2_5_Mid/3.png", "Image/P11_D1_P5_Medium_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the second plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2394.21386 seconds to 2406.21251 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P7_High_T3_4_Mid", "images": ["Image/P2_D1_P7_High_T3_4_Mid/1.png", "Image/P2_D1_P7_High_T3_4_Mid/2.png", "Image/P2_D1_P7_High_T3_4_Mid/3.png", "Image/P2_D1_P7_High_T3_4_Mid/4.png", "Image/P2_D1_P7_High_T3_4_Mid/5.png", "Image/P2_D1_P7_High_T3_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 972.3164 seconds to 986.4286 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P1_High_T1_15_Left", "images": ["Image/P26_D3_P1_High_T1_15_Left/1.png", "Image/P26_D3_P1_High_T1_15_Left/2.png", "Image/P26_D3_P1_High_T1_15_Left/3.png", "Image/P26_D3_P1_High_T1_15_Left/4.png", "Image/P26_D3_P1_High_T1_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant down on the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 732.9582 seconds to 743.3838000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P7_High_T3_11_Mid", "images": ["Image/P17_D3_P7_High_T3_11_Mid/1.png", "Image/P17_D3_P7_High_T3_11_Mid/2.png", "Image/P17_D3_P7_High_T3_11_Mid/3.png", "Image/P17_D3_P7_High_T3_11_Mid/4.png", "Image/P17_D3_P7_High_T3_11_Mid/5.png", "Image/P17_D3_P7_High_T3_11_Mid/6.png", "Image/P17_D3_P7_High_T3_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the brown vase on the floor under the desk? Finish\"\nThe frames roughly correspond to the speech interval from 1177.7473 seconds to 1190.5949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P1_Low_T3_23_Right", "images": ["Image/P6_D3_P1_Low_T3_23_Right/1.png", "Image/P6_D3_P1_Low_T3_23_Right/2.png", "Image/P6_D3_P1_Low_T3_23_Right/3.png", "Image/P6_D3_P1_Low_T3_23_Right/4.png", "Image/P6_D3_P1_Low_T3_23_Right/5.png", "Image/P6_D3_P1_Low_T3_23_Right/6.png", "Image/P6_D3_P1_Low_T3_23_Right/7.png", "Image/P6_D3_P1_Low_T3_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3488.1356 seconds to 3502.0421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P5_High_T2_27_Right", "images": ["Image/P26_D1_P5_High_T2_27_Right/1.png", "Image/P26_D1_P5_High_T2_27_Right/2.png", "Image/P26_D1_P5_High_T2_27_Right/3.png", "Image/P26_D1_P5_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into this white jug? Finish\"\nThe frames roughly correspond to the speech interval from 347.2422 seconds to 359.1291 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P9_High_T1_8_Mid", "images": ["Image/P24_D1_P9_High_T1_8_Mid/1.png", "Image/P24_D1_P9_High_T1_8_Mid/2.png", "Image/P24_D1_P9_High_T1_8_Mid/3.png", "Image/P24_D1_P9_High_T1_8_Mid/4.png", "Image/P24_D1_P9_High_T1_8_Mid/5.png", "Image/P24_D1_P9_High_T1_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this one over here\"\nThe frames roughly correspond to the speech interval from 985.251 seconds to 997.6662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P3_Low_T1_21_Mid", "images": ["Image/P24_D1_P3_Low_T1_21_Mid/1.png", "Image/P24_D1_P3_Low_T1_21_Mid/2.png", "Image/P24_D1_P3_Low_T1_21_Mid/3.png", "Image/P24_D1_P3_Low_T1_21_Mid/4.png", "Image/P24_D1_P3_Low_T1_21_Mid/5.png", "Image/P24_D1_P3_Low_T1_21_Mid/6.png", "Image/P24_D1_P3_Low_T1_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this bottle over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1072.6685 seconds to 1084.6657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P5_High_T2_17_Right", "images": ["Image/P13_D1_P5_High_T2_17_Right/1.png", "Image/P13_D1_P5_High_T2_17_Right/2.png", "Image/P13_D1_P5_High_T2_17_Right/3.png", "Image/P13_D1_P5_High_T2_17_Right/4.png", "Image/P13_D1_P5_High_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you pour water into this cup right here? Finish.\"\nThe frames roughly correspond to the speech interval from 176.4421 seconds to 188.09081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P4_Medium_T2_21_Left", "images": ["Image/P18_D3_P4_Medium_T2_21_Left/1.png", "Image/P18_D3_P4_Medium_T2_21_Left/2.png", "Image/P18_D3_P4_Medium_T2_21_Left/3.png", "Image/P18_D3_P4_Medium_T2_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1549.928 seconds to 1561.0076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P8_Low_T2_27_Left", "images": ["Image/P18_D3_P8_Low_T2_27_Left/1.png", "Image/P18_D3_P8_Low_T2_27_Left/2.png", "Image/P18_D3_P8_Low_T2_27_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1387.0647 seconds to 1396.6691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P5_Medium_T2_22_Left", "images": ["Image/P7_D1_P5_Medium_T2_22_Left/1.png", "Image/P7_D1_P5_Medium_T2_22_Left/2.png", "Image/P7_D1_P5_Medium_T2_22_Left/3.png", "Image/P7_D1_P5_Medium_T2_22_Left/4.png", "Image/P7_D1_P5_Medium_T2_22_Left/5.png", "Image/P7_D1_P5_Medium_T2_22_Left/6.png", "Image/P7_D1_P5_Medium_T2_22_Left/7.png", "Image/P7_D1_P5_Medium_T2_22_Left/8.png", "Image/P7_D1_P5_Medium_T2_22_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 377.2671 seconds to 394.402 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P6_High_T2_10_Left", "images": ["Image/P17_D2_P6_High_T2_10_Left/1.png", "Image/P17_D2_P6_High_T2_10_Left/2.png", "Image/P17_D2_P6_High_T2_10_Left/3.png", "Image/P17_D2_P6_High_T2_10_Left/4.png", "Image/P17_D2_P6_High_T2_10_Left/5.png", "Image/P17_D2_P6_High_T2_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that brown cup right there? Finish.\"\nThe frames roughly correspond to the speech interval from 464.7547 seconds to 477.4629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P7_Low_T1_20_Mid", "images": ["Image/P10_D3_P7_Low_T1_20_Mid/1.png", "Image/P10_D3_P7_Low_T1_20_Mid/2.png", "Image/P10_D3_P7_Low_T1_20_Mid/3.png", "Image/P10_D3_P7_Low_T1_20_Mid/4.png", "Image/P10_D3_P7_Low_T1_20_Mid/5.png", "Image/P10_D3_P7_Low_T1_20_Mid/6.png", "Image/P10_D3_P7_Low_T1_20_Mid/7.png", "Image/P10_D3_P7_Low_T1_20_Mid/8.png", "Image/P10_D3_P7_Low_T1_20_Mid/9.png", "Image/P10_D3_P7_Low_T1_20_Mid/10.png", "Image/P10_D3_P7_Low_T1_20_Mid/11.png", "Image/P10_D3_P7_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 521.46539 seconds to 538.75329 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P1_Medium_T3_5_Right", "images": ["Image/P19_D2_P1_Medium_T3_5_Right/1.png", "Image/P19_D2_P1_Medium_T3_5_Right/2.png", "Image/P19_D2_P1_Medium_T3_5_Right/3.png", "Image/P19_D2_P1_Medium_T3_5_Right/4.png", "Image/P19_D2_P1_Medium_T3_5_Right/5.png", "Image/P19_D2_P1_Medium_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1779.3505 seconds to 1792.0587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P9_Low_T1_7_Mid", "images": ["Image/P9_D1_P9_Low_T1_7_Mid/1.png", "Image/P9_D1_P9_Low_T1_7_Mid/2.png", "Image/P9_D1_P9_Low_T1_7_Mid/3.png", "Image/P9_D1_P9_Low_T1_7_Mid/4.png", "Image/P9_D1_P9_Low_T1_7_Mid/5.png", "Image/P9_D1_P9_Low_T1_7_Mid/6.png", "Image/P9_D1_P9_Low_T1_7_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the green cup in the box on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1885.1361 seconds to 1903.4104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P4_Medium_T3_5_Left", "images": ["Image/P9_D2_P4_Medium_T3_5_Left/1.png", "Image/P9_D2_P4_Medium_T3_5_Left/2.png", "Image/P9_D2_P4_Medium_T3_5_Left/3.png", "Image/P9_D2_P4_Medium_T3_5_Left/4.png", "Image/P9_D2_P4_Medium_T3_5_Left/5.png", "Image/P9_D2_P4_Medium_T3_5_Left/6.png", "Image/P9_D2_P4_Medium_T3_5_Left/7.png", "Image/P9_D2_P4_Medium_T3_5_Left/8.png", "Image/P9_D2_P4_Medium_T3_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the left plant on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2849.8638 seconds to 2864.8476 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P3_High_T2_14_Left", "images": ["Image/P22_D1_P3_High_T2_14_Left/1.png", "Image/P22_D1_P3_High_T2_14_Left/2.png", "Image/P22_D1_P3_High_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1326.8468 seconds to 1338.46773 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P9_Low_T2_15_Mid", "images": ["Image/P29_D1_P9_Low_T2_15_Mid/1.png", "Image/P29_D1_P9_Low_T2_15_Mid/2.png", "Image/P29_D1_P9_Low_T2_15_Mid/3.png", "Image/P29_D1_P9_Low_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2755.2846999999997 seconds to 2764.9019999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P4_Low_T3_9_Right", "images": ["Image/P7_D2_P4_Low_T3_9_Right/1.png", "Image/P7_D2_P4_Low_T3_9_Right/2.png", "Image/P7_D2_P4_Low_T3_9_Right/3.png", "Image/P7_D2_P4_Low_T3_9_Right/4.png", "Image/P7_D2_P4_Low_T3_9_Right/5.png", "Image/P7_D2_P4_Low_T3_9_Right/6.png", "Image/P7_D2_P4_Low_T3_9_Right/7.png", "Image/P7_D2_P4_Low_T3_9_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1251.8487000000002 seconds to 1267.8161000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P8_High_T2_25_Right", "images": ["Image/P30_D2_P8_High_T2_25_Right/1.png", "Image/P30_D2_P8_High_T2_25_Right/2.png", "Image/P30_D2_P8_High_T2_25_Right/3.png", "Image/P30_D2_P8_High_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 340.3751 seconds to 350.50739999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P7_Medium_T2_9_Right", "images": ["Image/P3_D2_P7_Medium_T2_9_Right/1.png", "Image/P3_D2_P7_Medium_T2_9_Right/2.png", "Image/P3_D2_P7_Medium_T2_9_Right/3.png", "Image/P3_D2_P7_Medium_T2_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2200.05502 seconds to 2210.5496700000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P3_High_T1_14_Right", "images": ["Image/P18_D3_P3_High_T1_14_Right/1.png", "Image/P18_D3_P3_High_T1_14_Right/2.png", "Image/P18_D3_P3_High_T1_14_Right/3.png", "Image/P18_D3_P3_High_T1_14_Right/4.png", "Image/P18_D3_P3_High_T1_14_Right/5.png", "Image/P18_D3_P3_High_T1_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1212.1046700000002 seconds to 1222.90558 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P3_Medium_T2_14_Mid", "images": ["Image/P21_D2_P3_Medium_T2_14_Mid/1.png", "Image/P21_D2_P3_Medium_T2_14_Mid/2.png", "Image/P21_D2_P3_Medium_T2_14_Mid/3.png", "Image/P21_D2_P3_Medium_T2_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1796.9160000000002 seconds to 1807.6067 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P1_Medium_T2_18_Left", "images": ["Image/P8_D3_P1_Medium_T2_18_Left/1.png", "Image/P8_D3_P1_Medium_T2_18_Left/2.png", "Image/P8_D3_P1_Medium_T2_18_Left/3.png", "Image/P8_D3_P1_Medium_T2_18_Left/4.png", "Image/P8_D3_P1_Medium_T2_18_Left/5.png", "Image/P8_D3_P1_Medium_T2_18_Left/6.png", "Image/P8_D3_P1_Medium_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1730.0967 seconds to 1742.8868 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P2_Low_T2_13_Mid", "images": ["Image/P8_D1_P2_Low_T2_13_Mid/1.png", "Image/P8_D1_P2_Low_T2_13_Mid/2.png", "Image/P8_D1_P2_Low_T2_13_Mid/3.png", "Image/P8_D1_P2_Low_T2_13_Mid/4.png", "Image/P8_D1_P2_Low_T2_13_Mid/5.png", "Image/P8_D1_P2_Low_T2_13_Mid/6.png", "Image/P8_D1_P2_Low_T2_13_Mid/7.png", "Image/P8_D1_P2_Low_T2_13_Mid/8.png", "Image/P8_D1_P2_Low_T2_13_Mid/9.png", "Image/P8_D1_P2_Low_T2_13_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Hi. Could you water the planet that is just in front of me? That's all. Finish.\"\nThe frames roughly correspond to the speech interval from 265.9735999999999 seconds to 284.3721999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P4_Low_T3_27_Right", "images": ["Image/P17_D1_P4_Low_T3_27_Right/1.png", "Image/P17_D1_P4_Low_T3_27_Right/2.png", "Image/P17_D1_P4_Low_T3_27_Right/3.png", "Image/P17_D1_P4_Low_T3_27_Right/4.png", "Image/P17_D1_P4_Low_T3_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white mug next to me? Finish\"\nThe frames roughly correspond to the speech interval from 1858.0685 seconds to 1869.4258 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P9_High_T2_20_Left", "images": ["Image/P26_D3_P9_High_T2_20_Left/1.png", "Image/P26_D3_P9_High_T2_20_Left/2.png", "Image/P26_D3_P9_High_T2_20_Left/3.png", "Image/P26_D3_P9_High_T2_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour water into the white vase? Finish.\"\nThe frames roughly correspond to the speech interval from 846.9203 seconds to 856.1357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P3_High_T2_25_Mid", "images": ["Image/P21_D2_P3_High_T2_25_Mid/1.png", "Image/P21_D2_P3_High_T2_25_Mid/2.png", "Image/P21_D2_P3_High_T2_25_Mid/3.png", "Image/P21_D2_P3_High_T2_25_Mid/4.png", "Image/P21_D2_P3_High_T2_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1640.0946 seconds to 1651.2435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P8_Medium_T1_4_Left", "images": ["Image/P9_D2_P8_Medium_T1_4_Left/1.png", "Image/P9_D2_P8_Medium_T1_4_Left/2.png", "Image/P9_D2_P8_Medium_T1_4_Left/3.png", "Image/P9_D2_P8_Medium_T1_4_Left/4.png", "Image/P9_D2_P8_Medium_T1_4_Left/5.png", "Image/P9_D2_P8_Medium_T1_4_Left/6.png", "Image/P9_D2_P8_Medium_T1_4_Left/7.png", "Image/P9_D2_P8_Medium_T1_4_Left/8.png", "Image/P9_D2_P8_Medium_T1_4_Left/9.png", "Image/P9_D2_P8_Medium_T1_4_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the left plant from the floor to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 3198.9429 seconds to 3215.759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P2_Medium_T2_5_Mid", "images": ["Image/P12_D3_P2_Medium_T2_5_Mid/1.png", "Image/P12_D3_P2_Medium_T2_5_Mid/2.png", "Image/P12_D3_P2_Medium_T2_5_Mid/3.png", "Image/P12_D3_P2_Medium_T2_5_Mid/4.png", "Image/P12_D3_P2_Medium_T2_5_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2646.01779 seconds to 2661.03639 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P9_Medium_T1_9_Right", "images": ["Image/P9_D3_P9_Medium_T1_9_Right/1.png", "Image/P9_D3_P9_Medium_T1_9_Right/2.png", "Image/P9_D3_P9_Medium_T1_9_Right/3.png", "Image/P9_D3_P9_Medium_T1_9_Right/4.png", "Image/P9_D3_P9_Medium_T1_9_Right/5.png", "Image/P9_D3_P9_Medium_T1_9_Right/6.png", "Image/P9_D3_P9_Medium_T1_9_Right/7.png", "Image/P9_D3_P9_Medium_T1_9_Right/8.png", "Image/P9_D3_P9_Medium_T1_9_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the most right cup to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 613.7278 seconds to 633.0442 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P4_High_T3_22_Right", "images": ["Image/P30_D2_P4_High_T3_22_Right/1.png", "Image/P30_D2_P4_High_T3_22_Right/2.png", "Image/P30_D2_P4_High_T3_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 395.6774 seconds to 403.82130000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P3_Medium_T2_24_Mid", "images": ["Image/P24_D3_P3_Medium_T2_24_Mid/1.png", "Image/P24_D3_P3_Medium_T2_24_Mid/2.png", "Image/P24_D3_P3_Medium_T2_24_Mid/3.png", "Image/P24_D3_P3_Medium_T2_24_Mid/4.png", "Image/P24_D3_P3_Medium_T2_24_Mid/5.png", "Image/P24_D3_P3_Medium_T2_24_Mid/6.png", "Image/P24_D3_P3_Medium_T2_24_Mid/7.png", "Image/P24_D3_P3_Medium_T2_24_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1650.5437 seconds to 1664.6297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P5_Low_T2_13_Left", "images": ["Image/P29_D3_P5_Low_T2_13_Left/1.png", "Image/P29_D3_P5_Low_T2_13_Left/2.png", "Image/P29_D3_P5_Low_T2_13_Left/3.png", "Image/P29_D3_P5_Low_T2_13_Left/4.png", "Image/P29_D3_P5_Low_T2_13_Left/5.png", "Image/P29_D3_P5_Low_T2_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 1275.58388 seconds to 1288.4999299999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P5_Medium_T2_25_Right", "images": ["Image/P25_D1_P5_Medium_T2_25_Right/1.png", "Image/P25_D1_P5_Medium_T2_25_Right/2.png", "Image/P25_D1_P5_Medium_T2_25_Right/3.png", "Image/P25_D1_P5_Medium_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the first one. Finish.\"\nThe frames roughly correspond to the speech interval from 530.5957 seconds to 539.3926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P6_Low_T1_11_Left", "images": ["Image/P29_D2_P6_Low_T1_11_Left/1.png", "Image/P29_D2_P6_Low_T1_11_Left/2.png", "Image/P29_D2_P6_Low_T1_11_Left/3.png", "Image/P29_D2_P6_Low_T1_11_Left/4.png", "Image/P29_D2_P6_Low_T1_11_Left/5.png", "Image/P29_D2_P6_Low_T1_11_Left/6.png", "Image/P29_D2_P6_Low_T1_11_Left/7.png", "Image/P29_D2_P6_Low_T1_11_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 207.0361 seconds to 221.2072 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P6_Low_T2_10_Mid", "images": ["Image/P15_D2_P6_Low_T2_10_Mid/1.png", "Image/P15_D2_P6_Low_T2_10_Mid/2.png", "Image/P15_D2_P6_Low_T2_10_Mid/3.png", "Image/P15_D2_P6_Low_T2_10_Mid/4.png", "Image/P15_D2_P6_Low_T2_10_Mid/5.png", "Image/P15_D2_P6_Low_T2_10_Mid/6.png", "Image/P15_D2_P6_Low_T2_10_Mid/7.png", "Image/P15_D2_P6_Low_T2_10_Mid/8.png", "Image/P15_D2_P6_Low_T2_10_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the brown Container at the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3099.6387 seconds to 3113.4182 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P4_High_T2_16_Right", "images": ["Image/P21_D3_P4_High_T2_16_Right/1.png", "Image/P21_D3_P4_High_T2_16_Right/2.png", "Image/P21_D3_P4_High_T2_16_Right/3.png", "Image/P21_D3_P4_High_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 287.04415 seconds to 300.65797999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P2_Low_T2_1_Mid", "images": ["Image/P17_D1_P2_Low_T2_1_Mid/1.png", "Image/P17_D1_P2_Low_T2_1_Mid/2.png", "Image/P17_D1_P2_Low_T2_1_Mid/3.png", "Image/P17_D1_P2_Low_T2_1_Mid/4.png", "Image/P17_D1_P2_Low_T2_1_Mid/5.png", "Image/P17_D1_P2_Low_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this green vase right in front of me? Finish\"\nThe frames roughly correspond to the speech interval from 1644.663 seconds to 1656.3967000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P9_Low_T3_26_Mid", "images": ["Image/P4_D3_P9_Low_T3_26_Mid/1.png", "Image/P4_D3_P9_Low_T3_26_Mid/2.png", "Image/P4_D3_P9_Low_T3_26_Mid/3.png", "Image/P4_D3_P9_Low_T3_26_Mid/4.png", "Image/P4_D3_P9_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the white bottle? Finish\"\nThe frames roughly correspond to the speech interval from 815.7847999999999 seconds to 827.8959 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P3_Low_T2_22_Mid", "images": ["Image/P27_D3_P3_Low_T2_22_Mid/1.png", "Image/P27_D3_P3_Low_T2_22_Mid/2.png", "Image/P27_D3_P3_Low_T2_22_Mid/3.png", "Image/P27_D3_P3_Low_T2_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 289.7903 seconds to 300.1741 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P5_High_T1_27_Left", "images": ["Image/P4_D2_P5_High_T1_27_Left/1.png", "Image/P4_D2_P5_High_T1_27_Left/2.png", "Image/P4_D2_P5_High_T1_27_Left/3.png", "Image/P4_D2_P5_High_T1_27_Left/4.png", "Image/P4_D2_P5_High_T1_27_Left/5.png", "Image/P4_D2_P5_High_T1_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white cap to the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1553.0678999999998 seconds to 1566.3331999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P3_Low_T1_2_Left", "images": ["Image/P12_D1_P3_Low_T1_2_Left/1.png", "Image/P12_D1_P3_Low_T1_2_Left/2.png", "Image/P12_D1_P3_Low_T1_2_Left/3.png", "Image/P12_D1_P3_Low_T1_2_Left/4.png", "Image/P12_D1_P3_Low_T1_2_Left/5.png", "Image/P12_D1_P3_Low_T1_2_Left/6.png", "Image/P12_D1_P3_Low_T1_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1723.4441000000002 seconds to 1737.8918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P4_Low_T3_11_Mid", "images": ["Image/P21_D3_P4_Low_T3_11_Mid/1.png", "Image/P21_D3_P4_Low_T3_11_Mid/2.png", "Image/P21_D3_P4_Low_T3_11_Mid/3.png", "Image/P21_D3_P4_Low_T3_11_Mid/4.png", "Image/P21_D3_P4_Low_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 310.8621 seconds to 322.652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P2_Medium_T3_21_Left", "images": ["Image/P23_D3_P2_Medium_T3_21_Left/1.png", "Image/P23_D3_P2_Medium_T3_21_Left/2.png", "Image/P23_D3_P2_Medium_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 698.9872 seconds to 708.8133 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P4_Medium_T2_3_Mid", "images": ["Image/P13_D3_P4_Medium_T2_3_Mid/1.png", "Image/P13_D3_P4_Medium_T2_3_Mid/2.png", "Image/P13_D3_P4_Medium_T2_3_Mid/3.png", "Image/P13_D3_P4_Medium_T2_3_Mid/4.png", "Image/P13_D3_P4_Medium_T2_3_Mid/5.png", "Image/P13_D3_P4_Medium_T2_3_Mid/6.png", "Image/P13_D3_P4_Medium_T2_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday Can you fill the vase in the middle with water\"\nThe frames roughly correspond to the speech interval from 1868.0001 seconds to 1880.7093 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P1_Low_T2_15_Mid", "images": ["Image/P18_D3_P1_Low_T2_15_Mid/1.png", "Image/P18_D3_P1_Low_T2_15_Mid/2.png", "Image/P18_D3_P1_Low_T2_15_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1471.1689 seconds to 1478.8524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P8_Medium_T3_12_Mid", "images": ["Image/P12_D3_P8_Medium_T3_12_Mid/1.png", "Image/P12_D3_P8_Medium_T3_12_Mid/2.png", "Image/P12_D3_P8_Medium_T3_12_Mid/3.png", "Image/P12_D3_P8_Medium_T3_12_Mid/4.png", "Image/P12_D3_P8_Medium_T3_12_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2493.0255899999997 seconds to 2505.66269 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P7_High_T2_20_Left", "images": ["Image/P28_D2_P7_High_T2_20_Left/1.png", "Image/P28_D2_P7_High_T2_20_Left/2.png", "Image/P28_D2_P7_High_T2_20_Left/3.png", "Image/P28_D2_P7_High_T2_20_Left/4.png", "Image/P28_D2_P7_High_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that white vase. Finish.\"\nThe frames roughly correspond to the speech interval from 926.5659 seconds to 937.3544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P4_Medium_T2_7_Mid", "images": ["Image/P2_D2_P4_Medium_T2_7_Mid/1.png", "Image/P2_D2_P4_Medium_T2_7_Mid/2.png", "Image/P2_D2_P4_Medium_T2_7_Mid/3.png", "Image/P2_D2_P4_Medium_T2_7_Mid/4.png", "Image/P2_D2_P4_Medium_T2_7_Mid/5.png", "Image/P2_D2_P4_Medium_T2_7_Mid/6.png", "Image/P2_D2_P4_Medium_T2_7_Mid/7.png", "Image/P2_D2_P4_Medium_T2_7_Mid/8.png", "Image/P2_D2_P4_Medium_T2_7_Mid/9.png", "Image/P2_D2_P4_Medium_T2_7_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the mug in the very middle? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2563.0672 seconds to 2579.9505999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P5_High_T3_19_Right", "images": ["Image/P27_D1_P5_High_T3_19_Right/1.png", "Image/P27_D1_P5_High_T3_19_Right/2.png", "Image/P27_D1_P5_High_T3_19_Right/3.png", "Image/P27_D1_P5_High_T3_19_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this glass? Finish.\"\nThe frames roughly correspond to the speech interval from 722.9256 seconds to 733.1984600000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P5_Medium_T2_21_Mid", "images": ["Image/P20_D1_P5_Medium_T2_21_Mid/1.png", "Image/P20_D1_P5_Medium_T2_21_Mid/2.png", "Image/P20_D1_P5_Medium_T2_21_Mid/3.png", "Image/P20_D1_P5_Medium_T2_21_Mid/4.png", "Image/P20_D1_P5_Medium_T2_21_Mid/5.png", "Image/P20_D1_P5_Medium_T2_21_Mid/6.png", "Image/P20_D1_P5_Medium_T2_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Pour some water into this thing. Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 409.81843 seconds to 422.2867700000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P7_High_T1_3_Right", "images": ["Image/P20_D3_P7_High_T1_3_Right/1.png", "Image/P20_D3_P7_High_T1_3_Right/2.png", "Image/P20_D3_P7_High_T1_3_Right/3.png", "Image/P20_D3_P7_High_T1_3_Right/4.png", "Image/P20_D3_P7_High_T1_3_Right/5.png", "Image/P20_D3_P7_High_T1_3_Right/6.png", "Image/P20_D3_P7_High_T1_3_Right/7.png", "Image/P20_D3_P7_High_T1_3_Right/8.png", "Image/P20_D3_P7_High_T1_3_Right/9.png", "Image/P20_D3_P7_High_T1_3_Right/10.png", "Image/P20_D3_P7_High_T1_3_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move the green bottle on the floor into the top of the shelf\"\nThe frames roughly correspond to the speech interval from 1317.3297 seconds to 1336.6357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P5_High_T2_23_Right", "images": ["Image/P19_D3_P5_High_T2_23_Right/1.png", "Image/P19_D3_P5_High_T2_23_Right/2.png", "Image/P19_D3_P5_High_T2_23_Right/3.png", "Image/P19_D3_P5_High_T2_23_Right/4.png", "Image/P19_D3_P5_High_T2_23_Right/5.png", "Image/P19_D3_P5_High_T2_23_Right/6.png", "Image/P19_D3_P5_High_T2_23_Right/7.png", "Image/P19_D3_P5_High_T2_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this flower. There. Finish.\"\nThe frames roughly correspond to the speech interval from 2064.8044 seconds to 2078.59766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P3_Medium_T3_6_Mid", "images": ["Image/P16_D3_P3_Medium_T3_6_Mid/1.png", "Image/P16_D3_P3_Medium_T3_6_Mid/2.png", "Image/P16_D3_P3_Medium_T3_6_Mid/3.png", "Image/P16_D3_P3_Medium_T3_6_Mid/4.png", "Image/P16_D3_P3_Medium_T3_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase? Finish.\"\nThe frames roughly correspond to the speech interval from 198.1018 seconds to 207.9288 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P1_Low_T3_21_Left", "images": ["Image/P21_D3_P1_Low_T3_21_Left/1.png", "Image/P21_D3_P1_Low_T3_21_Left/2.png", "Image/P21_D3_P1_Low_T3_21_Left/3.png", "Image/P21_D3_P1_Low_T3_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 347.05409999999995 seconds to 357.33989999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P2_Medium_T3_9_Mid", "images": ["Image/P16_D2_P2_Medium_T3_9_Mid/1.png", "Image/P16_D2_P2_Medium_T3_9_Mid/2.png", "Image/P16_D2_P2_Medium_T3_9_Mid/3.png", "Image/P16_D2_P2_Medium_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 919.9443 seconds to 929.4652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P6_High_T1_17_Right", "images": ["Image/P25_D3_P6_High_T1_17_Right/1.png", "Image/P25_D3_P6_High_T1_17_Right/2.png", "Image/P25_D3_P6_High_T1_17_Right/3.png", "Image/P25_D3_P6_High_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this onto the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 2041.3704 seconds to 2050.3355 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P8_High_T1_11_Left", "images": ["Image/P13_D1_P8_High_T1_11_Left/1.png", "Image/P13_D1_P8_High_T1_11_Left/2.png", "Image/P13_D1_P8_High_T1_11_Left/3.png", "Image/P13_D1_P8_High_T1_11_Left/4.png", "Image/P13_D1_P8_High_T1_11_Left/5.png", "Image/P13_D1_P8_High_T1_11_Left/6.png", "Image/P13_D1_P8_High_T1_11_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move that round cup up to here\"\nThe frames roughly correspond to the speech interval from 264.3307 seconds to 277.22130000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P6_Low_T3_10_Mid", "images": ["Image/P7_D1_P6_Low_T3_10_Mid/1.png", "Image/P7_D1_P6_Low_T3_10_Mid/2.png", "Image/P7_D1_P6_Low_T3_10_Mid/3.png", "Image/P7_D1_P6_Low_T3_10_Mid/4.png", "Image/P7_D1_P6_Low_T3_10_Mid/5.png", "Image/P7_D1_P6_Low_T3_10_Mid/6.png", "Image/P7_D1_P6_Low_T3_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 208.9745 seconds to 225.9973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P6_Low_T3_17_Left", "images": ["Image/P7_D2_P6_Low_T3_17_Left/1.png", "Image/P7_D2_P6_Low_T3_17_Left/2.png", "Image/P7_D2_P6_Low_T3_17_Left/3.png", "Image/P7_D2_P6_Low_T3_17_Left/4.png", "Image/P7_D2_P6_Low_T3_17_Left/5.png", "Image/P7_D2_P6_Low_T3_17_Left/6.png", "Image/P7_D2_P6_Low_T3_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Frederick\"\nThe frames roughly correspond to the speech interval from 1299.7324 seconds to 1314.1412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P7_Medium_T3_16_Left", "images": ["Image/P6_D3_P7_Medium_T3_16_Left/1.png", "Image/P6_D3_P7_Medium_T3_16_Left/2.png", "Image/P6_D3_P7_Medium_T3_16_Left/3.png", "Image/P6_D3_P7_Medium_T3_16_Left/4.png", "Image/P6_D3_P7_Medium_T3_16_Left/5.png", "Image/P6_D3_P7_Medium_T3_16_Left/6.png", "Image/P6_D3_P7_Medium_T3_16_Left/7.png", "Image/P6_D3_P7_Medium_T3_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3441.5597 seconds to 3455.7434 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P9_Low_T3_22_Right", "images": ["Image/P29_D2_P9_Low_T3_22_Right/1.png", "Image/P29_D2_P9_Low_T3_22_Right/2.png", "Image/P29_D2_P9_Low_T3_22_Right/3.png", "Image/P29_D2_P9_Low_T3_22_Right/4.png", "Image/P29_D2_P9_Low_T3_22_Right/5.png", "Image/P29_D2_P9_Low_T3_22_Right/6.png", "Image/P29_D2_P9_Low_T3_22_Right/7.png", "Image/P29_D2_P9_Low_T3_22_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 228.6263 seconds to 242.8391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P4_Medium_T2_7_Left", "images": ["Image/P30_D3_P4_Medium_T2_7_Left/1.png", "Image/P30_D3_P4_Medium_T2_7_Left/2.png", "Image/P30_D3_P4_Medium_T2_7_Left/3.png", "Image/P30_D3_P4_Medium_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1326.649 seconds to 1336.2691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P9_Low_T2_8_Left", "images": ["Image/P19_D2_P9_Low_T2_8_Left/1.png", "Image/P19_D2_P9_Low_T2_8_Left/2.png", "Image/P19_D2_P9_Low_T2_8_Left/3.png", "Image/P19_D2_P9_Low_T2_8_Left/4.png", "Image/P19_D2_P9_Low_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here.\"\nThe frames roughly correspond to the speech interval from 1635.4603 seconds to 1650.8826 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P7_High_T1_25_Right", "images": ["Image/P5_D1_P7_High_T1_25_Right/1.png", "Image/P5_D1_P7_High_T1_25_Right/2.png", "Image/P5_D1_P7_High_T1_25_Right/3.png", "Image/P5_D1_P7_High_T1_25_Right/4.png", "Image/P5_D1_P7_High_T1_25_Right/5.png", "Image/P5_D1_P7_High_T1_25_Right/6.png", "Image/P5_D1_P7_High_T1_25_Right/7.png", "Image/P5_D1_P7_High_T1_25_Right/8.png", "Image/P5_D1_P7_High_T1_25_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup on the ground to here on the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 1727.1209 seconds to 1741.8057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P3_Medium_T2_7_Left", "images": ["Image/P26_D1_P3_Medium_T2_7_Left/1.png", "Image/P26_D1_P3_Medium_T2_7_Left/2.png", "Image/P26_D1_P3_Medium_T2_7_Left/3.png", "Image/P26_D1_P3_Medium_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water just into this mug? Finish.\"\nThe frames roughly correspond to the speech interval from 555.3598999999999 seconds to 566.0491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P5_Medium_T2_8_Left", "images": ["Image/P22_D3_P5_Medium_T2_8_Left/1.png", "Image/P22_D3_P5_Medium_T2_8_Left/2.png", "Image/P22_D3_P5_Medium_T2_8_Left/3.png", "Image/P22_D3_P5_Medium_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the cup on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 300.3342 seconds to 310.6213 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P4_Medium_T2_20_Left", "images": ["Image/P5_D3_P4_Medium_T2_20_Left/1.png", "Image/P5_D3_P4_Medium_T2_20_Left/2.png", "Image/P5_D3_P4_Medium_T2_20_Left/3.png", "Image/P5_D3_P4_Medium_T2_20_Left/4.png", "Image/P5_D3_P4_Medium_T2_20_Left/5.png", "Image/P5_D3_P4_Medium_T2_20_Left/6.png", "Image/P5_D3_P4_Medium_T2_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the leftmost bottle placed on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 2883.4982999999997 seconds to 2896.5957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P1_Low_T1_10_Left", "images": ["Image/P13_D1_P1_Low_T1_10_Left/1.png", "Image/P13_D1_P1_Low_T1_10_Left/2.png", "Image/P13_D1_P1_Low_T1_10_Left/3.png", "Image/P13_D1_P1_Low_T1_10_Left/4.png", "Image/P13_D1_P1_Low_T1_10_Left/5.png", "Image/P13_D1_P1_Low_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this brown jug down here\"\nThe frames roughly correspond to the speech interval from 467.1866 seconds to 478.8235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P8_Low_T1_20_Mid", "images": ["Image/P15_D3_P8_Low_T1_20_Mid/1.png", "Image/P15_D3_P8_Low_T1_20_Mid/2.png", "Image/P15_D3_P8_Low_T1_20_Mid/3.png", "Image/P15_D3_P8_Low_T1_20_Mid/4.png", "Image/P15_D3_P8_Low_T1_20_Mid/5.png", "Image/P15_D3_P8_Low_T1_20_Mid/6.png", "Image/P15_D3_P8_Low_T1_20_Mid/7.png", "Image/P15_D3_P8_Low_T1_20_Mid/8.png", "Image/P15_D3_P8_Low_T1_20_Mid/9.png", "Image/P15_D3_P8_Low_T1_20_Mid/10.png", "Image/P15_D3_P8_Low_T1_20_Mid/11.png", "Image/P15_D3_P8_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white container from the under shelf to the top of the same shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 579.3399999999999 seconds to 597.0317 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P8_Low_T3_20_Right", "images": ["Image/P23_D3_P8_Low_T3_20_Right/1.png", "Image/P23_D3_P8_Low_T3_20_Right/2.png", "Image/P23_D3_P8_Low_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 1013.4522 seconds to 1022.1651 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P5_Medium_T2_24_Left", "images": ["Image/P29_D2_P5_Medium_T2_24_Left/1.png", "Image/P29_D2_P5_Medium_T2_24_Left/2.png", "Image/P29_D2_P5_Medium_T2_24_Left/3.png", "Image/P29_D2_P5_Medium_T2_24_Left/4.png", "Image/P29_D2_P5_Medium_T2_24_Left/5.png", "Image/P29_D2_P5_Medium_T2_24_Left/6.png", "Image/P29_D2_P5_Medium_T2_24_Left/7.png", "Image/P29_D2_P5_Medium_T2_24_Left/8.png", "Image/P29_D2_P5_Medium_T2_24_Left/9.png", "Image/P29_D2_P5_Medium_T2_24_Left/10.png", "Image/P29_D2_P5_Medium_T2_24_Left/11.png", "Image/P29_D2_P5_Medium_T2_24_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 293.74559999999997 seconds to 311.58979999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P1_High_T2_16_Mid", "images": ["Image/P3_D3_P1_High_T2_16_Mid/1.png", "Image/P3_D3_P1_High_T2_16_Mid/2.png", "Image/P3_D3_P1_High_T2_16_Mid/3.png", "Image/P3_D3_P1_High_T2_16_Mid/4.png", "Image/P3_D3_P1_High_T2_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 1052.9526 seconds to 1063.7145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P3_Medium_T1_15_Left", "images": ["Image/P29_D1_P3_Medium_T1_15_Left/1.png", "Image/P29_D1_P3_Medium_T1_15_Left/2.png", "Image/P29_D1_P3_Medium_T1_15_Left/3.png", "Image/P29_D1_P3_Medium_T1_15_Left/4.png", "Image/P29_D1_P3_Medium_T1_15_Left/5.png", "Image/P29_D1_P3_Medium_T1_15_Left/6.png", "Image/P29_D1_P3_Medium_T1_15_Left/7.png", "Image/P29_D1_P3_Medium_T1_15_Left/8.png", "Image/P29_D1_P3_Medium_T1_15_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2793.1308 seconds to 2807.2582 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P1_Low_T3_5_Mid", "images": ["Image/P2_D1_P1_Low_T3_5_Mid/1.png", "Image/P2_D1_P1_Low_T3_5_Mid/2.png", "Image/P2_D1_P1_Low_T3_5_Mid/3.png", "Image/P2_D1_P1_Low_T3_5_Mid/4.png", "Image/P2_D1_P1_Low_T3_5_Mid/5.png", "Image/P2_D1_P1_Low_T3_5_Mid/6.png", "Image/P2_D1_P1_Low_T3_5_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please claim this plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 874.8627999999999 seconds to 891.2438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P5_Medium_T2_25_Right", "images": ["Image/P16_D2_P5_Medium_T2_25_Right/1.png", "Image/P16_D2_P5_Medium_T2_25_Right/2.png", "Image/P16_D2_P5_Medium_T2_25_Right/3.png", "Image/P16_D2_P5_Medium_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 777.4479000000001 seconds to 787.6911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P6_High_T3_17_Right", "images": ["Image/P12_D1_P6_High_T3_17_Right/1.png", "Image/P12_D1_P6_High_T3_17_Right/2.png", "Image/P12_D1_P6_High_T3_17_Right/3.png", "Image/P12_D1_P6_High_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1492.7203 seconds to 1502.5893 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P4_Low_T1_15_Left", "images": ["Image/P22_D3_P4_Low_T1_15_Left/1.png", "Image/P22_D3_P4_Low_T1_15_Left/2.png", "Image/P22_D3_P4_Low_T1_15_Left/3.png", "Image/P22_D3_P4_Low_T1_15_Left/4.png", "Image/P22_D3_P4_Low_T1_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant to the shelf right there. Finish.\"\nThe frames roughly correspond to the speech interval from 231.125 seconds to 241.3414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P8_High_T1_5_Left", "images": ["Image/P4_D1_P8_High_T1_5_Left/1.png", "Image/P4_D1_P8_High_T1_5_Left/2.png", "Image/P4_D1_P8_High_T1_5_Left/3.png", "Image/P4_D1_P8_High_T1_5_Left/4.png", "Image/P4_D1_P8_High_T1_5_Left/5.png", "Image/P4_D1_P8_High_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant here? Finish\"\nThe frames roughly correspond to the speech interval from 2226.7045 seconds to 2241.1643999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P8_Low_T2_25_Right", "images": ["Image/P14_D1_P8_Low_T2_25_Right/1.png", "Image/P14_D1_P8_Low_T2_25_Right/2.png", "Image/P14_D1_P8_Low_T2_25_Right/3.png", "Image/P14_D1_P8_Low_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill that up. Finished.\"\nThe frames roughly correspond to the speech interval from 552.6773000000001 seconds to 561.9049 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P7_Medium_T1_23_Mid", "images": ["Image/P26_D1_P7_Medium_T1_23_Mid/1.png", "Image/P26_D1_P7_Medium_T1_23_Mid/2.png", "Image/P26_D1_P7_Medium_T1_23_Mid/3.png", "Image/P26_D1_P7_Medium_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move one of those plants up here? Finish.\"\nThe frames roughly correspond to the speech interval from 86.02247999999999 seconds to 98.63324 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P9_Low_T2_25_Right", "images": ["Image/P9_D3_P9_Low_T2_25_Right/1.png", "Image/P9_D3_P9_Low_T2_25_Right/2.png", "Image/P9_D3_P9_Low_T2_25_Right/3.png", "Image/P9_D3_P9_Low_T2_25_Right/4.png", "Image/P9_D3_P9_Low_T2_25_Right/5.png", "Image/P9_D3_P9_Low_T2_25_Right/6.png", "Image/P9_D3_P9_Low_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the cup on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 363.2854 seconds to 376.2972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P6_Low_T2_11_Mid", "images": ["Image/P25_D1_P6_Low_T2_11_Mid/1.png", "Image/P25_D1_P6_Low_T2_11_Mid/2.png", "Image/P25_D1_P6_Low_T2_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 441.9678 seconds to 450.3491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P9_Medium_T3_11_Right", "images": ["Image/P20_D2_P9_Medium_T3_11_Right/1.png", "Image/P20_D2_P9_Medium_T3_11_Right/2.png", "Image/P20_D2_P9_Medium_T3_11_Right/3.png", "Image/P20_D2_P9_Medium_T3_11_Right/4.png", "Image/P20_D2_P9_Medium_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the right bottle on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3010.5298000000003 seconds to 3022.277 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P9_Low_T1_6_Mid", "images": ["Image/P23_D1_P9_Low_T1_6_Mid/1.png", "Image/P23_D1_P9_Low_T1_6_Mid/2.png", "Image/P23_D1_P9_Low_T1_6_Mid/3.png", "Image/P23_D1_P9_Low_T1_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1912.6607 seconds to 1922.2783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P5_Low_T3_13_Mid", "images": ["Image/P18_D2_P5_Low_T3_13_Mid/1.png", "Image/P18_D2_P5_Low_T3_13_Mid/2.png", "Image/P18_D2_P5_Low_T3_13_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 525.8558 seconds to 534.1238000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P5_High_T1_5_Mid", "images": ["Image/P21_D3_P5_High_T1_5_Mid/1.png", "Image/P21_D3_P5_High_T1_5_Mid/2.png", "Image/P21_D3_P5_High_T1_5_Mid/3.png", "Image/P21_D3_P5_High_T1_5_Mid/4.png", "Image/P21_D3_P5_High_T1_5_Mid/5.png", "Image/P21_D3_P5_High_T1_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 714.1098999999999 seconds to 726.0523000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P6_Low_T2_23_Mid", "images": ["Image/P13_D3_P6_Low_T2_23_Mid/1.png", "Image/P13_D3_P6_Low_T2_23_Mid/2.png", "Image/P13_D3_P6_Low_T2_23_Mid/3.png", "Image/P13_D3_P6_Low_T2_23_Mid/4.png", "Image/P13_D3_P6_Low_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday\"\nThe frames roughly correspond to the speech interval from 1749.7639 seconds to 1760.1905 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P1_High_T3_24_Right", "images": ["Image/P26_D1_P1_High_T3_24_Right/1.png", "Image/P26_D1_P1_High_T3_24_Right/2.png", "Image/P26_D1_P1_High_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 427.9784 seconds to 446.8819 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P7_Medium_T2_13_Right", "images": ["Image/P19_D2_P7_Medium_T2_13_Right/1.png", "Image/P19_D2_P7_Medium_T2_13_Right/2.png", "Image/P19_D2_P7_Medium_T2_13_Right/3.png", "Image/P19_D2_P7_Medium_T2_13_Right/4.png", "Image/P19_D2_P7_Medium_T2_13_Right/5.png", "Image/P19_D2_P7_Medium_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1888.0508 seconds to 1900.7031 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P8_Medium_T3_1_Mid", "images": ["Image/P2_D2_P8_Medium_T3_1_Mid/1.png", "Image/P2_D2_P8_Medium_T3_1_Mid/2.png", "Image/P2_D2_P8_Medium_T3_1_Mid/3.png", "Image/P2_D2_P8_Medium_T3_1_Mid/4.png", "Image/P2_D2_P8_Medium_T3_1_Mid/5.png", "Image/P2_D2_P8_Medium_T3_1_Mid/6.png", "Image/P2_D2_P8_Medium_T3_1_Mid/7.png", "Image/P2_D2_P8_Medium_T3_1_Mid/8.png", "Image/P2_D2_P8_Medium_T3_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2819.4438 seconds to 2834.0193 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P7_Medium_T2_7_Left", "images": ["Image/P29_D3_P7_Medium_T2_7_Left/1.png", "Image/P29_D3_P7_Medium_T2_7_Left/2.png", "Image/P29_D3_P7_Medium_T2_7_Left/3.png", "Image/P29_D3_P7_Medium_T2_7_Left/4.png", "Image/P29_D3_P7_Medium_T2_7_Left/5.png", "Image/P29_D3_P7_Medium_T2_7_Left/6.png", "Image/P29_D3_P7_Medium_T2_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1634.7036999999998 seconds to 1646.8548999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P6_Medium_T3_2_Mid", "images": ["Image/P8_D1_P6_Medium_T3_2_Mid/1.png", "Image/P8_D1_P6_Medium_T3_2_Mid/2.png", "Image/P8_D1_P6_Medium_T3_2_Mid/3.png", "Image/P8_D1_P6_Medium_T3_2_Mid/4.png", "Image/P8_D1_P6_Medium_T3_2_Mid/5.png", "Image/P8_D1_P6_Medium_T3_2_Mid/6.png", "Image/P8_D1_P6_Medium_T3_2_Mid/7.png", "Image/P8_D1_P6_Medium_T3_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the green bottle that is in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 739.7114999999999 seconds to 754.8883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P6_Medium_T3_1_Right", "images": ["Image/P21_D3_P6_Medium_T3_1_Right/1.png", "Image/P21_D3_P6_Medium_T3_1_Right/2.png", "Image/P21_D3_P6_Medium_T3_1_Right/3.png", "Image/P21_D3_P6_Medium_T3_1_Right/4.png", "Image/P21_D3_P6_Medium_T3_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 566.6143 seconds to 577.2071 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P5_Low_T3_19_Right", "images": ["Image/P19_D2_P5_Low_T3_19_Right/1.png", "Image/P19_D2_P5_Low_T3_19_Right/2.png", "Image/P19_D2_P5_Low_T3_19_Right/3.png", "Image/P19_D2_P5_Low_T3_19_Right/4.png", "Image/P19_D2_P5_Low_T3_19_Right/5.png", "Image/P19_D2_P5_Low_T3_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1453.165 seconds to 1466.0407 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P8_Medium_T2_24_Mid", "images": ["Image/P8_D1_P8_Medium_T2_24_Mid/1.png", "Image/P8_D1_P8_Medium_T2_24_Mid/2.png", "Image/P8_D1_P8_Medium_T2_24_Mid/3.png", "Image/P8_D1_P8_Medium_T2_24_Mid/4.png", "Image/P8_D1_P8_Medium_T2_24_Mid/5.png", "Image/P8_D1_P8_Medium_T2_24_Mid/6.png", "Image/P8_D1_P8_Medium_T2_24_Mid/7.png", "Image/P8_D1_P8_Medium_T2_24_Mid/8.png", "Image/P8_D1_P8_Medium_T2_24_Mid/9.png", "Image/P8_D1_P8_Medium_T2_24_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 523.1375999999999 seconds to 539.6767 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P2_Low_T1_4_Left", "images": ["Image/P17_D3_P2_Low_T1_4_Left/1.png", "Image/P17_D3_P2_Low_T1_4_Left/2.png", "Image/P17_D3_P2_Low_T1_4_Left/3.png", "Image/P17_D3_P2_Low_T1_4_Left/4.png", "Image/P17_D3_P2_Low_T1_4_Left/5.png", "Image/P17_D3_P2_Low_T1_4_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant on the top shelf down to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 786.16787 seconds to 798.44538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P9_Medium_T1_24_Mid", "images": ["Image/P28_D2_P9_Medium_T1_24_Mid/1.png", "Image/P28_D2_P9_Medium_T1_24_Mid/2.png", "Image/P28_D2_P9_Medium_T1_24_Mid/3.png", "Image/P28_D2_P9_Medium_T1_24_Mid/4.png", "Image/P28_D2_P9_Medium_T1_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that middle plant to the very top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1245.6672 seconds to 1256.6084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P6_High_T3_18_Right", "images": ["Image/P6_D3_P6_High_T3_18_Right/1.png", "Image/P6_D3_P6_High_T3_18_Right/2.png", "Image/P6_D3_P6_High_T3_18_Right/3.png", "Image/P6_D3_P6_High_T3_18_Right/4.png", "Image/P6_D3_P6_High_T3_18_Right/5.png", "Image/P6_D3_P6_High_T3_18_Right/6.png", "Image/P6_D3_P6_High_T3_18_Right/7.png", "Image/P6_D3_P6_High_T3_18_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3464.6803 seconds to 3478.4609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P7_Medium_T2_22_Mid", "images": ["Image/P5_D3_P7_Medium_T2_22_Mid/1.png", "Image/P5_D3_P7_Medium_T2_22_Mid/2.png", "Image/P5_D3_P7_Medium_T2_22_Mid/3.png", "Image/P5_D3_P7_Medium_T2_22_Mid/4.png", "Image/P5_D3_P7_Medium_T2_22_Mid/5.png", "Image/P5_D3_P7_Medium_T2_22_Mid/6.png", "Image/P5_D3_P7_Medium_T2_22_Mid/7.png", "Image/P5_D3_P7_Medium_T2_22_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. There are three plants on the ground. Water\"\nThe frames roughly correspond to the speech interval from 2464.5901999999996 seconds to 2478.7347999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P5_High_T3_2_Left", "images": ["Image/P4_D1_P5_High_T3_2_Left/1.png", "Image/P4_D1_P5_High_T3_2_Left/2.png", "Image/P4_D1_P5_High_T3_2_Left/3.png", "Image/P4_D1_P5_High_T3_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2265.0951 seconds to 2276.2419999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P2_Medium_T2_17_Right", "images": ["Image/P1_D3_P2_Medium_T2_17_Right/1.png", "Image/P1_D3_P2_Medium_T2_17_Right/2.png", "Image/P1_D3_P2_Medium_T2_17_Right/3.png", "Image/P1_D3_P2_Medium_T2_17_Right/4.png", "Image/P1_D3_P2_Medium_T2_17_Right/5.png", "Image/P1_D3_P2_Medium_T2_17_Right/6.png", "Image/P1_D3_P2_Medium_T2_17_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the red cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1731.2254999999998 seconds to 1743.4620999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P8_Low_T3_18_Mid", "images": ["Image/P9_D2_P8_Low_T3_18_Mid/1.png", "Image/P9_D2_P8_Low_T3_18_Mid/2.png", "Image/P9_D2_P8_Low_T3_18_Mid/3.png", "Image/P9_D2_P8_Low_T3_18_Mid/4.png", "Image/P9_D2_P8_Low_T3_18_Mid/5.png", "Image/P9_D2_P8_Low_T3_18_Mid/6.png", "Image/P9_D2_P8_Low_T3_18_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the red cup under the desk on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 3250.0588 seconds to 3266.7087 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P8_Low_T1_25_Left", "images": ["Image/P25_D3_P8_Low_T1_25_Left/1.png", "Image/P25_D3_P8_Low_T1_25_Left/2.png", "Image/P25_D3_P8_Low_T1_25_Left/3.png", "Image/P25_D3_P8_Low_T1_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2102.5713 seconds to 2111.2156999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P6_High_T2_3_Mid", "images": ["Image/P25_D1_P6_High_T2_3_Mid/1.png", "Image/P25_D1_P6_High_T2_3_Mid/2.png", "Image/P25_D1_P6_High_T2_3_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 548.707 seconds to 557.2369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P8_High_T3_16_Mid", "images": ["Image/P20_D2_P8_High_T3_16_Mid/1.png", "Image/P20_D2_P8_High_T3_16_Mid/2.png", "Image/P20_D2_P8_High_T3_16_Mid/3.png", "Image/P20_D2_P8_High_T3_16_Mid/4.png", "Image/P20_D2_P8_High_T3_16_Mid/5.png", "Image/P20_D2_P8_High_T3_16_Mid/6.png", "Image/P20_D2_P8_High_T3_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that cup next to the plants? Finish.\"\nThe frames roughly correspond to the speech interval from 2876.2169000000004 seconds to 2888.5217 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P4_Low_T1_11_Left", "images": ["Image/P4_D1_P4_Low_T1_11_Left/1.png", "Image/P4_D1_P4_Low_T1_11_Left/2.png", "Image/P4_D1_P4_Low_T1_11_Left/3.png", "Image/P4_D1_P4_Low_T1_11_Left/4.png", "Image/P4_D1_P4_Low_T1_11_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the bottle to the middle of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2370.6358 seconds to 2382.8981 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P1_Low_T1_23_Right", "images": ["Image/P17_D3_P1_Low_T1_23_Right/1.png", "Image/P17_D3_P1_Low_T1_23_Right/2.png", "Image/P17_D3_P1_Low_T1_23_Right/3.png", "Image/P17_D3_P1_Low_T1_23_Right/4.png", "Image/P17_D3_P1_Low_T1_23_Right/5.png", "Image/P17_D3_P1_Low_T1_23_Right/6.png", "Image/P17_D3_P1_Low_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that white plant down to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1088.8825000000002 seconds to 1102.316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P8_Medium_T2_26_Mid", "images": ["Image/P13_D1_P8_Medium_T2_26_Mid/1.png", "Image/P13_D1_P8_Medium_T2_26_Mid/2.png", "Image/P13_D1_P8_Medium_T2_26_Mid/3.png", "Image/P13_D1_P8_Medium_T2_26_Mid/4.png", "Image/P13_D1_P8_Medium_T2_26_Mid/5.png", "Image/P13_D1_P8_Medium_T2_26_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the middle cup down there\"\nThe frames roughly correspond to the speech interval from 571.7241 seconds to 583.1942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P6_Medium_T3_3_Left", "images": ["Image/P11_D1_P6_Medium_T3_3_Left/1.png", "Image/P11_D1_P6_Medium_T3_3_Left/2.png", "Image/P11_D1_P6_Medium_T3_3_Left/3.png", "Image/P11_D1_P6_Medium_T3_3_Left/4.png", "Image/P11_D1_P6_Medium_T3_3_Left/5.png", "Image/P11_D1_P6_Medium_T3_3_Left/6.png", "Image/P11_D1_P6_Medium_T3_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the first green glass from the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 2644.8565999999996 seconds to 2662.2002999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P1_Low_T2_10_Mid", "images": ["Image/P17_D2_P1_Low_T2_10_Mid/1.png", "Image/P17_D2_P1_Low_T2_10_Mid/2.png", "Image/P17_D2_P1_Low_T2_10_Mid/3.png", "Image/P17_D2_P1_Low_T2_10_Mid/4.png", "Image/P17_D2_P1_Low_T2_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in this brown cup over here? Finish.\"\nThe frames roughly correspond to the speech interval from 605.9712000000001 seconds to 621.3936 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P1_High_T2_17_Mid", "images": ["Image/P17_D3_P1_High_T2_17_Mid/1.png", "Image/P17_D3_P1_High_T2_17_Mid/2.png", "Image/P17_D3_P1_High_T2_17_Mid/3.png", "Image/P17_D3_P1_High_T2_17_Mid/4.png", "Image/P17_D3_P1_High_T2_17_Mid/5.png", "Image/P17_D3_P1_High_T2_17_Mid/6.png", "Image/P17_D3_P1_High_T2_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in that brown mug on the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1239.4830000000002 seconds to 1252.3585 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P3_Low_T1_23_Left", "images": ["Image/P7_D2_P3_Low_T1_23_Left/1.png", "Image/P7_D2_P3_Low_T1_23_Left/2.png", "Image/P7_D2_P3_Low_T1_23_Left/3.png", "Image/P7_D2_P3_Low_T1_23_Left/4.png", "Image/P7_D2_P3_Low_T1_23_Left/5.png", "Image/P7_D2_P3_Low_T1_23_Left/6.png", "Image/P7_D2_P3_Low_T1_23_Left/7.png", "Image/P7_D2_P3_Low_T1_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Done. Hey Friday\"\nThe frames roughly correspond to the speech interval from 967.85631 seconds to 983.5989000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P7_Low_T1_15_Right", "images": ["Image/P17_D1_P7_Low_T1_15_Right/1.png", "Image/P17_D1_P7_Low_T1_15_Right/2.png", "Image/P17_D1_P7_Low_T1_15_Right/3.png", "Image/P17_D1_P7_Low_T1_15_Right/4.png", "Image/P17_D1_P7_Low_T1_15_Right/5.png", "Image/P17_D1_P7_Low_T1_15_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant in the basket to the shelf right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1993.3279 seconds to 2005.8701 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P8_Medium_T3_17_Right", "images": ["Image/P25_D2_P8_Medium_T3_17_Right/1.png", "Image/P25_D2_P8_Medium_T3_17_Right/2.png", "Image/P25_D2_P8_Medium_T3_17_Right/3.png", "Image/P25_D2_P8_Medium_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the first orange cup. Finish.\"\nThe frames roughly correspond to the speech interval from 658.63392 seconds to 667.77928 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P9_Medium_T3_16_Mid", "images": ["Image/P11_D3_P9_Medium_T3_16_Mid/1.png", "Image/P11_D3_P9_Medium_T3_16_Mid/2.png", "Image/P11_D3_P9_Medium_T3_16_Mid/3.png", "Image/P11_D3_P9_Medium_T3_16_Mid/4.png", "Image/P11_D3_P9_Medium_T3_16_Mid/5.png", "Image/P11_D3_P9_Medium_T3_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the second bottle in the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1642.7041 seconds to 1654.3673 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P5_High_T2_2_Left", "images": ["Image/P25_D3_P5_High_T2_2_Left/1.png", "Image/P25_D3_P5_High_T2_2_Left/2.png", "Image/P25_D3_P5_High_T2_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 2087.1588 seconds to 2095.5951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P4_Low_T1_10_Left", "images": ["Image/P24_D2_P4_Low_T1_10_Left/1.png", "Image/P24_D2_P4_Low_T1_10_Left/2.png", "Image/P24_D2_P4_Low_T1_10_Left/3.png", "Image/P24_D2_P4_Low_T1_10_Left/4.png", "Image/P24_D2_P4_Low_T1_10_Left/5.png", "Image/P24_D2_P4_Low_T1_10_Left/6.png", "Image/P24_D2_P4_Low_T1_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this bottle to this area? Please finish.\"\nThe frames roughly correspond to the speech interval from 681.5385 seconds to 694.0656 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P1_Medium_T3_15_Right", "images": ["Image/P27_D1_P1_Medium_T3_15_Right/1.png", "Image/P27_D1_P1_Medium_T3_15_Right/2.png", "Image/P27_D1_P1_Medium_T3_15_Right/3.png", "Image/P27_D1_P1_Medium_T3_15_Right/4.png", "Image/P27_D1_P1_Medium_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 901.3943 seconds to 912.0002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P1_Low_T3_25_Right", "images": ["Image/P23_D2_P1_Low_T3_25_Right/1.png", "Image/P23_D2_P1_Low_T3_25_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 102.09522 seconds to 110.80877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P8_Medium_T2_16_Right", "images": ["Image/P13_D3_P8_Medium_T2_16_Right/1.png", "Image/P13_D3_P8_Medium_T2_16_Right/2.png", "Image/P13_D3_P8_Medium_T2_16_Right/3.png", "Image/P13_D3_P8_Medium_T2_16_Right/4.png", "Image/P13_D3_P8_Medium_T2_16_Right/5.png", "Image/P13_D3_P8_Medium_T2_16_Right/6.png", "Image/P13_D3_P8_Medium_T2_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the right brown cup on the floor with water\"\nThe frames roughly correspond to the speech interval from 1488.8351 seconds to 1501.2938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P4_Medium_T2_2_Mid", "images": ["Image/P28_D1_P4_Medium_T2_2_Mid/1.png", "Image/P28_D1_P4_Medium_T2_2_Mid/2.png", "Image/P28_D1_P4_Medium_T2_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1346.34243 seconds to 1356.5735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P9_Low_T2_1_Left", "images": ["Image/P7_D3_P9_Low_T2_1_Left/1.png", "Image/P7_D3_P9_Low_T2_1_Left/2.png", "Image/P7_D3_P9_Low_T2_1_Left/3.png", "Image/P7_D3_P9_Low_T2_1_Left/4.png", "Image/P7_D3_P9_Low_T2_1_Left/5.png", "Image/P7_D3_P9_Low_T2_1_Left/6.png", "Image/P7_D3_P9_Low_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1982.2929 seconds to 1996.464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P6_Low_T2_14_Mid", "images": ["Image/P21_D1_P6_Low_T2_14_Mid/1.png", "Image/P21_D1_P6_Low_T2_14_Mid/2.png", "Image/P21_D1_P6_Low_T2_14_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 828.25119 seconds to 841.4867199999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P6_High_T2_15_Mid", "images": ["Image/P28_D2_P6_High_T2_15_Mid/1.png", "Image/P28_D2_P6_High_T2_15_Mid/2.png", "Image/P28_D2_P6_High_T2_15_Mid/3.png", "Image/P28_D2_P6_High_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1213.2886 seconds to 1222.2117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P2_Low_T2_24_Left", "images": ["Image/P13_D2_P2_Low_T2_24_Left/1.png", "Image/P13_D2_P2_Low_T2_24_Left/2.png", "Image/P13_D2_P2_Low_T2_24_Left/3.png", "Image/P13_D2_P2_Low_T2_24_Left/4.png", "Image/P13_D2_P2_Low_T2_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you water the plant here please? Finish.\"\nThe frames roughly correspond to the speech interval from 1192.48466 seconds to 1202.49326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P7_Low_T1_20_Mid", "images": ["Image/P28_D2_P7_Low_T1_20_Mid/1.png", "Image/P28_D2_P7_Low_T1_20_Mid/2.png", "Image/P28_D2_P7_Low_T1_20_Mid/3.png", "Image/P28_D2_P7_Low_T1_20_Mid/4.png", "Image/P28_D2_P7_Low_T1_20_Mid/5.png", "Image/P28_D2_P7_Low_T1_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that white base the very top of the bookshelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1028.9339 seconds to 1040.111 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P4_Low_T1_1_Right", "images": ["Image/P30_D3_P4_Low_T1_1_Right/1.png", "Image/P30_D3_P4_Low_T1_1_Right/2.png", "Image/P30_D3_P4_Low_T1_1_Right/3.png", "Image/P30_D3_P4_Low_T1_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase over to the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1579.6435 seconds to 1593.0764 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P9_Low_T2_13_Right", "images": ["Image/P15_D2_P9_Low_T2_13_Right/1.png", "Image/P15_D2_P9_Low_T2_13_Right/2.png", "Image/P15_D2_P9_Low_T2_13_Right/3.png", "Image/P15_D2_P9_Low_T2_13_Right/4.png", "Image/P15_D2_P9_Low_T2_13_Right/5.png", "Image/P15_D2_P9_Low_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the floor. Finished.\"\nThe frames roughly correspond to the speech interval from 3199.3739 seconds to 3210.1198999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P6_Low_T1_19_Mid", "images": ["Image/P1_D2_P6_Low_T1_19_Mid/1.png", "Image/P1_D2_P6_Low_T1_19_Mid/2.png", "Image/P1_D2_P6_Low_T1_19_Mid/3.png", "Image/P1_D2_P6_Low_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 970.6816 seconds to 980.4135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P1_High_T3_20_Right", "images": ["Image/P8_D3_P1_High_T3_20_Right/1.png", "Image/P8_D3_P1_High_T3_20_Right/2.png", "Image/P8_D3_P1_High_T3_20_Right/3.png", "Image/P8_D3_P1_High_T3_20_Right/4.png", "Image/P8_D3_P1_High_T3_20_Right/5.png", "Image/P8_D3_P1_High_T3_20_Right/6.png", "Image/P8_D3_P1_High_T3_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1687.7238 seconds to 1700.9338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P4_Medium_T3_1_Right", "images": ["Image/P3_D1_P4_Medium_T3_1_Right/1.png", "Image/P3_D1_P4_Medium_T3_1_Right/2.png", "Image/P3_D1_P4_Medium_T3_1_Right/3.png", "Image/P3_D1_P4_Medium_T3_1_Right/4.png", "Image/P3_D1_P4_Medium_T3_1_Right/5.png", "Image/P3_D1_P4_Medium_T3_1_Right/6.png", "Image/P3_D1_P4_Medium_T3_1_Right/7.png", "Image/P3_D1_P4_Medium_T3_1_Right/8.png", "Image/P3_D1_P4_Medium_T3_1_Right/9.png", "Image/P3_D1_P4_Medium_T3_1_Right/10.png", "Image/P3_D1_P4_Medium_T3_1_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1649.6369 seconds to 1666.0914 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P8_High_T3_12_Mid", "images": ["Image/P14_D2_P8_High_T3_12_Mid/1.png", "Image/P14_D2_P8_High_T3_12_Mid/2.png", "Image/P14_D2_P8_High_T3_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1853.6167 seconds to 1862.3771 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P6_Medium_T2_19_Mid", "images": ["Image/P9_D1_P6_Medium_T2_19_Mid/1.png", "Image/P9_D1_P6_Medium_T2_19_Mid/2.png", "Image/P9_D1_P6_Medium_T2_19_Mid/3.png", "Image/P9_D1_P6_Medium_T2_19_Mid/4.png", "Image/P9_D1_P6_Medium_T2_19_Mid/5.png", "Image/P9_D1_P6_Medium_T2_19_Mid/6.png", "Image/P9_D1_P6_Medium_T2_19_Mid/7.png", "Image/P9_D1_P6_Medium_T2_19_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the middle pot? Finish.\"\nThe frames roughly correspond to the speech interval from 1309.2493 seconds to 1323.6065999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P6_Medium_T2_20_Mid", "images": ["Image/P28_D3_P6_Medium_T2_20_Mid/1.png", "Image/P28_D3_P6_Medium_T2_20_Mid/2.png", "Image/P28_D3_P6_Medium_T2_20_Mid/3.png", "Image/P28_D3_P6_Medium_T2_20_Mid/4.png", "Image/P28_D3_P6_Medium_T2_20_Mid/5.png", "Image/P28_D3_P6_Medium_T2_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in the middle vase on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 234.4214 seconds to 246.0431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P8_Medium_T3_24_Mid", "images": ["Image/P15_D1_P8_Medium_T3_24_Mid/1.png", "Image/P15_D1_P8_Medium_T3_24_Mid/2.png", "Image/P15_D1_P8_Medium_T3_24_Mid/3.png", "Image/P15_D1_P8_Medium_T3_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1835.74057 seconds to 1845.31657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P3_Medium_T1_24_Mid", "images": ["Image/P23_D3_P3_Medium_T1_24_Mid/1.png", "Image/P23_D3_P3_Medium_T1_24_Mid/2.png", "Image/P23_D3_P3_Medium_T1_24_Mid/3.png", "Image/P23_D3_P3_Medium_T1_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 849.4466 seconds to 860.1088 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P9_High_T1_2_Right", "images": ["Image/P27_D1_P9_High_T1_2_Right/1.png", "Image/P27_D1_P9_High_T1_2_Right/2.png", "Image/P27_D1_P9_High_T1_2_Right/3.png", "Image/P27_D1_P9_High_T1_2_Right/4.png", "Image/P27_D1_P9_High_T1_2_Right/5.png", "Image/P27_D1_P9_High_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that green glass and put it on this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 826.0593000000001 seconds to 837.7233000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P8_Medium_T1_6_Right", "images": ["Image/P2_D1_P8_Medium_T1_6_Right/1.png", "Image/P2_D1_P8_Medium_T1_6_Right/2.png", "Image/P2_D1_P8_Medium_T1_6_Right/3.png", "Image/P2_D1_P8_Medium_T1_6_Right/4.png", "Image/P2_D1_P8_Medium_T1_6_Right/5.png", "Image/P2_D1_P8_Medium_T1_6_Right/6.png", "Image/P2_D1_P8_Medium_T1_6_Right/7.png", "Image/P2_D1_P8_Medium_T1_6_Right/8.png", "Image/P2_D1_P8_Medium_T1_6_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 541.2863 seconds to 556.4976999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P6_High_T2_6_Mid", "images": ["Image/P5_D2_P6_High_T2_6_Mid/1.png", "Image/P5_D2_P6_High_T2_6_Mid/2.png", "Image/P5_D2_P6_High_T2_6_Mid/3.png", "Image/P5_D2_P6_High_T2_6_Mid/4.png", "Image/P5_D2_P6_High_T2_6_Mid/5.png", "Image/P5_D2_P6_High_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant to the right-hand side of you. Finish.\"\nThe frames roughly correspond to the speech interval from 914.4311 seconds to 926.5958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P4_Low_T2_13_Mid", "images": ["Image/P23_D3_P4_Low_T2_13_Mid/1.png", "Image/P23_D3_P4_Low_T2_13_Mid/2.png", "Image/P23_D3_P4_Low_T2_13_Mid/3.png", "Image/P23_D3_P4_Low_T2_13_Mid/4.png", "Image/P23_D3_P4_Low_T2_13_Mid/5.png", "Image/P23_D3_P4_Low_T2_13_Mid/6.png", "Image/P23_D3_P4_Low_T2_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 648.86076 seconds to 658.2293900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P1_High_T3_6_Left", "images": ["Image/P11_D2_P1_High_T3_6_Left/1.png", "Image/P11_D2_P1_High_T3_6_Left/2.png", "Image/P11_D2_P1_High_T3_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Clean the plant\"\nThe frames roughly correspond to the speech interval from 879.6965 seconds to 888.0846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P9_Medium_T2_26_Right", "images": ["Image/P2_D1_P9_Medium_T2_26_Right/1.png", "Image/P2_D1_P9_Medium_T2_26_Right/2.png", "Image/P2_D1_P9_Medium_T2_26_Right/3.png", "Image/P2_D1_P9_Medium_T2_26_Right/4.png", "Image/P2_D1_P9_Medium_T2_26_Right/5.png", "Image/P2_D1_P9_Medium_T2_26_Right/6.png", "Image/P2_D1_P9_Medium_T2_26_Right/7.png", "Image/P2_D1_P9_Medium_T2_26_Right/8.png", "Image/P2_D1_P9_Medium_T2_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 849.4170999999999 seconds to 867.0503999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P5_High_T1_18_Right", "images": ["Image/P20_D2_P5_High_T1_18_Right/1.png", "Image/P20_D2_P5_High_T1_18_Right/2.png", "Image/P20_D2_P5_High_T1_18_Right/3.png", "Image/P20_D2_P5_High_T1_18_Right/4.png", "Image/P20_D2_P5_High_T1_18_Right/5.png", "Image/P20_D2_P5_High_T1_18_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the orange bottle over here? Finish.\"\nThe frames roughly correspond to the speech interval from 2815.17772 seconds to 2826.5085500000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P1_High_T3_12_Left", "images": ["Image/P9_D3_P1_High_T3_12_Left/1.png", "Image/P9_D3_P1_High_T3_12_Left/2.png", "Image/P9_D3_P1_High_T3_12_Left/3.png", "Image/P9_D3_P1_High_T3_12_Left/4.png", "Image/P9_D3_P1_High_T3_12_Left/5.png", "Image/P9_D3_P1_High_T3_12_Left/6.png", "Image/P9_D3_P1_High_T3_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to clean the right bottle\"\nThe frames roughly correspond to the speech interval from 471.9754 seconds to 485.9734 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_Low_T3_12_Right", "images": ["Image/P20_D2_P4_Low_T3_12_Right/1.png", "Image/P20_D2_P4_Low_T3_12_Right/2.png", "Image/P20_D2_P4_Low_T3_12_Right/3.png", "Image/P20_D2_P4_Low_T3_12_Right/4.png", "Image/P20_D2_P4_Low_T3_12_Right/5.png", "Image/P20_D2_P4_Low_T3_12_Right/6.png", "Image/P20_D2_P4_Low_T3_12_Right/7.png", "Image/P20_D2_P4_Low_T3_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that bottle over there? Finish.\"\nThe frames roughly correspond to the speech interval from 3107.8441000000003 seconds to 3121.3589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P6_High_T2_2_Left", "images": ["Image/P16_D3_P6_High_T2_2_Left/1.png", "Image/P16_D3_P6_High_T2_2_Left/2.png", "Image/P16_D3_P6_High_T2_2_Left/3.png", "Image/P16_D3_P6_High_T2_2_Left/4.png", "Image/P16_D3_P6_High_T2_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water into that vase? Finish\"\nThe frames roughly correspond to the speech interval from 160.2114 seconds to 170.51129999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P6_Medium_T2_1_Right", "images": ["Image/P4_D2_P6_Medium_T2_1_Right/1.png", "Image/P4_D2_P6_Medium_T2_1_Right/2.png", "Image/P4_D2_P6_Medium_T2_1_Right/3.png", "Image/P4_D2_P6_Medium_T2_1_Right/4.png", "Image/P4_D2_P6_Medium_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the right bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1910.2804 seconds to 1922.3602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P1_High_T3_13_Left", "images": ["Image/P3_D1_P1_High_T3_13_Left/1.png", "Image/P3_D1_P1_High_T3_13_Left/2.png", "Image/P3_D1_P1_High_T3_13_Left/3.png", "Image/P3_D1_P1_High_T3_13_Left/4.png", "Image/P3_D1_P1_High_T3_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1248.72419 seconds to 1260.20935 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P8_Medium_T2_4_Right", "images": ["Image/P20_D1_P8_Medium_T2_4_Right/1.png", "Image/P20_D1_P8_Medium_T2_4_Right/2.png", "Image/P20_D1_P8_Medium_T2_4_Right/3.png", "Image/P20_D1_P8_Medium_T2_4_Right/4.png", "Image/P20_D1_P8_Medium_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start.\"\nThe frames roughly correspond to the speech interval from 180.80629999999996 seconds to 192.37395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P8_Medium_T2_1_Mid", "images": ["Image/P12_D2_P8_Medium_T2_1_Mid/1.png", "Image/P12_D2_P8_Medium_T2_1_Mid/2.png", "Image/P12_D2_P8_Medium_T2_1_Mid/3.png", "Image/P12_D2_P8_Medium_T2_1_Mid/4.png", "Image/P12_D2_P8_Medium_T2_1_Mid/5.png", "Image/P12_D2_P8_Medium_T2_1_Mid/6.png", "Image/P12_D2_P8_Medium_T2_1_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 693.4811 seconds to 706.0504999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P7_Medium_T2_20_Mid", "images": ["Image/P9_D3_P7_Medium_T2_20_Mid/1.png", "Image/P9_D3_P7_Medium_T2_20_Mid/2.png", "Image/P9_D3_P7_Medium_T2_20_Mid/3.png", "Image/P9_D3_P7_Medium_T2_20_Mid/4.png", "Image/P9_D3_P7_Medium_T2_20_Mid/5.png", "Image/P9_D3_P7_Medium_T2_20_Mid/6.png", "Image/P9_D3_P7_Medium_T2_20_Mid/7.png", "Image/P9_D3_P7_Medium_T2_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the middle of the cup? Finish.\"\nThe frames roughly correspond to the speech interval from 404.5974 seconds to 420.1926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P8_Low_T3_18_Right", "images": ["Image/P16_D2_P8_Low_T3_18_Right/1.png", "Image/P16_D2_P8_Low_T3_18_Right/2.png", "Image/P16_D2_P8_Low_T3_18_Right/3.png", "Image/P16_D2_P8_Low_T3_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 814.1958000000001 seconds to 823.7595000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P4_Medium_T2_15_Right", "images": ["Image/P30_D1_P4_Medium_T2_15_Right/1.png", "Image/P30_D1_P4_Medium_T2_15_Right/2.png", "Image/P30_D1_P4_Medium_T2_15_Right/3.png", "Image/P30_D1_P4_Medium_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 646.93367 seconds to 656.66245 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P9_Medium_T2_9_Left", "images": ["Image/P23_D2_P9_Medium_T2_9_Left/1.png", "Image/P23_D2_P9_Medium_T2_9_Left/2.png", "Image/P23_D2_P9_Medium_T2_9_Left/3.png", "Image/P23_D2_P9_Medium_T2_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 501.2937 seconds to 511.3285 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P9_Medium_T1_16_Left", "images": ["Image/P16_D1_P9_Medium_T1_16_Left/1.png", "Image/P16_D1_P9_Medium_T1_16_Left/2.png", "Image/P16_D1_P9_Medium_T1_16_Left/3.png", "Image/P16_D1_P9_Medium_T1_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this mug up here? Finish.\"\nThe frames roughly correspond to the speech interval from 1589.904 seconds to 1601.2616 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P2_High_T1_14_Mid", "images": ["Image/P13_D2_P2_High_T1_14_Mid/1.png", "Image/P13_D2_P2_High_T1_14_Mid/2.png", "Image/P13_D2_P2_High_T1_14_Mid/3.png", "Image/P13_D2_P2_High_T1_14_Mid/4.png", "Image/P13_D2_P2_High_T1_14_Mid/5.png", "Image/P13_D2_P2_High_T1_14_Mid/6.png", "Image/P13_D2_P2_High_T1_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from the top shelf all the way to the bottom shelf down here\"\nThe frames roughly correspond to the speech interval from 819.55596 seconds to 832.25058 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P9_Low_T2_23_Right", "images": ["Image/P9_D2_P9_Low_T2_23_Right/1.png", "Image/P9_D2_P9_Low_T2_23_Right/2.png", "Image/P9_D2_P9_Low_T2_23_Right/3.png", "Image/P9_D2_P9_Low_T2_23_Right/4.png", "Image/P9_D2_P9_Low_T2_23_Right/5.png", "Image/P9_D2_P9_Low_T2_23_Right/6.png", "Image/P9_D2_P9_Low_T2_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3033.2634 seconds to 3046.2188 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P6_Low_T3_19_Mid", "images": ["Image/P10_D1_P6_Low_T3_19_Mid/1.png", "Image/P10_D1_P6_Low_T3_19_Mid/2.png", "Image/P10_D1_P6_Low_T3_19_Mid/3.png", "Image/P10_D1_P6_Low_T3_19_Mid/4.png", "Image/P10_D1_P6_Low_T3_19_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2570.4865 seconds to 2582.3860999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P2_High_T1_4_Left", "images": ["Image/P12_D3_P2_High_T1_4_Left/1.png", "Image/P12_D3_P2_High_T1_4_Left/2.png", "Image/P12_D3_P2_High_T1_4_Left/3.png", "Image/P12_D3_P2_High_T1_4_Left/4.png", "Image/P12_D3_P2_High_T1_4_Left/5.png", "Image/P12_D3_P2_High_T1_4_Left/6.png", "Image/P12_D3_P2_High_T1_4_Left/7.png", "Image/P12_D3_P2_High_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2396.84119 seconds to 2410.29749 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P3_Low_T3_11_Right", "images": ["Image/P15_D2_P3_Low_T3_11_Right/1.png", "Image/P15_D2_P3_Low_T3_11_Right/2.png", "Image/P15_D2_P3_Low_T3_11_Right/3.png", "Image/P15_D2_P3_Low_T3_11_Right/4.png", "Image/P15_D2_P3_Low_T3_11_Right/5.png", "Image/P15_D2_P3_Low_T3_11_Right/6.png", "Image/P15_D2_P3_Low_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown container at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3020.4614 seconds to 3033.8241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P4_Low_T1_23_Right", "images": ["Image/P3_D1_P4_Low_T1_23_Right/1.png", "Image/P3_D1_P4_Low_T1_23_Right/2.png", "Image/P3_D1_P4_Low_T1_23_Right/3.png", "Image/P3_D1_P4_Low_T1_23_Right/4.png", "Image/P3_D1_P4_Low_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this place? Finish.\"\nThe frames roughly correspond to the speech interval from 1344.1591 seconds to 1356.4936 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P7_High_T1_18_Left", "images": ["Image/P27_D2_P7_High_T1_18_Left/1.png", "Image/P27_D2_P7_High_T1_18_Left/2.png", "Image/P27_D2_P7_High_T1_18_Left/3.png", "Image/P27_D2_P7_High_T1_18_Left/4.png", "Image/P27_D2_P7_High_T1_18_Left/5.png", "Image/P27_D2_P7_High_T1_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the red cup to the top of that shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1331.6473 seconds to 1342.8386 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P6_Low_T1_10_Mid", "images": ["Image/P3_D2_P6_Low_T1_10_Mid/1.png", "Image/P3_D2_P6_Low_T1_10_Mid/2.png", "Image/P3_D2_P6_Low_T1_10_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2407.1854000000003 seconds to 2418.8235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P7_High_T2_15_Left", "images": ["Image/P5_D3_P7_High_T2_15_Left/1.png", "Image/P5_D3_P7_High_T2_15_Left/2.png", "Image/P5_D3_P7_High_T2_15_Left/3.png", "Image/P5_D3_P7_High_T2_15_Left/4.png", "Image/P5_D3_P7_High_T2_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant underneath the table. Finished.\"\nThe frames roughly correspond to the speech interval from 2861.7532 seconds to 2872.8480999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P9_High_T3_20_Left", "images": ["Image/P12_D2_P9_High_T3_20_Left/1.png", "Image/P12_D2_P9_High_T3_20_Left/2.png", "Image/P12_D2_P9_High_T3_20_Left/3.png", "Image/P12_D2_P9_High_T3_20_Left/4.png", "Image/P12_D2_P9_High_T3_20_Left/5.png", "Image/P12_D2_P9_High_T3_20_Left/6.png", "Image/P12_D2_P9_High_T3_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white vase on the bottom shelf that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 242.6957 seconds to 259.10630000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P4_Low_T2_15_Right", "images": ["Image/P6_D2_P4_Low_T2_15_Right/1.png", "Image/P6_D2_P4_Low_T2_15_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Is this a cup? I'm a little hungry.\"\nThe frames roughly correspond to the speech interval from 807.8870999999999 seconds to 824.9408 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P6_Low_T1_8_Left", "images": ["Image/P14_D1_P6_Low_T1_8_Left/1.png", "Image/P14_D1_P6_Low_T1_8_Left/2.png", "Image/P14_D1_P6_Low_T1_8_Left/3.png", "Image/P14_D1_P6_Low_T1_8_Left/4.png", "Image/P14_D1_P6_Low_T1_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup to the end of the table. Finish.\"\nThe frames roughly correspond to the speech interval from 347.7901 seconds to 357.5899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P6_Low_T1_9_Left", "images": ["Image/P4_D2_P6_Low_T1_9_Left/1.png", "Image/P4_D2_P6_Low_T1_9_Left/2.png", "Image/P4_D2_P6_Low_T1_9_Left/3.png", "Image/P4_D2_P6_Low_T1_9_Left/4.png", "Image/P4_D2_P6_Low_T1_9_Left/5.png", "Image/P4_D2_P6_Low_T1_9_Left/6.png", "Image/P4_D2_P6_Low_T1_9_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the green cap to the left on the shelter? Finish\"\nThe frames roughly correspond to the speech interval from 1528.0829 seconds to 1544.4916999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P7_High_T1_26_Right", "images": ["Image/P4_D1_P7_High_T1_26_Right/1.png", "Image/P4_D1_P7_High_T1_26_Right/2.png", "Image/P4_D1_P7_High_T1_26_Right/3.png", "Image/P4_D1_P7_High_T1_26_Right/4.png", "Image/P4_D1_P7_High_T1_26_Right/5.png", "Image/P4_D1_P7_High_T1_26_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right bottle up to the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 2304.7106 seconds to 2319.1310999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P1_Low_T2_4_Left", "images": ["Image/P24_D2_P1_Low_T2_4_Left/1.png", "Image/P24_D2_P1_Low_T2_4_Left/2.png", "Image/P24_D2_P1_Low_T2_4_Left/3.png", "Image/P24_D2_P1_Low_T2_4_Left/4.png", "Image/P24_D2_P1_Low_T2_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you pour water to this planet\"\nThe frames roughly correspond to the speech interval from 215.7488 seconds to 226.4122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P2_Medium_T2_10_Left", "images": ["Image/P25_D2_P2_Medium_T2_10_Left/1.png", "Image/P25_D2_P2_Medium_T2_10_Left/2.png", "Image/P25_D2_P2_Medium_T2_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 868.7544 seconds to 877.2473 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P9_Medium_T2_25_Right", "images": ["Image/P15_D3_P9_Medium_T2_25_Right/1.png", "Image/P15_D3_P9_Medium_T2_25_Right/2.png", "Image/P15_D3_P9_Medium_T2_25_Right/3.png", "Image/P15_D3_P9_Medium_T2_25_Right/4.png", "Image/P15_D3_P9_Medium_T2_25_Right/5.png", "Image/P15_D3_P9_Medium_T2_25_Right/6.png", "Image/P15_D3_P9_Medium_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 227.2212 seconds to 241.04260000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P9_High_T1_24_Right", "images": ["Image/P10_D1_P9_High_T1_24_Right/1.png", "Image/P10_D1_P9_High_T1_24_Right/2.png", "Image/P10_D1_P9_High_T1_24_Right/3.png", "Image/P10_D1_P9_High_T1_24_Right/4.png", "Image/P10_D1_P9_High_T1_24_Right/5.png", "Image/P10_D1_P9_High_T1_24_Right/6.png", "Image/P10_D1_P9_High_T1_24_Right/7.png", "Image/P10_D1_P9_High_T1_24_Right/8.png", "Image/P10_D1_P9_High_T1_24_Right/9.png", "Image/P10_D1_P9_High_T1_24_Right/10.png", "Image/P10_D1_P9_High_T1_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2546.3765 seconds to 2562.4125 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P7_Medium_T3_19_Mid", "images": ["Image/P26_D3_P7_Medium_T3_19_Mid/1.png", "Image/P26_D3_P7_Medium_T3_19_Mid/2.png", "Image/P26_D3_P7_Medium_T3_19_Mid/3.png", "Image/P26_D3_P7_Medium_T3_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the middle base on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1131.7504 seconds to 1141.8557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P7_Low_T3_5_Right", "images": ["Image/P9_D1_P7_Low_T3_5_Right/1.png", "Image/P9_D1_P7_Low_T3_5_Right/2.png", "Image/P9_D1_P7_Low_T3_5_Right/3.png", "Image/P9_D1_P7_Low_T3_5_Right/4.png", "Image/P9_D1_P7_Low_T3_5_Right/5.png", "Image/P9_D1_P7_Low_T3_5_Right/6.png", "Image/P9_D1_P7_Low_T3_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the plant in this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1715.6515 seconds to 1729.9683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P1_Low_T2_17_Left", "images": ["Image/P16_D2_P1_Low_T2_17_Left/1.png", "Image/P16_D2_P1_Low_T2_17_Left/2.png", "Image/P16_D2_P1_Low_T2_17_Left/3.png", "Image/P16_D2_P1_Low_T2_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 796.7382 seconds to 807.5561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P7_Medium_T2_3_Left", "images": ["Image/P15_D3_P7_Medium_T2_3_Left/1.png", "Image/P15_D3_P7_Medium_T2_3_Left/2.png", "Image/P15_D3_P7_Medium_T2_3_Left/3.png", "Image/P15_D3_P7_Medium_T2_3_Left/4.png", "Image/P15_D3_P7_Medium_T2_3_Left/5.png", "Image/P15_D3_P7_Medium_T2_3_Left/6.png", "Image/P15_D3_P7_Medium_T2_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 103.41720000000002 seconds to 116.2368 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P7_Low_T1_17_Left", "images": ["Image/P1_D3_P7_Low_T1_17_Left/1.png", "Image/P1_D3_P7_Low_T1_17_Left/2.png", "Image/P1_D3_P7_Low_T1_17_Left/3.png", "Image/P1_D3_P7_Low_T1_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1573.2522 seconds to 1583.2612999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P5_High_T2_27_Left", "images": ["Image/P16_D2_P5_High_T2_27_Left/1.png", "Image/P16_D2_P5_High_T2_27_Left/2.png", "Image/P16_D2_P5_High_T2_27_Left/3.png", "Image/P16_D2_P5_High_T2_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 904.2711 seconds to 914.307 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P6_Medium_T1_10_Left", "images": ["Image/P20_D1_P6_Medium_T1_10_Left/1.png", "Image/P20_D1_P6_Medium_T1_10_Left/2.png", "Image/P20_D1_P6_Medium_T1_10_Left/3.png", "Image/P20_D1_P6_Medium_T1_10_Left/4.png", "Image/P20_D1_P6_Medium_T1_10_Left/5.png", "Image/P20_D1_P6_Medium_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this bottle over to here. Finish.\"\nThe frames roughly correspond to the speech interval from 261.89 seconds to 274.529 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P7_Low_T3_8_Mid", "images": ["Image/P23_D2_P7_Low_T3_8_Mid/1.png", "Image/P23_D2_P7_Low_T3_8_Mid/2.png", "Image/P23_D2_P7_Low_T3_8_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 467.0769 seconds to 478.7422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P2_Medium_T2_12_Right", "images": ["Image/P26_D3_P2_Medium_T2_12_Right/1.png", "Image/P26_D3_P2_Medium_T2_12_Right/2.png", "Image/P26_D3_P2_Medium_T2_12_Right/3.png", "Image/P26_D3_P2_Medium_T2_12_Right/4.png", "Image/P26_D3_P2_Medium_T2_12_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour water into that vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 921.0174 seconds to 932.8494 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P2_High_T3_2_Right", "images": ["Image/P30_D1_P2_High_T3_2_Right/1.png", "Image/P30_D1_P2_High_T3_2_Right/2.png", "Image/P30_D1_P2_High_T3_2_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 907.0131 seconds to 915.5877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P5_Medium_T2_21_Left", "images": ["Image/P1_D1_P5_Medium_T2_21_Left/1.png", "Image/P1_D1_P5_Medium_T2_21_Left/2.png", "Image/P1_D1_P5_Medium_T2_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 207.5755 seconds to 216.2765 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P4_Low_T1_25_Mid", "images": ["Image/P13_D1_P4_Low_T1_25_Mid/1.png", "Image/P13_D1_P4_Low_T1_25_Mid/2.png", "Image/P13_D1_P4_Low_T1_25_Mid/3.png", "Image/P13_D1_P4_Low_T1_25_Mid/4.png", "Image/P13_D1_P4_Low_T1_25_Mid/5.png", "Image/P13_D1_P4_Low_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this white cup to the middle here\"\nThe frames roughly correspond to the speech interval from 379.1857 seconds to 390.9883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P9_Medium_T3_4_Mid", "images": ["Image/P11_D2_P9_Medium_T3_4_Mid/1.png", "Image/P11_D2_P9_Medium_T3_4_Mid/2.png", "Image/P11_D2_P9_Medium_T3_4_Mid/3.png", "Image/P11_D2_P9_Medium_T3_4_Mid/4.png", "Image/P11_D2_P9_Medium_T3_4_Mid/5.png", "Image/P11_D2_P9_Medium_T3_4_Mid/6.png", "Image/P11_D2_P9_Medium_T3_4_Mid/7.png", "Image/P11_D2_P9_Medium_T3_4_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the second plant on the bottom of the shelf. Please finish.\"\nThe frames roughly correspond to the speech interval from 616.2707 seconds to 630.0062 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P8_High_T2_10_Left", "images": ["Image/P22_D3_P8_High_T2_10_Left/1.png", "Image/P22_D3_P8_High_T2_10_Left/2.png", "Image/P22_D3_P8_High_T2_10_Left/3.png", "Image/P22_D3_P8_High_T2_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the orange bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 336.4013 seconds to 346.8133 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P1_Medium_T3_20_Right", "images": ["Image/P13_D2_P1_Medium_T3_20_Right/1.png", "Image/P13_D2_P1_Medium_T3_20_Right/2.png", "Image/P13_D2_P1_Medium_T3_20_Right/3.png", "Image/P13_D2_P1_Medium_T3_20_Right/4.png", "Image/P13_D2_P1_Medium_T3_20_Right/5.png", "Image/P13_D2_P1_Medium_T3_20_Right/6.png", "Image/P13_D2_P1_Medium_T3_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the white bottle on the right\"\nThe frames roughly correspond to the speech interval from 1139.53386 seconds to 1151.54516 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P2_High_T2_22_Right", "images": ["Image/P6_D1_P2_High_T2_22_Right/1.png", "Image/P6_D1_P2_High_T2_22_Right/2.png", "Image/P6_D1_P2_High_T2_22_Right/3.png", "Image/P6_D1_P2_High_T2_22_Right/4.png", "Image/P6_D1_P2_High_T2_22_Right/5.png", "Image/P6_D1_P2_High_T2_22_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2506.9977000000003 seconds to 2519.8818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P2_Low_T3_12_Mid", "images": ["Image/P13_D1_P2_Low_T3_12_Mid/1.png", "Image/P13_D1_P2_Low_T3_12_Mid/2.png", "Image/P13_D1_P2_Low_T3_12_Mid/3.png", "Image/P13_D1_P2_Low_T3_12_Mid/4.png", "Image/P13_D1_P2_Low_T3_12_Mid/5.png", "Image/P13_D1_P2_Low_T3_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean off the brown vase here\"\nThe frames roughly correspond to the speech interval from 399.9532 seconds to 411.3669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P5_Low_T1_3_Left", "images": ["Image/P11_D2_P5_Low_T1_3_Left/1.png", "Image/P11_D2_P5_Low_T1_3_Left/2.png", "Image/P11_D2_P5_Low_T1_3_Left/3.png", "Image/P11_D2_P5_Low_T1_3_Left/4.png", "Image/P11_D2_P5_Low_T1_3_Left/5.png", "Image/P11_D2_P5_Low_T1_3_Left/6.png", "Image/P11_D2_P5_Low_T1_3_Left/7.png", "Image/P11_D2_P5_Low_T1_3_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green glass to the top of the right shelf please. Finish.\"\nThe frames roughly correspond to the speech interval from 924.7037 seconds to 942.4502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P9_Medium_T1_22_Mid", "images": ["Image/P24_D3_P9_Medium_T1_22_Mid/1.png", "Image/P24_D3_P9_Medium_T1_22_Mid/2.png", "Image/P24_D3_P9_Medium_T1_22_Mid/3.png", "Image/P24_D3_P9_Medium_T1_22_Mid/4.png", "Image/P24_D3_P9_Medium_T1_22_Mid/5.png", "Image/P24_D3_P9_Medium_T1_22_Mid/6.png", "Image/P24_D3_P9_Medium_T1_22_Mid/7.png", "Image/P24_D3_P9_Medium_T1_22_Mid/8.png", "Image/P24_D3_P9_Medium_T1_22_Mid/9.png", "Image/P24_D3_P9_Medium_T1_22_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet up to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1960.2205 seconds to 1975.224 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P4_Medium_T3_2_Mid", "images": ["Image/P7_D2_P4_Medium_T3_2_Mid/1.png", "Image/P7_D2_P4_Medium_T3_2_Mid/2.png", "Image/P7_D2_P4_Medium_T3_2_Mid/3.png", "Image/P7_D2_P4_Medium_T3_2_Mid/4.png", "Image/P7_D2_P4_Medium_T3_2_Mid/5.png", "Image/P7_D2_P4_Medium_T3_2_Mid/6.png", "Image/P7_D2_P4_Medium_T3_2_Mid/7.png", "Image/P7_D2_P4_Medium_T3_2_Mid/8.png", "Image/P7_D2_P4_Medium_T3_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1395.1272 seconds to 1411.6087000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P7_Medium_T2_10_Left", "images": ["Image/P14_D3_P7_Medium_T2_10_Left/1.png", "Image/P14_D3_P7_Medium_T2_10_Left/2.png", "Image/P14_D3_P7_Medium_T2_10_Left/3.png", "Image/P14_D3_P7_Medium_T2_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water that first bottle\"\nThe frames roughly correspond to the speech interval from 1226.39576 seconds to 1235.58766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P7_Medium_T1_2_Mid", "images": ["Image/P24_D2_P7_Medium_T1_2_Mid/1.png", "Image/P24_D2_P7_Medium_T1_2_Mid/2.png", "Image/P24_D2_P7_Medium_T1_2_Mid/3.png", "Image/P24_D2_P7_Medium_T1_2_Mid/4.png", "Image/P24_D2_P7_Medium_T1_2_Mid/5.png", "Image/P24_D2_P7_Medium_T1_2_Mid/6.png", "Image/P24_D2_P7_Medium_T1_2_Mid/7.png", "Image/P24_D2_P7_Medium_T1_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this bottle up here\"\nThe frames roughly correspond to the speech interval from 234.5132 seconds to 248.7263 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P9_High_T3_17_Right", "images": ["Image/P14_D3_P9_High_T3_17_Right/1.png", "Image/P14_D3_P9_High_T3_17_Right/2.png", "Image/P14_D3_P9_High_T3_17_Right/3.png", "Image/P14_D3_P9_High_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1114.7650600000002 seconds to 1124.17686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P7_Medium_T3_22_Left", "images": ["Image/P11_D1_P7_Medium_T3_22_Left/1.png", "Image/P11_D1_P7_Medium_T3_22_Left/2.png", "Image/P11_D1_P7_Medium_T3_22_Left/3.png", "Image/P11_D1_P7_Medium_T3_22_Left/4.png", "Image/P11_D1_P7_Medium_T3_22_Left/5.png", "Image/P11_D1_P7_Medium_T3_22_Left/6.png", "Image/P11_D1_P7_Medium_T3_22_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first plant from the left side in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 2374.1717099999996 seconds to 2386.8095999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P5_Medium_T1_1_Mid", "images": ["Image/P23_D3_P5_Medium_T1_1_Mid/1.png", "Image/P23_D3_P5_Medium_T1_1_Mid/2.png", "Image/P23_D3_P5_Medium_T1_1_Mid/3.png", "Image/P23_D3_P5_Medium_T1_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put the middle one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1047.4857 seconds to 1056.6164 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P3_Medium_T2_7_Left", "images": ["Image/P10_D2_P3_Medium_T2_7_Left/1.png", "Image/P10_D2_P3_Medium_T2_7_Left/2.png", "Image/P10_D2_P3_Medium_T2_7_Left/3.png", "Image/P10_D2_P3_Medium_T2_7_Left/4.png", "Image/P10_D2_P3_Medium_T2_7_Left/5.png", "Image/P10_D2_P3_Medium_T2_7_Left/6.png", "Image/P10_D2_P3_Medium_T2_7_Left/7.png", "Image/P10_D2_P3_Medium_T2_7_Left/8.png", "Image/P10_D2_P3_Medium_T2_7_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1798.5799800000002 seconds to 1813.5025800000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P2_High_T3_8_Right", "images": ["Image/P5_D2_P2_High_T3_8_Right/1.png", "Image/P5_D2_P2_High_T3_8_Right/2.png", "Image/P5_D2_P2_High_T3_8_Right/3.png", "Image/P5_D2_P2_High_T3_8_Right/4.png", "Image/P5_D2_P2_High_T3_8_Right/5.png", "Image/P5_D2_P2_High_T3_8_Right/6.png", "Image/P5_D2_P2_High_T3_8_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle next to the plant over there. Finished.\"\nThe frames roughly correspond to the speech interval from 381.8999 seconds to 399.1739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P6_Medium_T2_23_Right", "images": ["Image/P22_D2_P6_Medium_T2_23_Right/1.png", "Image/P22_D2_P6_Medium_T2_23_Right/2.png", "Image/P22_D2_P6_Medium_T2_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Water the plant on the right. Finish\"\nThe frames roughly correspond to the speech interval from 969.2414 seconds to 979.347 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P1_Low_T3_13_Right", "images": ["Image/P4_D2_P1_Low_T3_13_Right/1.png", "Image/P4_D2_P1_Low_T3_13_Right/2.png", "Image/P4_D2_P1_Low_T3_13_Right/3.png", "Image/P4_D2_P1_Low_T3_13_Right/4.png", "Image/P4_D2_P1_Low_T3_13_Right/5.png", "Image/P4_D2_P1_Low_T3_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant on the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 1847.7378 seconds to 1862.0179 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P5_High_T1_23_Mid", "images": ["Image/P2_D1_P5_High_T1_23_Mid/1.png", "Image/P2_D1_P5_High_T1_23_Mid/2.png", "Image/P2_D1_P5_High_T1_23_Mid/3.png", "Image/P2_D1_P5_High_T1_23_Mid/4.png", "Image/P2_D1_P5_High_T1_23_Mid/5.png", "Image/P2_D1_P5_High_T1_23_Mid/6.png", "Image/P2_D1_P5_High_T1_23_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 385.1732999999999 seconds to 398.36639999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P7_Medium_T1_7_Left", "images": ["Image/P23_D2_P7_Medium_T1_7_Left/1.png", "Image/P23_D2_P7_Medium_T1_7_Left/2.png", "Image/P23_D2_P7_Medium_T1_7_Left/3.png", "Image/P23_D2_P7_Medium_T1_7_Left/4.png", "Image/P23_D2_P7_Medium_T1_7_Left/5.png", "Image/P23_D2_P7_Medium_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 268.09182999999996 seconds to 280.24372999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P7_Medium_T1_14_Right", "images": ["Image/P16_D1_P7_Medium_T1_14_Right/1.png", "Image/P16_D1_P7_Medium_T1_14_Right/2.png", "Image/P16_D1_P7_Medium_T1_14_Right/3.png", "Image/P16_D1_P7_Medium_T1_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move that plant up here? Finish.\"\nThe frames roughly correspond to the speech interval from 1194.48158 seconds to 1211.15609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P2_Low_T1_20_Mid", "images": ["Image/P3_D3_P2_Low_T1_20_Mid/1.png", "Image/P3_D3_P2_Low_T1_20_Mid/2.png", "Image/P3_D3_P2_Low_T1_20_Mid/3.png", "Image/P3_D3_P2_Low_T1_20_Mid/4.png", "Image/P3_D3_P2_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the bottle to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 846.1139 seconds to 857.1535999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P9_High_T2_21_Left", "images": ["Image/P2_D3_P9_High_T2_21_Left/1.png", "Image/P2_D3_P9_High_T2_21_Left/2.png", "Image/P2_D3_P9_High_T2_21_Left/3.png", "Image/P2_D3_P9_High_T2_21_Left/4.png", "Image/P2_D3_P9_High_T2_21_Left/5.png", "Image/P2_D3_P9_High_T2_21_Left/6.png", "Image/P2_D3_P9_High_T2_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that white vessel there? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2157.1991000000003 seconds to 2169.3515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P3_Medium_T1_27_Right", "images": ["Image/P29_D3_P3_Medium_T1_27_Right/1.png", "Image/P29_D3_P3_Medium_T1_27_Right/2.png", "Image/P29_D3_P3_Medium_T1_27_Right/3.png", "Image/P29_D3_P3_Medium_T1_27_Right/4.png", "Image/P29_D3_P3_Medium_T1_27_Right/5.png", "Image/P29_D3_P3_Medium_T1_27_Right/6.png", "Image/P29_D3_P3_Medium_T1_27_Right/7.png", "Image/P29_D3_P3_Medium_T1_27_Right/8.png", "Image/P29_D3_P3_Medium_T1_27_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Bridie\"\nThe frames roughly correspond to the speech interval from 1357.9761 seconds to 1373.1478999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P8_Low_T2_8_Left", "images": ["Image/P21_D1_P8_Low_T2_8_Left/1.png", "Image/P21_D1_P8_Low_T2_8_Left/2.png", "Image/P21_D1_P8_Low_T2_8_Left/3.png", "Image/P21_D1_P8_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 992.3122 seconds to 1002.25 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P9_Medium_T2_13_Right", "images": ["Image/P14_D2_P9_Medium_T2_13_Right/1.png", "Image/P14_D2_P9_Medium_T2_13_Right/2.png", "Image/P14_D2_P9_Medium_T2_13_Right/3.png", "Image/P14_D2_P9_Medium_T2_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1760.909 seconds to 1769.9614 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P2_Medium_T3_5_Mid", "images": ["Image/P23_D1_P2_Medium_T3_5_Mid/1.png", "Image/P23_D1_P2_Medium_T3_5_Mid/2.png", "Image/P23_D1_P2_Medium_T3_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start cleaning up this one\"\nThe frames roughly correspond to the speech interval from 1769.6332 seconds to 1778.2904 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P8_Low_T3_6_Right", "images": ["Image/P3_D3_P8_Low_T3_6_Right/1.png", "Image/P3_D3_P8_Low_T3_6_Right/2.png", "Image/P3_D3_P8_Low_T3_6_Right/3.png", "Image/P3_D3_P8_Low_T3_6_Right/4.png", "Image/P3_D3_P8_Low_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 729.602 seconds to 740.4875 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
