{"id": "P23_D2_P4_Low_T3_21_Left", "images": ["Image/P23_D2_P4_Low_T3_21_Left/1.png", "Image/P23_D2_P4_Low_T3_21_Left/2.png", "Image/P23_D2_P4_Low_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 376.1667 seconds to 384.7541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P2_High_T1_23_Right", "images": ["Image/P1_D3_P2_High_T1_23_Right/1.png", "Image/P1_D3_P2_High_T1_23_Right/2.png", "Image/P1_D3_P2_High_T1_23_Right/3.png", "Image/P1_D3_P2_High_T1_23_Right/4.png", "Image/P1_D3_P2_High_T1_23_Right/5.png", "Image/P1_D3_P2_High_T1_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1712.0005 seconds to 1723.2345999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P6_Low_T1_23_Mid", "images": ["Image/P18_D3_P6_Low_T1_23_Mid/1.png", "Image/P18_D3_P6_Low_T1_23_Mid/2.png", "Image/P18_D3_P6_Low_T1_23_Mid/3.png", "Image/P18_D3_P6_Low_T1_23_Mid/4.png", "Image/P18_D3_P6_Low_T1_23_Mid/5.png", "Image/P18_D3_P6_Low_T1_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1247.7401 seconds to 1259.2098 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P3_Medium_T2_11_Right", "images": ["Image/P28_D1_P3_Medium_T2_11_Right/1.png", "Image/P28_D1_P3_Medium_T2_11_Right/2.png", "Image/P28_D1_P3_Medium_T2_11_Right/3.png", "Image/P28_D1_P3_Medium_T2_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1641.9312 seconds to 1652.7609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P3_High_T1_5_Right", "images": ["Image/P26_D2_P3_High_T1_5_Right/1.png", "Image/P26_D2_P3_High_T1_5_Right/2.png", "Image/P26_D2_P3_High_T1_5_Right/3.png", "Image/P26_D2_P3_High_T1_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to the bottom shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1275.8949699999998 seconds to 1286.54283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P1_Medium_T1_13_Left", "images": ["Image/P4_D3_P1_Medium_T1_13_Left/1.png", "Image/P4_D3_P1_Medium_T1_13_Left/2.png", "Image/P4_D3_P1_Medium_T1_13_Left/3.png", "Image/P4_D3_P1_Medium_T1_13_Left/4.png", "Image/P4_D3_P1_Medium_T1_13_Left/5.png", "Image/P4_D3_P1_Medium_T1_13_Left/6.png", "Image/P4_D3_P1_Medium_T1_13_Left/7.png", "Image/P4_D3_P1_Medium_T1_13_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the left plant to the ground under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 626.394 seconds to 642.2896999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P9_High_T3_24_Mid", "images": ["Image/P18_D3_P9_High_T3_24_Mid/1.png", "Image/P18_D3_P9_High_T3_24_Mid/2.png", "Image/P18_D3_P9_High_T3_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1569.081 seconds to 1576.9316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P3_Medium_T3_19_Left", "images": ["Image/P18_D3_P3_Medium_T3_19_Left/1.png", "Image/P18_D3_P3_Medium_T3_19_Left/2.png", "Image/P18_D3_P3_Medium_T3_19_Left/3.png", "Image/P18_D3_P3_Medium_T3_19_Left/4.png", "Image/P18_D3_P3_Medium_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1270.7085000000002 seconds to 1281.7589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P8_Low_T1_14_Mid", "images": ["Image/P7_D3_P8_Low_T1_14_Mid/1.png", "Image/P7_D3_P8_Low_T1_14_Mid/2.png", "Image/P7_D3_P8_Low_T1_14_Mid/3.png", "Image/P7_D3_P8_Low_T1_14_Mid/4.png", "Image/P7_D3_P8_Low_T1_14_Mid/5.png", "Image/P7_D3_P8_Low_T1_14_Mid/6.png", "Image/P7_D3_P8_Low_T1_14_Mid/7.png", "Image/P7_D3_P8_Low_T1_14_Mid/8.png", "Image/P7_D3_P8_Low_T1_14_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1957.8357 seconds to 1974.5677 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P9_High_T1_15_Mid", "images": ["Image/P11_D2_P9_High_T1_15_Mid/1.png", "Image/P11_D2_P9_High_T1_15_Mid/2.png", "Image/P11_D2_P9_High_T1_15_Mid/3.png", "Image/P11_D2_P9_High_T1_15_Mid/4.png", "Image/P11_D2_P9_High_T1_15_Mid/5.png", "Image/P11_D2_P9_High_T1_15_Mid/6.png", "Image/P11_D2_P9_High_T1_15_Mid/7.png", "Image/P11_D2_P9_High_T1_15_Mid/8.png", "Image/P11_D2_P9_High_T1_15_Mid/9.png", "Image/P11_D2_P9_High_T1_15_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move the plant to the Top of the shelf on my right side\"\nThe frames roughly correspond to the speech interval from 852.3414 seconds to 870.6287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P7_Low_T1_5_Mid", "images": ["Image/P5_D3_P7_Low_T1_5_Mid/1.png", "Image/P5_D3_P7_Low_T1_5_Mid/2.png", "Image/P5_D3_P7_Low_T1_5_Mid/3.png", "Image/P5_D3_P7_Low_T1_5_Mid/4.png", "Image/P5_D3_P7_Low_T1_5_Mid/5.png", "Image/P5_D3_P7_Low_T1_5_Mid/6.png", "Image/P5_D3_P7_Low_T1_5_Mid/7.png", "Image/P5_D3_P7_Low_T1_5_Mid/8.png", "Image/P5_D3_P7_Low_T1_5_Mid/9.png", "Image/P5_D3_P7_Low_T1_5_Mid/10.png", "Image/P5_D3_P7_Low_T1_5_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are two levels of bookshelves on top of the table. Move the plant to the top level of the bookshelf. Finished.\"\nThe frames roughly correspond to the speech interval from 2438.7826 seconds to 2456.3775 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P2_Low_T2_14_Left", "images": ["Image/P22_D1_P2_Low_T2_14_Left/1.png", "Image/P22_D1_P2_Low_T2_14_Left/2.png", "Image/P22_D1_P2_Low_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1311.54778 seconds to 1320.35832 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P8_Medium_T2_21_Left", "images": ["Image/P28_D3_P8_Medium_T2_21_Left/1.png", "Image/P28_D3_P8_Medium_T2_21_Left/2.png", "Image/P28_D3_P8_Medium_T2_21_Left/3.png", "Image/P28_D3_P8_Medium_T2_21_Left/4.png", "Image/P28_D3_P8_Medium_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 375.2899 seconds to 385.8549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P1_Low_T2_1_Left", "images": ["Image/P29_D2_P1_Low_T2_1_Left/1.png", "Image/P29_D2_P1_Low_T2_1_Left/2.png", "Image/P29_D2_P1_Low_T2_1_Left/3.png", "Image/P29_D2_P1_Low_T2_1_Left/4.png", "Image/P29_D2_P1_Low_T2_1_Left/5.png", "Image/P29_D2_P1_Low_T2_1_Left/6.png", "Image/P29_D2_P1_Low_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 185.5173 seconds to 197.8214 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P4_Low_T1_19_Mid", "images": ["Image/P11_D3_P4_Low_T1_19_Mid/1.png", "Image/P11_D3_P4_Low_T1_19_Mid/2.png", "Image/P11_D3_P4_Low_T1_19_Mid/3.png", "Image/P11_D3_P4_Low_T1_19_Mid/4.png", "Image/P11_D3_P4_Low_T1_19_Mid/5.png", "Image/P11_D3_P4_Low_T1_19_Mid/6.png", "Image/P11_D3_P4_Low_T1_19_Mid/7.png", "Image/P11_D3_P4_Low_T1_19_Mid/8.png", "Image/P11_D3_P4_Low_T1_19_Mid/9.png", "Image/P11_D3_P4_Low_T1_19_Mid/10.png", "Image/P11_D3_P4_Low_T1_19_Mid/11.png", "Image/P11_D3_P4_Low_T1_19_Mid/12.png", "Image/P11_D3_P4_Low_T1_19_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the bottle on the desk to the third layer of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 1900.8192 seconds to 1921.561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P8_Low_T2_24_Right", "images": ["Image/P10_D2_P8_Low_T2_24_Right/1.png", "Image/P10_D2_P8_Low_T2_24_Right/2.png", "Image/P10_D2_P8_Low_T2_24_Right/3.png", "Image/P10_D2_P8_Low_T2_24_Right/4.png", "Image/P10_D2_P8_Low_T2_24_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1720.3793800000003 seconds to 1731.3192800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P3_High_T3_11_Mid", "images": ["Image/P3_D2_P3_High_T3_11_Mid/1.png", "Image/P3_D2_P3_High_T3_11_Mid/2.png", "Image/P3_D2_P3_High_T3_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you move the item to this table?\"\nThe frames roughly correspond to the speech interval from 2506.2251 seconds to 2515.4589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P8_High_T2_8_Right", "images": ["Image/P12_D1_P8_High_T2_8_Right/1.png", "Image/P12_D1_P8_High_T2_8_Right/2.png", "Image/P12_D1_P8_High_T2_8_Right/3.png", "Image/P12_D1_P8_High_T2_8_Right/4.png", "Image/P12_D1_P8_High_T2_8_Right/5.png", "Image/P12_D1_P8_High_T2_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean the green mug that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 1266.73042 seconds to 1279.31452 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P6_Low_T1_15_Right", "images": ["Image/P23_D3_P6_Low_T1_15_Right/1.png", "Image/P23_D3_P6_Low_T1_15_Right/2.png", "Image/P23_D3_P6_Low_T1_15_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 784.0235 seconds to 793.0713000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P4_Medium_T3_19_Right", "images": ["Image/P1_D2_P4_Medium_T3_19_Right/1.png", "Image/P1_D2_P4_Medium_T3_19_Right/2.png", "Image/P1_D2_P4_Medium_T3_19_Right/3.png", "Image/P1_D2_P4_Medium_T3_19_Right/4.png", "Image/P1_D2_P4_Medium_T3_19_Right/5.png", "Image/P1_D2_P4_Medium_T3_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 704.07677 seconds to 715.1014100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P8_High_T2_19_Right", "images": ["Image/P8_D2_P8_High_T2_19_Right/1.png", "Image/P8_D2_P8_High_T2_19_Right/2.png", "Image/P8_D2_P8_High_T2_19_Right/3.png", "Image/P8_D2_P8_High_T2_19_Right/4.png", "Image/P8_D2_P8_High_T2_19_Right/5.png", "Image/P8_D2_P8_High_T2_19_Right/6.png", "Image/P8_D2_P8_High_T2_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the right bottom under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2316.3196000000003 seconds to 2329.5619 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P6_Medium_T3_14_Left", "images": ["Image/P8_D3_P6_Medium_T3_14_Left/1.png", "Image/P8_D3_P6_Medium_T3_14_Left/2.png", "Image/P8_D3_P6_Medium_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 1594.5437 seconds to 1607.1398 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P3_Medium_T1_11_Right", "images": ["Image/P15_D3_P3_Medium_T1_11_Right/1.png", "Image/P15_D3_P3_Medium_T1_11_Right/2.png", "Image/P15_D3_P3_Medium_T1_11_Right/3.png", "Image/P15_D3_P3_Medium_T1_11_Right/4.png", "Image/P15_D3_P3_Medium_T1_11_Right/5.png", "Image/P15_D3_P3_Medium_T1_11_Right/6.png", "Image/P15_D3_P3_Medium_T1_11_Right/7.png", "Image/P15_D3_P3_Medium_T1_11_Right/8.png", "Image/P15_D3_P3_Medium_T1_11_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the right side pot into the lowest level of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 169.2726 seconds to 184.3337 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P5_High_T2_5_Mid", "images": ["Image/P5_D1_P5_High_T2_5_Mid/1.png", "Image/P5_D1_P5_High_T2_5_Mid/2.png", "Image/P5_D1_P5_High_T2_5_Mid/3.png", "Image/P5_D1_P5_High_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1840.959 seconds to 1851.5492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_High_T3_16_Left", "images": ["Image/P8_D3_P3_High_T3_16_Left/1.png", "Image/P8_D3_P3_High_T3_16_Left/2.png", "Image/P8_D3_P3_High_T3_16_Left/3.png", "Image/P8_D3_P3_High_T3_16_Left/4.png", "Image/P8_D3_P3_High_T3_16_Left/5.png", "Image/P8_D3_P3_High_T3_16_Left/6.png", "Image/P8_D3_P3_High_T3_16_Left/7.png", "Image/P8_D3_P3_High_T3_16_Left/8.png", "Image/P8_D3_P3_High_T3_16_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Hey\"\nThe frames roughly correspond to the speech interval from 1638.6703 seconds to 1653.1593 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P2_High_T2_18_Left", "images": ["Image/P3_D3_P2_High_T2_18_Left/1.png", "Image/P3_D3_P2_High_T2_18_Left/2.png", "Image/P3_D3_P2_High_T2_18_Left/3.png", "Image/P3_D3_P2_High_T2_18_Left/4.png", "Image/P3_D3_P2_High_T2_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you put some water into the first item? Finish.\"\nThe frames roughly correspond to the speech interval from 767.3424 seconds to 779.8304999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P5_High_T1_7_Mid", "images": ["Image/P25_D2_P5_High_T1_7_Mid/1.png", "Image/P25_D2_P5_High_T1_7_Mid/2.png", "Image/P25_D2_P5_High_T1_7_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 641.97224 seconds to 650.5733 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P2_Medium_T3_8_Mid", "images": ["Image/P19_D1_P2_Medium_T3_8_Mid/1.png", "Image/P19_D1_P2_Medium_T3_8_Mid/2.png", "Image/P19_D1_P2_Medium_T3_8_Mid/3.png", "Image/P19_D1_P2_Medium_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this glass. Finish.\"\nThe frames roughly correspond to the speech interval from 582.6706 seconds to 597.2869000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P8_High_T3_24_Left", "images": ["Image/P4_D2_P8_High_T3_24_Left/1.png", "Image/P4_D2_P8_High_T3_24_Left/2.png", "Image/P4_D2_P8_High_T3_24_Left/3.png", "Image/P4_D2_P8_High_T3_24_Left/4.png", "Image/P4_D2_P8_High_T3_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1763.2036999999998 seconds to 1776.357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P4_High_T3_20_Mid", "images": ["Image/P5_D2_P4_High_T3_20_Mid/1.png", "Image/P5_D2_P4_High_T3_20_Mid/2.png", "Image/P5_D2_P4_High_T3_20_Mid/3.png", "Image/P5_D2_P4_High_T3_20_Mid/4.png", "Image/P5_D2_P4_High_T3_20_Mid/5.png", "Image/P5_D2_P4_High_T3_20_Mid/6.png", "Image/P5_D2_P4_High_T3_20_Mid/7.png", "Image/P5_D2_P4_High_T3_20_Mid/8.png", "Image/P5_D2_P4_High_T3_20_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean the bottle in white color\"\nThe frames roughly correspond to the speech interval from 167.70989999999998 seconds to 183.4535 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P9_Medium_T2_17_Right", "images": ["Image/P6_D2_P9_Medium_T2_17_Right/1.png", "Image/P6_D2_P9_Medium_T2_17_Right/2.png", "Image/P6_D2_P9_Medium_T2_17_Right/3.png", "Image/P6_D2_P9_Medium_T2_17_Right/4.png", "Image/P6_D2_P9_Medium_T2_17_Right/5.png", "Image/P6_D2_P9_Medium_T2_17_Right/6.png", "Image/P6_D2_P9_Medium_T2_17_Right/7.png", "Image/P6_D2_P9_Medium_T2_17_Right/8.png", "Image/P6_D2_P9_Medium_T2_17_Right/9.png", "Image/P6_D2_P9_Medium_T2_17_Right/10.png", "Image/P6_D2_P9_Medium_T2_17_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 611.3224 seconds to 629.9078999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P4_High_T2_27_Right", "images": ["Image/P11_D1_P4_High_T2_27_Right/1.png", "Image/P11_D1_P4_High_T2_27_Right/2.png", "Image/P11_D1_P4_High_T2_27_Right/3.png", "Image/P11_D1_P4_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 2835.5053 seconds to 2845.5545999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P9_Medium_T1_12_Right", "images": ["Image/P4_D3_P9_Medium_T1_12_Right/1.png", "Image/P4_D3_P9_Medium_T1_12_Right/2.png", "Image/P4_D3_P9_Medium_T1_12_Right/3.png", "Image/P4_D3_P9_Medium_T1_12_Right/4.png", "Image/P4_D3_P9_Medium_T1_12_Right/5.png", "Image/P4_D3_P9_Medium_T1_12_Right/6.png", "Image/P4_D3_P9_Medium_T1_12_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right item to the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 471.04979999999995 seconds to 485.0938999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P6_High_T2_13_Mid", "images": ["Image/P23_D3_P6_High_T2_13_Mid/1.png", "Image/P23_D3_P6_High_T2_13_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 767.6532 seconds to 775.8667 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P3_Medium_T2_17_Mid", "images": ["Image/P24_D2_P3_Medium_T2_17_Mid/1.png", "Image/P24_D2_P3_Medium_T2_17_Mid/2.png", "Image/P24_D2_P3_Medium_T2_17_Mid/3.png", "Image/P24_D2_P3_Medium_T2_17_Mid/4.png", "Image/P24_D2_P3_Medium_T2_17_Mid/5.png", "Image/P24_D2_P3_Medium_T2_17_Mid/6.png", "Image/P24_D2_P3_Medium_T2_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you pour water in this cup\"\nThe frames roughly correspond to the speech interval from 395.289 seconds to 408.1644 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P4_High_T1_25_Mid", "images": ["Image/P14_D2_P4_High_T1_25_Mid/1.png", "Image/P14_D2_P4_High_T1_25_Mid/2.png", "Image/P14_D2_P4_High_T1_25_Mid/3.png", "Image/P14_D2_P4_High_T1_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that white cup to the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 1777.4197 seconds to 1786.3629999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P1_Medium_T2_20_Right", "images": ["Image/P12_D2_P1_Medium_T2_20_Right/1.png", "Image/P12_D2_P1_Medium_T2_20_Right/2.png", "Image/P12_D2_P1_Medium_T2_20_Right/3.png", "Image/P12_D2_P1_Medium_T2_20_Right/4.png", "Image/P12_D2_P1_Medium_T2_20_Right/5.png", "Image/P12_D2_P1_Medium_T2_20_Right/6.png", "Image/P12_D2_P1_Medium_T2_20_Right/7.png", "Image/P12_D2_P1_Medium_T2_20_Right/8.png", "Image/P12_D2_P1_Medium_T2_20_Right/9.png", "Image/P12_D2_P1_Medium_T2_20_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the middle white vase on the shelf that I'm pointing at. Finish\"\nThe frames roughly correspond to the speech interval from 122.95588 seconds to 139.92427 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P8_Low_T3_16_Mid", "images": ["Image/P10_D1_P8_Low_T3_16_Mid/1.png", "Image/P10_D1_P8_Low_T3_16_Mid/2.png", "Image/P10_D1_P8_Low_T3_16_Mid/3.png", "Image/P10_D1_P8_Low_T3_16_Mid/4.png", "Image/P10_D1_P8_Low_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 2403.1973 seconds to 2414.9174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P3_High_T2_1_Mid", "images": ["Image/P5_D1_P3_High_T2_1_Mid/1.png", "Image/P5_D1_P3_High_T2_1_Mid/2.png", "Image/P5_D1_P3_High_T2_1_Mid/3.png", "Image/P5_D1_P3_High_T2_1_Mid/4.png", "Image/P5_D1_P3_High_T2_1_Mid/5.png", "Image/P5_D1_P3_High_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle which is next to the plant. Finished.\"\nThe frames roughly correspond to the speech interval from 1748.8781 seconds to 1761.1119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P6_High_T3_22_Right", "images": ["Image/P5_D1_P6_High_T3_22_Right/1.png", "Image/P5_D1_P6_High_T3_22_Right/2.png", "Image/P5_D1_P6_High_T3_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finished.\"\nThe frames roughly correspond to the speech interval from 1622.4448 seconds to 1631.2545 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P7_High_T1_9_Left", "images": ["Image/P26_D2_P7_High_T1_9_Left/1.png", "Image/P26_D2_P7_High_T1_9_Left/2.png", "Image/P26_D2_P7_High_T1_9_Left/3.png", "Image/P26_D2_P7_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this green vase to the top shelf\"\nThe frames roughly correspond to the speech interval from 1345.7166 seconds to 1355.7804999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P3_Low_T2_14_Left", "images": ["Image/P29_D3_P3_Low_T2_14_Left/1.png", "Image/P29_D3_P3_Low_T2_14_Left/2.png", "Image/P29_D3_P3_Low_T2_14_Left/3.png", "Image/P29_D3_P3_Low_T2_14_Left/4.png", "Image/P29_D3_P3_Low_T2_14_Left/5.png", "Image/P29_D3_P3_Low_T2_14_Left/6.png", "Image/P29_D3_P3_Low_T2_14_Left/7.png", "Image/P29_D3_P3_Low_T2_14_Left/8.png", "Image/P29_D3_P3_Low_T2_14_Left/9.png", "Image/P29_D3_P3_Low_T2_14_Left/10.png", "Image/P29_D3_P3_Low_T2_14_Left/11.png", "Image/P29_D3_P3_Low_T2_14_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1572.0489999999998 seconds to 1589.6991999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P5_High_T1_11_Right", "images": ["Image/P9_D2_P5_High_T1_11_Right/1.png", "Image/P9_D2_P5_High_T1_11_Right/2.png", "Image/P9_D2_P5_High_T1_11_Right/3.png", "Image/P9_D2_P5_High_T1_11_Right/4.png", "Image/P9_D2_P5_High_T1_11_Right/5.png", "Image/P9_D2_P5_High_T1_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the right bottle on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 2750.3683 seconds to 2764.9622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P2_Medium_T2_1_Left", "images": ["Image/P13_D3_P2_Medium_T2_1_Left/1.png", "Image/P13_D3_P2_Medium_T2_1_Left/2.png", "Image/P13_D3_P2_Medium_T2_1_Left/3.png", "Image/P13_D3_P2_Medium_T2_1_Left/4.png", "Image/P13_D3_P2_Medium_T2_1_Left/5.png", "Image/P13_D3_P2_Medium_T2_1_Left/6.png", "Image/P13_D3_P2_Medium_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you fill up the left green bottle with water\"\nThe frames roughly correspond to the speech interval from 1673.3856 seconds to 1686.2197 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P5_High_T3_3_Right", "images": ["Image/P26_D2_P5_High_T3_3_Right/1.png", "Image/P26_D2_P5_High_T3_3_Right/2.png", "Image/P26_D2_P5_High_T3_3_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the green vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1646.3727 seconds to 1655.4887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P9_Low_T2_2_Left", "images": ["Image/P16_D1_P9_Low_T2_2_Left/1.png", "Image/P16_D1_P9_Low_T2_2_Left/2.png", "Image/P16_D1_P9_Low_T2_2_Left/3.png", "Image/P16_D1_P9_Low_T2_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into this mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1501.1914000000002 seconds to 1511.4915 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P2_Low_T3_26_Mid", "images": ["Image/P24_D2_P2_Low_T3_26_Mid/1.png", "Image/P24_D2_P2_Low_T3_26_Mid/2.png", "Image/P24_D2_P2_Low_T3_26_Mid/3.png", "Image/P24_D2_P2_Low_T3_26_Mid/4.png", "Image/P24_D2_P2_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 189.1776 seconds to 199.659 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P4_High_T3_3_Right", "images": ["Image/P27_D2_P4_High_T3_3_Right/1.png", "Image/P27_D2_P4_High_T3_3_Right/2.png", "Image/P27_D2_P4_High_T3_3_Right/3.png", "Image/P27_D2_P4_High_T3_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that vase? Finish\"\nThe frames roughly correspond to the speech interval from 1369.2156 seconds to 1379.3782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P8_High_T2_3_Left", "images": ["Image/P23_D2_P8_High_T2_3_Left/1.png", "Image/P23_D2_P8_High_T2_3_Left/2.png", "Image/P23_D2_P8_High_T2_3_Left/3.png", "Image/P23_D2_P8_High_T2_3_Left/4.png", "Image/P23_D2_P8_High_T2_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 250.10687 seconds to 260.93708 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P7_High_T3_22_Right", "images": ["Image/P4_D2_P7_High_T3_22_Right/1.png", "Image/P4_D2_P7_High_T3_22_Right/2.png", "Image/P4_D2_P7_High_T3_22_Right/3.png", "Image/P4_D2_P7_High_T3_22_Right/4.png", "Image/P4_D2_P7_High_T3_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the plant? Finish\"\nThe frames roughly correspond to the speech interval from 1784.6112999999998 seconds to 1798.1123999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P7_Low_T1_21_Mid", "images": ["Image/P14_D3_P7_Low_T1_21_Mid/1.png", "Image/P14_D3_P7_Low_T1_21_Mid/2.png", "Image/P14_D3_P7_Low_T1_21_Mid/3.png", "Image/P14_D3_P7_Low_T1_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1163.21276 seconds to 1172.15486 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P1_Medium_T2_22_Right", "images": ["Image/P26_D3_P1_Medium_T2_22_Right/1.png", "Image/P26_D3_P1_Medium_T2_22_Right/2.png", "Image/P26_D3_P1_Medium_T2_22_Right/3.png", "Image/P26_D3_P1_Medium_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1070.1694 seconds to 1080.9424 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P9_Medium_T1_25_Mid", "images": ["Image/P27_D3_P9_Medium_T1_25_Mid/1.png", "Image/P27_D3_P9_Medium_T1_25_Mid/2.png", "Image/P27_D3_P9_Medium_T1_25_Mid/3.png", "Image/P27_D3_P9_Medium_T1_25_Mid/4.png", "Image/P27_D3_P9_Medium_T1_25_Mid/5.png", "Image/P27_D3_P9_Medium_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the middle glass to the top of that cabinet? Finish\"\nThe frames roughly correspond to the speech interval from 205.25590000000005 seconds to 219.8001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P1_Medium_T1_25_Mid", "images": ["Image/P6_D2_P1_Medium_T1_25_Mid/1.png", "Image/P6_D2_P1_Medium_T1_25_Mid/2.png", "Image/P6_D2_P1_Medium_T1_25_Mid/3.png", "Image/P6_D2_P1_Medium_T1_25_Mid/4.png", "Image/P6_D2_P1_Medium_T1_25_Mid/5.png", "Image/P6_D2_P1_Medium_T1_25_Mid/6.png", "Image/P6_D2_P1_Medium_T1_25_Mid/7.png", "Image/P6_D2_P1_Medium_T1_25_Mid/8.png", "Image/P6_D2_P1_Medium_T1_25_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1114.0609 seconds to 1128.9556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P3_High_T2_2_Mid", "images": ["Image/P28_D3_P3_High_T2_2_Mid/1.png", "Image/P28_D3_P3_High_T2_2_Mid/2.png", "Image/P28_D3_P3_High_T2_2_Mid/3.png", "Image/P28_D3_P3_High_T2_2_Mid/4.png", "Image/P28_D3_P3_High_T2_2_Mid/5.png", "Image/P28_D3_P3_High_T2_2_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 294.9708 seconds to 305.94120000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P5_High_T1_22_Mid", "images": ["Image/P29_D2_P5_High_T1_22_Mid/1.png", "Image/P29_D2_P5_High_T1_22_Mid/2.png", "Image/P29_D2_P5_High_T1_22_Mid/3.png", "Image/P29_D2_P5_High_T1_22_Mid/4.png", "Image/P29_D2_P5_High_T1_22_Mid/5.png", "Image/P29_D2_P5_High_T1_22_Mid/6.png", "Image/P29_D2_P5_High_T1_22_Mid/7.png", "Image/P29_D2_P5_High_T1_22_Mid/8.png", "Image/P29_D2_P5_High_T1_22_Mid/9.png", "Image/P29_D2_P5_High_T1_22_Mid/10.png", "Image/P29_D2_P5_High_T1_22_Mid/11.png", "Image/P29_D2_P5_High_T1_22_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 466.00340000000006 seconds to 484.1129 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P1_High_T1_20_Right", "images": ["Image/P21_D3_P1_High_T1_20_Right/1.png", "Image/P21_D3_P1_High_T1_20_Right/2.png", "Image/P21_D3_P1_High_T1_20_Right/3.png", "Image/P21_D3_P1_High_T1_20_Right/4.png", "Image/P21_D3_P1_High_T1_20_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 383.3849 seconds to 394.7849 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P5_Low_T2_20_Left", "images": ["Image/P4_D3_P5_Low_T2_20_Left/1.png", "Image/P4_D3_P5_Low_T2_20_Left/2.png", "Image/P4_D3_P5_Low_T2_20_Left/3.png", "Image/P4_D3_P5_Low_T2_20_Left/4.png", "Image/P4_D3_P5_Low_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you fill in the white bottle? Finish\"\nThe frames roughly correspond to the speech interval from 768.8973999999998 seconds to 781.0209 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P2_Low_T3_15_Left", "images": ["Image/P24_D1_P2_Low_T3_15_Left/1.png", "Image/P24_D1_P2_Low_T3_15_Left/2.png", "Image/P24_D1_P2_Low_T3_15_Left/3.png", "Image/P24_D1_P2_Low_T3_15_Left/4.png", "Image/P24_D1_P2_Low_T3_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 1215.9738 seconds to 1225.6613 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P4_Low_T2_24_Mid", "images": ["Image/P25_D2_P4_Low_T2_24_Mid/1.png", "Image/P25_D2_P4_Low_T2_24_Mid/2.png", "Image/P25_D2_P4_Low_T2_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 1061.0176 seconds to 1069.5487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P1_Low_T3_2_Mid", "images": ["Image/P9_D1_P1_Low_T3_2_Mid/1.png", "Image/P9_D1_P1_Low_T3_2_Mid/2.png", "Image/P9_D1_P1_Low_T3_2_Mid/3.png", "Image/P9_D1_P1_Low_T3_2_Mid/4.png", "Image/P9_D1_P1_Low_T3_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1387.5838 seconds to 1420.5075 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P9_High_T3_11_Left", "images": ["Image/P6_D3_P9_High_T3_11_Left/1.png", "Image/P6_D3_P9_High_T3_11_Left/2.png", "Image/P6_D3_P9_High_T3_11_Left/3.png", "Image/P6_D3_P9_High_T3_11_Left/4.png", "Image/P6_D3_P9_High_T3_11_Left/5.png", "Image/P6_D3_P9_High_T3_11_Left/6.png", "Image/P6_D3_P9_High_T3_11_Left/7.png", "Image/P6_D3_P9_High_T3_11_Left/8.png", "Image/P6_D3_P9_High_T3_11_Left/9.png", "Image/P6_D3_P9_High_T3_11_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4417.564969999999 seconds to 4433.83858 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P7_Low_T1_1_Left", "images": ["Image/P19_D2_P7_Low_T1_1_Left/1.png", "Image/P19_D2_P7_Low_T1_1_Left/2.png", "Image/P19_D2_P7_Low_T1_1_Left/3.png", "Image/P19_D2_P7_Low_T1_1_Left/4.png", "Image/P19_D2_P7_Low_T1_1_Left/5.png", "Image/P19_D2_P7_Low_T1_1_Left/6.png", "Image/P19_D2_P7_Low_T1_1_Left/7.png", "Image/P19_D2_P7_Low_T1_1_Left/8.png", "Image/P19_D2_P7_Low_T1_1_Left/9.png", "Image/P19_D2_P7_Low_T1_1_Left/10.png", "Image/P19_D2_P7_Low_T1_1_Left/11.png", "Image/P19_D2_P7_Low_T1_1_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1329.44391 seconds to 1345.31253 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P6_Low_T2_15_Right", "images": ["Image/P30_D1_P6_Low_T2_15_Right/1.png", "Image/P30_D1_P6_Low_T2_15_Right/2.png", "Image/P30_D1_P6_Low_T2_15_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 923.9531 seconds to 933.0558 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P3_High_T2_6_Mid", "images": ["Image/P3_D3_P3_High_T2_6_Mid/1.png", "Image/P3_D3_P3_High_T2_6_Mid/2.png", "Image/P3_D3_P3_High_T2_6_Mid/3.png", "Image/P3_D3_P3_High_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1033.839 seconds to 1044.1951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P9_Medium_T1_14_Mid", "images": ["Image/P26_D1_P9_Medium_T1_14_Mid/1.png", "Image/P26_D1_P9_Medium_T1_14_Mid/2.png", "Image/P26_D1_P9_Medium_T1_14_Mid/3.png", "Image/P26_D1_P9_Medium_T1_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the middle plant up to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 170.20929999999998 seconds to 181.2898 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P9_High_T1_2_Mid", "images": ["Image/P13_D3_P9_High_T1_2_Mid/1.png", "Image/P13_D3_P9_High_T1_2_Mid/2.png", "Image/P13_D3_P9_High_T1_2_Mid/3.png", "Image/P13_D3_P9_High_T1_2_Mid/4.png", "Image/P13_D3_P9_High_T1_2_Mid/5.png", "Image/P13_D3_P9_High_T1_2_Mid/6.png", "Image/P13_D3_P9_High_T1_2_Mid/7.png", "Image/P13_D3_P9_High_T1_2_Mid/8.png", "Image/P13_D3_P9_High_T1_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move the green bottle in the middle on the floor from the bottom shelf to the top shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1534.005 seconds to 1548.9414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P2_Low_T2_6_Mid", "images": ["Image/P19_D3_P2_Low_T2_6_Mid/1.png", "Image/P19_D3_P2_Low_T2_6_Mid/2.png", "Image/P19_D3_P2_Low_T2_6_Mid/3.png", "Image/P19_D3_P2_Low_T2_6_Mid/4.png", "Image/P19_D3_P2_Low_T2_6_Mid/5.png", "Image/P19_D3_P2_Low_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2207.7187999999996 seconds to 2220.5951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P3_Low_T2_6_Left", "images": ["Image/P2_D2_P3_Low_T2_6_Left/1.png", "Image/P2_D2_P3_Low_T2_6_Left/2.png", "Image/P2_D2_P3_Low_T2_6_Left/3.png", "Image/P2_D2_P3_Low_T2_6_Left/4.png", "Image/P2_D2_P3_Low_T2_6_Left/5.png", "Image/P2_D2_P3_Low_T2_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3036.7061 seconds to 3047.9246999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P9_Low_T1_20_Right", "images": ["Image/P17_D3_P9_Low_T1_20_Right/1.png", "Image/P17_D3_P9_Low_T1_20_Right/2.png", "Image/P17_D3_P9_Low_T1_20_Right/3.png", "Image/P17_D3_P9_Low_T1_20_Right/4.png", "Image/P17_D3_P9_Low_T1_20_Right/5.png", "Image/P17_D3_P9_Low_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that white vase up to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 808.03516 seconds to 822.3167500000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P3_High_T3_8_Left", "images": ["Image/P11_D2_P3_High_T3_8_Left/1.png", "Image/P11_D2_P3_High_T3_8_Left/2.png", "Image/P11_D2_P3_High_T3_8_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the green glass. Finish.\"\nThe frames roughly correspond to the speech interval from 992.1501 seconds to 1001.6761 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P4_High_T3_7_Left", "images": ["Image/P3_D2_P4_High_T3_7_Left/1.png", "Image/P3_D2_P4_High_T3_7_Left/2.png", "Image/P3_D2_P4_High_T3_7_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2489.4387 seconds to 2498.8478 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P4_Medium_T2_27_Left", "images": ["Image/P25_D2_P4_Medium_T2_27_Left/1.png", "Image/P25_D2_P4_Medium_T2_27_Left/2.png", "Image/P25_D2_P4_Medium_T2_27_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 837.3917 seconds to 846.2592 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P7_Low_T1_13_Mid", "images": ["Image/P11_D2_P7_Low_T1_13_Mid/1.png", "Image/P11_D2_P7_Low_T1_13_Mid/2.png", "Image/P11_D2_P7_Low_T1_13_Mid/3.png", "Image/P11_D2_P7_Low_T1_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the top of the shelf please. Finish.\"\nThe frames roughly correspond to the speech interval from 716.4757 seconds to 727.3905 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P2_Medium_T3_14_Right", "images": ["Image/P24_D1_P2_Medium_T3_14_Right/1.png", "Image/P24_D1_P2_Medium_T3_14_Right/2.png", "Image/P24_D1_P2_Medium_T3_14_Right/3.png", "Image/P24_D1_P2_Medium_T3_14_Right/4.png", "Image/P24_D1_P2_Medium_T3_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1337.536 seconds to 1347.7103 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P1_Medium_T3_7_Left", "images": ["Image/P20_D3_P1_Medium_T3_7_Left/1.png", "Image/P20_D3_P1_Medium_T3_7_Left/2.png", "Image/P20_D3_P1_Medium_T3_7_Left/3.png", "Image/P20_D3_P1_Medium_T3_7_Left/4.png", "Image/P20_D3_P1_Medium_T3_7_Left/5.png", "Image/P20_D3_P1_Medium_T3_7_Left/6.png", "Image/P20_D3_P1_Medium_T3_7_Left/7.png", "Image/P20_D3_P1_Medium_T3_7_Left/8.png", "Image/P20_D3_P1_Medium_T3_7_Left/9.png", "Image/P20_D3_P1_Medium_T3_7_Left/10.png", "Image/P20_D3_P1_Medium_T3_7_Left/11.png", "Image/P20_D3_P1_Medium_T3_7_Left/12.png", "Image/P20_D3_P1_Medium_T3_7_Left/13.png", "Image/P20_D3_P1_Medium_T3_7_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move water into the left bottle over there on the shelf? White cup? Green cup? Yeah. Finish.\"\nThe frames roughly correspond to the speech interval from 1224.1906 seconds to 1244.4587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P9_Medium_T2_4_Mid", "images": ["Image/P13_D2_P9_Medium_T2_4_Mid/1.png", "Image/P13_D2_P9_Medium_T2_4_Mid/2.png", "Image/P13_D2_P9_Medium_T2_4_Mid/3.png", "Image/P13_D2_P9_Medium_T2_4_Mid/4.png", "Image/P13_D2_P9_Medium_T2_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the middle plant down there\"\nThe frames roughly correspond to the speech interval from 919.16616 seconds to 929.95426 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P5_Medium_T3_13_Right", "images": ["Image/P3_D2_P5_Medium_T3_13_Right/1.png", "Image/P3_D2_P5_Medium_T3_13_Right/2.png", "Image/P3_D2_P5_Medium_T3_13_Right/3.png", "Image/P3_D2_P5_Medium_T3_13_Right/4.png", "Image/P3_D2_P5_Medium_T3_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this table? Finish.\"\nThe frames roughly correspond to the speech interval from 2424.6284 seconds to 2433.4712 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P8_Medium_T3_18_Left", "images": ["Image/P17_D1_P8_Medium_T3_18_Left/1.png", "Image/P17_D1_P8_Medium_T3_18_Left/2.png", "Image/P17_D1_P8_Medium_T3_18_Left/3.png", "Image/P17_D1_P8_Medium_T3_18_Left/4.png", "Image/P17_D1_P8_Medium_T3_18_Left/5.png", "Image/P17_D1_P8_Medium_T3_18_Left/6.png", "Image/P17_D1_P8_Medium_T3_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this brown mug in the center of the basket on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 2032.5273 seconds to 2045.7085 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P9_Low_T1_15_Left", "images": ["Image/P26_D3_P9_Low_T1_15_Left/1.png", "Image/P26_D3_P9_Low_T1_15_Left/2.png", "Image/P26_D3_P9_Low_T1_15_Left/3.png", "Image/P26_D3_P9_Low_T1_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the plant to the top shelf of that cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 1089.4477000000002 seconds to 1099.9422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P9_Medium_T1_13_Left", "images": ["Image/P7_D1_P9_Medium_T1_13_Left/1.png", "Image/P7_D1_P9_Medium_T1_13_Left/2.png", "Image/P7_D1_P9_Medium_T1_13_Left/3.png", "Image/P7_D1_P9_Medium_T1_13_Left/4.png", "Image/P7_D1_P9_Medium_T1_13_Left/5.png", "Image/P7_D1_P9_Medium_T1_13_Left/6.png", "Image/P7_D1_P9_Medium_T1_13_Left/7.png", "Image/P7_D1_P9_Medium_T1_13_Left/8.png", "Image/P7_D1_P9_Medium_T1_13_Left/9.png", "Image/P7_D1_P9_Medium_T1_13_Left/10.png", "Image/P7_D1_P9_Medium_T1_13_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 593.6795 seconds to 612.4564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P9_Low_T3_25_Right", "images": ["Image/P3_D1_P9_Low_T3_25_Right/1.png", "Image/P3_D1_P9_Low_T3_25_Right/2.png", "Image/P3_D1_P9_Low_T3_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1504.8055 seconds to 1518.3239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P3_High_T1_22_Mid", "images": ["Image/P28_D2_P3_High_T1_22_Mid/1.png", "Image/P28_D2_P3_High_T1_22_Mid/2.png", "Image/P28_D2_P3_High_T1_22_Mid/3.png", "Image/P28_D2_P3_High_T1_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1130.4108 seconds to 1140.4743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P6_High_T2_26_Mid", "images": ["Image/P19_D2_P6_High_T2_26_Mid/1.png", "Image/P19_D2_P6_High_T2_26_Mid/2.png", "Image/P19_D2_P6_High_T2_26_Mid/3.png", "Image/P19_D2_P6_High_T2_26_Mid/4.png", "Image/P19_D2_P6_High_T2_26_Mid/5.png", "Image/P19_D2_P6_High_T2_26_Mid/6.png", "Image/P19_D2_P6_High_T2_26_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here. Start.\"\nThe frames roughly correspond to the speech interval from 1801.3015999999998 seconds to 1816.2513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P2_Medium_T1_13_Right", "images": ["Image/P24_D2_P2_Medium_T1_13_Right/1.png", "Image/P24_D2_P2_Medium_T1_13_Right/2.png", "Image/P24_D2_P2_Medium_T1_13_Right/3.png", "Image/P24_D2_P2_Medium_T1_13_Right/4.png", "Image/P24_D2_P2_Medium_T1_13_Right/5.png", "Image/P24_D2_P2_Medium_T1_13_Right/6.png", "Image/P24_D2_P2_Medium_T1_13_Right/7.png", "Image/P24_D2_P2_Medium_T1_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this planet down to this area\"\nThe frames roughly correspond to the speech interval from 488.47 seconds to 501.1363 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P4_Medium_T2_12_Left", "images": ["Image/P19_D2_P4_Medium_T2_12_Left/1.png", "Image/P19_D2_P4_Medium_T2_12_Left/2.png", "Image/P19_D2_P4_Medium_T2_12_Left/3.png", "Image/P19_D2_P4_Medium_T2_12_Left/4.png", "Image/P19_D2_P4_Medium_T2_12_Left/5.png", "Image/P19_D2_P4_Medium_T2_12_Left/6.png", "Image/P19_D2_P4_Medium_T2_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1517.9616999999998 seconds to 1531.5897 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P1_High_T2_19_Mid", "images": ["Image/P11_D3_P1_High_T2_19_Mid/1.png", "Image/P11_D3_P1_High_T2_19_Mid/2.png", "Image/P11_D3_P1_High_T2_19_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1875.0119 seconds to 1884.6306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P6_High_T3_21_Mid", "images": ["Image/P19_D3_P6_High_T3_21_Mid/1.png", "Image/P19_D3_P6_High_T3_21_Mid/2.png", "Image/P19_D3_P6_High_T3_21_Mid/3.png", "Image/P19_D3_P6_High_T3_21_Mid/4.png", "Image/P19_D3_P6_High_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 2146.8338 seconds to 2158.1359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P5_Low_T2_1_Right", "images": ["Image/P18_D3_P5_Low_T2_1_Right/1.png", "Image/P18_D3_P5_Low_T2_1_Right/2.png", "Image/P18_D3_P5_Low_T2_1_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1535.4362 seconds to 1543.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P3_Medium_T1_10_Mid", "images": ["Image/P11_D1_P3_Medium_T1_10_Mid/1.png", "Image/P11_D1_P3_Medium_T1_10_Mid/2.png", "Image/P11_D1_P3_Medium_T1_10_Mid/3.png", "Image/P11_D1_P3_Medium_T1_10_Mid/4.png", "Image/P11_D1_P3_Medium_T1_10_Mid/5.png", "Image/P11_D1_P3_Medium_T1_10_Mid/6.png", "Image/P11_D1_P3_Medium_T1_10_Mid/7.png", "Image/P11_D1_P3_Medium_T1_10_Mid/8.png", "Image/P11_D1_P3_Medium_T1_10_Mid/9.png", "Image/P11_D1_P3_Medium_T1_10_Mid/10.png", "Image/P11_D1_P3_Medium_T1_10_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the second red glass to the right basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2750.5101999999997 seconds to 2769.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P7_Low_T2_21_Mid", "images": ["Image/P22_D1_P7_Low_T2_21_Mid/1.png", "Image/P22_D1_P7_Low_T2_21_Mid/2.png", "Image/P22_D1_P7_Low_T2_21_Mid/3.png", "Image/P22_D1_P7_Low_T2_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Put water in that vase. Finish\"\nThe frames roughly correspond to the speech interval from 1767.8983 seconds to 1777.1275 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P1_Low_T3_24_Left", "images": ["Image/P29_D1_P1_Low_T3_24_Left/1.png", "Image/P29_D1_P1_Low_T3_24_Left/2.png", "Image/P29_D1_P1_Low_T3_24_Left/3.png", "Image/P29_D1_P1_Low_T3_24_Left/4.png", "Image/P29_D1_P1_Low_T3_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2772.7263999999996 seconds to 2783.6938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P4_Medium_T1_4_Right", "images": ["Image/P29_D3_P4_Medium_T1_4_Right/1.png", "Image/P29_D3_P4_Medium_T1_4_Right/2.png", "Image/P29_D3_P4_Medium_T1_4_Right/3.png", "Image/P29_D3_P4_Medium_T1_4_Right/4.png", "Image/P29_D3_P4_Medium_T1_4_Right/5.png", "Image/P29_D3_P4_Medium_T1_4_Right/6.png", "Image/P29_D3_P4_Medium_T1_4_Right/7.png", "Image/P29_D3_P4_Medium_T1_4_Right/8.png", "Image/P29_D3_P4_Medium_T1_4_Right/9.png", "Image/P29_D3_P4_Medium_T1_4_Right/10.png", "Image/P29_D3_P4_Medium_T1_4_Right/11.png", "Image/P29_D3_P4_Medium_T1_4_Right/12.png", "Image/P29_D3_P4_Medium_T1_4_Right/13.png", "Image/P29_D3_P4_Medium_T1_4_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1527.3243999999995 seconds to 1546.4647999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P6_Low_T1_15_Mid", "images": ["Image/P1_D1_P6_Low_T1_15_Mid/1.png", "Image/P1_D1_P6_Low_T1_15_Mid/2.png", "Image/P1_D1_P6_Low_T1_15_Mid/3.png", "Image/P1_D1_P6_Low_T1_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 426.8892 seconds to 436.3005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P1_Medium_T1_18_Mid", "images": ["Image/P9_D3_P1_Medium_T1_18_Mid/1.png", "Image/P9_D3_P1_Medium_T1_18_Mid/2.png", "Image/P9_D3_P1_Medium_T1_18_Mid/3.png", "Image/P9_D3_P1_Medium_T1_18_Mid/4.png", "Image/P9_D3_P1_Medium_T1_18_Mid/5.png", "Image/P9_D3_P1_Medium_T1_18_Mid/6.png", "Image/P9_D3_P1_Medium_T1_18_Mid/7.png", "Image/P9_D3_P1_Medium_T1_18_Mid/8.png", "Image/P9_D3_P1_Medium_T1_18_Mid/9.png", "Image/P9_D3_P1_Medium_T1_18_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the middle cup to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 681.1903 seconds to 698.1445 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P1_Low_T3_20_Mid", "images": ["Image/P22_D2_P1_Low_T3_20_Mid/1.png", "Image/P22_D2_P1_Low_T3_20_Mid/2.png", "Image/P22_D2_P1_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 724.9465999999999 seconds to 734.4264999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P4_High_T1_1_Right", "images": ["Image/P19_D3_P4_High_T1_1_Right/1.png", "Image/P19_D3_P4_High_T1_1_Right/2.png", "Image/P19_D3_P4_High_T1_1_Right/3.png", "Image/P19_D3_P4_High_T1_1_Right/4.png", "Image/P19_D3_P4_High_T1_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please move this vase over here. Finish.\"\nThe frames roughly correspond to the speech interval from 2127.87462 seconds to 2138.87081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P5_Low_T3_18_Mid", "images": ["Image/P25_D3_P5_Low_T3_18_Mid/1.png", "Image/P25_D3_P5_Low_T3_18_Mid/2.png", "Image/P25_D3_P5_Low_T3_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1740.37555 seconds to 1749.31316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P6_Low_T1_25_Left", "images": ["Image/P20_D2_P6_Low_T1_25_Left/1.png", "Image/P20_D2_P6_Low_T1_25_Left/2.png", "Image/P20_D2_P6_Low_T1_25_Left/3.png", "Image/P20_D2_P6_Low_T1_25_Left/4.png", "Image/P20_D2_P6_Low_T1_25_Left/5.png", "Image/P20_D2_P6_Low_T1_25_Left/6.png", "Image/P20_D2_P6_Low_T1_25_Left/7.png", "Image/P20_D2_P6_Low_T1_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that white cup to the middle layer of that shelf over there. Finish\"\nThe frames roughly correspond to the speech interval from 3315.4317 seconds to 3330.0746000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P1_Medium_T1_27_Mid", "images": ["Image/P9_D2_P1_Medium_T1_27_Mid/1.png", "Image/P9_D2_P1_Medium_T1_27_Mid/2.png", "Image/P9_D2_P1_Medium_T1_27_Mid/3.png", "Image/P9_D2_P1_Medium_T1_27_Mid/4.png", "Image/P9_D2_P1_Medium_T1_27_Mid/5.png", "Image/P9_D2_P1_Medium_T1_27_Mid/6.png", "Image/P9_D2_P1_Medium_T1_27_Mid/7.png", "Image/P9_D2_P1_Medium_T1_27_Mid/8.png", "Image/P9_D2_P1_Medium_T1_27_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the middle cup from the top to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3054.8293 seconds to 3073.9229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P8_High_T1_27_Left", "images": ["Image/P20_D3_P8_High_T1_27_Left/1.png", "Image/P20_D3_P8_High_T1_27_Left/2.png", "Image/P20_D3_P8_High_T1_27_Left/3.png", "Image/P20_D3_P8_High_T1_27_Left/4.png", "Image/P20_D3_P8_High_T1_27_Left/5.png", "Image/P20_D3_P8_High_T1_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white cup to the top of the shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1743.0973 seconds to 1756.3620999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P9_Medium_T3_1_Right", "images": ["Image/P27_D2_P9_Medium_T3_1_Right/1.png", "Image/P27_D2_P9_Medium_T3_1_Right/2.png", "Image/P27_D2_P9_Medium_T3_1_Right/3.png", "Image/P27_D2_P9_Medium_T3_1_Right/4.png", "Image/P27_D2_P9_Medium_T3_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1261.2822 seconds to 1271.03859 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P4_Medium_T1_5_Right", "images": ["Image/P18_D2_P4_Medium_T1_5_Right/1.png", "Image/P18_D2_P4_Medium_T1_5_Right/2.png", "Image/P18_D2_P4_Medium_T1_5_Right/3.png", "Image/P18_D2_P4_Medium_T1_5_Right/4.png", "Image/P18_D2_P4_Medium_T1_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that first plant to the table in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 256.71400000000006 seconds to 267.7674 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P6_High_T1_16_Mid", "images": ["Image/P16_D2_P6_High_T1_16_Mid/1.png", "Image/P16_D2_P6_High_T1_16_Mid/2.png", "Image/P16_D2_P6_High_T1_16_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 608.4317000000001 seconds to 617.61788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P8_Medium_T2_27_Right", "images": ["Image/P18_D3_P8_Medium_T2_27_Right/1.png", "Image/P18_D3_P8_Medium_T2_27_Right/2.png", "Image/P18_D3_P8_Medium_T2_27_Right/3.png", "Image/P18_D3_P8_Medium_T2_27_Right/4.png", "Image/P18_D3_P8_Medium_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1425.9835 seconds to 1436.3971 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P9_Low_T1_7_Mid", "images": ["Image/P8_D3_P9_Low_T1_7_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Finish.\"\nThe frames roughly correspond to the speech interval from 1504.09183 seconds to 1516.18851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P7_High_T2_22_Left", "images": ["Image/P7_D1_P7_High_T2_22_Left/1.png", "Image/P7_D1_P7_High_T2_22_Left/2.png", "Image/P7_D1_P7_High_T2_22_Left/3.png", "Image/P7_D1_P7_High_T2_22_Left/4.png", "Image/P7_D1_P7_High_T2_22_Left/5.png", "Image/P7_D1_P7_High_T2_22_Left/6.png", "Image/P7_D1_P7_High_T2_22_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 519.5008 seconds to 534.2692999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P1_Low_T3_26_Left", "images": ["Image/P28_D3_P1_Low_T3_26_Left/1.png", "Image/P28_D3_P1_Low_T3_26_Left/2.png", "Image/P28_D3_P1_Low_T3_26_Left/3.png", "Image/P28_D3_P1_Low_T3_26_Left/4.png", "Image/P28_D3_P1_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 215.6987 seconds to 226.2911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P9_High_T2_25_Right", "images": ["Image/P19_D2_P9_High_T2_25_Right/1.png", "Image/P19_D2_P9_High_T2_25_Right/2.png", "Image/P19_D2_P9_High_T2_25_Right/3.png", "Image/P19_D2_P9_High_T2_25_Right/4.png", "Image/P19_D2_P9_High_T2_25_Right/5.png", "Image/P19_D2_P9_High_T2_25_Right/6.png", "Image/P19_D2_P9_High_T2_25_Right/7.png", "Image/P19_D2_P9_High_T2_25_Right/8.png", "Image/P19_D2_P9_High_T2_25_Right/9.png", "Image/P19_D2_P9_High_T2_25_Right/10.png", "Image/P19_D2_P9_High_T2_25_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1683.4991 seconds to 1698.6419999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P1_Low_T2_7_Right", "images": ["Image/P10_D1_P1_Low_T2_7_Right/1.png", "Image/P10_D1_P1_Low_T2_7_Right/2.png", "Image/P10_D1_P1_Low_T2_7_Right/3.png", "Image/P10_D1_P1_Low_T2_7_Right/4.png", "Image/P10_D1_P1_Low_T2_7_Right/5.png", "Image/P10_D1_P1_Low_T2_7_Right/6.png", "Image/P10_D1_P1_Low_T2_7_Right/7.png", "Image/P10_D1_P1_Low_T2_7_Right/8.png", "Image/P10_D1_P1_Low_T2_7_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2521.1268 seconds to 2535.9091 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P8_High_T1_19_Mid", "images": ["Image/P18_D3_P8_High_T1_19_Mid/1.png", "Image/P18_D3_P8_High_T1_19_Mid/2.png", "Image/P18_D3_P8_High_T1_19_Mid/3.png", "Image/P18_D3_P8_High_T1_19_Mid/4.png", "Image/P18_D3_P8_High_T1_19_Mid/5.png", "Image/P18_D3_P8_High_T1_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1229.75535 seconds to 1240.76658 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P2_Medium_T1_6_Left", "images": ["Image/P19_D3_P2_Medium_T1_6_Left/1.png", "Image/P19_D3_P2_Medium_T1_6_Left/2.png", "Image/P19_D3_P2_Medium_T1_6_Left/3.png", "Image/P19_D3_P2_Medium_T1_6_Left/4.png", "Image/P19_D3_P2_Medium_T1_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that flowerpot down here. Finish.\"\nThe frames roughly correspond to the speech interval from 2250.2025999999996 seconds to 2262.7306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P1_High_T3_10_Left", "images": ["Image/P23_D2_P1_High_T3_10_Left/1.png", "Image/P23_D2_P1_High_T3_10_Left/2.png", "Image/P23_D2_P1_High_T3_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 83.41482000000002 seconds to 93.84086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P1_Medium_T1_17_Right", "images": ["Image/P23_D2_P1_Medium_T1_17_Right/1.png", "Image/P23_D2_P1_Medium_T1_17_Right/2.png", "Image/P23_D2_P1_Medium_T1_17_Right/3.png", "Image/P23_D2_P1_Medium_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the one on the right over there. Finish.\"\nThe frames roughly correspond to the speech interval from 344.0806 seconds to 354.284 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P7_Medium_T2_4_Mid", "images": ["Image/P18_D1_P7_Medium_T2_4_Mid/1.png", "Image/P18_D1_P7_Medium_T2_4_Mid/2.png", "Image/P18_D1_P7_Medium_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 920.339 seconds to 928.9119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P9_Medium_T2_26_Left", "images": ["Image/P4_D2_P9_Medium_T2_26_Left/1.png", "Image/P4_D2_P9_Medium_T2_26_Left/2.png", "Image/P4_D2_P9_Medium_T2_26_Left/3.png", "Image/P4_D2_P9_Medium_T2_26_Left/4.png", "Image/P4_D2_P9_Medium_T2_26_Left/5.png", "Image/P4_D2_P9_Medium_T2_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the left photo? Finish.\"\nThe frames roughly correspond to the speech interval from 1670.9563999999998 seconds to 1684.5116999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P3_Medium_T3_14_Left", "images": ["Image/P9_D1_P3_Medium_T3_14_Left/1.png", "Image/P9_D1_P3_Medium_T3_14_Left/2.png", "Image/P9_D1_P3_Medium_T3_14_Left/3.png", "Image/P9_D1_P3_Medium_T3_14_Left/4.png", "Image/P9_D1_P3_Medium_T3_14_Left/5.png", "Image/P9_D1_P3_Medium_T3_14_Left/6.png", "Image/P9_D1_P3_Medium_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1515.5477999999998 seconds to 1536.3629999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P1_Low_T3_23_Mid", "images": ["Image/P10_D3_P1_Low_T3_23_Mid/1.png", "Image/P10_D3_P1_Low_T3_23_Mid/2.png", "Image/P10_D3_P1_Low_T3_23_Mid/3.png", "Image/P10_D3_P1_Low_T3_23_Mid/4.png", "Image/P10_D3_P1_Low_T3_23_Mid/5.png", "Image/P10_D3_P1_Low_T3_23_Mid/6.png", "Image/P10_D3_P1_Low_T3_23_Mid/7.png", "Image/P10_D3_P1_Low_T3_23_Mid/8.png", "Image/P10_D3_P1_Low_T3_23_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 805.6959899999999 seconds to 823.91689 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P4_Low_T3_19_Right", "images": ["Image/P29_D3_P4_Low_T3_19_Right/1.png", "Image/P29_D3_P4_Low_T3_19_Right/2.png", "Image/P29_D3_P4_Low_T3_19_Right/3.png", "Image/P29_D3_P4_Low_T3_19_Right/4.png", "Image/P29_D3_P4_Low_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddie\"\nThe frames roughly correspond to the speech interval from 1475.4314999999997 seconds to 1485.6065999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P5_Medium_T2_10_Mid", "images": ["Image/P2_D3_P5_Medium_T2_10_Mid/1.png", "Image/P2_D3_P5_Medium_T2_10_Mid/2.png", "Image/P2_D3_P5_Medium_T2_10_Mid/3.png", "Image/P2_D3_P5_Medium_T2_10_Mid/4.png", "Image/P2_D3_P5_Medium_T2_10_Mid/5.png", "Image/P2_D3_P5_Medium_T2_10_Mid/6.png", "Image/P2_D3_P5_Medium_T2_10_Mid/7.png", "Image/P2_D3_P5_Medium_T2_10_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the brown vessel in the very middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2279.0815 seconds to 2293.6136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P3_Medium_T1_9_Mid", "images": ["Image/P13_D2_P3_Medium_T1_9_Mid/1.png", "Image/P13_D2_P3_Medium_T1_9_Mid/2.png", "Image/P13_D2_P3_Medium_T1_9_Mid/3.png", "Image/P13_D2_P3_Medium_T1_9_Mid/4.png", "Image/P13_D2_P3_Medium_T1_9_Mid/5.png", "Image/P13_D2_P3_Medium_T1_9_Mid/6.png", "Image/P13_D2_P3_Medium_T1_9_Mid/7.png", "Image/P13_D2_P3_Medium_T1_9_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the middle green glass down to all the way to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 879.09046 seconds to 892.18976 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P7_Medium_T3_27_Left", "images": ["Image/P19_D1_P7_Medium_T3_27_Left/1.png", "Image/P19_D1_P7_Medium_T3_27_Left/2.png", "Image/P19_D1_P7_Medium_T3_27_Left/3.png", "Image/P19_D1_P7_Medium_T3_27_Left/4.png", "Image/P19_D1_P7_Medium_T3_27_Left/5.png", "Image/P19_D1_P7_Medium_T3_27_Left/6.png", "Image/P19_D1_P7_Medium_T3_27_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this glass here. Finish.\"\nThe frames roughly correspond to the speech interval from 950.1841 seconds to 964.2841 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P7_High_T3_8_Mid", "images": ["Image/P13_D2_P7_High_T3_8_Mid/1.png", "Image/P13_D2_P7_High_T3_8_Mid/2.png", "Image/P13_D2_P7_High_T3_8_Mid/3.png", "Image/P13_D2_P7_High_T3_8_Mid/4.png", "Image/P13_D2_P7_High_T3_8_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the green glass down there\"\nThe frames roughly correspond to the speech interval from 959.96526 seconds to 970.51666 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P4_Medium_T1_10_Left", "images": ["Image/P9_D1_P4_Medium_T1_10_Left/1.png", "Image/P9_D1_P4_Medium_T1_10_Left/2.png", "Image/P9_D1_P4_Medium_T1_10_Left/3.png", "Image/P9_D1_P4_Medium_T1_10_Left/4.png", "Image/P9_D1_P4_Medium_T1_10_Left/5.png", "Image/P9_D1_P4_Medium_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this bottle here? Finish.\"\nThe frames roughly correspond to the speech interval from 1856.225 seconds to 1870.8186 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P9_High_T2_23_Left", "images": ["Image/P9_D1_P9_High_T2_23_Left/1.png", "Image/P9_D1_P9_High_T2_23_Left/2.png", "Image/P9_D1_P9_High_T2_23_Left/3.png", "Image/P9_D1_P9_High_T2_23_Left/4.png", "Image/P9_D1_P9_High_T2_23_Left/5.png", "Image/P9_D1_P9_High_T2_23_Left/6.png", "Image/P9_D1_P9_High_T2_23_Left/7.png", "Image/P9_D1_P9_High_T2_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to pour some water in this plant in the box? Finish.\"\nThe frames roughly correspond to the speech interval from 1742.8971 seconds to 1761.032 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P7_Low_T1_18_Left", "images": ["Image/P12_D3_P7_Low_T1_18_Left/1.png", "Image/P12_D3_P7_Low_T1_18_Left/2.png", "Image/P12_D3_P7_Low_T1_18_Left/3.png", "Image/P12_D3_P7_Low_T1_18_Left/4.png", "Image/P12_D3_P7_Low_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2359.0300899999997 seconds to 2370.62439 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P1_Low_T1_7_Mid", "images": ["Image/P27_D3_P1_Low_T1_7_Mid/1.png", "Image/P27_D3_P1_Low_T1_7_Mid/2.png", "Image/P27_D3_P1_Low_T1_7_Mid/3.png", "Image/P27_D3_P1_Low_T1_7_Mid/4.png", "Image/P27_D3_P1_Low_T1_7_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that cup and put it on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 269.88620000000003 seconds to 280.8815 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P9_Medium_T1_12_Right", "images": ["Image/P3_D1_P9_Medium_T1_12_Right/1.png", "Image/P3_D1_P9_Medium_T1_12_Right/2.png", "Image/P3_D1_P9_Medium_T1_12_Right/3.png", "Image/P3_D1_P9_Medium_T1_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Would you move this item to this way?\"\nThe frames roughly correspond to the speech interval from 1425.3941 seconds to 1436.754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P9_High_T3_25_Left", "images": ["Image/P7_D2_P9_High_T3_25_Left/1.png", "Image/P7_D2_P9_High_T3_25_Left/2.png", "Image/P7_D2_P9_High_T3_25_Left/3.png", "Image/P7_D2_P9_High_T3_25_Left/4.png", "Image/P7_D2_P9_High_T3_25_Left/5.png", "Image/P7_D2_P9_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1512.7231000000002 seconds to 1528.5787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P3_High_T3_11_Mid", "images": ["Image/P5_D3_P3_High_T3_11_Mid/1.png", "Image/P5_D3_P3_High_T3_11_Mid/2.png", "Image/P5_D3_P3_High_T3_11_Mid/3.png", "Image/P5_D3_P3_High_T3_11_Mid/4.png", "Image/P5_D3_P3_High_T3_11_Mid/5.png", "Image/P5_D3_P3_High_T3_11_Mid/6.png", "Image/P5_D3_P3_High_T3_11_Mid/7.png", "Image/P5_D3_P3_High_T3_11_Mid/8.png", "Image/P5_D3_P3_High_T3_11_Mid/9.png", "Image/P5_D3_P3_High_T3_11_Mid/10.png", "Image/P5_D3_P3_High_T3_11_Mid/11.png", "Image/P5_D3_P3_High_T3_11_Mid/12.png", "Image/P5_D3_P3_High_T3_11_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle bottle place on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2346.6882 seconds to 2365.5780999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P9_High_T2_4_Right", "images": ["Image/P17_D1_P9_High_T2_4_Right/1.png", "Image/P17_D1_P9_High_T2_4_Right/2.png", "Image/P17_D1_P9_High_T2_4_Right/3.png", "Image/P17_D1_P9_High_T2_4_Right/4.png", "Image/P17_D1_P9_High_T2_4_Right/5.png", "Image/P17_D1_P9_High_T2_4_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that plant on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1563.9827 seconds to 1576.4263 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P4_High_T3_8_Mid", "images": ["Image/P29_D2_P4_High_T3_8_Mid/1.png", "Image/P29_D2_P4_High_T3_8_Mid/2.png", "Image/P29_D2_P4_High_T3_8_Mid/3.png", "Image/P29_D2_P4_High_T3_8_Mid/4.png", "Image/P29_D2_P4_High_T3_8_Mid/5.png", "Image/P29_D2_P4_High_T3_8_Mid/6.png", "Image/P29_D2_P4_High_T3_8_Mid/7.png", "Image/P29_D2_P4_High_T3_8_Mid/8.png", "Image/P29_D2_P4_High_T3_8_Mid/9.png", "Image/P29_D2_P4_High_T3_8_Mid/10.png", "Image/P29_D2_P4_High_T3_8_Mid/11.png", "Image/P29_D2_P4_High_T3_8_Mid/12.png", "Image/P29_D2_P4_High_T3_8_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 387.1607 seconds to 407.4842 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P8_Medium_T2_14_Right", "images": ["Image/P22_D3_P8_Medium_T2_14_Right/1.png", "Image/P22_D3_P8_Medium_T2_14_Right/2.png", "Image/P22_D3_P8_Medium_T2_14_Right/3.png", "Image/P22_D3_P8_Medium_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the little plant on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 87.84753 seconds to 98.10577 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P7_High_T2_5_Mid", "images": ["Image/P8_D3_P7_High_T2_5_Mid/1.png", "Image/P8_D3_P7_High_T2_5_Mid/2.png", "Image/P8_D3_P7_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 1575.2511 seconds to 1586.7487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P1_Low_T2_23_Mid", "images": ["Image/P29_D3_P1_Low_T2_23_Mid/1.png", "Image/P29_D3_P1_Low_T2_23_Mid/2.png", "Image/P29_D3_P1_Low_T2_23_Mid/3.png", "Image/P29_D3_P1_Low_T2_23_Mid/4.png", "Image/P29_D3_P1_Low_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1422.5633999999998 seconds to 1432.6549999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P3_Low_T1_22_Right", "images": ["Image/P11_D3_P3_Low_T1_22_Right/1.png", "Image/P11_D3_P3_Low_T1_22_Right/2.png", "Image/P11_D3_P3_Low_T1_22_Right/3.png", "Image/P11_D3_P3_Low_T1_22_Right/4.png", "Image/P11_D3_P3_Low_T1_22_Right/5.png", "Image/P11_D3_P3_Low_T1_22_Right/6.png", "Image/P11_D3_P3_Low_T1_22_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from top of the shelf to the bottom of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1431.71972 seconds to 1444.77669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P5_Low_T3_12_Left", "images": ["Image/P24_D1_P5_Low_T3_12_Left/1.png", "Image/P24_D1_P5_Low_T3_12_Left/2.png", "Image/P24_D1_P5_Low_T3_12_Left/3.png", "Image/P24_D1_P5_Low_T3_12_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you clean this bottle\"\nThe frames roughly correspond to the speech interval from 835.1123299999999 seconds to 844.5475899999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P1_High_T2_24_Mid", "images": ["Image/P22_D3_P1_High_T2_24_Mid/1.png", "Image/P22_D3_P1_High_T2_24_Mid/2.png", "Image/P22_D3_P1_High_T2_24_Mid/3.png", "Image/P22_D3_P1_High_T2_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 319.5999 seconds to 329.8595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P8_Medium_T3_6_Left", "images": ["Image/P13_D2_P8_Medium_T3_6_Left/1.png", "Image/P13_D2_P8_Medium_T3_6_Left/2.png", "Image/P13_D2_P8_Medium_T3_6_Left/3.png", "Image/P13_D2_P8_Medium_T3_6_Left/4.png", "Image/P13_D2_P8_Medium_T3_6_Left/5.png", "Image/P13_D2_P8_Medium_T3_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the left green plant\"\nThe frames roughly correspond to the speech interval from 1059.00596 seconds to 1070.19726 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P9_Low_T3_18_Right", "images": ["Image/P30_D3_P9_Low_T3_18_Right/1.png", "Image/P30_D3_P9_Low_T3_18_Right/2.png", "Image/P30_D3_P9_Low_T3_18_Right/3.png", "Image/P30_D3_P9_Low_T3_18_Right/4.png", "Image/P30_D3_P9_Low_T3_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1207.0902999999998 seconds to 1217.4189 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P8_Medium_T1_27_Mid", "images": ["Image/P14_D2_P8_Medium_T1_27_Mid/1.png", "Image/P14_D2_P8_Medium_T1_27_Mid/2.png", "Image/P14_D2_P8_Medium_T1_27_Mid/3.png", "Image/P14_D2_P8_Medium_T1_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1745.0368 seconds to 1754.2156999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P7_Medium_T2_15_Right", "images": ["Image/P28_D2_P7_Medium_T2_15_Right/1.png", "Image/P28_D2_P7_Medium_T2_15_Right/2.png", "Image/P28_D2_P7_Medium_T2_15_Right/3.png", "Image/P28_D2_P7_Medium_T2_15_Right/4.png", "Image/P28_D2_P7_Medium_T2_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 994.8295 seconds to 1005.2834 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P9_Low_T3_11_Left", "images": ["Image/P28_D3_P9_Low_T3_11_Left/1.png", "Image/P28_D3_P9_Low_T3_11_Left/2.png", "Image/P28_D3_P9_Low_T3_11_Left/3.png", "Image/P28_D3_P9_Low_T3_11_Left/4.png", "Image/P28_D3_P9_Low_T3_11_Left/5.png", "Image/P28_D3_P9_Low_T3_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 171.36370000000002 seconds to 184.8794 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P8_Low_T3_9_Mid", "images": ["Image/P21_D2_P8_Low_T3_9_Mid/1.png", "Image/P21_D2_P8_Low_T3_9_Mid/2.png", "Image/P21_D2_P8_Low_T3_9_Mid/3.png", "Image/P21_D2_P8_Low_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1829.1821 seconds to 1842.4194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P4_Medium_T2_12_Mid", "images": ["Image/P20_D1_P4_Medium_T2_12_Mid/1.png", "Image/P20_D1_P4_Medium_T2_12_Mid/2.png", "Image/P20_D1_P4_Medium_T2_12_Mid/3.png", "Image/P20_D1_P4_Medium_T2_12_Mid/4.png", "Image/P20_D1_P4_Medium_T2_12_Mid/5.png", "Image/P20_D1_P4_Medium_T2_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour some water into this thingy. Finish.\"\nThe frames roughly correspond to the speech interval from 388.47714 seconds to 401.33851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P7_Medium_T3_23_Right", "images": ["Image/P5_D2_P7_Medium_T3_23_Right/1.png", "Image/P5_D2_P7_Medium_T3_23_Right/2.png", "Image/P5_D2_P7_Medium_T3_23_Right/3.png", "Image/P5_D2_P7_Medium_T3_23_Right/4.png", "Image/P5_D2_P7_Medium_T3_23_Right/5.png", "Image/P5_D2_P7_Medium_T3_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Clean the plant as dry most of free plants. Finish.\"\nThe frames roughly correspond to the speech interval from 622.1772000000001 seconds to 635.8200999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P9_High_T1_14_Mid", "images": ["Image/P8_D2_P9_High_T1_14_Mid/1.png", "Image/P8_D2_P9_High_T1_14_Mid/2.png", "Image/P8_D2_P9_High_T1_14_Mid/3.png", "Image/P8_D2_P9_High_T1_14_Mid/4.png", "Image/P8_D2_P9_High_T1_14_Mid/5.png", "Image/P8_D2_P9_High_T1_14_Mid/6.png", "Image/P8_D2_P9_High_T1_14_Mid/7.png", "Image/P8_D2_P9_High_T1_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Dar\"\nThe frames roughly correspond to the speech interval from 2097.05413 seconds to 2110.9881 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P4_High_T2_3_Mid", "images": ["Image/P29_D1_P4_High_T2_3_Mid/1.png", "Image/P29_D1_P4_High_T2_3_Mid/2.png", "Image/P29_D1_P4_High_T2_3_Mid/3.png", "Image/P29_D1_P4_High_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2652.7634 seconds to 2662.9255999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P7_Low_T2_7_Right", "images": ["Image/P23_D1_P7_Low_T2_7_Right/1.png", "Image/P23_D1_P7_Low_T2_7_Right/2.png", "Image/P23_D1_P7_Low_T2_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1784.3179 seconds to 1793.4231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P2_Medium_T2_6_Mid", "images": ["Image/P24_D3_P2_Medium_T2_6_Mid/1.png", "Image/P24_D3_P2_Medium_T2_6_Mid/2.png", "Image/P24_D3_P2_Medium_T2_6_Mid/3.png", "Image/P24_D3_P2_Medium_T2_6_Mid/4.png", "Image/P24_D3_P2_Medium_T2_6_Mid/5.png", "Image/P24_D3_P2_Medium_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water into this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1631.3623 seconds to 1643.1931 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P4_Low_T2_8_Right", "images": ["Image/P5_D2_P4_Low_T2_8_Right/1.png", "Image/P5_D2_P4_Low_T2_8_Right/2.png", "Image/P5_D2_P4_Low_T2_8_Right/3.png", "Image/P5_D2_P4_Low_T2_8_Right/4.png", "Image/P5_D2_P4_Low_T2_8_Right/5.png", "Image/P5_D2_P4_Low_T2_8_Right/6.png", "Image/P5_D2_P4_Low_T2_8_Right/7.png", "Image/P5_D2_P4_Low_T2_8_Right/8.png", "Image/P5_D2_P4_Low_T2_8_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water to the cup on the left side of me. Finish.\"\nThe frames roughly correspond to the speech interval from 893.2125000000001 seconds to 907.4391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P5_Low_T1_20_Left", "images": ["Image/P16_D2_P5_Low_T1_20_Left/1.png", "Image/P16_D2_P5_Low_T1_20_Left/2.png", "Image/P16_D2_P5_Low_T1_20_Left/3.png", "Image/P16_D2_P5_Low_T1_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 867.3009000000001 seconds to 877.7813000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P9_Medium_T3_13_Mid", "images": ["Image/P5_D3_P9_Medium_T3_13_Mid/1.png", "Image/P5_D3_P9_Medium_T3_13_Mid/2.png", "Image/P5_D3_P9_Medium_T3_13_Mid/3.png", "Image/P5_D3_P9_Medium_T3_13_Mid/4.png", "Image/P5_D3_P9_Medium_T3_13_Mid/5.png", "Image/P5_D3_P9_Medium_T3_13_Mid/6.png", "Image/P5_D3_P9_Medium_T3_13_Mid/7.png", "Image/P5_D3_P9_Medium_T3_13_Mid/8.png", "Image/P5_D3_P9_Medium_T3_13_Mid/9.png", "Image/P5_D3_P9_Medium_T3_13_Mid/10.png", "Image/P5_D3_P9_Medium_T3_13_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle plant on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2541.5531 seconds to 2557.8244 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P7_Medium_T3_6_Right", "images": ["Image/P14_D1_P7_Medium_T3_6_Right/1.png", "Image/P14_D1_P7_Medium_T3_6_Right/2.png", "Image/P14_D1_P7_Medium_T3_6_Right/3.png", "Image/P14_D1_P7_Medium_T3_6_Right/4.png", "Image/P14_D1_P7_Medium_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean that plant up\"\nThe frames roughly correspond to the speech interval from 115.2713 seconds to 126.51917 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P8_Medium_T3_19_Right", "images": ["Image/P17_D2_P8_Medium_T3_19_Right/1.png", "Image/P17_D2_P8_Medium_T3_19_Right/2.png", "Image/P17_D2_P8_Medium_T3_19_Right/3.png", "Image/P17_D2_P8_Medium_T3_19_Right/4.png", "Image/P17_D2_P8_Medium_T3_19_Right/5.png", "Image/P17_D2_P8_Medium_T3_19_Right/6.png", "Image/P17_D2_P8_Medium_T3_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that bottle to the right down there? Finish.\"\nThe frames roughly correspond to the speech interval from 211.1191 seconds to 224.7051 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P7_Low_T1_20_Mid", "images": ["Image/P25_D3_P7_Low_T1_20_Mid/1.png", "Image/P25_D3_P7_Low_T1_20_Mid/2.png", "Image/P25_D3_P7_Low_T1_20_Mid/3.png", "Image/P25_D3_P7_Low_T1_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1725.38356 seconds to 1734.41628 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P7_Medium_T2_14_Mid", "images": ["Image/P20_D1_P7_Medium_T2_14_Mid/1.png", "Image/P20_D1_P7_Medium_T2_14_Mid/2.png", "Image/P20_D1_P7_Medium_T2_14_Mid/3.png", "Image/P20_D1_P7_Medium_T2_14_Mid/4.png", "Image/P20_D1_P7_Medium_T2_14_Mid/5.png", "Image/P20_D1_P7_Medium_T2_14_Mid/6.png", "Image/P20_D1_P7_Medium_T2_14_Mid/7.png", "Image/P20_D1_P7_Medium_T2_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you water the plant on the floor\"\nThe frames roughly correspond to the speech interval from 44.52692999999999 seconds to 60.06147 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P3_Medium_T3_19_Mid", "images": ["Image/P3_D1_P3_Medium_T3_19_Mid/1.png", "Image/P3_D1_P3_Medium_T3_19_Mid/2.png", "Image/P3_D1_P3_Medium_T3_19_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 1226.74158 seconds to 1241.90171 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P7_High_T1_9_Right", "images": ["Image/P5_D2_P7_High_T1_9_Right/1.png", "Image/P5_D2_P7_High_T1_9_Right/2.png", "Image/P5_D2_P7_High_T1_9_Right/3.png", "Image/P5_D2_P7_High_T1_9_Right/4.png", "Image/P5_D2_P7_High_T1_9_Right/5.png", "Image/P5_D2_P7_High_T1_9_Right/6.png", "Image/P5_D2_P7_High_T1_9_Right/7.png", "Image/P5_D2_P7_High_T1_9_Right/8.png", "Image/P5_D2_P7_High_T1_9_Right/9.png", "Image/P5_D2_P7_High_T1_9_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the cup\"\nThe frames roughly correspond to the speech interval from 680.1091999999999 seconds to 696.2013 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P1_High_T3_9_Left", "images": ["Image/P22_D2_P1_High_T3_9_Left/1.png", "Image/P22_D2_P1_High_T3_9_Left/2.png", "Image/P22_D2_P1_High_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 772.3991999999998 seconds to 787.3217999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P7_High_T2_16_Left", "images": ["Image/P26_D1_P7_High_T2_16_Left/1.png", "Image/P26_D1_P7_High_T2_16_Left/2.png", "Image/P26_D1_P7_High_T2_16_Left/3.png", "Image/P26_D1_P7_High_T2_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into that jug? Finish\"\nThe frames roughly correspond to the speech interval from 111.3151 seconds to 123.24366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P9_High_T2_24_Right", "images": ["Image/P28_D1_P9_High_T2_24_Right/1.png", "Image/P28_D1_P9_High_T2_24_Right/2.png", "Image/P28_D1_P9_High_T2_24_Right/3.png", "Image/P28_D1_P9_High_T2_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1329.08234 seconds to 1338.9369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P6_Medium_T1_6_Right", "images": ["Image/P6_D1_P6_Medium_T1_6_Right/1.png", "Image/P6_D1_P6_Medium_T1_6_Right/2.png", "Image/P6_D1_P6_Medium_T1_6_Right/3.png", "Image/P6_D1_P6_Medium_T1_6_Right/4.png", "Image/P6_D1_P6_Medium_T1_6_Right/5.png", "Image/P6_D1_P6_Medium_T1_6_Right/6.png", "Image/P6_D1_P6_Medium_T1_6_Right/7.png", "Image/P6_D1_P6_Medium_T1_6_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 1945.2701 seconds to 1962.6673000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P5_Medium_T3_27_Right", "images": ["Image/P24_D3_P5_Medium_T3_27_Right/1.png", "Image/P24_D3_P5_Medium_T3_27_Right/2.png", "Image/P24_D3_P5_Medium_T3_27_Right/3.png", "Image/P24_D3_P5_Medium_T3_27_Right/4.png", "Image/P24_D3_P5_Medium_T3_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1889.8835 seconds to 1899.4731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P1_High_T2_13_Right", "images": ["Image/P8_D1_P1_High_T2_13_Right/1.png", "Image/P8_D1_P1_High_T2_13_Right/2.png", "Image/P8_D1_P1_High_T2_13_Right/3.png", "Image/P8_D1_P1_High_T2_13_Right/4.png", "Image/P8_D1_P1_High_T2_13_Right/5.png", "Image/P8_D1_P1_High_T2_13_Right/6.png", "Image/P8_D1_P1_High_T2_13_Right/7.png", "Image/P8_D1_P1_High_T2_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. So could you water the planet just on the left side of me? Finish.\"\nThe frames roughly correspond to the speech interval from 615.4410999999999 seconds to 629.4796999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P3_Medium_T1_2_Left", "images": ["Image/P17_D1_P3_Medium_T1_2_Left/1.png", "Image/P17_D1_P3_Medium_T1_2_Left/2.png", "Image/P17_D1_P3_Medium_T1_2_Left/3.png", "Image/P17_D1_P3_Medium_T1_2_Left/4.png", "Image/P17_D1_P3_Medium_T1_2_Left/5.png", "Image/P17_D1_P3_Medium_T1_2_Left/6.png", "Image/P17_D1_P3_Medium_T1_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green object right here and put it in the basket on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 1497.04165 seconds to 1510.6270700000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P6_Low_T1_17_Mid", "images": ["Image/P9_D1_P6_Low_T1_17_Mid/1.png", "Image/P9_D1_P6_Low_T1_17_Mid/2.png", "Image/P9_D1_P6_Low_T1_17_Mid/3.png", "Image/P9_D1_P6_Low_T1_17_Mid/4.png", "Image/P9_D1_P6_Low_T1_17_Mid/5.png", "Image/P9_D1_P6_Low_T1_17_Mid/6.png", "Image/P9_D1_P6_Low_T1_17_Mid/7.png", "Image/P9_D1_P6_Low_T1_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to move this cup from the right to the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1276.492 seconds to 1298.9309999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P1_Medium_T1_13_Mid", "images": ["Image/P6_D3_P1_Medium_T1_13_Mid/1.png", "Image/P6_D3_P1_Medium_T1_13_Mid/2.png", "Image/P6_D3_P1_Medium_T1_13_Mid/3.png", "Image/P6_D3_P1_Medium_T1_13_Mid/4.png", "Image/P6_D3_P1_Medium_T1_13_Mid/5.png", "Image/P6_D3_P1_Medium_T1_13_Mid/6.png", "Image/P6_D3_P1_Medium_T1_13_Mid/7.png", "Image/P6_D3_P1_Medium_T1_13_Mid/8.png", "Image/P6_D3_P1_Medium_T1_13_Mid/9.png", "Image/P6_D3_P1_Medium_T1_13_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4659.1662 seconds to 4676.5422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P7_High_T3_7_Mid", "images": ["Image/P24_D1_P7_High_T3_7_Mid/1.png", "Image/P24_D1_P7_High_T3_7_Mid/2.png", "Image/P24_D1_P7_High_T3_7_Mid/3.png", "Image/P24_D1_P7_High_T3_7_Mid/4.png", "Image/P24_D1_P7_High_T3_7_Mid/5.png", "Image/P24_D1_P7_High_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1356.7186 seconds to 1368.159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P6_Low_T2_6_Left", "images": ["Image/P1_D3_P6_Low_T2_6_Left/1.png", "Image/P1_D3_P6_Low_T2_6_Left/2.png", "Image/P1_D3_P6_Low_T2_6_Left/3.png", "Image/P1_D3_P6_Low_T2_6_Left/4.png", "Image/P1_D3_P6_Low_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1406.1749099999995 seconds to 1416.8937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P8_Low_T3_24_Left", "images": ["Image/P9_D3_P8_Low_T3_24_Left/1.png", "Image/P9_D3_P8_Low_T3_24_Left/2.png", "Image/P9_D3_P8_Low_T3_24_Left/3.png", "Image/P9_D3_P8_Low_T3_24_Left/4.png", "Image/P9_D3_P8_Low_T3_24_Left/5.png", "Image/P9_D3_P8_Low_T3_24_Left/6.png", "Image/P9_D3_P8_Low_T3_24_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the land on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 907.0107 seconds to 921.328 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P9_Low_T3_17_Mid", "images": ["Image/P2_D3_P9_Low_T3_17_Mid/1.png", "Image/P2_D3_P9_Low_T3_17_Mid/2.png", "Image/P2_D3_P9_Low_T3_17_Mid/3.png", "Image/P2_D3_P9_Low_T3_17_Mid/4.png", "Image/P2_D3_P9_Low_T3_17_Mid/5.png", "Image/P2_D3_P9_Low_T3_17_Mid/6.png", "Image/P2_D3_P9_Low_T3_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please claim that brown mug for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2226.2019 seconds to 2241.8032000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P4_High_T1_18_Mid", "images": ["Image/P26_D1_P4_High_T1_18_Mid/1.png", "Image/P26_D1_P4_High_T1_18_Mid/2.png", "Image/P26_D1_P4_High_T1_18_Mid/3.png", "Image/P26_D1_P4_High_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the vase here in the middle? Finish\"\nThe frames roughly correspond to the speech interval from 367.03649999999993 seconds to 380.8448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P2_High_T1_6_Mid", "images": ["Image/P8_D2_P2_High_T1_6_Mid/1.png", "Image/P8_D2_P2_High_T1_6_Mid/2.png", "Image/P8_D2_P2_High_T1_6_Mid/3.png", "Image/P8_D2_P2_High_T1_6_Mid/4.png", "Image/P8_D2_P2_High_T1_6_Mid/5.png", "Image/P8_D2_P2_High_T1_6_Mid/6.png", "Image/P8_D2_P2_High_T1_6_Mid/7.png", "Image/P8_D2_P2_High_T1_6_Mid/8.png", "Image/P8_D2_P2_High_T1_6_Mid/9.png", "Image/P8_D2_P2_High_T1_6_Mid/10.png", "Image/P8_D2_P2_High_T1_6_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you move that planet to the ground floor of the desk at the bottom? Just that desk. Finish.\"\nThe frames roughly correspond to the speech interval from 2209.8209 seconds to 2232.802 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P1_Medium_T3_10_Right", "images": ["Image/P18_D1_P1_Medium_T3_10_Right/1.png", "Image/P18_D1_P1_Medium_T3_10_Right/2.png", "Image/P18_D1_P1_Medium_T3_10_Right/3.png", "Image/P18_D1_P1_Medium_T3_10_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1080.2351 seconds to 1090.4101 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P5_Low_T2_15_Mid", "images": ["Image/P21_D3_P5_Low_T2_15_Mid/1.png", "Image/P21_D3_P5_Low_T2_15_Mid/2.png", "Image/P21_D3_P5_Low_T2_15_Mid/3.png", "Image/P21_D3_P5_Low_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 583.612 seconds to 594.6764000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P1_Medium_T2_20_Left", "images": ["Image/P1_D3_P1_Medium_T2_20_Left/1.png", "Image/P1_D3_P1_Medium_T2_20_Left/2.png", "Image/P1_D3_P1_Medium_T2_20_Left/3.png", "Image/P1_D3_P1_Medium_T2_20_Left/4.png", "Image/P1_D3_P1_Medium_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1626.3638 seconds to 1637.2938999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P1_Medium_T3_21_Left", "images": ["Image/P14_D2_P1_Medium_T3_21_Left/1.png", "Image/P14_D2_P1_Medium_T3_21_Left/2.png", "Image/P14_D2_P1_Medium_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1572.78968 seconds to 1581.3142999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P7_Low_T3_8_Right", "images": ["Image/P26_D3_P7_Low_T3_8_Right/1.png", "Image/P26_D3_P7_Low_T3_8_Right/2.png", "Image/P26_D3_P7_Low_T3_8_Right/3.png", "Image/P26_D3_P7_Low_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that vase down there? Finish.\"\nThe frames roughly correspond to the speech interval from 942.0219 seconds to 953.1292 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P1_Medium_T1_12_Left", "images": ["Image/P22_D2_P1_Medium_T1_12_Left/1.png", "Image/P22_D2_P1_Medium_T1_12_Left/2.png", "Image/P22_D2_P1_Medium_T1_12_Left/3.png", "Image/P22_D2_P1_Medium_T1_12_Left/4.png", "Image/P22_D2_P1_Medium_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 602.6470699999999 seconds to 612.96084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P6_Medium_T1_24_Left", "images": ["Image/P30_D3_P6_Medium_T1_24_Left/1.png", "Image/P30_D3_P6_Medium_T1_24_Left/2.png", "Image/P30_D3_P6_Medium_T1_24_Left/3.png", "Image/P30_D3_P6_Medium_T1_24_Left/4.png", "Image/P30_D3_P6_Medium_T1_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the plant furthest away from me\"\nThe frames roughly correspond to the speech interval from 1126.10652 seconds to 1136.6282 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P9_High_T1_23_Right", "images": ["Image/P13_D1_P9_High_T1_23_Right/1.png", "Image/P13_D1_P9_High_T1_23_Right/2.png", "Image/P13_D1_P9_High_T1_23_Right/3.png", "Image/P13_D1_P9_High_T1_23_Right/4.png", "Image/P13_D1_P9_High_T1_23_Right/5.png", "Image/P13_D1_P9_High_T1_23_Right/6.png", "Image/P13_D1_P9_High_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this plant down here up to here\"\nThe frames roughly correspond to the speech interval from 593.6483000000001 seconds to 605.6753 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P8_High_T1_18_Left", "images": ["Image/P2_D3_P8_High_T1_18_Left/1.png", "Image/P2_D3_P8_High_T1_18_Left/2.png", "Image/P2_D3_P8_High_T1_18_Left/3.png", "Image/P2_D3_P8_High_T1_18_Left/4.png", "Image/P2_D3_P8_High_T1_18_Left/5.png", "Image/P2_D3_P8_High_T1_18_Left/6.png", "Image/P2_D3_P8_High_T1_18_Left/7.png", "Image/P2_D3_P8_High_T1_18_Left/8.png", "Image/P2_D3_P8_High_T1_18_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please move that brown cup all the way to the top of the shelf? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2201.8966 seconds to 2218.7669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P1_High_T1_15_Right", "images": ["Image/P17_D2_P1_High_T1_15_Right/1.png", "Image/P17_D2_P1_High_T1_15_Right/2.png", "Image/P17_D2_P1_High_T1_15_Right/3.png", "Image/P17_D2_P1_High_T1_15_Right/4.png", "Image/P17_D2_P1_High_T1_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant down to the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 444.2913 seconds to 456.1793 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P3_Medium_T2_25_Mid", "images": ["Image/P20_D2_P3_Medium_T2_25_Mid/1.png", "Image/P20_D2_P3_Medium_T2_25_Mid/2.png", "Image/P20_D2_P3_Medium_T2_25_Mid/3.png", "Image/P20_D2_P3_Medium_T2_25_Mid/4.png", "Image/P20_D2_P3_Medium_T2_25_Mid/5.png", "Image/P20_D2_P3_Medium_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into the middle cup on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2916.2078 seconds to 2927.6346000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P2_Low_T1_24_Right", "images": ["Image/P5_D1_P2_Low_T1_24_Right/1.png", "Image/P5_D1_P2_Low_T1_24_Right/2.png", "Image/P5_D1_P2_Low_T1_24_Right/3.png", "Image/P5_D1_P2_Low_T1_24_Right/4.png", "Image/P5_D1_P2_Low_T1_24_Right/5.png", "Image/P5_D1_P2_Low_T1_24_Right/6.png", "Image/P5_D1_P2_Low_T1_24_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant into the bucket next to the wall. Finished.\"\nThe frames roughly correspond to the speech interval from 1937.9259 seconds to 1952.7235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P6_Low_T2_12_Right", "images": ["Image/P16_D2_P6_Low_T2_12_Right/1.png", "Image/P16_D2_P6_Low_T2_12_Right/2.png", "Image/P16_D2_P6_Low_T2_12_Right/3.png", "Image/P16_D2_P6_Low_T2_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish\"\nThe frames roughly correspond to the speech interval from 851.1115 seconds to 860.9939 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P3_High_T1_24_Right", "images": ["Image/P16_D3_P3_High_T1_24_Right/1.png", "Image/P16_D3_P3_High_T1_24_Right/2.png", "Image/P16_D3_P3_High_T1_24_Right/3.png", "Image/P16_D3_P3_High_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase down there. Finish.\"\nThe frames roughly correspond to the speech interval from 499.6341 seconds to 509.0719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P4_High_T3_7_Left", "images": ["Image/P5_D1_P4_High_T3_7_Left/1.png", "Image/P5_D1_P4_High_T3_7_Left/2.png", "Image/P5_D1_P4_High_T3_7_Left/3.png", "Image/P5_D1_P4_High_T3_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this cup. Finished.\"\nThe frames roughly correspond to the speech interval from 1960.2652 seconds to 1969.8976 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P2_High_T1_9_Left", "images": ["Image/P15_D1_P2_High_T1_9_Left/1.png", "Image/P15_D1_P2_High_T1_9_Left/2.png", "Image/P15_D1_P2_High_T1_9_Left/3.png", "Image/P15_D1_P2_High_T1_9_Left/4.png", "Image/P15_D1_P2_High_T1_9_Left/5.png", "Image/P15_D1_P2_High_T1_9_Left/6.png", "Image/P15_D1_P2_High_T1_9_Left/7.png", "Image/P15_D1_P2_High_T1_9_Left/8.png", "Image/P15_D1_P2_High_T1_9_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1639.4692699999998 seconds to 1653.86187 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P2_Medium_T2_13_Right", "images": ["Image/P17_D2_P2_Medium_T2_13_Right/1.png", "Image/P17_D2_P2_Medium_T2_13_Right/2.png", "Image/P17_D2_P2_Medium_T2_13_Right/3.png", "Image/P17_D2_P2_Medium_T2_13_Right/4.png", "Image/P17_D2_P2_Medium_T2_13_Right/5.png", "Image/P17_D2_P2_Medium_T2_13_Right/6.png", "Image/P17_D2_P2_Medium_T2_13_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in the right-nose plant on the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 283.37710000000004 seconds to 297.3963 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P3_High_T2_21_Mid", "images": ["Image/P11_D3_P3_High_T2_21_Mid/1.png", "Image/P11_D3_P3_High_T2_21_Mid/2.png", "Image/P11_D3_P3_High_T2_21_Mid/3.png", "Image/P11_D3_P3_High_T2_21_Mid/4.png", "Image/P11_D3_P3_High_T2_21_Mid/5.png", "Image/P11_D3_P3_High_T2_21_Mid/6.png", "Image/P11_D3_P3_High_T2_21_Mid/7.png", "Image/P11_D3_P3_High_T2_21_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the white glass on the top of the shell. Finish.\"\nThe frames roughly correspond to the speech interval from 1532.8619 seconds to 1546.2803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P9_Low_T1_2_Mid", "images": ["Image/P1_D3_P9_Low_T1_2_Mid/1.png", "Image/P1_D3_P9_Low_T1_2_Mid/2.png", "Image/P1_D3_P9_Low_T1_2_Mid/3.png", "Image/P1_D3_P9_Low_T1_2_Mid/4.png", "Image/P1_D3_P9_Low_T1_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the bottle to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1769.1072 seconds to 1779.9112999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P8_Medium_T3_17_Mid", "images": ["Image/P9_D3_P8_Medium_T3_17_Mid/1.png", "Image/P9_D3_P8_Medium_T3_17_Mid/2.png", "Image/P9_D3_P8_Medium_T3_17_Mid/3.png", "Image/P9_D3_P8_Medium_T3_17_Mid/4.png", "Image/P9_D3_P8_Medium_T3_17_Mid/5.png", "Image/P9_D3_P8_Medium_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the middle cup on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 930.9663 seconds to 944.2399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P2_Medium_T2_18_Mid", "images": ["Image/P30_D1_P2_Medium_T2_18_Mid/1.png", "Image/P30_D1_P2_Medium_T2_18_Mid/2.png", "Image/P30_D1_P2_Medium_T2_18_Mid/3.png", "Image/P30_D1_P2_Medium_T2_18_Mid/4.png", "Image/P30_D1_P2_Medium_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 623.3826300000001 seconds to 636.1590100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P9_Medium_T3_17_Mid", "images": ["Image/P27_D1_P9_Medium_T3_17_Mid/1.png", "Image/P27_D1_P9_Medium_T3_17_Mid/2.png", "Image/P27_D1_P9_Medium_T3_17_Mid/3.png", "Image/P27_D1_P9_Medium_T3_17_Mid/4.png", "Image/P27_D1_P9_Medium_T3_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 781.6689 seconds to 793.9308000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P9_High_T1_12_Mid", "images": ["Image/P30_D2_P9_High_T1_12_Mid/1.png", "Image/P30_D2_P9_High_T1_12_Mid/2.png", "Image/P30_D2_P9_High_T1_12_Mid/3.png", "Image/P30_D2_P9_High_T1_12_Mid/4.png", "Image/P30_D2_P9_High_T1_12_Mid/5.png", "Image/P30_D2_P9_High_T1_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that red object up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 222.0685 seconds to 233.66350000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P7_High_T2_27_Mid", "images": ["Image/P7_D3_P7_High_T2_27_Mid/1.png", "Image/P7_D3_P7_High_T2_27_Mid/2.png", "Image/P7_D3_P7_High_T2_27_Mid/3.png", "Image/P7_D3_P7_High_T2_27_Mid/4.png", "Image/P7_D3_P7_High_T2_27_Mid/5.png", "Image/P7_D3_P7_High_T2_27_Mid/6.png", "Image/P7_D3_P7_High_T2_27_Mid/7.png", "Image/P7_D3_P7_High_T2_27_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1886.593 seconds to 1901.4457 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P4_Low_T3_11_Right", "images": ["Image/P7_D1_P4_Low_T3_11_Right/1.png", "Image/P7_D1_P4_Low_T3_11_Right/2.png", "Image/P7_D1_P4_Low_T3_11_Right/3.png", "Image/P7_D1_P4_Low_T3_11_Right/4.png", "Image/P7_D1_P4_Low_T3_11_Right/5.png", "Image/P7_D1_P4_Low_T3_11_Right/6.png", "Image/P7_D1_P4_Low_T3_11_Right/7.png", "Image/P7_D1_P4_Low_T3_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 542.7318 seconds to 558.2943 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P2_Low_T3_1_Left", "images": ["Image/P6_D1_P2_Low_T3_1_Left/1.png", "Image/P6_D1_P2_Low_T3_1_Left/2.png", "Image/P6_D1_P2_Low_T3_1_Left/3.png", "Image/P6_D1_P2_Low_T3_1_Left/4.png", "Image/P6_D1_P2_Low_T3_1_Left/5.png", "Image/P6_D1_P2_Low_T3_1_Left/6.png", "Image/P6_D1_P2_Low_T3_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2170.954 seconds to 2183.9794 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P3_Low_T1_7_Mid", "images": ["Image/P7_D3_P3_Low_T1_7_Mid/1.png", "Image/P7_D3_P3_Low_T1_7_Mid/2.png", "Image/P7_D3_P3_Low_T1_7_Mid/3.png", "Image/P7_D3_P3_Low_T1_7_Mid/4.png", "Image/P7_D3_P3_Low_T1_7_Mid/5.png", "Image/P7_D3_P3_Low_T1_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1728.8109000000002 seconds to 1740.8231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P7_Low_T1_13_Mid", "images": ["Image/P2_D3_P7_Low_T1_13_Mid/1.png", "Image/P2_D3_P7_Low_T1_13_Mid/2.png", "Image/P2_D3_P7_Low_T1_13_Mid/3.png", "Image/P2_D3_P7_Low_T1_13_Mid/4.png", "Image/P2_D3_P7_Low_T1_13_Mid/5.png", "Image/P2_D3_P7_Low_T1_13_Mid/6.png", "Image/P2_D3_P7_Low_T1_13_Mid/7.png", "Image/P2_D3_P7_Low_T1_13_Mid/8.png", "Image/P2_D3_P7_Low_T1_13_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please move that plant\"\nThe frames roughly correspond to the speech interval from 2307.6050000000005 seconds to 2324.8634 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P7_Medium_T1_2_Right", "images": ["Image/P3_D1_P7_Medium_T1_2_Right/1.png", "Image/P3_D1_P7_Medium_T1_2_Right/2.png", "Image/P3_D1_P7_Medium_T1_2_Right/3.png", "Image/P3_D1_P7_Medium_T1_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1483.9089 seconds to 1493.9036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P9_Medium_T2_3_Right", "images": ["Image/P20_D1_P9_Medium_T2_3_Right/1.png", "Image/P20_D1_P9_Medium_T2_3_Right/2.png", "Image/P20_D1_P9_Medium_T2_3_Right/3.png", "Image/P20_D1_P9_Medium_T2_3_Right/4.png", "Image/P20_D1_P9_Medium_T2_3_Right/5.png", "Image/P20_D1_P9_Medium_T2_3_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour some water into a vessel over there\"\nThe frames roughly correspond to the speech interval from 802.3855000000001 seconds to 814.5091 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P6_Low_T2_14_Right", "images": ["Image/P13_D2_P6_Low_T2_14_Right/1.png", "Image/P13_D2_P6_Low_T2_14_Right/2.png", "Image/P13_D2_P6_Low_T2_14_Right/3.png", "Image/P13_D2_P6_Low_T2_14_Right/4.png", "Image/P13_D2_P6_Low_T2_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you water the plant please over here? Finish.\"\nThe frames roughly correspond to the speech interval from 1175.31996 seconds to 1185.2868600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P7_Low_T3_16_Mid", "images": ["Image/P29_D2_P7_Low_T3_16_Mid/1.png", "Image/P29_D2_P7_Low_T3_16_Mid/2.png", "Image/P29_D2_P7_Low_T3_16_Mid/3.png", "Image/P29_D2_P7_Low_T3_16_Mid/4.png", "Image/P29_D2_P7_Low_T3_16_Mid/5.png", "Image/P29_D2_P7_Low_T3_16_Mid/6.png", "Image/P29_D2_P7_Low_T3_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 512.5792 seconds to 524.6189 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P9_High_T2_1_Mid", "images": ["Image/P7_D1_P9_High_T2_1_Mid/1.png", "Image/P7_D1_P9_High_T2_1_Mid/2.png", "Image/P7_D1_P9_High_T2_1_Mid/3.png", "Image/P7_D1_P9_High_T2_1_Mid/4.png", "Image/P7_D1_P9_High_T2_1_Mid/5.png", "Image/P7_D1_P9_High_T2_1_Mid/6.png", "Image/P7_D1_P9_High_T2_1_Mid/7.png", "Image/P7_D1_P9_High_T2_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 691.5225 seconds to 707.697 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P7_Medium_T3_2_Left", "images": ["Image/P18_D3_P7_Medium_T3_2_Left/1.png", "Image/P18_D3_P7_Medium_T3_2_Left/2.png", "Image/P18_D3_P7_Medium_T3_2_Left/3.png", "Image/P18_D3_P7_Medium_T3_2_Left/4.png", "Image/P18_D3_P7_Medium_T3_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the bottle on the far left-hand side of the room. Finish.\"\nThe frames roughly correspond to the speech interval from 1620.9462 seconds to 1632.3744000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P5_Low_T2_16_Right", "images": ["Image/P6_D3_P5_Low_T2_16_Right/1.png", "Image/P6_D3_P5_Low_T2_16_Right/2.png", "Image/P6_D3_P5_Low_T2_16_Right/3.png", "Image/P6_D3_P5_Low_T2_16_Right/4.png", "Image/P6_D3_P5_Low_T2_16_Right/5.png", "Image/P6_D3_P5_Low_T2_16_Right/6.png", "Image/P6_D3_P5_Low_T2_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3326.62154 seconds to 3340.06892 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P7_Medium_T1_3_Left", "images": ["Image/P29_D2_P7_Medium_T1_3_Left/1.png", "Image/P29_D2_P7_Medium_T1_3_Left/2.png", "Image/P29_D2_P7_Medium_T1_3_Left/3.png", "Image/P29_D2_P7_Medium_T1_3_Left/4.png", "Image/P29_D2_P7_Medium_T1_3_Left/5.png", "Image/P29_D2_P7_Medium_T1_3_Left/6.png", "Image/P29_D2_P7_Medium_T1_3_Left/7.png", "Image/P29_D2_P7_Medium_T1_3_Left/8.png", "Image/P29_D2_P7_Medium_T1_3_Left/9.png", "Image/P29_D2_P7_Medium_T1_3_Left/10.png", "Image/P29_D2_P7_Medium_T1_3_Left/11.png", "Image/P29_D2_P7_Medium_T1_3_Left/12.png", "Image/P29_D2_P7_Medium_T1_3_Left/13.png", "Image/P29_D2_P7_Medium_T1_3_Left/14.png", "Image/P29_D2_P7_Medium_T1_3_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 642.7435 seconds to 663.8185 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P3_Medium_T2_10_Right", "images": ["Image/P5_D1_P3_Medium_T2_10_Right/1.png", "Image/P5_D1_P3_Medium_T2_10_Right/2.png", "Image/P5_D1_P3_Medium_T2_10_Right/3.png", "Image/P5_D1_P3_Medium_T2_10_Right/4.png", "Image/P5_D1_P3_Medium_T2_10_Right/5.png", "Image/P5_D1_P3_Medium_T2_10_Right/6.png", "Image/P5_D1_P3_Medium_T2_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle that is farthest to me and closest to you. Finished.\"\nThe frames roughly correspond to the speech interval from 2068.2163 seconds to 2081.8831 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P3_Low_T1_10_Left", "images": ["Image/P4_D3_P3_Low_T1_10_Left/1.png", "Image/P4_D3_P3_Low_T1_10_Left/2.png", "Image/P4_D3_P3_Low_T1_10_Left/3.png", "Image/P4_D3_P3_Low_T1_10_Left/4.png", "Image/P4_D3_P3_Low_T1_10_Left/5.png", "Image/P4_D3_P3_Low_T1_10_Left/6.png", "Image/P4_D3_P3_Low_T1_10_Left/7.png", "Image/P4_D3_P3_Low_T1_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that right bottle to the bottom of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 551.5753 seconds to 566.0379999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P2_Medium_T2_19_Left", "images": ["Image/P11_D2_P2_Medium_T2_19_Left/1.png", "Image/P11_D2_P2_Medium_T2_19_Left/2.png", "Image/P11_D2_P2_Medium_T2_19_Left/3.png", "Image/P11_D2_P2_Medium_T2_19_Left/4.png", "Image/P11_D2_P2_Medium_T2_19_Left/5.png", "Image/P11_D2_P2_Medium_T2_19_Left/6.png", "Image/P11_D2_P2_Medium_T2_19_Left/7.png", "Image/P11_D2_P2_Medium_T2_19_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the first white glass from left side. Finish.\"\nThe frames roughly correspond to the speech interval from 1055.611 seconds to 1072.1086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P8_Low_T1_9_Right", "images": ["Image/P20_D2_P8_Low_T1_9_Right/1.png", "Image/P20_D2_P8_Low_T1_9_Right/2.png", "Image/P20_D2_P8_Low_T1_9_Right/3.png", "Image/P20_D2_P8_Low_T1_9_Right/4.png", "Image/P20_D2_P8_Low_T1_9_Right/5.png", "Image/P20_D2_P8_Low_T1_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that green cup to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3033.1650000000004 seconds to 3044.9786000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P3_Low_T2_6_Right", "images": ["Image/P29_D2_P3_Low_T2_6_Right/1.png", "Image/P29_D2_P3_Low_T2_6_Right/2.png", "Image/P29_D2_P3_Low_T2_6_Right/3.png", "Image/P29_D2_P3_Low_T2_6_Right/4.png", "Image/P29_D2_P3_Low_T2_6_Right/5.png", "Image/P29_D2_P3_Low_T2_6_Right/6.png", "Image/P29_D2_P3_Low_T2_6_Right/7.png", "Image/P29_D2_P3_Low_T2_6_Right/8.png", "Image/P29_D2_P3_Low_T2_6_Right/9.png", "Image/P29_D2_P3_Low_T2_6_Right/10.png", "Image/P29_D2_P3_Low_T2_6_Right/11.png", "Image/P29_D2_P3_Low_T2_6_Right/12.png", "Image/P29_D2_P3_Low_T2_6_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Just know for the\"\nThe frames roughly correspond to the speech interval from 88.67669000000001 seconds to 102.13573 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P3_Medium_T2_25_Mid", "images": ["Image/P19_D1_P3_Medium_T2_25_Mid/1.png", "Image/P19_D1_P3_Medium_T2_25_Mid/2.png", "Image/P19_D1_P3_Medium_T2_25_Mid/3.png", "Image/P19_D1_P3_Medium_T2_25_Mid/4.png", "Image/P19_D1_P3_Medium_T2_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour from this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 892.6397 seconds to 904.3312 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P1_High_T2_12_Right", "images": ["Image/P13_D1_P1_High_T2_12_Right/1.png", "Image/P13_D1_P1_High_T2_12_Right/2.png", "Image/P13_D1_P1_High_T2_12_Right/3.png", "Image/P13_D1_P1_High_T2_12_Right/4.png", "Image/P13_D1_P1_High_T2_12_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the brown jug here\"\nThe frames roughly correspond to the speech interval from 311.6313 seconds to 322.837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P2_Medium_T1_2_Right", "images": ["Image/P30_D3_P2_Medium_T1_2_Right/1.png", "Image/P30_D3_P2_Medium_T1_2_Right/2.png", "Image/P30_D3_P2_Medium_T1_2_Right/3.png", "Image/P30_D3_P2_Medium_T1_2_Right/4.png", "Image/P30_D3_P2_Medium_T1_2_Right/5.png", "Image/P30_D3_P2_Medium_T1_2_Right/6.png", "Image/P30_D3_P2_Medium_T1_2_Right/7.png", "Image/P30_D3_P2_Medium_T1_2_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase furthest on the right down to the bottom underneath the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1303.7376 seconds to 1318.7146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P5_Medium_T1_4_Left", "images": ["Image/P24_D2_P5_Medium_T1_4_Left/1.png", "Image/P24_D2_P5_Medium_T1_4_Left/2.png", "Image/P24_D2_P5_Medium_T1_4_Left/3.png", "Image/P24_D2_P5_Medium_T1_4_Left/4.png", "Image/P24_D2_P5_Medium_T1_4_Left/5.png", "Image/P24_D2_P5_Medium_T1_4_Left/6.png", "Image/P24_D2_P5_Medium_T1_4_Left/7.png", "Image/P24_D2_P5_Medium_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shell? Please finish.\"\nThe frames roughly correspond to the speech interval from 616.9355 seconds to 630.5784 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P3_High_T2_7_Right", "images": ["Image/P21_D1_P3_High_T2_7_Right/1.png", "Image/P21_D1_P3_High_T2_7_Right/2.png", "Image/P21_D1_P3_High_T2_7_Right/3.png", "Image/P21_D1_P3_High_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1101.1237999999998 seconds to 1111.8135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P5_High_T2_22_Right", "images": ["Image/P9_D3_P5_High_T2_22_Right/1.png", "Image/P9_D3_P5_High_T2_22_Right/2.png", "Image/P9_D3_P5_High_T2_22_Right/3.png", "Image/P9_D3_P5_High_T2_22_Right/4.png", "Image/P9_D3_P5_High_T2_22_Right/5.png", "Image/P9_D3_P5_High_T2_22_Right/6.png", "Image/P9_D3_P5_High_T2_22_Right/7.png", "Image/P9_D3_P5_High_T2_22_Right/8.png", "Image/P9_D3_P5_High_T2_22_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water on the plant on the third layer? Finish.\"\nThe frames roughly correspond to the speech interval from 1064.0675 seconds to 1080.065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P4_High_T1_15_Left", "images": ["Image/P16_D2_P4_High_T1_15_Left/1.png", "Image/P16_D2_P4_High_T1_15_Left/2.png", "Image/P16_D2_P4_High_T1_15_Left/3.png", "Image/P16_D2_P4_High_T1_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1025.4721 seconds to 1035.2984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P1_High_T3_9_Mid", "images": ["Image/P2_D3_P1_High_T3_9_Mid/1.png", "Image/P2_D3_P1_High_T3_9_Mid/2.png", "Image/P2_D3_P1_High_T3_9_Mid/3.png", "Image/P2_D3_P1_High_T3_9_Mid/4.png", "Image/P2_D3_P1_High_T3_9_Mid/5.png", "Image/P2_D3_P1_High_T3_9_Mid/6.png", "Image/P2_D3_P1_High_T3_9_Mid/7.png", "Image/P2_D3_P1_High_T3_9_Mid/8.png", "Image/P2_D3_P1_High_T3_9_Mid/9.png", "Image/P2_D3_P1_High_T3_9_Mid/10.png", "Image/P2_D3_P1_High_T3_9_Mid/11.png", "Image/P2_D3_P1_High_T3_9_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that ring cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1807.9777000000004 seconds to 1825.6815 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P9_Medium_T3_23_Mid", "images": ["Image/P23_D1_P9_Medium_T3_23_Mid/1.png", "Image/P23_D1_P9_Medium_T3_23_Mid/2.png", "Image/P23_D1_P9_Medium_T3_23_Mid/3.png", "Image/P23_D1_P9_Medium_T3_23_Mid/4.png", "Image/P23_D1_P9_Medium_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1703.0979 seconds to 1714.1351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P2_High_T1_14_Right", "images": ["Image/P9_D2_P2_High_T1_14_Right/1.png", "Image/P9_D2_P2_High_T1_14_Right/2.png", "Image/P9_D2_P2_High_T1_14_Right/3.png", "Image/P9_D2_P2_High_T1_14_Right/4.png", "Image/P9_D2_P2_High_T1_14_Right/5.png", "Image/P9_D2_P2_High_T1_14_Right/6.png", "Image/P9_D2_P2_High_T1_14_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this plant to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2920.8252 seconds to 2934.1009 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P8_Medium_T3_18_Right", "images": ["Image/P7_D3_P8_Medium_T3_18_Right/1.png", "Image/P7_D3_P8_Medium_T3_18_Right/2.png", "Image/P7_D3_P8_Medium_T3_18_Right/3.png", "Image/P7_D3_P8_Medium_T3_18_Right/4.png", "Image/P7_D3_P8_Medium_T3_18_Right/5.png", "Image/P7_D3_P8_Medium_T3_18_Right/6.png", "Image/P7_D3_P8_Medium_T3_18_Right/7.png", "Image/P7_D3_P8_Medium_T3_18_Right/8.png", "Image/P7_D3_P8_Medium_T3_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2182.8641 seconds to 2199.5820000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P4_High_T3_20_Mid", "images": ["Image/P30_D3_P4_High_T3_20_Mid/1.png", "Image/P30_D3_P4_High_T3_20_Mid/2.png", "Image/P30_D3_P4_High_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1529.2952 seconds to 1537.4806 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P3_Low_T2_25_Left", "images": ["Image/P24_D3_P3_Low_T2_25_Left/1.png", "Image/P24_D3_P3_Low_T2_25_Left/2.png", "Image/P24_D3_P3_Low_T2_25_Left/3.png", "Image/P24_D3_P3_Low_T2_25_Left/4.png", "Image/P24_D3_P3_Low_T2_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1530.73486 seconds to 1540.92345 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P8_Low_T3_10_Right", "images": ["Image/P29_D2_P8_Low_T3_10_Right/1.png", "Image/P29_D2_P8_Low_T3_10_Right/2.png", "Image/P29_D2_P8_Low_T3_10_Right/3.png", "Image/P29_D2_P8_Low_T3_10_Right/4.png", "Image/P29_D2_P8_Low_T3_10_Right/5.png", "Image/P29_D2_P8_Low_T3_10_Right/6.png", "Image/P29_D2_P8_Low_T3_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 531.2323 seconds to 545.4159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P7_Low_T1_2_Left", "images": ["Image/P27_D3_P7_Low_T1_2_Left/1.png", "Image/P27_D3_P7_Low_T1_2_Left/2.png", "Image/P27_D3_P7_Low_T1_2_Left/3.png", "Image/P27_D3_P7_Low_T1_2_Left/4.png", "Image/P27_D3_P7_Low_T1_2_Left/5.png", "Image/P27_D3_P7_Low_T1_2_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that glass to the top shelf? Finish\"\nThe frames roughly correspond to the speech interval from 566.115 seconds to 577.1388 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P6_Medium_T2_27_Right", "images": ["Image/P3_D1_P6_Medium_T2_27_Right/1.png", "Image/P3_D1_P6_Medium_T2_27_Right/2.png", "Image/P3_D1_P6_Medium_T2_27_Right/3.png", "Image/P3_D1_P6_Medium_T2_27_Right/4.png", "Image/P3_D1_P6_Medium_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1401.908 seconds to 1412.7524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P2_Low_T3_8_Right", "images": ["Image/P25_D3_P2_Low_T3_8_Right/1.png", "Image/P25_D3_P2_Low_T3_8_Right/2.png", "Image/P25_D3_P2_Low_T3_8_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1835.1736 seconds to 1845.6536 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P2_Low_T2_10_Left", "images": ["Image/P1_D1_P2_Low_T2_10_Left/1.png", "Image/P1_D1_P2_Low_T2_10_Left/2.png", "Image/P1_D1_P2_Low_T2_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this one? Finish.\"\nThe frames roughly correspond to the speech interval from 475.102 seconds to 483.246 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P8_Low_T2_13_Mid", "images": ["Image/P27_D2_P8_Low_T2_13_Mid/1.png", "Image/P27_D2_P8_Low_T2_13_Mid/2.png", "Image/P27_D2_P8_Low_T2_13_Mid/3.png", "Image/P27_D2_P8_Low_T2_13_Mid/4.png", "Image/P27_D2_P8_Low_T2_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1465.3192 seconds to 1475.23 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P9_Medium_T3_9_Left", "images": ["Image/P29_D3_P9_Medium_T3_9_Left/1.png", "Image/P29_D3_P9_Medium_T3_9_Left/2.png", "Image/P29_D3_P9_Medium_T3_9_Left/3.png", "Image/P29_D3_P9_Medium_T3_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1670.9371 seconds to 1680.4851999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P3_Medium_T2_16_Right", "images": ["Image/P19_D2_P3_Medium_T2_16_Right/1.png", "Image/P19_D2_P3_Medium_T2_16_Right/2.png", "Image/P19_D2_P3_Medium_T2_16_Right/3.png", "Image/P19_D2_P3_Medium_T2_16_Right/4.png", "Image/P19_D2_P3_Medium_T2_16_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1496.1789 seconds to 1509.9287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P4_High_T2_5_Mid", "images": ["Image/P14_D3_P4_High_T2_5_Mid/1.png", "Image/P14_D3_P4_High_T2_5_Mid/2.png", "Image/P14_D3_P4_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1147.50746 seconds to 1156.01856 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P6_Low_T1_8_Mid", "images": ["Image/P8_D2_P6_Low_T1_8_Mid/1.png", "Image/P8_D2_P6_Low_T1_8_Mid/2.png", "Image/P8_D2_P6_Low_T1_8_Mid/3.png", "Image/P8_D2_P6_Low_T1_8_Mid/4.png", "Image/P8_D2_P6_Low_T1_8_Mid/5.png", "Image/P8_D2_P6_Low_T1_8_Mid/6.png", "Image/P8_D2_P6_Low_T1_8_Mid/7.png", "Image/P8_D2_P6_Low_T1_8_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2024.67158 seconds to 2039.52339 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P9_High_T3_6_Mid", "images": ["Image/P14_D1_P9_High_T3_6_Mid/1.png", "Image/P14_D1_P9_High_T3_6_Mid/2.png", "Image/P14_D1_P9_High_T3_6_Mid/3.png", "Image/P14_D1_P9_High_T3_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that plant up for me. Finish.\"\nThe frames roughly correspond to the speech interval from 517.7943 seconds to 527.4680000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P3_High_T2_7_Right", "images": ["Image/P4_D2_P3_High_T2_7_Right/1.png", "Image/P4_D2_P3_High_T2_7_Right/2.png", "Image/P4_D2_P3_High_T2_7_Right/3.png", "Image/P4_D2_P3_High_T2_7_Right/4.png", "Image/P4_D2_P3_High_T2_7_Right/5.png", "Image/P4_D2_P3_High_T2_7_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the green bottle in the right hand? Finish.\"\nThe frames roughly correspond to the speech interval from 1889.6103 seconds to 1904.2792999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P8_High_T2_4_Mid", "images": ["Image/P26_D2_P8_High_T2_4_Mid/1.png", "Image/P26_D2_P8_High_T2_4_Mid/2.png", "Image/P26_D2_P8_High_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1453.2480999999998 seconds to 1465.8449999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P3_High_T1_4_Left", "images": ["Image/P22_D2_P3_High_T1_4_Left/1.png", "Image/P22_D2_P3_High_T1_4_Left/2.png", "Image/P22_D2_P3_High_T1_4_Left/3.png", "Image/P22_D2_P3_High_T1_4_Left/4.png", "Image/P22_D2_P3_High_T1_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the bottom of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 636.2492899999999 seconds to 645.8528699999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P4_High_T3_16_Mid", "images": ["Image/P24_D3_P4_High_T3_16_Mid/1.png", "Image/P24_D3_P4_High_T3_16_Mid/2.png", "Image/P24_D3_P4_High_T3_16_Mid/3.png", "Image/P24_D3_P4_High_T3_16_Mid/4.png", "Image/P24_D3_P4_High_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1812.9051 seconds to 1823.1087 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P5_Low_T2_19_Left", "images": ["Image/P7_D1_P5_Low_T2_19_Left/1.png", "Image/P7_D1_P5_Low_T2_19_Left/2.png", "Image/P7_D1_P5_Low_T2_19_Left/3.png", "Image/P7_D1_P5_Low_T2_19_Left/4.png", "Image/P7_D1_P5_Low_T2_19_Left/5.png", "Image/P7_D1_P5_Low_T2_19_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 283.529 seconds to 297.5464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P3_Low_T2_2_Left", "images": ["Image/P14_D2_P3_Low_T2_2_Left/1.png", "Image/P14_D2_P3_Low_T2_2_Left/2.png", "Image/P14_D2_P3_Low_T2_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1883.1797 seconds to 1891.0933999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P2_Medium_T3_1_Left", "images": ["Image/P22_D2_P2_Medium_T3_1_Left/1.png", "Image/P22_D2_P2_Medium_T3_1_Left/2.png", "Image/P22_D2_P2_Medium_T3_1_Left/3.png", "Image/P22_D2_P2_Medium_T3_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 666.9843999999999 seconds to 677.3120999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P5_High_T2_1_Right", "images": ["Image/P30_D2_P5_High_T2_1_Right/1.png", "Image/P30_D2_P5_High_T2_1_Right/2.png", "Image/P30_D2_P5_High_T2_1_Right/3.png", "Image/P30_D2_P5_High_T2_1_Right/4.png", "Image/P30_D2_P5_High_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in the green base. Finish.\"\nThe frames roughly correspond to the speech interval from 184.1092 seconds to 195.8855 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P1_High_T1_6_Left", "images": ["Image/P4_D2_P1_High_T1_6_Left/1.png", "Image/P4_D2_P1_High_T1_6_Left/2.png", "Image/P4_D2_P1_High_T1_6_Left/3.png", "Image/P4_D2_P1_High_T1_6_Left/4.png", "Image/P4_D2_P1_High_T1_6_Left/5.png", "Image/P4_D2_P1_High_T1_6_Left/6.png", "Image/P4_D2_P1_High_T1_6_Left/7.png", "Image/P4_D2_P1_High_T1_6_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant in the green bottle to the ground? Finished.\"\nThe frames roughly correspond to the speech interval from 1696.6808 seconds to 1713.0888999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P4_Low_T1_4_Right", "images": ["Image/P28_D3_P4_Low_T1_4_Right/1.png", "Image/P28_D3_P4_Low_T1_4_Right/2.png", "Image/P28_D3_P4_Low_T1_4_Right/3.png", "Image/P28_D3_P4_Low_T1_4_Right/4.png", "Image/P28_D3_P4_Low_T1_4_Right/5.png", "Image/P28_D3_P4_Low_T1_4_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 82.58232000000001 seconds to 93.92716 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P6_Low_T2_21_Mid", "images": ["Image/P30_D2_P6_Low_T2_21_Mid/1.png", "Image/P30_D2_P6_Low_T2_21_Mid/2.png", "Image/P30_D2_P6_Low_T2_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 411.2825 seconds to 420.1772 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P9_High_T1_20_Left", "images": ["Image/P19_D3_P9_High_T1_20_Left/1.png", "Image/P19_D3_P9_High_T1_20_Left/2.png", "Image/P19_D3_P9_High_T1_20_Left/3.png", "Image/P19_D3_P9_High_T1_20_Left/4.png", "Image/P19_D3_P9_High_T1_20_Left/5.png", "Image/P19_D3_P9_High_T1_20_Left/6.png", "Image/P19_D3_P9_High_T1_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this container up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2187.3118999999997 seconds to 2200.3822 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P4_Low_T3_19_Mid", "images": ["Image/P17_D3_P4_Low_T3_19_Mid/1.png", "Image/P17_D3_P4_Low_T3_19_Mid/2.png", "Image/P17_D3_P4_Low_T3_19_Mid/3.png", "Image/P17_D3_P4_Low_T3_19_Mid/4.png", "Image/P17_D3_P4_Low_T3_19_Mid/5.png", "Image/P17_D3_P4_Low_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white vase on the desk over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1280.1855 seconds to 1293.2972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P5_Medium_T1_11_Right", "images": ["Image/P4_D3_P5_Medium_T1_11_Right/1.png", "Image/P4_D3_P5_Medium_T1_11_Right/2.png", "Image/P4_D3_P5_Medium_T1_11_Right/3.png", "Image/P4_D3_P5_Medium_T1_11_Right/4.png", "Image/P4_D3_P5_Medium_T1_11_Right/5.png", "Image/P4_D3_P5_Medium_T1_11_Right/6.png", "Image/P4_D3_P5_Medium_T1_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that bottle to the middle of the flag shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 887.3319999999999 seconds to 901.4458 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P6_Medium_T3_4_Left", "images": ["Image/P26_D2_P6_Medium_T3_4_Left/1.png", "Image/P26_D2_P6_Medium_T3_4_Left/2.png", "Image/P26_D2_P6_Medium_T3_4_Left/3.png", "Image/P26_D2_P6_Medium_T3_4_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the plant on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1662.5172 seconds to 1672.3057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P1_High_T3_15_Mid", "images": ["Image/P21_D2_P1_High_T3_15_Mid/1.png", "Image/P21_D2_P1_High_T3_15_Mid/2.png", "Image/P21_D2_P1_High_T3_15_Mid/3.png", "Image/P21_D2_P1_High_T3_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1391.31752 seconds to 1401.22846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P8_High_T1_12_Right", "images": ["Image/P4_D3_P8_High_T1_12_Right/1.png", "Image/P4_D3_P8_High_T1_12_Right/2.png", "Image/P4_D3_P8_High_T1_12_Right/3.png", "Image/P4_D3_P8_High_T1_12_Right/4.png", "Image/P4_D3_P8_High_T1_12_Right/5.png", "Image/P4_D3_P8_High_T1_12_Right/6.png", "Image/P4_D3_P8_High_T1_12_Right/7.png", "Image/P4_D3_P8_High_T1_12_Right/8.png", "Image/P4_D3_P8_High_T1_12_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right item next to the plants to the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 413.3578999999999 seconds to 431.1759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P3_Medium_T1_1_Mid", "images": ["Image/P15_D2_P3_Medium_T1_1_Mid/1.png", "Image/P15_D2_P3_Medium_T1_1_Mid/2.png", "Image/P15_D2_P3_Medium_T1_1_Mid/3.png", "Image/P15_D2_P3_Medium_T1_1_Mid/4.png", "Image/P15_D2_P3_Medium_T1_1_Mid/5.png", "Image/P15_D2_P3_Medium_T1_1_Mid/6.png", "Image/P15_D2_P3_Medium_T1_1_Mid/7.png", "Image/P15_D2_P3_Medium_T1_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the middle container from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3046.45 seconds to 3061.5951999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P3_High_T1_24_Left", "images": ["Image/P25_D1_P3_High_T1_24_Left/1.png", "Image/P25_D1_P3_High_T1_24_Left/2.png", "Image/P25_D1_P3_High_T1_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 318.4848 seconds to 326.8225 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P7_High_T1_25_Mid", "images": ["Image/P10_D1_P7_High_T1_25_Mid/1.png", "Image/P10_D1_P7_High_T1_25_Mid/2.png", "Image/P10_D1_P7_High_T1_25_Mid/3.png", "Image/P10_D1_P7_High_T1_25_Mid/4.png", "Image/P10_D1_P7_High_T1_25_Mid/5.png", "Image/P10_D1_P7_High_T1_25_Mid/6.png", "Image/P10_D1_P7_High_T1_25_Mid/7.png", "Image/P10_D1_P7_High_T1_25_Mid/8.png", "Image/P10_D1_P7_High_T1_25_Mid/9.png", "Image/P10_D1_P7_High_T1_25_Mid/10.png", "Image/P10_D1_P7_High_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2245.7077 seconds to 2263.0094 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P2_Medium_T3_11_Mid", "images": ["Image/P11_D3_P2_Medium_T3_11_Mid/1.png", "Image/P11_D3_P2_Medium_T3_11_Mid/2.png", "Image/P11_D3_P2_Medium_T3_11_Mid/3.png", "Image/P11_D3_P2_Medium_T3_11_Mid/4.png", "Image/P11_D3_P2_Medium_T3_11_Mid/5.png", "Image/P11_D3_P2_Medium_T3_11_Mid/6.png", "Image/P11_D3_P2_Medium_T3_11_Mid/7.png", "Image/P11_D3_P2_Medium_T3_11_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the second red bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1491.6301 seconds to 1505.2996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P1_Low_T1_12_Left", "images": ["Image/P3_D3_P1_Low_T1_12_Left/1.png", "Image/P3_D3_P1_Low_T1_12_Left/2.png", "Image/P3_D3_P1_Low_T1_12_Left/3.png", "Image/P3_D3_P1_Low_T1_12_Left/4.png", "Image/P3_D3_P1_Low_T1_12_Left/5.png", "Image/P3_D3_P1_Low_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 621.6907199999999 seconds to 633.95579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P6_Medium_T1_16_Right", "images": ["Image/P15_D1_P6_Medium_T1_16_Right/1.png", "Image/P15_D1_P6_Medium_T1_16_Right/2.png", "Image/P15_D1_P6_Medium_T1_16_Right/3.png", "Image/P15_D1_P6_Medium_T1_16_Right/4.png", "Image/P15_D1_P6_Medium_T1_16_Right/5.png", "Image/P15_D1_P6_Medium_T1_16_Right/6.png", "Image/P15_D1_P6_Medium_T1_16_Right/7.png", "Image/P15_D1_P6_Medium_T1_16_Right/8.png", "Image/P15_D1_P6_Medium_T1_16_Right/9.png", "Image/P15_D1_P6_Medium_T1_16_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the right side cup into the left of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1760.8090699999998 seconds to 1776.82937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P9_Low_T3_20_Right", "images": ["Image/P29_D3_P9_Low_T3_20_Right/1.png", "Image/P29_D3_P9_Low_T3_20_Right/2.png", "Image/P29_D3_P9_Low_T3_20_Right/3.png", "Image/P29_D3_P9_Low_T3_20_Right/4.png", "Image/P29_D3_P9_Low_T3_20_Right/5.png", "Image/P29_D3_P9_Low_T3_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1616.5521 seconds to 1628.1740999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P6_Medium_T1_2_Left", "images": ["Image/P25_D1_P6_Medium_T1_2_Left/1.png", "Image/P25_D1_P6_Medium_T1_2_Left/2.png", "Image/P25_D1_P6_Medium_T1_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this there. Finish.\"\nThe frames roughly correspond to the speech interval from 197.1719 seconds to 205.6068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P8_Low_T3_19_Right", "images": ["Image/P4_D1_P8_Low_T3_19_Right/1.png", "Image/P4_D1_P8_Low_T3_19_Right/2.png", "Image/P4_D1_P8_Low_T3_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start and clean the bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 2186.4853 seconds to 2198.1231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P4_Low_T1_19_Left", "images": ["Image/P23_D1_P4_Low_T1_19_Left/1.png", "Image/P23_D1_P4_Low_T1_19_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1542.58574 seconds to 1551.18735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P5_High_T3_26_Mid", "images": ["Image/P22_D2_P5_High_T3_26_Mid/1.png", "Image/P22_D2_P5_High_T3_26_Mid/2.png", "Image/P22_D2_P5_High_T3_26_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1001.4661 seconds to 1010.2487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P4_High_T2_15_Mid", "images": ["Image/P22_D1_P4_High_T2_15_Mid/1.png", "Image/P22_D1_P4_High_T2_15_Mid/2.png", "Image/P22_D1_P4_High_T2_15_Mid/3.png", "Image/P22_D1_P4_High_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1600.902 seconds to 1610.1027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P9_High_T2_21_Right", "images": ["Image/P27_D2_P9_High_T2_21_Right/1.png", "Image/P27_D2_P9_High_T2_21_Right/2.png", "Image/P27_D2_P9_High_T2_21_Right/3.png", "Image/P27_D2_P9_High_T2_21_Right/4.png", "Image/P27_D2_P9_High_T2_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the white vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1245.09379 seconds to 1255.50399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P1_Medium_T3_25_Right", "images": ["Image/P2_D2_P1_Medium_T3_25_Right/1.png", "Image/P2_D2_P1_Medium_T3_25_Right/2.png", "Image/P2_D2_P1_Medium_T3_25_Right/3.png", "Image/P2_D2_P1_Medium_T3_25_Right/4.png", "Image/P2_D2_P1_Medium_T3_25_Right/5.png", "Image/P2_D2_P1_Medium_T3_25_Right/6.png", "Image/P2_D2_P1_Medium_T3_25_Right/7.png", "Image/P2_D2_P1_Medium_T3_25_Right/8.png", "Image/P2_D2_P1_Medium_T3_25_Right/9.png", "Image/P2_D2_P1_Medium_T3_25_Right/10.png", "Image/P2_D2_P1_Medium_T3_25_Right/11.png", "Image/P2_D2_P1_Medium_T3_25_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please clean the mug on the very right side on top of the shelf? Yeah\"\nThe frames roughly correspond to the speech interval from 2866.6178999999997 seconds to 2884.1695999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P8_Medium_T2_27_Mid", "images": ["Image/P11_D1_P8_Medium_T2_27_Mid/1.png", "Image/P11_D1_P8_Medium_T2_27_Mid/2.png", "Image/P11_D1_P8_Medium_T2_27_Mid/3.png", "Image/P11_D1_P8_Medium_T2_27_Mid/4.png", "Image/P11_D1_P8_Medium_T2_27_Mid/5.png", "Image/P11_D1_P8_Medium_T2_27_Mid/6.png", "Image/P11_D1_P8_Medium_T2_27_Mid/7.png", "Image/P11_D1_P8_Medium_T2_27_Mid/8.png", "Image/P11_D1_P8_Medium_T2_27_Mid/9.png", "Image/P11_D1_P8_Medium_T2_27_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please pour the water in the In the white glass in the basket\"\nThe frames roughly correspond to the speech interval from 2518.6764 seconds to 2535.0303 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P9_Medium_T2_1_Right", "images": ["Image/P1_D1_P9_Medium_T2_1_Right/1.png", "Image/P1_D1_P9_Medium_T2_1_Right/2.png", "Image/P1_D1_P9_Medium_T2_1_Right/3.png", "Image/P1_D1_P9_Medium_T2_1_Right/4.png", "Image/P1_D1_P9_Medium_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 240.2939 seconds to 249.8306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P8_Medium_T1_6_Mid", "images": ["Image/P7_D2_P8_Medium_T1_6_Mid/1.png", "Image/P7_D2_P8_Medium_T1_6_Mid/2.png", "Image/P7_D2_P8_Medium_T1_6_Mid/3.png", "Image/P7_D2_P8_Medium_T1_6_Mid/4.png", "Image/P7_D2_P8_Medium_T1_6_Mid/5.png", "Image/P7_D2_P8_Medium_T1_6_Mid/6.png", "Image/P7_D2_P8_Medium_T1_6_Mid/7.png", "Image/P7_D2_P8_Medium_T1_6_Mid/8.png", "Image/P7_D2_P8_Medium_T1_6_Mid/9.png", "Image/P7_D2_P8_Medium_T1_6_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1319.9891000000002 seconds to 1338.8357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P4_Low_T3_23_Right", "images": ["Image/P22_D1_P4_Low_T3_23_Right/1.png", "Image/P22_D1_P4_Low_T3_23_Right/2.png", "Image/P22_D1_P4_Low_T3_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 1426.61082 seconds to 1436.28565 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P8_Medium_T3_25_Left", "images": ["Image/P14_D1_P8_Medium_T3_25_Left/1.png", "Image/P14_D1_P8_Medium_T3_25_Left/2.png", "Image/P14_D1_P8_Medium_T3_25_Left/3.png", "Image/P14_D1_P8_Medium_T3_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that mug up. Finish.\"\nThe frames roughly correspond to the speech interval from 330.2222 seconds to 339.5212 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P8_Low_T1_9_Mid", "images": ["Image/P11_D1_P8_Low_T1_9_Mid/1.png", "Image/P11_D1_P8_Low_T1_9_Mid/2.png", "Image/P11_D1_P8_Low_T1_9_Mid/3.png", "Image/P11_D1_P8_Low_T1_9_Mid/4.png", "Image/P11_D1_P8_Low_T1_9_Mid/5.png", "Image/P11_D1_P8_Low_T1_9_Mid/6.png", "Image/P11_D1_P8_Low_T1_9_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green glass in the basket on the floor to the top of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2870.7933999999996 seconds to 2882.9862999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P4_Low_T1_22_Left", "images": ["Image/P1_D2_P4_Low_T1_22_Left/1.png", "Image/P1_D2_P4_Low_T1_22_Left/2.png", "Image/P1_D2_P4_Low_T1_22_Left/3.png", "Image/P1_D2_P4_Low_T1_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1041.4896 seconds to 1050.6513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P8_Low_T1_23_Mid", "images": ["Image/P23_D1_P8_Low_T1_23_Mid/1.png", "Image/P23_D1_P8_Low_T1_23_Mid/2.png", "Image/P23_D1_P8_Low_T1_23_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1719.814 seconds to 1729.0144 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P7_Low_T3_13_Mid", "images": ["Image/P17_D3_P7_Low_T3_13_Mid/1.png", "Image/P17_D3_P7_Low_T3_13_Mid/2.png", "Image/P17_D3_P7_Low_T3_13_Mid/3.png", "Image/P17_D3_P7_Low_T3_13_Mid/4.png", "Image/P17_D3_P7_Low_T3_13_Mid/5.png", "Image/P17_D3_P7_Low_T3_13_Mid/6.png", "Image/P17_D3_P7_Low_T3_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that plant on the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1259.7244 seconds to 1272.5146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P2_Low_T1_1_Mid", "images": ["Image/P15_D2_P2_Low_T1_1_Mid/1.png", "Image/P15_D2_P2_Low_T1_1_Mid/2.png", "Image/P15_D2_P2_Low_T1_1_Mid/3.png", "Image/P15_D2_P2_Low_T1_1_Mid/4.png", "Image/P15_D2_P2_Low_T1_1_Mid/5.png", "Image/P15_D2_P2_Low_T1_1_Mid/6.png", "Image/P15_D2_P2_Low_T1_1_Mid/7.png", "Image/P15_D2_P2_Low_T1_1_Mid/8.png", "Image/P15_D2_P2_Low_T1_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green container from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2899.4843 seconds to 2913.7785 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P1_High_T1_19_Mid", "images": ["Image/P29_D2_P1_High_T1_19_Mid/1.png", "Image/P29_D2_P1_High_T1_19_Mid/2.png", "Image/P29_D2_P1_High_T1_19_Mid/3.png", "Image/P29_D2_P1_High_T1_19_Mid/4.png", "Image/P29_D2_P1_High_T1_19_Mid/5.png", "Image/P29_D2_P1_High_T1_19_Mid/6.png", "Image/P29_D2_P1_High_T1_19_Mid/7.png", "Image/P29_D2_P1_High_T1_19_Mid/8.png", "Image/P29_D2_P1_High_T1_19_Mid/9.png", "Image/P29_D2_P1_High_T1_19_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 552.7367 seconds to 568.1323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P5_Medium_T3_11_Mid", "images": ["Image/P29_D3_P5_Medium_T3_11_Mid/1.png", "Image/P29_D3_P5_Medium_T3_11_Mid/2.png", "Image/P29_D3_P5_Medium_T3_11_Mid/3.png", "Image/P29_D3_P5_Medium_T3_11_Mid/4.png", "Image/P29_D3_P5_Medium_T3_11_Mid/5.png", "Image/P29_D3_P5_Medium_T3_11_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1457.4331999999995 seconds to 1468.8195999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P1_Medium_T1_3_Left", "images": ["Image/P19_D3_P1_Medium_T1_3_Left/1.png", "Image/P19_D3_P1_Medium_T1_3_Left/2.png", "Image/P19_D3_P1_Medium_T1_3_Left/3.png", "Image/P19_D3_P1_Medium_T1_3_Left/4.png", "Image/P19_D3_P1_Medium_T1_3_Left/5.png", "Image/P19_D3_P1_Medium_T1_3_Left/6.png", "Image/P19_D3_P1_Medium_T1_3_Left/7.png", "Image/P19_D3_P1_Medium_T1_3_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase to the floor right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2469.8727 seconds to 2484.3354999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P2_High_T1_1_Left", "images": ["Image/P17_D1_P2_High_T1_1_Left/1.png", "Image/P17_D1_P2_High_T1_1_Left/2.png", "Image/P17_D1_P2_High_T1_1_Left/3.png", "Image/P17_D1_P2_High_T1_1_Left/4.png", "Image/P17_D1_P2_High_T1_1_Left/5.png", "Image/P17_D1_P2_High_T1_1_Left/6.png", "Image/P17_D1_P2_High_T1_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green vase to the basket closest to the wall? Finish\"\nThe frames roughly correspond to the speech interval from 1604.6721000000002 seconds to 1617.7962000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P9_Medium_T1_8_Right", "images": ["Image/P7_D3_P9_Medium_T1_8_Right/1.png", "Image/P7_D3_P9_Medium_T1_8_Right/2.png", "Image/P7_D3_P9_Medium_T1_8_Right/3.png", "Image/P7_D3_P9_Medium_T1_8_Right/4.png", "Image/P7_D3_P9_Medium_T1_8_Right/5.png", "Image/P7_D3_P9_Medium_T1_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1613.1081600000002 seconds to 1625.84541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P5_Low_T2_17_Left", "images": ["Image/P10_D2_P5_Low_T2_17_Left/1.png", "Image/P10_D2_P5_Low_T2_17_Left/2.png", "Image/P10_D2_P5_Low_T2_17_Left/3.png", "Image/P10_D2_P5_Low_T2_17_Left/4.png", "Image/P10_D2_P5_Low_T2_17_Left/5.png", "Image/P10_D2_P5_Low_T2_17_Left/6.png", "Image/P10_D2_P5_Low_T2_17_Left/7.png", "Image/P10_D2_P5_Low_T2_17_Left/8.png", "Image/P10_D2_P5_Low_T2_17_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1506.6522800000002 seconds to 1524.0248800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P5_Medium_T1_19_Left", "images": ["Image/P8_D2_P5_Medium_T1_19_Left/1.png", "Image/P8_D2_P5_Medium_T1_19_Left/2.png", "Image/P8_D2_P5_Medium_T1_19_Left/3.png", "Image/P8_D2_P5_Medium_T1_19_Left/4.png", "Image/P8_D2_P5_Medium_T1_19_Left/5.png", "Image/P8_D2_P5_Medium_T1_19_Left/6.png", "Image/P8_D2_P5_Medium_T1_19_Left/7.png", "Image/P8_D2_P5_Medium_T1_19_Left/8.png", "Image/P8_D2_P5_Medium_T1_19_Left/9.png", "Image/P8_D2_P5_Medium_T1_19_Left/10.png", "Image/P8_D2_P5_Medium_T1_19_Left/11.png", "Image/P8_D2_P5_Medium_T1_19_Left/12.png", "Image/P8_D2_P5_Medium_T1_19_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the left bottle here? Just on the right shelf. I mean the lower one. Just on it. Finish.\"\nThe frames roughly correspond to the speech interval from 2463.5957000000003 seconds to 2482.9283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P4_Low_T3_20_Left", "images": ["Image/P13_D2_P4_Low_T3_20_Left/1.png", "Image/P13_D2_P4_Low_T3_20_Left/2.png", "Image/P13_D2_P4_Low_T3_20_Left/3.png", "Image/P13_D2_P4_Low_T3_20_Left/4.png", "Image/P13_D2_P4_Low_T3_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the white bottle over there\"\nThe frames roughly correspond to the speech interval from 1096.10246 seconds to 1106.31956 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P1_Low_T2_23_Mid", "images": ["Image/P26_D2_P1_Low_T2_23_Mid/1.png", "Image/P26_D2_P1_Low_T2_23_Mid/2.png", "Image/P26_D2_P1_Low_T2_23_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant over there? Finish\"\nThe frames roughly correspond to the speech interval from 1328.4132 seconds to 1338.6015 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P5_Low_T2_18_Mid", "images": ["Image/P28_D3_P5_Low_T2_18_Mid/1.png", "Image/P28_D3_P5_Low_T2_18_Mid/2.png", "Image/P28_D3_P5_Low_T2_18_Mid/3.png", "Image/P28_D3_P5_Low_T2_18_Mid/4.png", "Image/P28_D3_P5_Low_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish\"\nThe frames roughly correspond to the speech interval from 508.6707 seconds to 518.7209 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P5_Medium_T1_14_Mid", "images": ["Image/P18_D3_P5_Medium_T1_14_Mid/1.png", "Image/P18_D3_P5_Medium_T1_14_Mid/2.png", "Image/P18_D3_P5_Medium_T1_14_Mid/3.png", "Image/P18_D3_P5_Medium_T1_14_Mid/4.png", "Image/P18_D3_P5_Medium_T1_14_Mid/5.png", "Image/P18_D3_P5_Medium_T1_14_Mid/6.png", "Image/P18_D3_P5_Medium_T1_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that middle plant over here to the second level from the top. Finish.\"\nThe frames roughly correspond to the speech interval from 1601.069 seconds to 1613.8895 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P3_Low_T1_1_Right", "images": ["Image/P8_D2_P3_Low_T1_1_Right/1.png", "Image/P8_D2_P3_Low_T1_1_Right/2.png", "Image/P8_D2_P3_Low_T1_1_Right/3.png", "Image/P8_D2_P3_Low_T1_1_Right/4.png", "Image/P8_D2_P3_Low_T1_1_Right/5.png", "Image/P8_D2_P3_Low_T1_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move that bottle here? There. Never mind. Finish.\"\nThe frames roughly correspond to the speech interval from 2351.9289 seconds to 2364.1362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P3_Low_T3_22_Mid", "images": ["Image/P6_D2_P3_Low_T3_22_Mid/1.png", "Image/P6_D2_P3_Low_T3_22_Mid/2.png", "Image/P6_D2_P3_Low_T3_22_Mid/3.png", "Image/P6_D2_P3_Low_T3_22_Mid/4.png", "Image/P6_D2_P3_Low_T3_22_Mid/5.png", "Image/P6_D2_P3_Low_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Start. Hi Friday\"\nThe frames roughly correspond to the speech interval from 1143.8955999999998 seconds to 1168.4529 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P2_Medium_T2_12_Left", "images": ["Image/P14_D2_P2_Medium_T2_12_Left/1.png", "Image/P14_D2_P2_Medium_T2_12_Left/2.png", "Image/P14_D2_P2_Medium_T2_12_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1601.9635999999998 seconds to 1610.448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P9_High_T3_25_Left", "images": ["Image/P16_D1_P9_High_T3_25_Left/1.png", "Image/P16_D1_P9_High_T3_25_Left/2.png", "Image/P16_D1_P9_High_T3_25_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish\"\nThe frames roughly correspond to the speech interval from 1451.6782 seconds to 1460.6988 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P9_Low_T1_21_Left", "images": ["Image/P27_D2_P9_Low_T1_21_Left/1.png", "Image/P27_D2_P9_Low_T1_21_Left/2.png", "Image/P27_D2_P9_Low_T1_21_Left/3.png", "Image/P27_D2_P9_Low_T1_21_Left/4.png", "Image/P27_D2_P9_Low_T1_21_Left/5.png", "Image/P27_D2_P9_Low_T1_21_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take that vase and put it on top of that cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 1634.336 seconds to 1645.6248 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P2_Low_T3_5_Right", "images": ["Image/P20_D2_P2_Low_T3_5_Right/1.png", "Image/P20_D2_P2_Low_T3_5_Right/2.png", "Image/P20_D2_P2_Low_T3_5_Right/3.png", "Image/P20_D2_P2_Low_T3_5_Right/4.png", "Image/P20_D2_P2_Low_T3_5_Right/5.png", "Image/P20_D2_P2_Low_T3_5_Right/6.png", "Image/P20_D2_P2_Low_T3_5_Right/7.png", "Image/P20_D2_P2_Low_T3_5_Right/8.png", "Image/P20_D2_P2_Low_T3_5_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that plant on the top of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 3128.1119000000003 seconds to 3143.8427 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P1_Medium_T2_5_Mid", "images": ["Image/P26_D2_P1_Medium_T2_5_Mid/1.png", "Image/P26_D2_P1_Medium_T2_5_Mid/2.png", "Image/P26_D2_P1_Medium_T2_5_Mid/3.png", "Image/P26_D2_P1_Medium_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the middle plant please? Finish.\"\nThe frames roughly correspond to the speech interval from 1220.56272 seconds to 1234.3419199999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P4_High_T1_3_Right", "images": ["Image/P5_D3_P4_High_T1_3_Right/1.png", "Image/P5_D3_P4_High_T1_3_Right/2.png", "Image/P5_D3_P4_High_T1_3_Right/3.png", "Image/P5_D3_P4_High_T1_3_Right/4.png", "Image/P5_D3_P4_High_T1_3_Right/5.png", "Image/P5_D3_P4_High_T1_3_Right/6.png", "Image/P5_D3_P4_High_T1_3_Right/7.png", "Image/P5_D3_P4_High_T1_3_Right/8.png", "Image/P5_D3_P4_High_T1_3_Right/9.png", "Image/P5_D3_P4_High_T1_3_Right/10.png", "Image/P5_D3_P4_High_T1_3_Right/11.png", "Image/P5_D3_P4_High_T1_3_Right/12.png", "Image/P5_D3_P4_High_T1_3_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottle on the table to the shelf in front of me. To the second level of the shelf in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 2175.9484199999997 seconds to 2196.00556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P9_High_T1_6_Left", "images": ["Image/P6_D1_P9_High_T1_6_Left/1.png", "Image/P6_D1_P9_High_T1_6_Left/2.png", "Image/P6_D1_P9_High_T1_6_Left/3.png", "Image/P6_D1_P9_High_T1_6_Left/4.png", "Image/P6_D1_P9_High_T1_6_Left/5.png", "Image/P6_D1_P9_High_T1_6_Left/6.png", "Image/P6_D1_P9_High_T1_6_Left/7.png", "Image/P6_D1_P9_High_T1_6_Left/8.png", "Image/P6_D1_P9_High_T1_6_Left/9.png", "Image/P6_D1_P9_High_T1_6_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2584.208 seconds to 2600.1159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P2_Medium_T2_13_Right", "images": ["Image/P10_D3_P2_Medium_T2_13_Right/1.png", "Image/P10_D3_P2_Medium_T2_13_Right/2.png", "Image/P10_D3_P2_Medium_T2_13_Right/3.png", "Image/P10_D3_P2_Medium_T2_13_Right/4.png", "Image/P10_D3_P2_Medium_T2_13_Right/5.png", "Image/P10_D3_P2_Medium_T2_13_Right/6.png", "Image/P10_D3_P2_Medium_T2_13_Right/7.png", "Image/P10_D3_P2_Medium_T2_13_Right/8.png", "Image/P10_D3_P2_Medium_T2_13_Right/9.png", "Image/P10_D3_P2_Medium_T2_13_Right/10.png", "Image/P10_D3_P2_Medium_T2_13_Right/11.png", "Image/P10_D3_P2_Medium_T2_13_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 297.34157999999996 seconds to 315.77115 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P5_Low_T1_24_Mid", "images": ["Image/P17_D1_P5_Low_T1_24_Mid/1.png", "Image/P17_D1_P5_Low_T1_24_Mid/2.png", "Image/P17_D1_P5_Low_T1_24_Mid/3.png", "Image/P17_D1_P5_Low_T1_24_Mid/4.png", "Image/P17_D1_P5_Low_T1_24_Mid/5.png", "Image/P17_D1_P5_Low_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant to the spot on the table? Finish\"\nThe frames roughly correspond to the speech interval from 1518.01816 seconds to 1530.99131 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P4_Medium_T2_16_Left", "images": ["Image/P3_D2_P4_Medium_T2_16_Left/1.png", "Image/P3_D2_P4_Medium_T2_16_Left/2.png", "Image/P3_D2_P4_Medium_T2_16_Left/3.png", "Image/P3_D2_P4_Medium_T2_16_Left/4.png", "Image/P3_D2_P4_Medium_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the item to this space? Finish.\"\nThe frames roughly correspond to the speech interval from 2340.1244 seconds to 2350.6203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P6_Low_T3_22_Right", "images": ["Image/P21_D2_P6_Low_T3_22_Right/1.png", "Image/P21_D2_P6_Low_T3_22_Right/2.png", "Image/P21_D2_P6_Low_T3_22_Right/3.png", "Image/P21_D2_P6_Low_T3_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1758.4554 seconds to 1773.7949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P6_High_T3_1_Mid", "images": ["Image/P6_D2_P6_High_T3_1_Mid/1.png", "Image/P6_D2_P6_High_T3_1_Mid/2.png", "Image/P6_D2_P6_High_T3_1_Mid/3.png", "Image/P6_D2_P6_High_T3_1_Mid/4.png", "Image/P6_D2_P6_High_T3_1_Mid/5.png", "Image/P6_D2_P6_High_T3_1_Mid/6.png", "Image/P6_D2_P6_High_T3_1_Mid/7.png", "Image/P6_D2_P6_High_T3_1_Mid/8.png", "Image/P6_D2_P6_High_T3_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 719.5655999999999 seconds to 735.6868999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P6_High_T3_21_Mid", "images": ["Image/P30_D3_P6_High_T3_21_Mid/1.png", "Image/P30_D3_P6_High_T3_21_Mid/2.png", "Image/P30_D3_P6_High_T3_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1620.1230999999998 seconds to 1628.1117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P8_Medium_T1_17_Left", "images": ["Image/P11_D2_P8_Medium_T1_17_Left/1.png", "Image/P11_D2_P8_Medium_T1_17_Left/2.png", "Image/P11_D2_P8_Medium_T1_17_Left/3.png", "Image/P11_D2_P8_Medium_T1_17_Left/4.png", "Image/P11_D2_P8_Medium_T1_17_Left/5.png", "Image/P11_D2_P8_Medium_T1_17_Left/6.png", "Image/P11_D2_P8_Medium_T1_17_Left/7.png", "Image/P11_D2_P8_Medium_T1_17_Left/8.png", "Image/P11_D2_P8_Medium_T1_17_Left/9.png", "Image/P11_D2_P8_Medium_T1_17_Left/10.png", "Image/P11_D2_P8_Medium_T1_17_Left/11.png", "Image/P11_D2_P8_Medium_T1_17_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the first red glass to the shelf on the wall in front of me. Please finish.\"\nThe frames roughly correspond to the speech interval from 756.4135 seconds to 777.9787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P1_High_T3_7_Mid", "images": ["Image/P7_D2_P1_High_T3_7_Mid/1.png", "Image/P7_D2_P1_High_T3_7_Mid/2.png", "Image/P7_D2_P1_High_T3_7_Mid/3.png", "Image/P7_D2_P1_High_T3_7_Mid/4.png", "Image/P7_D2_P1_High_T3_7_Mid/5.png", "Image/P7_D2_P1_High_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1491.2588 seconds to 1504.3715000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P5_Medium_T2_14_Mid", "images": ["Image/P7_D3_P5_Medium_T2_14_Mid/1.png", "Image/P7_D3_P5_Medium_T2_14_Mid/2.png", "Image/P7_D3_P5_Medium_T2_14_Mid/3.png", "Image/P7_D3_P5_Medium_T2_14_Mid/4.png", "Image/P7_D3_P5_Medium_T2_14_Mid/5.png", "Image/P7_D3_P5_Medium_T2_14_Mid/6.png", "Image/P7_D3_P5_Medium_T2_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1841.0196 seconds to 1853.7837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P7_Low_T3_16_Left", "images": ["Image/P24_D3_P7_Low_T3_16_Left/1.png", "Image/P24_D3_P7_Low_T3_16_Left/2.png", "Image/P24_D3_P7_Low_T3_16_Left/3.png", "Image/P24_D3_P7_Low_T3_16_Left/4.png", "Image/P24_D3_P7_Low_T3_16_Left/5.png", "Image/P24_D3_P7_Low_T3_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1776.1568 seconds to 1787.2643 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P1_Medium_T1_13_Mid", "images": ["Image/P30_D2_P1_Medium_T1_13_Mid/1.png", "Image/P30_D2_P1_Medium_T1_13_Mid/2.png", "Image/P30_D2_P1_Medium_T1_13_Mid/3.png", "Image/P30_D2_P1_Medium_T1_13_Mid/4.png", "Image/P30_D2_P1_Medium_T1_13_Mid/5.png", "Image/P30_D2_P1_Medium_T1_13_Mid/6.png", "Image/P30_D2_P1_Medium_T1_13_Mid/7.png", "Image/P30_D2_P1_Medium_T1_13_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the middle plant on the top shelf to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 60.18126000000001 seconds to 75.42291000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P7_High_T2_10_Left", "images": ["Image/P2_D3_P7_High_T2_10_Left/1.png", "Image/P2_D3_P7_High_T2_10_Left/2.png", "Image/P2_D3_P7_High_T2_10_Left/3.png", "Image/P2_D3_P7_High_T2_10_Left/4.png", "Image/P2_D3_P7_High_T2_10_Left/5.png", "Image/P2_D3_P7_High_T2_10_Left/6.png", "Image/P2_D3_P7_High_T2_10_Left/7.png", "Image/P2_D3_P7_High_T2_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that brown bottle? Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 2054.2478 seconds to 2067.6918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P1_Low_T3_17_Mid", "images": ["Image/P25_D3_P1_Low_T3_17_Mid/1.png", "Image/P25_D3_P1_Low_T3_17_Mid/2.png", "Image/P25_D3_P1_Low_T3_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1869.7672 seconds to 1877.6888 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P7_High_T2_26_Left", "images": ["Image/P3_D1_P7_High_T2_26_Left/1.png", "Image/P3_D1_P7_High_T2_26_Left/2.png", "Image/P3_D1_P7_High_T2_26_Left/3.png", "Image/P3_D1_P7_High_T2_26_Left/4.png", "Image/P3_D1_P7_High_T2_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1694.7295 seconds to 1705.366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P1_Low_T3_22_Mid", "images": ["Image/P5_D3_P1_Low_T3_22_Mid/1.png", "Image/P5_D3_P1_Low_T3_22_Mid/2.png", "Image/P5_D3_P1_Low_T3_22_Mid/3.png", "Image/P5_D3_P1_Low_T3_22_Mid/4.png", "Image/P5_D3_P1_Low_T3_22_Mid/5.png", "Image/P5_D3_P1_Low_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant on the table over there. Finished.\"\nThe frames roughly correspond to the speech interval from 2565.9001 seconds to 2577.5773999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P7_Low_T3_9_Mid", "images": ["Image/P22_D2_P7_Low_T3_9_Mid/1.png", "Image/P22_D2_P7_Low_T3_9_Mid/2.png", "Image/P22_D2_P7_Low_T3_9_Mid/3.png", "Image/P22_D2_P7_Low_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 844.5038999999999 seconds to 854.0671 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P2_Medium_T3_10_Left", "images": ["Image/P13_D2_P2_Medium_T3_10_Left/1.png", "Image/P13_D2_P2_Medium_T3_10_Left/2.png", "Image/P13_D2_P2_Medium_T3_10_Left/3.png", "Image/P13_D2_P2_Medium_T3_10_Left/4.png", "Image/P13_D2_P2_Medium_T3_10_Left/5.png", "Image/P13_D2_P2_Medium_T3_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Friday\"\nThe frames roughly correspond to the speech interval from 1022.28526 seconds to 1032.96226 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P8_Medium_T2_12_Right", "images": ["Image/P29_D3_P8_Medium_T2_12_Right/1.png", "Image/P29_D3_P8_Medium_T2_12_Right/2.png", "Image/P29_D3_P8_Medium_T2_12_Right/3.png", "Image/P29_D3_P8_Medium_T2_12_Right/4.png", "Image/P29_D3_P8_Medium_T2_12_Right/5.png", "Image/P29_D3_P8_Medium_T2_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1404.5372999999995 seconds to 1415.8970999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P8_Medium_T2_23_Right", "images": ["Image/P14_D3_P8_Medium_T2_23_Right/1.png", "Image/P14_D3_P8_Medium_T2_23_Right/2.png", "Image/P14_D3_P8_Medium_T2_23_Right/3.png", "Image/P14_D3_P8_Medium_T2_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1178.65306 seconds to 1187.43056 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P5_High_T3_21_Mid", "images": ["Image/P13_D3_P5_High_T3_21_Mid/1.png", "Image/P13_D3_P5_High_T3_21_Mid/2.png", "Image/P13_D3_P5_High_T3_21_Mid/3.png", "Image/P13_D3_P5_High_T3_21_Mid/4.png", "Image/P13_D3_P5_High_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the white vase in the middle\"\nThe frames roughly correspond to the speech interval from 1767.4705 seconds to 1778.2436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P4_Medium_T2_24_Right", "images": ["Image/P2_D3_P4_Medium_T2_24_Right/1.png", "Image/P2_D3_P4_Medium_T2_24_Right/2.png", "Image/P2_D3_P4_Medium_T2_24_Right/3.png", "Image/P2_D3_P4_Medium_T2_24_Right/4.png", "Image/P2_D3_P4_Medium_T2_24_Right/5.png", "Image/P2_D3_P4_Medium_T2_24_Right/6.png", "Image/P2_D3_P4_Medium_T2_24_Right/7.png", "Image/P2_D3_P4_Medium_T2_24_Right/8.png", "Image/P2_D3_P4_Medium_T2_24_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please water the plant on the very right side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1651.0149700000002 seconds to 1678.5356400000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P2_High_T3_20_Left", "images": ["Image/P2_D3_P2_High_T3_20_Left/1.png", "Image/P2_D3_P2_High_T3_20_Left/2.png", "Image/P2_D3_P2_High_T3_20_Left/3.png", "Image/P2_D3_P2_High_T3_20_Left/4.png", "Image/P2_D3_P2_High_T3_20_Left/5.png", "Image/P2_D3_P2_High_T3_20_Left/6.png", "Image/P2_D3_P2_High_T3_20_Left/7.png", "Image/P2_D3_P2_High_T3_20_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 1986.0552 seconds to 1999.5675 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P3_High_T3_9_Right", "images": ["Image/P18_D1_P3_High_T3_9_Right/1.png", "Image/P18_D1_P3_High_T3_9_Right/2.png", "Image/P18_D1_P3_High_T3_9_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1046.2435 seconds to 1055.3887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P8_Low_T3_8_Mid", "images": ["Image/P2_D1_P8_Low_T3_8_Mid/1.png", "Image/P2_D1_P8_Low_T3_8_Mid/2.png", "Image/P2_D1_P8_Low_T3_8_Mid/3.png", "Image/P2_D1_P8_Low_T3_8_Mid/4.png", "Image/P2_D1_P8_Low_T3_8_Mid/5.png", "Image/P2_D1_P8_Low_T3_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 338.98723999999993 seconds to 353.76886999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P3_Medium_T2_27_Mid", "images": ["Image/P23_D1_P3_Medium_T2_27_Mid/1.png", "Image/P23_D1_P3_Medium_T2_27_Mid/2.png", "Image/P23_D1_P3_Medium_T2_27_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start pour water into the middle one\"\nThe frames roughly correspond to the speech interval from 1751.5652 seconds to 1761.3092 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P7_High_T1_8_Mid", "images": ["Image/P27_D1_P7_High_T1_8_Mid/1.png", "Image/P27_D1_P7_High_T1_8_Mid/2.png", "Image/P27_D1_P7_High_T1_8_Mid/3.png", "Image/P27_D1_P7_High_T1_8_Mid/4.png", "Image/P27_D1_P7_High_T1_8_Mid/5.png", "Image/P27_D1_P7_High_T1_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that cup onto this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 741.8986600000001 seconds to 752.9098200000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P6_Low_T3_25_Mid", "images": ["Image/P21_D3_P6_Low_T3_25_Mid/1.png", "Image/P21_D3_P6_Low_T3_25_Mid/2.png", "Image/P21_D3_P6_Low_T3_25_Mid/3.png", "Image/P21_D3_P6_Low_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 330.3642 seconds to 340.3026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P2_Medium_T1_21_Left", "images": ["Image/P25_D1_P2_Medium_T1_21_Left/1.png", "Image/P25_D1_P2_Medium_T1_21_Left/2.png", "Image/P25_D1_P2_Medium_T1_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down. Finish.\"\nThe frames roughly correspond to the speech interval from 495.1426 seconds to 503.2429 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P5_High_T2_1_Right", "images": ["Image/P1_D3_P5_High_T2_1_Right/1.png", "Image/P1_D3_P5_High_T2_1_Right/2.png", "Image/P1_D3_P5_High_T2_1_Right/3.png", "Image/P1_D3_P5_High_T2_1_Right/4.png", "Image/P1_D3_P5_High_T2_1_Right/5.png", "Image/P1_D3_P5_High_T2_1_Right/6.png", "Image/P1_D3_P5_High_T2_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you watering the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1660.3628 seconds to 1673.2124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P9_Medium_T2_20_Right", "images": ["Image/P14_D1_P9_Medium_T2_20_Right/1.png", "Image/P14_D1_P9_Medium_T2_20_Right/2.png", "Image/P14_D1_P9_Medium_T2_20_Right/3.png", "Image/P14_D1_P9_Medium_T2_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 446.2458 seconds to 455.8782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P9_Medium_T3_14_Left", "images": ["Image/P5_D2_P9_Medium_T3_14_Left/1.png", "Image/P5_D2_P9_Medium_T3_14_Left/2.png", "Image/P5_D2_P9_Medium_T3_14_Left/3.png", "Image/P5_D2_P9_Medium_T3_14_Left/4.png", "Image/P5_D2_P9_Medium_T3_14_Left/5.png", "Image/P5_D2_P9_Medium_T3_14_Left/6.png", "Image/P5_D2_P9_Medium_T3_14_Left/7.png", "Image/P5_D2_P9_Medium_T3_14_Left/8.png", "Image/P5_D2_P9_Medium_T3_14_Left/9.png", "Image/P5_D2_P9_Medium_T3_14_Left/10.png", "Image/P5_D2_P9_Medium_T3_14_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are three plants to the right side of me and clean the plant on the leftmost finish.\"\nThe frames roughly correspond to the speech interval from 476.7928 seconds to 495.5561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P6_Low_T3_20_Left", "images": ["Image/P6_D2_P6_Low_T3_20_Left/1.png", "Image/P6_D2_P6_Low_T3_20_Left/2.png", "Image/P6_D2_P6_Low_T3_20_Left/3.png", "Image/P6_D2_P6_Low_T3_20_Left/4.png", "Image/P6_D2_P6_Low_T3_20_Left/5.png", "Image/P6_D2_P6_Low_T3_20_Left/6.png", "Image/P6_D2_P6_Low_T3_20_Left/7.png", "Image/P6_D2_P6_Low_T3_20_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hi Friday\"\nThe frames roughly correspond to the speech interval from 771.8291999999999 seconds to 785.4176 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P3_Low_T2_10_Mid", "images": ["Image/P25_D1_P3_Low_T2_10_Mid/1.png", "Image/P25_D1_P3_Low_T2_10_Mid/2.png", "Image/P25_D1_P3_Low_T2_10_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 213.9472 seconds to 222.4647 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P5_High_T1_13_Right", "images": ["Image/P24_D3_P5_High_T1_13_Right/1.png", "Image/P24_D3_P5_High_T1_13_Right/2.png", "Image/P24_D3_P5_High_T1_13_Right/3.png", "Image/P24_D3_P5_High_T1_13_Right/4.png", "Image/P24_D3_P5_High_T1_13_Right/5.png", "Image/P24_D3_P5_High_T1_13_Right/6.png", "Image/P24_D3_P5_High_T1_13_Right/7.png", "Image/P24_D3_P5_High_T1_13_Right/8.png", "Image/P24_D3_P5_High_T1_13_Right/9.png", "Image/P24_D3_P5_High_T1_13_Right/10.png", "Image/P24_D3_P5_High_T1_13_Right/11.png", "Image/P24_D3_P5_High_T1_13_Right/12.png", "Image/P24_D3_P5_High_T1_13_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1595.54646 seconds to 1608.3801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P1_Low_T2_7_Right", "images": ["Image/P1_D1_P1_Low_T2_7_Right/1.png", "Image/P1_D1_P1_Low_T2_7_Right/2.png", "Image/P1_D1_P1_Low_T2_7_Right/3.png", "Image/P1_D1_P1_Low_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 222.7513 seconds to 231.3416 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P1_Low_T1_6_Left", "images": ["Image/P21_D2_P1_Low_T1_6_Left/1.png", "Image/P21_D2_P1_Low_T1_6_Left/2.png", "Image/P21_D2_P1_Low_T1_6_Left/3.png", "Image/P21_D2_P1_Low_T1_6_Left/4.png", "Image/P21_D2_P1_Low_T1_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1602.3845 seconds to 1614.3010000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P8_Low_T1_17_Right", "images": ["Image/P30_D3_P8_Low_T1_17_Right/1.png", "Image/P30_D3_P8_Low_T1_17_Right/2.png", "Image/P30_D3_P8_Low_T1_17_Right/3.png", "Image/P30_D3_P8_Low_T1_17_Right/4.png", "Image/P30_D3_P8_Low_T1_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1493.9944 seconds to 1505.4081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P1_Medium_T3_12_Right", "images": ["Image/P21_D3_P1_Medium_T3_12_Right/1.png", "Image/P21_D3_P1_Medium_T3_12_Right/2.png", "Image/P21_D3_P1_Medium_T3_12_Right/3.png", "Image/P21_D3_P1_Medium_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 460.2503 seconds to 470.9131 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P2_Low_T2_11_Right", "images": ["Image/P5_D3_P2_Low_T2_11_Right/1.png", "Image/P5_D3_P2_Low_T2_11_Right/2.png", "Image/P5_D3_P2_Low_T2_11_Right/3.png", "Image/P5_D3_P2_Low_T2_11_Right/4.png", "Image/P5_D3_P2_Low_T2_11_Right/5.png", "Image/P5_D3_P2_Low_T2_11_Right/6.png", "Image/P5_D3_P2_Low_T2_11_Right/7.png", "Image/P5_D3_P2_Low_T2_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour water into the Next\"\nThe frames roughly correspond to the speech interval from 2717.4595 seconds to 2733.9404999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P1_Low_T2_22_Right", "images": ["Image/P8_D1_P1_Low_T2_22_Right/1.png", "Image/P8_D1_P1_Low_T2_22_Right/2.png", "Image/P8_D1_P1_Low_T2_22_Right/3.png", "Image/P8_D1_P1_Low_T2_22_Right/4.png", "Image/P8_D1_P1_Low_T2_22_Right/5.png", "Image/P8_D1_P1_Low_T2_22_Right/6.png", "Image/P8_D1_P1_Low_T2_22_Right/7.png", "Image/P8_D1_P1_Low_T2_22_Right/8.png", "Image/P8_D1_P1_Low_T2_22_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 171.82389999999992 seconds to 187.79279999999991 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P9_High_T2_19_Left", "images": ["Image/P23_D3_P9_High_T2_19_Left/1.png", "Image/P23_D3_P9_High_T2_19_Left/2.png", "Image/P23_D3_P9_High_T2_19_Left/3.png", "Image/P23_D3_P9_High_T2_19_Left/4.png", "Image/P23_D3_P9_High_T2_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water into the white waste. Finish.\"\nThe frames roughly correspond to the speech interval from 1028.3181 seconds to 1039.5648 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P2_Low_T3_5_Mid", "images": ["Image/P21_D1_P2_Low_T3_5_Mid/1.png", "Image/P21_D1_P2_Low_T3_5_Mid/2.png", "Image/P21_D1_P2_Low_T3_5_Mid/3.png", "Image/P21_D1_P2_Low_T3_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1229.7025 seconds to 1240.4749 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P5_High_T1_17_Right", "images": ["Image/P7_D3_P5_High_T1_17_Right/1.png", "Image/P7_D3_P5_High_T1_17_Right/2.png", "Image/P7_D3_P5_High_T1_17_Right/3.png", "Image/P7_D3_P5_High_T1_17_Right/4.png", "Image/P7_D3_P5_High_T1_17_Right/5.png", "Image/P7_D3_P5_High_T1_17_Right/6.png", "Image/P7_D3_P5_High_T1_17_Right/7.png", "Image/P7_D3_P5_High_T1_17_Right/8.png", "Image/P7_D3_P5_High_T1_17_Right/9.png", "Image/P7_D3_P5_High_T1_17_Right/10.png", "Image/P7_D3_P5_High_T1_17_Right/11.png", "Image/P7_D3_P5_High_T1_17_Right/12.png", "Image/P7_D3_P5_High_T1_17_Right/13.png", "Image/P7_D3_P5_High_T1_17_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Before they move the brown mug to the second level of the shelf\"\nThe frames roughly correspond to the speech interval from 1769.6521 seconds to 1789.947 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P3_Low_T1_7_Mid", "images": ["Image/P3_D3_P3_Low_T1_7_Mid/1.png", "Image/P3_D3_P3_Low_T1_7_Mid/2.png", "Image/P3_D3_P3_Low_T1_7_Mid/3.png", "Image/P3_D3_P3_Low_T1_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1013.8322 seconds to 1025.0818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P7_High_T3_16_Left", "images": ["Image/P17_D1_P7_High_T3_16_Left/1.png", "Image/P17_D1_P7_High_T3_16_Left/2.png", "Image/P17_D1_P7_High_T3_16_Left/3.png", "Image/P17_D1_P7_High_T3_16_Left/4.png", "Image/P17_D1_P7_High_T3_16_Left/5.png", "Image/P17_D1_P7_High_T3_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this brown mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1947.8512 seconds to 1960.6996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P4_High_T1_21_Mid", "images": ["Image/P17_D3_P4_High_T1_21_Mid/1.png", "Image/P17_D3_P4_High_T1_21_Mid/2.png", "Image/P17_D3_P4_High_T1_21_Mid/3.png", "Image/P17_D3_P4_High_T1_21_Mid/4.png", "Image/P17_D3_P4_High_T1_21_Mid/5.png", "Image/P17_D3_P4_High_T1_21_Mid/6.png", "Image/P17_D3_P4_High_T1_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this white object on the desk to the second shelf over here on the right? Finish\"\nThe frames roughly correspond to the speech interval from 1040.7483 seconds to 1055.3495 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P4_Low_T2_17_Left", "images": ["Image/P27_D3_P4_Low_T2_17_Left/1.png", "Image/P27_D3_P4_Low_T2_17_Left/2.png", "Image/P27_D3_P4_Low_T2_17_Left/3.png", "Image/P27_D3_P4_Low_T2_17_Left/4.png", "Image/P27_D3_P4_Low_T2_17_Left/5.png", "Image/P27_D3_P4_Low_T2_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that glass on the table? Finish\"\nThe frames roughly correspond to the speech interval from 228.98800000000003 seconds to 240.123 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P5_Medium_T1_11_Right", "images": ["Image/P22_D1_P5_Medium_T1_11_Right/1.png", "Image/P22_D1_P5_Medium_T1_11_Right/2.png", "Image/P22_D1_P5_Medium_T1_11_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1707.348 seconds to 1718.9967 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P7_Medium_T1_17_Mid", "images": ["Image/P4_D2_P7_Medium_T1_17_Mid/1.png", "Image/P4_D2_P7_Medium_T1_17_Mid/2.png", "Image/P4_D2_P7_Medium_T1_17_Mid/3.png", "Image/P4_D2_P7_Medium_T1_17_Mid/4.png", "Image/P4_D2_P7_Medium_T1_17_Mid/5.png", "Image/P4_D2_P7_Medium_T1_17_Mid/6.png", "Image/P4_D2_P7_Medium_T1_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the cap in the middle to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1599.4071 seconds to 1616.0126999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P4_Low_T1_16_Right", "images": ["Image/P3_D2_P4_Low_T1_16_Right/1.png", "Image/P3_D2_P4_Low_T1_16_Right/2.png", "Image/P3_D2_P4_Low_T1_16_Right/3.png", "Image/P3_D2_P4_Low_T1_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2553.4366 seconds to 2563.9465 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P7_Medium_T1_3_Mid", "images": ["Image/P28_D1_P7_Medium_T1_3_Mid/1.png", "Image/P28_D1_P7_Medium_T1_3_Mid/2.png", "Image/P28_D1_P7_Medium_T1_3_Mid/3.png", "Image/P28_D1_P7_Medium_T1_3_Mid/4.png", "Image/P28_D1_P7_Medium_T1_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that middle vase to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1404.1104 seconds to 1415.3563 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P4_Low_T3_6_Right", "images": ["Image/P16_D1_P4_Low_T3_6_Right/1.png", "Image/P16_D1_P4_Low_T3_6_Right/2.png", "Image/P16_D1_P4_Low_T3_6_Right/3.png", "Image/P16_D1_P4_Low_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1570.6777 seconds to 1581.1184 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P9_High_T3_7_Left", "images": ["Image/P23_D1_P9_High_T3_7_Left/1.png", "Image/P23_D1_P9_High_T3_7_Left/2.png", "Image/P23_D1_P9_High_T3_7_Left/3.png", "Image/P23_D1_P9_High_T3_7_Left/4.png", "Image/P23_D1_P9_High_T3_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1880.5326 seconds to 1890.847 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P3_Low_T2_26_Left", "images": ["Image/P3_D2_P3_Low_T2_26_Left/1.png", "Image/P3_D2_P3_Low_T2_26_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2270.3621000000003 seconds to 2280.6759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P5_High_T1_2_Right", "images": ["Image/P7_D1_P5_High_T1_2_Right/1.png", "Image/P7_D1_P5_High_T1_2_Right/2.png", "Image/P7_D1_P5_High_T1_2_Right/3.png", "Image/P7_D1_P5_High_T1_2_Right/4.png", "Image/P7_D1_P5_High_T1_2_Right/5.png", "Image/P7_D1_P5_High_T1_2_Right/6.png", "Image/P7_D1_P5_High_T1_2_Right/7.png", "Image/P7_D1_P5_High_T1_2_Right/8.png", "Image/P7_D1_P5_High_T1_2_Right/9.png", "Image/P7_D1_P5_High_T1_2_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Before they move the green bottle from in front of me to my right hand side. Finish.\"\nThe frames roughly correspond to the speech interval from 736.6094 seconds to 753.4793 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P1_Medium_T2_26_Right", "images": ["Image/P14_D3_P1_Medium_T2_26_Right/1.png", "Image/P14_D3_P1_Medium_T2_26_Right/2.png", "Image/P14_D3_P1_Medium_T2_26_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill up that mug. Finished.\"\nThe frames roughly correspond to the speech interval from 1341.16676 seconds to 1349.75136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P3_Low_T1_11_Right", "images": ["Image/P19_D2_P3_Low_T1_11_Right/1.png", "Image/P19_D2_P3_Low_T1_11_Right/2.png", "Image/P19_D2_P3_Low_T1_11_Right/3.png", "Image/P19_D2_P3_Low_T1_11_Right/4.png", "Image/P19_D2_P3_Low_T1_11_Right/5.png", "Image/P19_D2_P3_Low_T1_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. And please move this container down here. Finish.\"\nThe frames roughly correspond to the speech interval from 1431.6165999999998 seconds to 1444.5767999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P5_High_T2_10_Left", "images": ["Image/P12_D2_P5_High_T2_10_Left/1.png", "Image/P12_D2_P5_High_T2_10_Left/2.png", "Image/P12_D2_P5_High_T2_10_Left/3.png", "Image/P12_D2_P5_High_T2_10_Left/4.png", "Image/P12_D2_P5_High_T2_10_Left/5.png", "Image/P12_D2_P5_High_T2_10_Left/6.png", "Image/P12_D2_P5_High_T2_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the orange vase in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 148.16558 seconds to 161.63924 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P4_Medium_T3_27_Left", "images": ["Image/P30_D2_P4_Medium_T3_27_Left/1.png", "Image/P30_D2_P4_Medium_T3_27_Left/2.png", "Image/P30_D2_P4_Medium_T3_27_Left/3.png", "Image/P30_D2_P4_Medium_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup closest to me. Finish.\"\nThe frames roughly correspond to the speech interval from 465.4032 seconds to 474.298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P7_High_T3_2_Mid", "images": ["Image/P12_D3_P7_High_T3_2_Mid/1.png", "Image/P12_D3_P7_High_T3_2_Mid/2.png", "Image/P12_D3_P7_High_T3_2_Mid/3.png", "Image/P12_D3_P7_High_T3_2_Mid/4.png", "Image/P12_D3_P7_High_T3_2_Mid/5.png", "Image/P12_D3_P7_High_T3_2_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2273.17188 seconds to 2287.14703 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P6_High_T3_10_Mid", "images": ["Image/P26_D3_P6_High_T3_10_Mid/1.png", "Image/P26_D3_P6_High_T3_10_Mid/2.png", "Image/P26_D3_P6_High_T3_10_Mid/3.png", "Image/P26_D3_P6_High_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 789.5983 seconds to 799.5228 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P6_Low_T3_11_Right", "images": ["Image/P2_D3_P6_Low_T3_11_Right/1.png", "Image/P2_D3_P6_Low_T3_11_Right/2.png", "Image/P2_D3_P6_Low_T3_11_Right/3.png", "Image/P2_D3_P6_Low_T3_11_Right/4.png", "Image/P2_D3_P6_Low_T3_11_Right/5.png", "Image/P2_D3_P6_Low_T3_11_Right/6.png", "Image/P2_D3_P6_Low_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that round bottle? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1754.7187000000004 seconds to 1772.9251000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P9_Low_T2_3_Left", "images": ["Image/P13_D3_P9_Low_T2_3_Left/1.png", "Image/P13_D3_P9_Low_T2_3_Left/2.png", "Image/P13_D3_P9_Low_T2_3_Left/3.png", "Image/P13_D3_P9_Low_T2_3_Left/4.png", "Image/P13_D3_P9_Low_T2_3_Left/5.png", "Image/P13_D3_P9_Low_T2_3_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Friday\"\nThe frames roughly correspond to the speech interval from 1594.8072 seconds to 1606.3471 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P8_Medium_T3_14_Left", "images": ["Image/P19_D3_P8_Medium_T3_14_Left/1.png", "Image/P19_D3_P8_Medium_T3_14_Left/2.png", "Image/P19_D3_P8_Medium_T3_14_Left/3.png", "Image/P19_D3_P8_Medium_T3_14_Left/4.png", "Image/P19_D3_P8_Medium_T3_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2384.6141 seconds to 2398.4357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P8_Low_T1_7_Right", "images": ["Image/P22_D2_P8_Low_T1_7_Right/1.png", "Image/P22_D2_P8_Low_T1_7_Right/2.png", "Image/P22_D2_P8_Low_T1_7_Right/3.png", "Image/P22_D2_P8_Low_T1_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup up there. Finish.\"\nThe frames roughly correspond to the speech interval from 861.6670999999999 seconds to 870.4654999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P3_Low_T1_15_Right", "images": ["Image/P9_D3_P3_Low_T1_15_Right/1.png", "Image/P9_D3_P3_Low_T1_15_Right/2.png", "Image/P9_D3_P3_Low_T1_15_Right/3.png", "Image/P9_D3_P3_Low_T1_15_Right/4.png", "Image/P9_D3_P3_Low_T1_15_Right/5.png", "Image/P9_D3_P3_Low_T1_15_Right/6.png", "Image/P9_D3_P3_Low_T1_15_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the top to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 301.28290000000004 seconds to 315.7928 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P3_High_T3_4_Mid", "images": ["Image/P8_D2_P3_High_T3_4_Mid/1.png", "Image/P8_D2_P3_High_T3_4_Mid/2.png", "Image/P8_D2_P3_High_T3_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the planet. Finish.\"\nThe frames roughly correspond to the speech interval from 2430.2553000000003 seconds to 2439.4556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P8_Low_T3_27_Left", "images": ["Image/P28_D2_P8_Low_T3_27_Left/1.png", "Image/P28_D2_P8_Low_T3_27_Left/2.png", "Image/P28_D2_P8_Low_T3_27_Left/3.png", "Image/P28_D2_P8_Low_T3_27_Left/4.png", "Image/P28_D2_P8_Low_T3_27_Left/5.png", "Image/P28_D2_P8_Low_T3_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that object over there. Finish.\"\nThe frames roughly correspond to the speech interval from 796.0539200000001 seconds to 808.92925 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P3_Low_T3_22_Left", "images": ["Image/P26_D2_P3_Low_T3_22_Left/1.png", "Image/P26_D2_P3_Low_T3_22_Left/2.png", "Image/P26_D2_P3_Low_T3_22_Left/3.png", "Image/P26_D2_P3_Low_T3_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1363.7006 seconds to 1375.8103999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P7_Low_T2_12_Right", "images": ["Image/P29_D1_P7_Low_T2_12_Right/1.png", "Image/P29_D1_P7_Low_T2_12_Right/2.png", "Image/P29_D1_P7_Low_T2_12_Right/3.png", "Image/P29_D1_P7_Low_T2_12_Right/4.png", "Image/P29_D1_P7_Low_T2_12_Right/5.png", "Image/P29_D1_P7_Low_T2_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2689.2765999999997 seconds to 2700.6065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P1_High_T3_13_Left", "images": ["Image/P11_D1_P1_High_T3_13_Left/1.png", "Image/P11_D1_P1_High_T3_13_Left/2.png", "Image/P11_D1_P1_High_T3_13_Left/3.png", "Image/P11_D1_P1_High_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the plant in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2705.9244999999996 seconds to 2717.9929999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P7_Low_T1_22_Mid", "images": ["Image/P15_D1_P7_Low_T1_22_Mid/1.png", "Image/P15_D1_P7_Low_T1_22_Mid/2.png", "Image/P15_D1_P7_Low_T1_22_Mid/3.png", "Image/P15_D1_P7_Low_T1_22_Mid/4.png", "Image/P15_D1_P7_Low_T1_22_Mid/5.png", "Image/P15_D1_P7_Low_T1_22_Mid/6.png", "Image/P15_D1_P7_Low_T1_22_Mid/7.png", "Image/P15_D1_P7_Low_T1_22_Mid/8.png", "Image/P15_D1_P7_Low_T1_22_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the floor to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2007.23297 seconds to 2020.98557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P7_Low_T1_22_Mid", "images": ["Image/P30_D1_P7_Low_T1_22_Mid/1.png", "Image/P30_D1_P7_Low_T1_22_Mid/2.png", "Image/P30_D1_P7_Low_T1_22_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1061.3703 seconds to 1069.8469 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P1_Medium_T3_18_Left", "images": ["Image/P5_D2_P1_Medium_T3_18_Left/1.png", "Image/P5_D2_P1_Medium_T3_18_Left/2.png", "Image/P5_D2_P1_Medium_T3_18_Left/3.png", "Image/P5_D2_P1_Medium_T3_18_Left/4.png", "Image/P5_D2_P1_Medium_T3_18_Left/5.png", "Image/P5_D2_P1_Medium_T3_18_Left/6.png", "Image/P5_D2_P1_Medium_T3_18_Left/7.png", "Image/P5_D2_P1_Medium_T3_18_Left/8.png", "Image/P5_D2_P1_Medium_T3_18_Left/9.png", "Image/P5_D2_P1_Medium_T3_18_Left/10.png", "Image/P5_D2_P1_Medium_T3_18_Left/11.png", "Image/P5_D2_P1_Medium_T3_18_Left/12.png", "Image/P5_D2_P1_Medium_T3_18_Left/13.png", "Image/P5_D2_P1_Medium_T3_18_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are three cups on the shelf in front of you. Clean the one that is closer to the middle cup. Finish.\"\nThe frames roughly correspond to the speech interval from 548.6424 seconds to 569.5939000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P4_High_T3_22_Left", "images": ["Image/P27_D1_P4_High_T3_22_Left/1.png", "Image/P27_D1_P4_High_T3_22_Left/2.png", "Image/P27_D1_P4_High_T3_22_Left/3.png", "Image/P27_D1_P4_High_T3_22_Left/4.png", "Image/P27_D1_P4_High_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1072.3155000000002 seconds to 1082.4066 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P3_High_T1_16_Left", "images": ["Image/P13_D1_P3_High_T1_16_Left/1.png", "Image/P13_D1_P3_High_T1_16_Left/2.png", "Image/P13_D1_P3_High_T1_16_Left/3.png", "Image/P13_D1_P3_High_T1_16_Left/4.png", "Image/P13_D1_P3_High_T1_16_Left/5.png", "Image/P13_D1_P3_High_T1_16_Left/6.png", "Image/P13_D1_P3_High_T1_16_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move this brown cup down here please? Finish.\"\nThe frames roughly correspond to the speech interval from 357.3167 seconds to 370.1914 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P3_Medium_T1_18_Mid", "images": ["Image/P16_D1_P3_Medium_T1_18_Mid/1.png", "Image/P16_D1_P3_Medium_T1_18_Mid/2.png", "Image/P16_D1_P3_Medium_T1_18_Mid/3.png", "Image/P16_D1_P3_Medium_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this mug down here. Finish.\"\nThe frames roughly correspond to the speech interval from 1316.6142 seconds to 1326.0788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P3_Low_T3_13_Mid", "images": ["Image/P12_D3_P3_Low_T3_13_Mid/1.png", "Image/P12_D3_P3_Low_T3_13_Mid/2.png", "Image/P12_D3_P3_Low_T3_13_Mid/3.png", "Image/P12_D3_P3_Low_T3_13_Mid/4.png", "Image/P12_D3_P3_Low_T3_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2455.8571899999997 seconds to 2466.17269 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P5_Medium_T1_22_Right", "images": ["Image/P30_D2_P5_Medium_T1_22_Right/1.png", "Image/P30_D2_P5_Medium_T1_22_Right/2.png", "Image/P30_D2_P5_Medium_T1_22_Right/3.png", "Image/P30_D2_P5_Medium_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant furthest on the right over here. Finish.\"\nThe frames roughly correspond to the speech interval from 358.5961 seconds to 368.8261 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P7_Medium_T1_17_Right", "images": ["Image/P29_D1_P7_Medium_T1_17_Right/1.png", "Image/P29_D1_P7_Medium_T1_17_Right/2.png", "Image/P29_D1_P7_Medium_T1_17_Right/3.png", "Image/P29_D1_P7_Medium_T1_17_Right/4.png", "Image/P29_D1_P7_Medium_T1_17_Right/5.png", "Image/P29_D1_P7_Medium_T1_17_Right/6.png", "Image/P29_D1_P7_Medium_T1_17_Right/7.png", "Image/P29_D1_P7_Medium_T1_17_Right/8.png", "Image/P29_D1_P7_Medium_T1_17_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2712.8134999999997 seconds to 2728.9896999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P1_Low_T3_19_Left", "images": ["Image/P28_D1_P1_Low_T3_19_Left/1.png", "Image/P28_D1_P1_Low_T3_19_Left/2.png", "Image/P28_D1_P1_Low_T3_19_Left/3.png", "Image/P28_D1_P1_Low_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1457.395 seconds to 1466.679 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P1_Medium_T1_23_Mid", "images": ["Image/P28_D3_P1_Medium_T1_23_Mid/1.png", "Image/P28_D3_P1_Medium_T1_23_Mid/2.png", "Image/P28_D3_P1_Medium_T1_23_Mid/3.png", "Image/P28_D3_P1_Medium_T1_23_Mid/4.png", "Image/P28_D3_P1_Medium_T1_23_Mid/5.png", "Image/P28_D3_P1_Medium_T1_23_Mid/6.png", "Image/P28_D3_P1_Medium_T1_23_Mid/7.png", "Image/P28_D3_P1_Medium_T1_23_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant in the middle underneath the desk on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 145.9455 seconds to 162.0786 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P8_High_T3_8_Right", "images": ["Image/P30_D3_P8_High_T3_8_Right/1.png", "Image/P30_D3_P8_High_T3_8_Right/2.png", "Image/P30_D3_P8_High_T3_8_Right/3.png", "Image/P30_D3_P8_High_T3_8_Right/4.png", "Image/P30_D3_P8_High_T3_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the green cup over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1250.1744 seconds to 1262.2001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P5_Low_T2_7_Mid", "images": ["Image/P4_D1_P5_Low_T2_7_Mid/1.png", "Image/P4_D1_P5_Low_T2_7_Mid/2.png", "Image/P4_D1_P5_Low_T2_7_Mid/3.png", "Image/P4_D1_P5_Low_T2_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2585.1418 seconds to 2597.5975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P8_Low_T3_11_Mid", "images": ["Image/P14_D3_P8_Low_T3_11_Mid/1.png", "Image/P14_D3_P8_Low_T3_11_Mid/2.png", "Image/P14_D3_P8_Low_T3_11_Mid/3.png", "Image/P14_D3_P8_Low_T3_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 1024.72799 seconds to 1033.80416 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D1_P2_Low_T1_1_Right", "images": ["Image/P10_D1_P2_Low_T1_1_Right/1.png", "Image/P10_D1_P2_Low_T1_1_Right/2.png", "Image/P10_D1_P2_Low_T1_1_Right/3.png", "Image/P10_D1_P2_Low_T1_1_Right/4.png", "Image/P10_D1_P2_Low_T1_1_Right/5.png", "Image/P10_D1_P2_Low_T1_1_Right/6.png", "Image/P10_D1_P2_Low_T1_1_Right/7.png", "Image/P10_D1_P2_Low_T1_1_Right/8.png", "Image/P10_D1_P2_Low_T1_1_Right/9.png", "Image/P10_D1_P2_Low_T1_1_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1988.4869 seconds to 2011.0789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P2_Medium_T3_15_Mid", "images": ["Image/P3_D2_P2_Medium_T3_15_Mid/1.png", "Image/P3_D2_P2_Medium_T3_15_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2520.8314 seconds to 2529.9479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P1_High_T1_20_Right", "images": ["Image/P13_D2_P1_High_T1_20_Right/1.png", "Image/P13_D2_P1_High_T1_20_Right/2.png", "Image/P13_D2_P1_High_T1_20_Right/3.png", "Image/P13_D2_P1_High_T1_20_Right/4.png", "Image/P13_D2_P1_High_T1_20_Right/5.png", "Image/P13_D2_P1_High_T1_20_Right/6.png", "Image/P13_D2_P1_High_T1_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this white bottle down to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 774.35856 seconds to 786.3562800000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P8_Medium_T2_21_Right", "images": ["Image/P5_D3_P8_Medium_T2_21_Right/1.png", "Image/P5_D3_P8_Medium_T2_21_Right/2.png", "Image/P5_D3_P8_Medium_T2_21_Right/3.png", "Image/P5_D3_P8_Medium_T2_21_Right/4.png", "Image/P5_D3_P8_Medium_T2_21_Right/5.png", "Image/P5_D3_P8_Medium_T2_21_Right/6.png", "Image/P5_D3_P8_Medium_T2_21_Right/7.png", "Image/P5_D3_P8_Medium_T2_21_Right/8.png", "Image/P5_D3_P8_Medium_T2_21_Right/9.png", "Image/P5_D3_P8_Medium_T2_21_Right/10.png", "Image/P5_D3_P8_Medium_T2_21_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water. into the bottle into the rightmost bottle on the ground. Finished.\"\nThe frames roughly correspond to the speech interval from 2811.0841 seconds to 2827.1346 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P2_Low_T3_7_Mid", "images": ["Image/P16_D3_P2_Low_T3_7_Mid/1.png", "Image/P16_D3_P2_Low_T3_7_Mid/2.png", "Image/P16_D3_P2_Low_T3_7_Mid/3.png", "Image/P16_D3_P2_Low_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 464.529 seconds to 473.53469999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P4_Low_T1_19_Mid", "images": ["Image/P20_D3_P4_Low_T1_19_Mid/1.png", "Image/P20_D3_P4_Low_T1_19_Mid/2.png", "Image/P20_D3_P4_Low_T1_19_Mid/3.png", "Image/P20_D3_P4_Low_T1_19_Mid/4.png", "Image/P20_D3_P4_Low_T1_19_Mid/5.png", "Image/P20_D3_P4_Low_T1_19_Mid/6.png", "Image/P20_D3_P4_Low_T1_19_Mid/7.png", "Image/P20_D3_P4_Low_T1_19_Mid/8.png", "Image/P20_D3_P4_Low_T1_19_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey robot\"\nThe frames roughly correspond to the speech interval from 1861.5 seconds to 1876.9911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P4_Medium_T3_14_Left", "images": ["Image/P1_D3_P4_Medium_T3_14_Left/1.png", "Image/P1_D3_P4_Medium_T3_14_Left/2.png", "Image/P1_D3_P4_Medium_T3_14_Left/3.png", "Image/P1_D3_P4_Medium_T3_14_Left/4.png", "Image/P1_D3_P4_Medium_T3_14_Left/5.png", "Image/P1_D3_P4_Medium_T3_14_Left/6.png", "Image/P1_D3_P4_Medium_T3_14_Left/7.png", "Image/P1_D3_P4_Medium_T3_14_Left/8.png", "Image/P1_D3_P4_Medium_T3_14_Left/9.png", "Image/P1_D3_P4_Medium_T3_14_Left/10.png", "Image/P1_D3_P4_Medium_T3_14_Left/11.png", "Image/P1_D3_P4_Medium_T3_14_Left/12.png", "Image/P1_D3_P4_Medium_T3_14_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you help clean the... The left plant\"\nThe frames roughly correspond to the speech interval from 1472.5962999999997 seconds to 1492.1703999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P5_High_T2_21_Left", "images": ["Image/P8_D1_P5_High_T2_21_Left/1.png", "Image/P8_D1_P5_High_T2_21_Left/2.png", "Image/P8_D1_P5_High_T2_21_Left/3.png", "Image/P8_D1_P5_High_T2_21_Left/4.png", "Image/P8_D1_P5_High_T2_21_Left/5.png", "Image/P8_D1_P5_High_T2_21_Left/6.png", "Image/P8_D1_P5_High_T2_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Start. So\"\nThe frames roughly correspond to the speech interval from 718.4091999999999 seconds to 732.4069999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P1_Medium_T3_1_Right", "images": ["Image/P29_D2_P1_Medium_T3_1_Right/1.png", "Image/P29_D2_P1_Medium_T3_1_Right/2.png", "Image/P29_D2_P1_Medium_T3_1_Right/3.png", "Image/P29_D2_P1_Medium_T3_1_Right/4.png", "Image/P29_D2_P1_Medium_T3_1_Right/5.png", "Image/P29_D2_P1_Medium_T3_1_Right/6.png", "Image/P29_D2_P1_Medium_T3_1_Right/7.png", "Image/P29_D2_P1_Medium_T3_1_Right/8.png", "Image/P29_D2_P1_Medium_T3_1_Right/9.png", "Image/P29_D2_P1_Medium_T3_1_Right/10.png", "Image/P29_D2_P1_Medium_T3_1_Right/11.png", "Image/P29_D2_P1_Medium_T3_1_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 320.12199999999996 seconds to 337.6883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P7_Medium_T3_21_Mid", "images": ["Image/P30_D2_P7_Medium_T3_21_Mid/1.png", "Image/P30_D2_P7_Medium_T3_21_Mid/2.png", "Image/P30_D2_P7_Medium_T3_21_Mid/3.png", "Image/P30_D2_P7_Medium_T3_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle base. Finish.\"\nThe frames roughly correspond to the speech interval from 561.3249 seconds to 570.2201 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P2_High_T3_6_Right", "images": ["Image/P13_D3_P2_High_T3_6_Right/1.png", "Image/P13_D3_P2_High_T3_6_Right/2.png", "Image/P13_D3_P2_High_T3_6_Right/3.png", "Image/P13_D3_P2_High_T3_6_Right/4.png", "Image/P13_D3_P2_High_T3_6_Right/5.png", "Image/P13_D3_P2_High_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean the plant over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1693.9315 seconds to 1705.109 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P3_Low_T1_18_Left", "images": ["Image/P5_D1_P3_Low_T1_18_Left/1.png", "Image/P5_D1_P3_Low_T1_18_Left/2.png", "Image/P5_D1_P3_Low_T1_18_Left/3.png", "Image/P5_D1_P3_Low_T1_18_Left/4.png", "Image/P5_D1_P3_Low_T1_18_Left/5.png", "Image/P5_D1_P3_Low_T1_18_Left/6.png", "Image/P5_D1_P3_Low_T1_18_Left/7.png", "Image/P5_D1_P3_Low_T1_18_Left/8.png", "Image/P5_D1_P3_Low_T1_18_Left/9.png", "Image/P5_D1_P3_Low_T1_18_Left/10.png", "Image/P5_D1_P3_Low_T1_18_Left/11.png", "Image/P5_D1_P3_Low_T1_18_Left/12.png", "Image/P5_D1_P3_Low_T1_18_Left/13.png", "Image/P5_D1_P3_Low_T1_18_Left/14.png", "Image/P5_D1_P3_Low_T1_18_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup on the shelf into the bucket which is the closest to you. Finished.\"\nThe frames roughly correspond to the speech interval from 1813.8019 seconds to 1834.554 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P6_High_T1_2_Right", "images": ["Image/P18_D1_P6_High_T1_2_Right/1.png", "Image/P18_D1_P6_High_T1_2_Right/2.png", "Image/P18_D1_P6_High_T1_2_Right/3.png", "Image/P18_D1_P6_High_T1_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1013.8926 seconds to 1023.7901 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P4_High_T3_26_Mid", "images": ["Image/P10_D3_P4_High_T3_26_Mid/1.png", "Image/P10_D3_P4_High_T3_26_Mid/2.png", "Image/P10_D3_P4_High_T3_26_Mid/3.png", "Image/P10_D3_P4_High_T3_26_Mid/4.png", "Image/P10_D3_P4_High_T3_26_Mid/5.png", "Image/P10_D3_P4_High_T3_26_Mid/6.png", "Image/P10_D3_P4_High_T3_26_Mid/7.png", "Image/P10_D3_P4_High_T3_26_Mid/8.png", "Image/P10_D3_P4_High_T3_26_Mid/9.png", "Image/P10_D3_P4_High_T3_26_Mid/10.png", "Image/P10_D3_P4_High_T3_26_Mid/11.png", "Image/P10_D3_P4_High_T3_26_Mid/12.png", "Image/P10_D3_P4_High_T3_26_Mid/13.png", "Image/P10_D3_P4_High_T3_26_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey\"\nThe frames roughly correspond to the speech interval from 899.76749 seconds to 921.34229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P6_Low_T3_20_Mid", "images": ["Image/P14_D3_P6_Low_T3_20_Mid/1.png", "Image/P14_D3_P6_Low_T3_20_Mid/2.png", "Image/P14_D3_P6_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1388.71436 seconds to 1397.26886 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P9_Medium_T2_19_Left", "images": ["Image/P10_D3_P9_Medium_T2_19_Left/1.png", "Image/P10_D3_P9_Medium_T2_19_Left/2.png", "Image/P10_D3_P9_Medium_T2_19_Left/3.png", "Image/P10_D3_P9_Medium_T2_19_Left/4.png", "Image/P10_D3_P9_Medium_T2_19_Left/5.png", "Image/P10_D3_P9_Medium_T2_19_Left/6.png", "Image/P10_D3_P9_Medium_T2_19_Left/7.png", "Image/P10_D3_P9_Medium_T2_19_Left/8.png", "Image/P10_D3_P9_Medium_T2_19_Left/9.png", "Image/P10_D3_P9_Medium_T2_19_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 778.46819 seconds to 796.2994899999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P1_High_T3_21_Right", "images": ["Image/P10_D3_P1_High_T3_21_Right/1.png", "Image/P10_D3_P1_High_T3_21_Right/2.png", "Image/P10_D3_P1_High_T3_21_Right/3.png", "Image/P10_D3_P1_High_T3_21_Right/4.png", "Image/P10_D3_P1_High_T3_21_Right/5.png", "Image/P10_D3_P1_High_T3_21_Right/6.png", "Image/P10_D3_P1_High_T3_21_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 431.55719 seconds to 443.61049 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P9_High_T1_25_Mid", "images": ["Image/P29_D3_P9_High_T1_25_Mid/1.png", "Image/P29_D3_P9_High_T1_25_Mid/2.png", "Image/P29_D3_P9_High_T1_25_Mid/3.png", "Image/P29_D3_P9_High_T1_25_Mid/4.png", "Image/P29_D3_P9_High_T1_25_Mid/5.png", "Image/P29_D3_P9_High_T1_25_Mid/6.png", "Image/P29_D3_P9_High_T1_25_Mid/7.png", "Image/P29_D3_P9_High_T1_25_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1596.4099 seconds to 1610.2594 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P5_High_T3_27_Right", "images": ["Image/P8_D3_P5_High_T3_27_Right/1.png", "Image/P8_D3_P5_High_T3_27_Right/2.png", "Image/P8_D3_P5_High_T3_27_Right/3.png", "Image/P8_D3_P5_High_T3_27_Right/4.png", "Image/P8_D3_P5_High_T3_27_Right/5.png", "Image/P8_D3_P5_High_T3_27_Right/6.png", "Image/P8_D3_P5_High_T3_27_Right/7.png", "Image/P8_D3_P5_High_T3_27_Right/8.png", "Image/P8_D3_P5_High_T3_27_Right/9.png", "Image/P8_D3_P5_High_T3_27_Right/10.png", "Image/P8_D3_P5_High_T3_27_Right/11.png", "Image/P8_D3_P5_High_T3_27_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1013.35462 seconds to 1030.55957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P6_Low_T2_11_Left", "images": ["Image/P22_D1_P6_Low_T2_11_Left/1.png", "Image/P22_D1_P6_Low_T2_11_Left/2.png", "Image/P22_D1_P6_Low_T2_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Put water in that. Finish\"\nThe frames roughly correspond to the speech interval from 1637.483 seconds to 1647.4203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P3_Low_T3_18_Left", "images": ["Image/P6_D1_P3_Low_T3_18_Left/1.png", "Image/P6_D1_P3_Low_T3_18_Left/2.png", "Image/P6_D1_P3_Low_T3_18_Left/3.png", "Image/P6_D1_P3_Low_T3_18_Left/4.png", "Image/P6_D1_P3_Low_T3_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2038.7280000000003 seconds to 2049.2493000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P8_High_T1_27_Mid", "images": ["Image/P17_D1_P8_High_T1_27_Mid/1.png", "Image/P17_D1_P8_High_T1_27_Mid/2.png", "Image/P17_D1_P8_High_T1_27_Mid/3.png", "Image/P17_D1_P8_High_T1_27_Mid/4.png", "Image/P17_D1_P8_High_T1_27_Mid/5.png", "Image/P17_D1_P8_High_T1_27_Mid/6.png", "Image/P17_D1_P8_High_T1_27_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start? Can you move this white mug to this top shelf right here in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 1788.7057000000002 seconds to 1805.8822 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P7_High_T3_14_Right", "images": ["Image/P11_D2_P7_High_T3_14_Right/1.png", "Image/P11_D2_P7_High_T3_14_Right/2.png", "Image/P11_D2_P7_High_T3_14_Right/3.png", "Image/P11_D2_P7_High_T3_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Please finish.\"\nThe frames roughly correspond to the speech interval from 698.7991 seconds to 708.5598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P6_Medium_T3_11_Right", "images": ["Image/P9_D2_P6_Medium_T3_11_Right/1.png", "Image/P9_D2_P6_Medium_T3_11_Right/2.png", "Image/P9_D2_P6_Medium_T3_11_Right/3.png", "Image/P9_D2_P6_Medium_T3_11_Right/4.png", "Image/P9_D2_P6_Medium_T3_11_Right/5.png", "Image/P9_D2_P6_Medium_T3_11_Right/6.png", "Image/P9_D2_P6_Medium_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right bottle on that desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3010.795 seconds to 3024.7651 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P3_Medium_T1_23_Mid", "images": ["Image/P4_D2_P3_Medium_T1_23_Mid/1.png", "Image/P4_D2_P3_Medium_T1_23_Mid/2.png", "Image/P4_D2_P3_Medium_T1_23_Mid/3.png", "Image/P4_D2_P3_Medium_T1_23_Mid/4.png", "Image/P4_D2_P3_Medium_T1_23_Mid/5.png", "Image/P4_D2_P3_Medium_T1_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant in the middle to the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 1827.2633 seconds to 1841.2238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P3_High_T3_16_Right", "images": ["Image/P7_D1_P3_High_T3_16_Right/1.png", "Image/P7_D1_P3_High_T3_16_Right/2.png", "Image/P7_D1_P3_High_T3_16_Right/3.png", "Image/P7_D1_P3_High_T3_16_Right/4.png", "Image/P7_D1_P3_High_T3_16_Right/5.png", "Image/P7_D1_P3_High_T3_16_Right/6.png", "Image/P7_D1_P3_High_T3_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 355.566 seconds to 370.1391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P2_Medium_T1_13_Left", "images": ["Image/P28_D1_P2_Medium_T1_13_Left/1.png", "Image/P28_D1_P2_Medium_T1_13_Left/2.png", "Image/P28_D1_P2_Medium_T1_13_Left/3.png", "Image/P28_D1_P2_Medium_T1_13_Left/4.png", "Image/P28_D1_P2_Medium_T1_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1684.3455 seconds to 1695.2173 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P9_Low_T3_4_Left", "images": ["Image/P30_D2_P9_Low_T3_4_Left/1.png", "Image/P30_D2_P9_Low_T3_4_Left/2.png", "Image/P30_D2_P9_Low_T3_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 242.2665 seconds to 250.8685 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P2_Low_T2_14_Right", "images": ["Image/P4_D3_P2_Low_T2_14_Right/1.png", "Image/P4_D3_P2_Low_T2_14_Right/2.png", "Image/P4_D3_P2_Low_T2_14_Right/3.png", "Image/P4_D3_P2_Low_T2_14_Right/4.png", "Image/P4_D3_P2_Low_T2_14_Right/5.png", "Image/P4_D3_P2_Low_T2_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help me watering the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 839.5658999999998 seconds to 852.9696999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P6_Medium_T3_11_Right", "images": ["Image/P10_D3_P6_Medium_T3_11_Right/1.png", "Image/P10_D3_P6_Medium_T3_11_Right/2.png", "Image/P10_D3_P6_Medium_T3_11_Right/3.png", "Image/P10_D3_P6_Medium_T3_11_Right/4.png", "Image/P10_D3_P6_Medium_T3_11_Right/5.png", "Image/P10_D3_P6_Medium_T3_11_Right/6.png", "Image/P10_D3_P6_Medium_T3_11_Right/7.png", "Image/P10_D3_P6_Medium_T3_11_Right/8.png", "Image/P10_D3_P6_Medium_T3_11_Right/9.png", "Image/P10_D3_P6_Medium_T3_11_Right/10.png", "Image/P10_D3_P6_Medium_T3_11_Right/11.png", "Image/P10_D3_P6_Medium_T3_11_Right/12.png", "Image/P10_D3_P6_Medium_T3_11_Right/13.png", "Image/P10_D3_P6_Medium_T3_11_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 832.5748899999999 seconds to 852.2706900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P1_Medium_T3_4_Mid", "images": ["Image/P21_D2_P1_Medium_T3_4_Mid/1.png", "Image/P21_D2_P1_Medium_T3_4_Mid/2.png", "Image/P21_D2_P1_Medium_T3_4_Mid/3.png", "Image/P21_D2_P1_Medium_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1708.2581 seconds to 1719.1731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P9_High_T1_4_Left", "images": ["Image/P25_D2_P9_High_T1_4_Left/1.png", "Image/P25_D2_P9_High_T1_4_Left/2.png", "Image/P25_D2_P9_High_T1_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 922.1795 seconds to 930.9484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P4_High_T3_25_Left", "images": ["Image/P8_D2_P4_High_T3_25_Left/1.png", "Image/P8_D2_P4_High_T3_25_Left/2.png", "Image/P8_D2_P4_High_T3_25_Left/3.png", "Image/P8_D2_P4_High_T3_25_Left/4.png", "Image/P8_D2_P4_High_T3_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean that white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 2239.8589 seconds to 2249.9779000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P2_High_T1_23_Mid", "images": ["Image/P23_D3_P2_High_T1_23_Mid/1.png", "Image/P23_D3_P2_High_T1_23_Mid/2.png", "Image/P23_D3_P2_High_T1_23_Mid/3.png", "Image/P23_D3_P2_High_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 800.0032 seconds to 809.4409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P7_High_T2_11_Right", "images": ["Image/P19_D2_P7_High_T2_11_Right/1.png", "Image/P19_D2_P7_High_T2_11_Right/2.png", "Image/P19_D2_P7_High_T2_11_Right/3.png", "Image/P19_D2_P7_High_T2_11_Right/4.png", "Image/P19_D2_P7_High_T2_11_Right/5.png", "Image/P19_D2_P7_High_T2_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here.\"\nThe frames roughly correspond to the speech interval from 1908.43 seconds to 1921.3757 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P2_High_T2_18_Left", "images": ["Image/P5_D1_P2_High_T2_18_Left/1.png", "Image/P5_D1_P2_High_T2_18_Left/2.png", "Image/P5_D1_P2_High_T2_18_Left/3.png", "Image/P5_D1_P2_High_T2_18_Left/4.png", "Image/P5_D1_P2_High_T2_18_Left/5.png", "Image/P5_D1_P2_High_T2_18_Left/6.png", "Image/P5_D1_P2_High_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill water into the cup which is in red color. Finished.\"\nThe frames roughly correspond to the speech interval from 1560.8355 seconds to 1573.3333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P4_Low_T1_27_Right", "images": ["Image/P29_D1_P4_Low_T1_27_Right/1.png", "Image/P29_D1_P4_Low_T1_27_Right/2.png", "Image/P29_D1_P4_Low_T1_27_Right/3.png", "Image/P29_D1_P4_Low_T1_27_Right/4.png", "Image/P29_D1_P4_Low_T1_27_Right/5.png", "Image/P29_D1_P4_Low_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2918.9559 seconds to 2930.5067999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P7_High_T3_14_Mid", "images": ["Image/P25_D3_P7_High_T3_14_Mid/1.png", "Image/P25_D3_P7_High_T3_14_Mid/2.png", "Image/P25_D3_P7_High_T3_14_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2134.7858 seconds to 2142.9036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P2_Low_T1_27_Right", "images": ["Image/P21_D2_P2_Low_T1_27_Right/1.png", "Image/P21_D2_P2_Low_T1_27_Right/2.png", "Image/P21_D2_P2_Low_T1_27_Right/3.png", "Image/P21_D2_P2_Low_T1_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1371.9140900000002 seconds to 1383.86991 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P1_Low_T1_25_Mid", "images": ["Image/P2_D3_P1_Low_T1_25_Mid/1.png", "Image/P2_D3_P1_Low_T1_25_Mid/2.png", "Image/P2_D3_P1_Low_T1_25_Mid/3.png", "Image/P2_D3_P1_Low_T1_25_Mid/4.png", "Image/P2_D3_P1_Low_T1_25_Mid/5.png", "Image/P2_D3_P1_Low_T1_25_Mid/6.png", "Image/P2_D3_P1_Low_T1_25_Mid/7.png", "Image/P2_D3_P1_Low_T1_25_Mid/8.png", "Image/P2_D3_P1_Low_T1_25_Mid/9.png", "Image/P2_D3_P1_Low_T1_25_Mid/10.png", "Image/P2_D3_P1_Low_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1854.4971000000005 seconds to 1871.7019000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P3_High_T3_3_Left", "images": ["Image/P11_D1_P3_High_T3_3_Left/1.png", "Image/P11_D1_P3_High_T3_3_Left/2.png", "Image/P11_D1_P3_High_T3_3_Left/3.png", "Image/P11_D1_P3_High_T3_3_Left/4.png", "Image/P11_D1_P3_High_T3_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the green bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2777.1092 seconds to 2789.8877999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P3_Low_T3_3_Right", "images": ["Image/P2_D1_P3_Low_T3_3_Right/1.png", "Image/P2_D1_P3_Low_T3_3_Right/2.png", "Image/P2_D1_P3_Low_T3_3_Right/3.png", "Image/P2_D1_P3_Low_T3_3_Right/4.png", "Image/P2_D1_P3_Low_T3_3_Right/5.png", "Image/P2_D1_P3_Low_T3_3_Right/6.png", "Image/P2_D1_P3_Low_T3_3_Right/7.png", "Image/P2_D1_P3_Low_T3_3_Right/8.png", "Image/P2_D1_P3_Low_T3_3_Right/9.png", "Image/P2_D1_P3_Low_T3_3_Right/10.png", "Image/P2_D1_P3_Low_T3_3_Right/11.png", "Image/P2_D1_P3_Low_T3_3_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 590.4922999999999 seconds to 614.516 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P7_Medium_T1_11_Mid", "images": ["Image/P18_D2_P7_Medium_T1_11_Mid/1.png", "Image/P18_D2_P7_Medium_T1_11_Mid/2.png", "Image/P18_D2_P7_Medium_T1_11_Mid/3.png", "Image/P18_D2_P7_Medium_T1_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that middle bottle to this top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 495.8423 seconds to 505.476 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P6_Medium_T1_24_Right", "images": ["Image/P16_D3_P6_Medium_T1_24_Right/1.png", "Image/P16_D3_P6_Medium_T1_24_Right/2.png", "Image/P16_D3_P6_Medium_T1_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 282.1636 seconds to 291.43449999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P6_Low_T1_6_Mid", "images": ["Image/P9_D3_P6_Low_T1_6_Mid/1.png", "Image/P9_D3_P6_Low_T1_6_Mid/2.png", "Image/P9_D3_P6_Low_T1_6_Mid/3.png", "Image/P9_D3_P6_Low_T1_6_Mid/4.png", "Image/P9_D3_P6_Low_T1_6_Mid/5.png", "Image/P9_D3_P6_Low_T1_6_Mid/6.png", "Image/P9_D3_P6_Low_T1_6_Mid/7.png", "Image/P9_D3_P6_Low_T1_6_Mid/8.png", "Image/P9_D3_P6_Low_T1_6_Mid/9.png", "Image/P9_D3_P6_Low_T1_6_Mid/10.png", "Image/P9_D3_P6_Low_T1_6_Mid/11.png", "Image/P9_D3_P6_Low_T1_6_Mid/12.png", "Image/P9_D3_P6_Low_T1_6_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the second layer to the desk on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 802.3491 seconds to 822.2203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P4_Medium_T3_17_Mid", "images": ["Image/P8_D2_P4_Medium_T3_17_Mid/1.png", "Image/P8_D2_P4_Medium_T3_17_Mid/2.png", "Image/P8_D2_P4_Medium_T3_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you water the planet that is on the right? Could you clean the middle brown glass?\"\nThe frames roughly correspond to the speech interval from 2193.5759000000003 seconds to 2204.3628000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P5_Medium_T3_25_Left", "images": ["Image/P9_D3_P5_Medium_T3_25_Left/1.png", "Image/P9_D3_P5_Medium_T3_25_Left/2.png", "Image/P9_D3_P5_Medium_T3_25_Left/3.png", "Image/P9_D3_P5_Medium_T3_25_Left/4.png", "Image/P9_D3_P5_Medium_T3_25_Left/5.png", "Image/P9_D3_P5_Medium_T3_25_Left/6.png", "Image/P9_D3_P5_Medium_T3_25_Left/7.png", "Image/P9_D3_P5_Medium_T3_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the cup on the third layer? Finish.\"\nThe frames roughly correspond to the speech interval from 868.0873 seconds to 887.653 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P4_High_T3_13_Right", "images": ["Image/P28_D1_P4_High_T3_13_Right/1.png", "Image/P28_D1_P4_High_T3_13_Right/2.png", "Image/P28_D1_P4_High_T3_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1475.352 seconds to 1484.0932 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P4_Medium_T2_23_Right", "images": ["Image/P3_D3_P4_Medium_T2_23_Right/1.png", "Image/P3_D3_P4_Medium_T2_23_Right/2.png", "Image/P3_D3_P4_Medium_T2_23_Right/3.png", "Image/P3_D3_P4_Medium_T2_23_Right/4.png", "Image/P3_D3_P4_Medium_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 747.7130999999999 seconds to 758.5298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P4_Low_T3_18_Left", "images": ["Image/P10_D2_P4_Low_T3_18_Left/1.png", "Image/P10_D2_P4_Low_T3_18_Left/2.png", "Image/P10_D2_P4_Low_T3_18_Left/3.png", "Image/P10_D2_P4_Low_T3_18_Left/4.png", "Image/P10_D2_P4_Low_T3_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1648.7192800000005 seconds to 1664.14228 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P2_Low_T1_25_Left", "images": ["Image/P18_D2_P2_Low_T1_25_Left/1.png", "Image/P18_D2_P2_Low_T1_25_Left/2.png", "Image/P18_D2_P2_Low_T1_25_Left/3.png", "Image/P18_D2_P2_Low_T1_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 103.02439000000004 seconds to 113.36664000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P9_Medium_T2_17_Right", "images": ["Image/P28_D3_P9_Medium_T2_17_Right/1.png", "Image/P28_D3_P9_Medium_T2_17_Right/2.png", "Image/P28_D3_P9_Medium_T2_17_Right/3.png", "Image/P28_D3_P9_Medium_T2_17_Right/4.png", "Image/P28_D3_P9_Medium_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 127.09767 seconds to 137.58118000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P3_High_T2_10_Right", "images": ["Image/P10_D2_P3_High_T2_10_Right/1.png", "Image/P10_D2_P3_High_T2_10_Right/2.png", "Image/P10_D2_P3_High_T2_10_Right/3.png", "Image/P10_D2_P3_High_T2_10_Right/4.png", "Image/P10_D2_P3_High_T2_10_Right/5.png", "Image/P10_D2_P3_High_T2_10_Right/6.png", "Image/P10_D2_P3_High_T2_10_Right/7.png", "Image/P10_D2_P3_High_T2_10_Right/8.png", "Image/P10_D2_P3_High_T2_10_Right/9.png", "Image/P10_D2_P3_High_T2_10_Right/10.png", "Image/P10_D2_P3_High_T2_10_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1110.3114 seconds to 1128.18423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P4_Medium_T3_5_Right", "images": ["Image/P14_D3_P4_Medium_T3_5_Right/1.png", "Image/P14_D3_P4_Medium_T3_5_Right/2.png", "Image/P14_D3_P4_Medium_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1356.65186 seconds to 1365.45686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P3_Low_T1_1_Left", "images": ["Image/P26_D1_P3_Low_T1_1_Left/1.png", "Image/P26_D1_P3_Low_T1_1_Left/2.png", "Image/P26_D1_P3_Low_T1_1_Left/3.png", "Image/P26_D1_P3_Low_T1_1_Left/4.png", "Image/P26_D1_P3_Low_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this mug down here? Finish\"\nThe frames roughly correspond to the speech interval from 574.6936999999999 seconds to 585.6207999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P3_High_T2_25_Left", "images": ["Image/P26_D3_P3_High_T2_25_Left/1.png", "Image/P26_D3_P3_High_T2_25_Left/2.png", "Image/P26_D3_P3_High_T2_25_Left/3.png", "Image/P26_D3_P3_High_T2_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour water into that white vase over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1148.9951 seconds to 1160.7587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P3_High_T3_14_Left", "images": ["Image/P14_D2_P3_High_T3_14_Left/1.png", "Image/P14_D2_P3_High_T3_14_Left/2.png", "Image/P14_D2_P3_High_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1793.5958999999998 seconds to 1801.9022999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P5_High_T3_5_Right", "images": ["Image/P3_D2_P5_High_T3_5_Right/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you move the item to this space?\"\nThe frames roughly correspond to the speech interval from 2356.2034000000003 seconds to 2364.8359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P1_Low_T2_21_Right", "images": ["Image/P4_D3_P1_Low_T2_21_Right/1.png", "Image/P4_D3_P1_Low_T2_21_Right/2.png", "Image/P4_D3_P1_Low_T2_21_Right/3.png", "Image/P4_D3_P1_Low_T2_21_Right/4.png", "Image/P4_D3_P1_Low_T2_21_Right/5.png", "Image/P4_D3_P1_Low_T2_21_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in that white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 578.7787999999999 seconds to 591.7538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P4_High_T1_9_Left", "images": ["Image/P23_D3_P4_High_T1_9_Left/1.png", "Image/P23_D3_P4_High_T1_9_Left/2.png", "Image/P23_D3_P4_High_T1_9_Left/3.png", "Image/P23_D3_P4_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 884.6781000000001 seconds to 894.3803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P1_High_T1_25_Left", "images": ["Image/P23_D1_P1_High_T1_25_Left/1.png", "Image/P23_D1_P1_High_T1_25_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Pour water there. Finish.\"\nThe frames roughly correspond to the speech interval from 1509.06734 seconds to 1517.5853499999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P1_Low_T2_19_Left", "images": ["Image/P5_D2_P1_Low_T2_19_Left/1.png", "Image/P5_D2_P1_Low_T2_19_Left/2.png", "Image/P5_D2_P1_Low_T2_19_Left/3.png", "Image/P5_D2_P1_Low_T2_19_Left/4.png", "Image/P5_D2_P1_Low_T2_19_Left/5.png", "Image/P5_D2_P1_Low_T2_19_Left/6.png", "Image/P5_D2_P1_Low_T2_19_Left/7.png", "Image/P5_D2_P1_Low_T2_19_Left/8.png", "Image/P5_D2_P1_Low_T2_19_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour water into the bottle in front of you on the top of the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 67.86126999999999 seconds to 83.60336999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P7_High_T1_24_Right", "images": ["Image/P2_D2_P7_High_T1_24_Right/1.png", "Image/P2_D2_P7_High_T1_24_Right/2.png", "Image/P2_D2_P7_High_T1_24_Right/3.png", "Image/P2_D2_P7_High_T1_24_Right/4.png", "Image/P2_D2_P7_High_T1_24_Right/5.png", "Image/P2_D2_P7_High_T1_24_Right/6.png", "Image/P2_D2_P7_High_T1_24_Right/7.png", "Image/P2_D2_P7_High_T1_24_Right/8.png", "Image/P2_D2_P7_High_T1_24_Right/9.png", "Image/P2_D2_P7_High_T1_24_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2588.9163 seconds to 2606.3431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P4_Low_T2_7_Left", "images": ["Image/P26_D1_P4_Low_T2_7_Left/1.png", "Image/P26_D1_P4_Low_T2_7_Left/2.png", "Image/P26_D1_P4_Low_T2_7_Left/3.png", "Image/P26_D1_P4_Low_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in here? Finish\"\nThe frames roughly correspond to the speech interval from 475.362 seconds to 485.46629999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P4_Medium_T3_26_Left", "images": ["Image/P16_D2_P4_Medium_T3_26_Left/1.png", "Image/P16_D2_P4_Medium_T3_26_Left/2.png", "Image/P16_D2_P4_Medium_T3_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 955.7036 seconds to 965.1287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P9_Low_T1_6_Right", "images": ["Image/P22_D1_P9_Low_T1_6_Right/1.png", "Image/P22_D1_P9_Low_T1_6_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Move the plant up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1672.5481 seconds to 1680.5656 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P9_Medium_T2_26_Right", "images": ["Image/P17_D2_P9_Medium_T2_26_Right/1.png", "Image/P17_D2_P9_Medium_T2_26_Right/2.png", "Image/P17_D2_P9_Medium_T2_26_Right/3.png", "Image/P17_D2_P9_Medium_T2_26_Right/4.png", "Image/P17_D2_P9_Medium_T2_26_Right/5.png", "Image/P17_D2_P9_Medium_T2_26_Right/6.png", "Image/P17_D2_P9_Medium_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that mug down there on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 183.7801 seconds to 196.8657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P5_Medium_T1_11_Mid", "images": ["Image/P6_D2_P5_Medium_T1_11_Mid/1.png", "Image/P6_D2_P5_Medium_T1_11_Mid/2.png", "Image/P6_D2_P5_Medium_T1_11_Mid/3.png", "Image/P6_D2_P5_Medium_T1_11_Mid/4.png", "Image/P6_D2_P5_Medium_T1_11_Mid/5.png", "Image/P6_D2_P5_Medium_T1_11_Mid/6.png", "Image/P6_D2_P5_Medium_T1_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1031.7812 seconds to 1050.993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P8_Low_T1_2_Left", "images": ["Image/P3_D2_P8_Low_T1_2_Left/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2286.5365 seconds to 2297.2693 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P1_Low_T1_11_Mid", "images": ["Image/P30_D1_P1_Low_T1_11_Mid/1.png", "Image/P30_D1_P1_Low_T1_11_Mid/2.png", "Image/P30_D1_P1_Low_T1_11_Mid/3.png", "Image/P30_D1_P1_Low_T1_11_Mid/4.png", "Image/P30_D1_P1_Low_T1_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this face down here. Finish.\"\nThe frames roughly correspond to the speech interval from 667.0613900000001 seconds to 676.6523400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P9_Low_T3_23_Right", "images": ["Image/P21_D3_P9_Low_T3_23_Right/1.png", "Image/P21_D3_P9_Low_T3_23_Right/2.png", "Image/P21_D3_P9_Low_T3_23_Right/3.png", "Image/P21_D3_P9_Low_T3_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 402.0657 seconds to 414.2026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P7_High_T2_1_Mid", "images": ["Image/P25_D1_P7_High_T2_1_Mid/1.png", "Image/P25_D1_P7_High_T2_1_Mid/2.png", "Image/P25_D1_P7_High_T2_1_Mid/3.png", "Image/P25_D1_P7_High_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 404.3985 seconds to 413.9607 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P7_Medium_T1_25_Right", "images": ["Image/P24_D1_P7_Medium_T1_25_Right/1.png", "Image/P24_D1_P7_Medium_T1_25_Right/2.png", "Image/P24_D1_P7_Medium_T1_25_Right/3.png", "Image/P24_D1_P7_Medium_T1_25_Right/4.png", "Image/P24_D1_P7_Medium_T1_25_Right/5.png", "Image/P24_D1_P7_Medium_T1_25_Right/6.png", "Image/P24_D1_P7_Medium_T1_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this cup over here\"\nThe frames roughly correspond to the speech interval from 1155.0037 seconds to 1168.3948 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P4_High_T1_8_Mid", "images": ["Image/P8_D3_P4_High_T1_8_Mid/1.png", "Image/P8_D3_P4_High_T1_8_Mid/2.png", "Image/P8_D3_P4_High_T1_8_Mid/3.png", "Image/P8_D3_P4_High_T1_8_Mid/4.png", "Image/P8_D3_P4_High_T1_8_Mid/5.png", "Image/P8_D3_P4_High_T1_8_Mid/6.png", "Image/P8_D3_P4_High_T1_8_Mid/7.png", "Image/P8_D3_P4_High_T1_8_Mid/8.png", "Image/P8_D3_P4_High_T1_8_Mid/9.png", "Image/P8_D3_P4_High_T1_8_Mid/10.png", "Image/P8_D3_P4_High_T1_8_Mid/11.png", "Image/P8_D3_P4_High_T1_8_Mid/12.png", "Image/P8_D3_P4_High_T1_8_Mid/13.png", "Image/P8_D3_P4_High_T1_8_Mid/14.png", "Image/P8_D3_P4_High_T1_8_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi. Star. Hi. Could you move the green glass to the wooden shelf? I mean the right side. And it is the second level\"\nThe frames roughly correspond to the speech interval from 1899.1545 seconds to 1920.3958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P7_Low_T3_27_Left", "images": ["Image/P7_D3_P7_Low_T3_27_Left/1.png", "Image/P7_D3_P7_Low_T3_27_Left/2.png", "Image/P7_D3_P7_Low_T3_27_Left/3.png", "Image/P7_D3_P7_Low_T3_27_Left/4.png", "Image/P7_D3_P7_Low_T3_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey Friday\"\nThe frames roughly correspond to the speech interval from 1908.3361 seconds to 1921.435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P5_Low_T3_3_Left", "images": ["Image/P23_D2_P5_Low_T3_3_Left/1.png", "Image/P23_D2_P5_Low_T3_3_Left/2.png", "Image/P23_D2_P5_Low_T3_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 360.3538999999999 seconds to 369.1648 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P6_High_T2_8_Left", "images": ["Image/P14_D2_P6_High_T2_8_Left/1.png", "Image/P14_D2_P6_High_T2_8_Left/2.png", "Image/P14_D2_P6_High_T2_8_Left/3.png", "Image/P14_D2_P6_High_T2_8_Left/4.png", "Image/P14_D2_P6_High_T2_8_Left/5.png", "Image/P14_D2_P6_High_T2_8_Left/6.png", "Image/P14_D2_P6_High_T2_8_Left/7.png", "Image/P14_D2_P6_High_T2_8_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1632.6944 seconds to 1646.399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P6_High_T2_24_Right", "images": ["Image/P8_D2_P6_High_T2_24_Right/1.png", "Image/P8_D2_P6_High_T2_24_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"water the planet. Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2532.1912 seconds to 2542.755 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P6_Medium_T2_6_Left", "images": ["Image/P18_D2_P6_Medium_T2_6_Left/1.png", "Image/P18_D2_P6_Medium_T2_6_Left/2.png", "Image/P18_D2_P6_Medium_T2_6_Left/3.png", "Image/P18_D2_P6_Medium_T2_6_Left/4.png", "Image/P18_D2_P6_Medium_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the first plant that's separate from the other two. Finish.\"\nThe frames roughly correspond to the speech interval from 335.1118 seconds to 347.0403 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P7_Medium_T1_7_Mid", "images": ["Image/P7_D1_P7_Medium_T1_7_Mid/1.png", "Image/P7_D1_P7_Medium_T1_7_Mid/2.png", "Image/P7_D1_P7_Medium_T1_7_Mid/3.png", "Image/P7_D1_P7_Medium_T1_7_Mid/4.png", "Image/P7_D1_P7_Medium_T1_7_Mid/5.png", "Image/P7_D1_P7_Medium_T1_7_Mid/6.png", "Image/P7_D1_P7_Medium_T1_7_Mid/7.png", "Image/P7_D1_P7_Medium_T1_7_Mid/8.png", "Image/P7_D1_P7_Medium_T1_7_Mid/9.png", "Image/P7_D1_P7_Medium_T1_7_Mid/10.png", "Image/P7_D1_P7_Medium_T1_7_Mid/11.png", "Image/P7_D1_P7_Medium_T1_7_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 156.1072 seconds to 176.1376 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P3_Low_T1_5_Mid", "images": ["Image/P10_D2_P3_Low_T1_5_Mid/1.png", "Image/P10_D2_P3_Low_T1_5_Mid/2.png", "Image/P10_D2_P3_Low_T1_5_Mid/3.png", "Image/P10_D2_P3_Low_T1_5_Mid/4.png", "Image/P10_D2_P3_Low_T1_5_Mid/5.png", "Image/P10_D2_P3_Low_T1_5_Mid/6.png", "Image/P10_D2_P3_Low_T1_5_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1434.70208 seconds to 1446.7130800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P5_High_T3_12_Left", "images": ["Image/P17_D1_P5_High_T3_12_Left/1.png", "Image/P17_D1_P5_High_T3_12_Left/2.png", "Image/P17_D1_P5_High_T3_12_Left/3.png", "Image/P17_D1_P5_High_T3_12_Left/4.png", "Image/P17_D1_P5_High_T3_12_Left/5.png", "Image/P17_D1_P5_High_T3_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this brown object next to the green mug? Finish\"\nThe frames roughly correspond to the speech interval from 1764.4428000000005 seconds to 1781.9257000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P9_Medium_T1_16_Left", "images": ["Image/P14_D3_P9_Medium_T1_16_Left/1.png", "Image/P14_D3_P9_Medium_T1_16_Left/2.png", "Image/P14_D3_P9_Medium_T1_16_Left/3.png", "Image/P14_D3_P9_Medium_T1_16_Left/4.png", "Image/P14_D3_P9_Medium_T1_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1290.96716 seconds to 1300.7297600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P2_Medium_T2_18_Right", "images": ["Image/P21_D2_P2_Medium_T2_18_Right/1.png", "Image/P21_D2_P2_Medium_T2_18_Right/2.png", "Image/P21_D2_P2_Medium_T2_18_Right/3.png", "Image/P21_D2_P2_Medium_T2_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill the right cup with water. Finish.\"\nThe frames roughly correspond to the speech interval from 1585.2782000000002 seconds to 1595.8294 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P8_High_T2_11_Right", "images": ["Image/P7_D3_P8_High_T2_11_Right/1.png", "Image/P7_D3_P8_High_T2_11_Right/2.png", "Image/P7_D3_P8_High_T2_11_Right/3.png", "Image/P7_D3_P8_High_T2_11_Right/4.png", "Image/P7_D3_P8_High_T2_11_Right/5.png", "Image/P7_D3_P8_High_T2_11_Right/6.png", "Image/P7_D3_P8_High_T2_11_Right/7.png", "Image/P7_D3_P8_High_T2_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 2138.7801 seconds to 2152.6171000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P8_High_T1_6_Mid", "images": ["Image/P24_D2_P8_High_T1_6_Mid/1.png", "Image/P24_D2_P8_High_T1_6_Mid/2.png", "Image/P24_D2_P8_High_T1_6_Mid/3.png", "Image/P24_D2_P8_High_T1_6_Mid/4.png", "Image/P24_D2_P8_High_T1_6_Mid/5.png", "Image/P24_D2_P8_High_T1_6_Mid/6.png", "Image/P24_D2_P8_High_T1_6_Mid/7.png", "Image/P24_D2_P8_High_T1_6_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this planet up to this shelf\"\nThe frames roughly correspond to the speech interval from 464.8329 seconds to 480.06139999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P8_High_T1_18_Left", "images": ["Image/P1_D3_P8_High_T1_18_Left/1.png", "Image/P1_D3_P8_High_T1_18_Left/2.png", "Image/P1_D3_P8_High_T1_18_Left/3.png", "Image/P1_D3_P8_High_T1_18_Left/4.png", "Image/P1_D3_P8_High_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start can help move the left cup to there\"\nThe frames roughly correspond to the speech interval from 1497.8638 seconds to 1509.6573 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P6_Medium_T2_6_Left", "images": ["Image/P23_D2_P6_Medium_T2_6_Left/1.png", "Image/P23_D2_P6_Medium_T2_6_Left/2.png", "Image/P23_D2_P6_Medium_T2_6_Left/3.png", "Image/P23_D2_P6_Medium_T2_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nSome frames may include a red triangle marker when valid gaze data is available; when present, it indicates the user's estimated gaze point (the average gaze position within ±0.5s of that frame), which may help identify the referred object.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant to the left. Finish.\"\nThe frames roughly correspond to the speech interval from 159.29290000000003 seconds to 169.4674 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
