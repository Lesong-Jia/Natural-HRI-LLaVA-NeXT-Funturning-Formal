{"id": "P2_D2_P6_Low_T1_2_Mid", "images": ["Image/P2_D2_P6_Low_T1_2_Mid/1.png", "Image/P2_D2_P6_Low_T1_2_Mid/2.png", "Image/P2_D2_P6_Low_T1_2_Mid/3.png", "Image/P2_D2_P6_Low_T1_2_Mid/4.png", "Image/P2_D2_P6_Low_T1_2_Mid/5.png", "Image/P2_D2_P6_Low_T1_2_Mid/6.png", "Image/P2_D2_P6_Low_T1_2_Mid/7.png", "Image/P2_D2_P6_Low_T1_2_Mid/8.png", "Image/P2_D2_P6_Low_T1_2_Mid/9.png", "Image/P2_D2_P6_Low_T1_2_Mid/10.png", "Image/P2_D2_P6_Low_T1_2_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that green bottle? To that shelf. Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2445.10858 seconds to 2461.10229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P7_High_T2_8_Left", "images": ["Image/P19_D1_P7_High_T2_8_Left/1.png", "Image/P19_D1_P7_High_T2_8_Left/2.png", "Image/P19_D1_P7_High_T2_8_Left/3.png", "Image/P19_D1_P7_High_T2_8_Left/4.png", "Image/P19_D1_P7_High_T2_8_Left/5.png", "Image/P19_D1_P7_High_T2_8_Left/6.png", "Image/P19_D1_P7_High_T2_8_Left/7.png", "Image/P19_D1_P7_High_T2_8_Left/8.png", "Image/P19_D1_P7_High_T2_8_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour from this container and do that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 689.5612 seconds to 709.8978999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P7_High_T3_25_Mid", "images": ["Image/P18_D1_P7_High_T3_25_Mid/1.png", "Image/P18_D1_P7_High_T3_25_Mid/2.png", "Image/P18_D1_P7_High_T3_25_Mid/3.png", "Image/P18_D1_P7_High_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the cook. Finish.\"\nThe frames roughly correspond to the speech interval from 658.57348 seconds to 668.76206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P4_High_T2_2_Left", "images": ["Image/P22_D3_P4_High_T2_2_Left/1.png", "Image/P22_D3_P4_High_T2_2_Left/2.png", "Image/P22_D3_P4_High_T2_2_Left/3.png", "Image/P22_D3_P4_High_T2_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water in the green jug. Finish.\"\nThe frames roughly correspond to the speech interval from 211.3037 seconds to 222.6616 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P5_Low_T1_24_Left", "images": ["Image/P30_D1_P5_Low_T1_24_Left/1.png", "Image/P30_D1_P5_Low_T1_24_Left/2.png", "Image/P30_D1_P5_Low_T1_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1009.5466 seconds to 1017.9808 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P3_Low_T3_12_Right", "images": ["Image/P11_D1_P3_Low_T3_12_Right/1.png", "Image/P11_D1_P3_Low_T3_12_Right/2.png", "Image/P11_D1_P3_Low_T3_12_Right/3.png", "Image/P11_D1_P3_Low_T3_12_Right/4.png", "Image/P11_D1_P3_Low_T3_12_Right/5.png", "Image/P11_D1_P3_Low_T3_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the red bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2890.8374999999996 seconds to 2902.5703999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P3_Low_T2_26_Right", "images": ["Image/P8_D1_P3_Low_T2_26_Right/1.png", "Image/P8_D1_P3_Low_T2_26_Right/2.png", "Image/P8_D1_P3_Low_T2_26_Right/3.png", "Image/P8_D1_P3_Low_T2_26_Right/4.png", "Image/P8_D1_P3_Low_T2_26_Right/5.png", "Image/P8_D1_P3_Low_T2_26_Right/6.png", "Image/P8_D1_P3_Low_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. There's a white cap here. Could you pour some water to it? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 208.5261999999999 seconds to 221.74419999999992 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P4_High_T2_18_Left", "images": ["Image/P9_D2_P4_High_T2_18_Left/1.png", "Image/P9_D2_P4_High_T2_18_Left/2.png", "Image/P9_D2_P4_High_T2_18_Left/3.png", "Image/P9_D2_P4_High_T2_18_Left/4.png", "Image/P9_D2_P4_High_T2_18_Left/5.png", "Image/P9_D2_P4_High_T2_18_Left/6.png", "Image/P9_D2_P4_High_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the right cup on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2945.0141 seconds to 2958.7754999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P2_Medium_T1_13_Right", "images": ["Image/P24_D2_P2_Medium_T1_13_Right/1.png", "Image/P24_D2_P2_Medium_T1_13_Right/2.png", "Image/P24_D2_P2_Medium_T1_13_Right/3.png", "Image/P24_D2_P2_Medium_T1_13_Right/4.png", "Image/P24_D2_P2_Medium_T1_13_Right/5.png", "Image/P24_D2_P2_Medium_T1_13_Right/6.png", "Image/P24_D2_P2_Medium_T1_13_Right/7.png", "Image/P24_D2_P2_Medium_T1_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this planet down to this area\"\nThe frames roughly correspond to the speech interval from 488.47 seconds to 501.1363 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P7_High_T3_18_Right", "images": ["Image/P16_D1_P7_High_T3_18_Right/1.png", "Image/P16_D1_P7_High_T3_18_Right/2.png", "Image/P16_D1_P7_High_T3_18_Right/3.png", "Image/P16_D1_P7_High_T3_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1173.48933 seconds to 1185.40486 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P1_Low_T2_22_Right", "images": ["Image/P8_D1_P1_Low_T2_22_Right/1.png", "Image/P8_D1_P1_Low_T2_22_Right/2.png", "Image/P8_D1_P1_Low_T2_22_Right/3.png", "Image/P8_D1_P1_Low_T2_22_Right/4.png", "Image/P8_D1_P1_Low_T2_22_Right/5.png", "Image/P8_D1_P1_Low_T2_22_Right/6.png", "Image/P8_D1_P1_Low_T2_22_Right/7.png", "Image/P8_D1_P1_Low_T2_22_Right/8.png", "Image/P8_D1_P1_Low_T2_22_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 171.82389999999992 seconds to 187.79279999999991 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P5_Medium_T1_19_Right", "images": ["Image/P13_D3_P5_Medium_T1_19_Right/1.png", "Image/P13_D3_P5_Medium_T1_19_Right/2.png", "Image/P13_D3_P5_Medium_T1_19_Right/3.png", "Image/P13_D3_P5_Medium_T1_19_Right/4.png", "Image/P13_D3_P5_Medium_T1_19_Right/5.png", "Image/P13_D3_P5_Medium_T1_19_Right/6.png", "Image/P13_D3_P5_Medium_T1_19_Right/7.png", "Image/P13_D3_P5_Medium_T1_19_Right/8.png", "Image/P13_D3_P5_Medium_T1_19_Right/9.png", "Image/P13_D3_P5_Medium_T1_19_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move the right white bottle from there over to that shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1465.8264 seconds to 1481.054 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P6_Low_T1_11_Left", "images": ["Image/P29_D2_P6_Low_T1_11_Left/1.png", "Image/P29_D2_P6_Low_T1_11_Left/2.png", "Image/P29_D2_P6_Low_T1_11_Left/3.png", "Image/P29_D2_P6_Low_T1_11_Left/4.png", "Image/P29_D2_P6_Low_T1_11_Left/5.png", "Image/P29_D2_P6_Low_T1_11_Left/6.png", "Image/P29_D2_P6_Low_T1_11_Left/7.png", "Image/P29_D2_P6_Low_T1_11_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 207.0361 seconds to 221.2072 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P8_High_T3_19_Left", "images": ["Image/P3_D3_P8_High_T3_19_Left/1.png", "Image/P3_D3_P8_High_T3_19_Left/2.png", "Image/P3_D3_P8_High_T3_19_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 938.4168 seconds to 947.5497 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P2_High_T3_7_Left", "images": ["Image/P15_D3_P2_High_T3_7_Left/1.png", "Image/P15_D3_P2_High_T3_7_Left/2.png", "Image/P15_D3_P2_High_T3_7_Left/3.png", "Image/P15_D3_P2_High_T3_7_Left/4.png", "Image/P15_D3_P2_High_T3_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 487.9137 seconds to 502.2641 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P5_Low_T3_5_Left", "images": ["Image/P1_D3_P5_Low_T3_5_Left/1.png", "Image/P1_D3_P5_Low_T3_5_Left/2.png", "Image/P1_D3_P5_Low_T3_5_Left/3.png", "Image/P1_D3_P5_Low_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1554.583 seconds to 1565.4144 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P2_High_T3_10_Right", "images": ["Image/P18_D1_P2_High_T3_10_Right/1.png", "Image/P18_D1_P2_High_T3_10_Right/2.png", "Image/P18_D1_P2_High_T3_10_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1031.4458 seconds to 1039.492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P9_High_T2_9_Mid", "images": ["Image/P10_D2_P9_High_T2_9_Mid/1.png", "Image/P10_D2_P9_High_T2_9_Mid/2.png", "Image/P10_D2_P9_High_T2_9_Mid/3.png", "Image/P10_D2_P9_High_T2_9_Mid/4.png", "Image/P10_D2_P9_High_T2_9_Mid/5.png", "Image/P10_D2_P9_High_T2_9_Mid/6.png", "Image/P10_D2_P9_High_T2_9_Mid/7.png", "Image/P10_D2_P9_High_T2_9_Mid/8.png", "Image/P10_D2_P9_High_T2_9_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1622.38238 seconds to 1637.80678 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P2_Medium_T2_14_Mid", "images": ["Image/P15_D2_P2_Medium_T2_14_Mid/1.png", "Image/P15_D2_P2_Medium_T2_14_Mid/2.png", "Image/P15_D2_P2_Medium_T2_14_Mid/3.png", "Image/P15_D2_P2_Medium_T2_14_Mid/4.png", "Image/P15_D2_P2_Medium_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2878.8255 seconds to 2889.1967999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P2_Low_T3_9_Left", "images": ["Image/P14_D3_P2_Low_T3_9_Left/1.png", "Image/P14_D3_P2_Low_T3_9_Left/2.png", "Image/P14_D3_P2_Low_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1210.24356 seconds to 1218.8257600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P1_Medium_T3_12_Right", "images": ["Image/P21_D3_P1_Medium_T3_12_Right/1.png", "Image/P21_D3_P1_Medium_T3_12_Right/2.png", "Image/P21_D3_P1_Medium_T3_12_Right/3.png", "Image/P21_D3_P1_Medium_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 460.2503 seconds to 470.9131 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P3_Low_T2_24_Left", "images": ["Image/P4_D1_P3_Low_T2_24_Left/1.png", "Image/P4_D1_P3_Low_T2_24_Left/2.png", "Image/P4_D1_P3_Low_T2_24_Left/3.png", "Image/P4_D1_P3_Low_T2_24_Left/4.png", "Image/P4_D1_P3_Low_T2_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 2327.1362 seconds to 2339.2446999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P9_Low_T3_24_Mid", "images": ["Image/P1_D2_P9_Low_T3_24_Mid/1.png", "Image/P1_D2_P9_Low_T3_24_Mid/2.png", "Image/P1_D2_P9_Low_T3_24_Mid/3.png", "Image/P1_D2_P9_Low_T3_24_Mid/4.png", "Image/P1_D2_P9_Low_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 1159.6738 seconds to 1170.1288 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P8_Medium_T3_8_Right", "images": ["Image/P8_D2_P8_Medium_T3_8_Right/1.png", "Image/P8_D2_P8_Medium_T3_8_Right/2.png", "Image/P8_D2_P8_Medium_T3_8_Right/3.png", "Image/P8_D2_P8_Medium_T3_8_Right/4.png", "Image/P8_D2_P8_Medium_T3_8_Right/5.png", "Image/P8_D2_P8_Medium_T3_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the right green cups. Finish.\"\nThe frames roughly correspond to the speech interval from 2512.8133000000003 seconds to 2525.7994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P3_High_T2_26_Left", "images": ["Image/P20_D3_P3_High_T2_26_Left/1.png", "Image/P20_D3_P3_High_T2_26_Left/2.png", "Image/P20_D3_P3_High_T2_26_Left/3.png", "Image/P20_D3_P3_High_T2_26_Left/4.png", "Image/P20_D3_P3_High_T2_26_Left/5.png", "Image/P20_D3_P3_High_T2_26_Left/6.png", "Image/P20_D3_P3_High_T2_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Finished. Hey\"\nThe frames roughly correspond to the speech interval from 1195.38995 seconds to 1209.2544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P2_Medium_T1_3_Mid", "images": ["Image/P22_D3_P2_Medium_T1_3_Mid/1.png", "Image/P22_D3_P2_Medium_T1_3_Mid/2.png", "Image/P22_D3_P2_Medium_T1_3_Mid/3.png", "Image/P22_D3_P2_Medium_T1_3_Mid/4.png", "Image/P22_D3_P2_Medium_T1_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move the middle vase to the floor there. Finish\"\nThe frames roughly correspond to the speech interval from 159.312 seconds to 169.77890000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P3_Medium_T2_21_Mid", "images": ["Image/P13_D1_P3_Medium_T2_21_Mid/1.png", "Image/P13_D1_P3_Medium_T2_21_Mid/2.png", "Image/P13_D1_P3_Medium_T2_21_Mid/3.png", "Image/P13_D1_P3_Medium_T2_21_Mid/4.png", "Image/P13_D1_P3_Medium_T2_21_Mid/5.png", "Image/P13_D1_P3_Medium_T2_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the middle white vase\"\nThe frames roughly correspond to the speech interval from 615.1138000000001 seconds to 627.1122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P8_High_T1_5_Left", "images": ["Image/P4_D1_P8_High_T1_5_Left/1.png", "Image/P4_D1_P8_High_T1_5_Left/2.png", "Image/P4_D1_P8_High_T1_5_Left/3.png", "Image/P4_D1_P8_High_T1_5_Left/4.png", "Image/P4_D1_P8_High_T1_5_Left/5.png", "Image/P4_D1_P8_High_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant here? Finish\"\nThe frames roughly correspond to the speech interval from 2226.7045 seconds to 2241.1643999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P4_Low_T1_4_Mid", "images": ["Image/P12_D2_P4_Low_T1_4_Mid/1.png", "Image/P12_D2_P4_Low_T1_4_Mid/2.png", "Image/P12_D2_P4_Low_T1_4_Mid/3.png", "Image/P12_D2_P4_Low_T1_4_Mid/4.png", "Image/P12_D2_P4_Low_T1_4_Mid/5.png", "Image/P12_D2_P4_Low_T1_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this plant over to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 428.31590000000006 seconds to 441.2621 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P8_Medium_T1_6_Mid", "images": ["Image/P7_D2_P8_Medium_T1_6_Mid/1.png", "Image/P7_D2_P8_Medium_T1_6_Mid/2.png", "Image/P7_D2_P8_Medium_T1_6_Mid/3.png", "Image/P7_D2_P8_Medium_T1_6_Mid/4.png", "Image/P7_D2_P8_Medium_T1_6_Mid/5.png", "Image/P7_D2_P8_Medium_T1_6_Mid/6.png", "Image/P7_D2_P8_Medium_T1_6_Mid/7.png", "Image/P7_D2_P8_Medium_T1_6_Mid/8.png", "Image/P7_D2_P8_Medium_T1_6_Mid/9.png", "Image/P7_D2_P8_Medium_T1_6_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1319.9891000000002 seconds to 1338.8357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P3_Low_T3_10_Mid", "images": ["Image/P21_D1_P3_Low_T3_10_Mid/1.png", "Image/P21_D1_P3_Low_T3_10_Mid/2.png", "Image/P21_D1_P3_Low_T3_10_Mid/3.png", "Image/P21_D1_P3_Low_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1176.4025 seconds to 1187.3015 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P8_High_T1_26_Left", "images": ["Image/P13_D3_P8_High_T1_26_Left/1.png", "Image/P13_D3_P8_High_T1_26_Left/2.png", "Image/P13_D3_P8_High_T1_26_Left/3.png", "Image/P13_D3_P8_High_T1_26_Left/4.png", "Image/P13_D3_P8_High_T1_26_Left/5.png", "Image/P13_D3_P8_High_T1_26_Left/6.png", "Image/P13_D3_P8_High_T1_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup from the bottom to the top shelf\"\nThe frames roughly correspond to the speech interval from 1384.35296 seconds to 1396.64369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P4_Low_T2_18_Right", "images": ["Image/P1_D1_P4_Low_T2_18_Right/1.png", "Image/P1_D1_P4_Low_T2_18_Right/2.png", "Image/P1_D1_P4_Low_T2_18_Right/3.png", "Image/P1_D1_P4_Low_T2_18_Right/4.png", "Image/P1_D1_P4_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 379.609 seconds to 389.5372 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P5_Medium_T2_21_Left", "images": ["Image/P10_D1_P5_Medium_T2_21_Left/1.png", "Image/P10_D1_P5_Medium_T2_21_Left/2.png", "Image/P10_D1_P5_Medium_T2_21_Left/3.png", "Image/P10_D1_P5_Medium_T2_21_Left/4.png", "Image/P10_D1_P5_Medium_T2_21_Left/5.png", "Image/P10_D1_P5_Medium_T2_21_Left/6.png", "Image/P10_D1_P5_Medium_T2_21_Left/7.png", "Image/P10_D1_P5_Medium_T2_21_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2293.3403 seconds to 2307.3719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P2_Medium_T2_12_Left", "images": ["Image/P9_D2_P2_Medium_T2_12_Left/1.png", "Image/P9_D2_P2_Medium_T2_12_Left/2.png", "Image/P9_D2_P2_Medium_T2_12_Left/3.png", "Image/P9_D2_P2_Medium_T2_12_Left/4.png", "Image/P9_D2_P2_Medium_T2_12_Left/5.png", "Image/P9_D2_P2_Medium_T2_12_Left/6.png", "Image/P9_D2_P2_Medium_T2_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2728.87207 seconds to 2742.00753 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P5_High_T2_8_Left", "images": ["Image/P28_D3_P5_High_T2_8_Left/1.png", "Image/P28_D3_P5_High_T2_8_Left/2.png", "Image/P28_D3_P5_High_T2_8_Left/3.png", "Image/P28_D3_P5_High_T2_8_Left/4.png", "Image/P28_D3_P5_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 433.0716 seconds to 444.5966 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P7_Low_T1_22_Mid", "images": ["Image/P30_D1_P7_Low_T1_22_Mid/1.png", "Image/P30_D1_P7_Low_T1_22_Mid/2.png", "Image/P30_D1_P7_Low_T1_22_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1061.3703 seconds to 1069.8469 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P4_Medium_T3_22_Mid", "images": ["Image/P10_D2_P4_Medium_T3_22_Mid/1.png", "Image/P10_D2_P4_Medium_T3_22_Mid/2.png", "Image/P10_D2_P4_Medium_T3_22_Mid/3.png", "Image/P10_D2_P4_Medium_T3_22_Mid/4.png", "Image/P10_D2_P4_Medium_T3_22_Mid/5.png", "Image/P10_D2_P4_Medium_T3_22_Mid/6.png", "Image/P10_D2_P4_Medium_T3_22_Mid/7.png", "Image/P10_D2_P4_Medium_T3_22_Mid/8.png", "Image/P10_D2_P4_Medium_T3_22_Mid/9.png", "Image/P10_D2_P4_Medium_T3_22_Mid/10.png", "Image/P10_D2_P4_Medium_T3_22_Mid/11.png", "Image/P10_D2_P4_Medium_T3_22_Mid/12.png", "Image/P10_D2_P4_Medium_T3_22_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1480.3579800000002 seconds to 1499.27508 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P5_Medium_T2_7_Mid", "images": ["Image/P11_D3_P5_Medium_T2_7_Mid/1.png", "Image/P11_D3_P5_Medium_T2_7_Mid/2.png", "Image/P11_D3_P5_Medium_T2_7_Mid/3.png", "Image/P11_D3_P5_Medium_T2_7_Mid/4.png", "Image/P11_D3_P5_Medium_T2_7_Mid/5.png", "Image/P11_D3_P5_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour the water in the second green glass on the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 1473.2584 seconds to 1485.326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P1_Medium_T2_1_Left", "images": ["Image/P19_D1_P1_Medium_T2_1_Left/1.png", "Image/P19_D1_P1_Medium_T2_1_Left/2.png", "Image/P19_D1_P1_Medium_T2_1_Left/3.png", "Image/P19_D1_P1_Medium_T2_1_Left/4.png", "Image/P19_D1_P1_Medium_T2_1_Left/5.png", "Image/P19_D1_P1_Medium_T2_1_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour from this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1154.8176 seconds to 1166.8723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P7_Low_T3_11_Left", "images": ["Image/P12_D1_P7_Low_T3_11_Left/1.png", "Image/P12_D1_P7_Low_T3_11_Left/2.png", "Image/P12_D1_P7_Low_T3_11_Left/3.png", "Image/P12_D1_P7_Low_T3_11_Left/4.png", "Image/P12_D1_P7_Low_T3_11_Left/5.png", "Image/P12_D1_P7_Low_T3_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown base that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 1237.47311 seconds to 1252.89449 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P9_Low_T3_6_Left", "images": ["Image/P27_D1_P9_Low_T3_6_Left/1.png", "Image/P27_D1_P9_Low_T3_6_Left/2.png", "Image/P27_D1_P9_Low_T3_6_Left/3.png", "Image/P27_D1_P9_Low_T3_6_Left/4.png", "Image/P27_D1_P9_Low_T3_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 938.9351 seconds to 949.6818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P3_High_T1_13_Right", "images": ["Image/P25_D3_P3_High_T1_13_Right/1.png", "Image/P25_D3_P3_High_T1_13_Right/2.png", "Image/P25_D3_P3_High_T1_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down there. Finish.\"\nThe frames roughly correspond to the speech interval from 2071.1621 seconds to 2080.7823 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P3_Low_T1_23_Left", "images": ["Image/P11_D2_P3_Low_T1_23_Left/1.png", "Image/P11_D2_P3_Low_T1_23_Left/2.png", "Image/P11_D2_P3_Low_T1_23_Left/3.png", "Image/P11_D2_P3_Low_T1_23_Left/4.png", "Image/P11_D2_P3_Low_T1_23_Left/5.png", "Image/P11_D2_P3_Low_T1_23_Left/6.png", "Image/P11_D2_P3_Low_T1_23_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please move the plant to the bottom of the shelf on the right side\"\nThe frames roughly correspond to the speech interval from 488.9466 seconds to 503.35899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P7_Medium_T2_12_Right", "images": ["Image/P2_D1_P7_Medium_T2_12_Right/1.png", "Image/P2_D1_P7_Medium_T2_12_Right/2.png", "Image/P2_D1_P7_Medium_T2_12_Right/3.png", "Image/P2_D1_P7_Medium_T2_12_Right/4.png", "Image/P2_D1_P7_Medium_T2_12_Right/5.png", "Image/P2_D1_P7_Medium_T2_12_Right/6.png", "Image/P2_D1_P7_Medium_T2_12_Right/7.png", "Image/P2_D1_P7_Medium_T2_12_Right/8.png", "Image/P2_D1_P7_Medium_T2_12_Right/9.png", "Image/P2_D1_P7_Medium_T2_12_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello. Could you please clean the brown vessel on the very right\"\nThe frames roughly correspond to the speech interval from 409.18549999999993 seconds to 426.9453999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P8_High_T2_11_Right", "images": ["Image/P7_D3_P8_High_T2_11_Right/1.png", "Image/P7_D3_P8_High_T2_11_Right/2.png", "Image/P7_D3_P8_High_T2_11_Right/3.png", "Image/P7_D3_P8_High_T2_11_Right/4.png", "Image/P7_D3_P8_High_T2_11_Right/5.png", "Image/P7_D3_P8_High_T2_11_Right/6.png", "Image/P7_D3_P8_High_T2_11_Right/7.png", "Image/P7_D3_P8_High_T2_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 2138.7801 seconds to 2152.6171000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P6_High_T2_10_Mid", "images": ["Image/P6_D1_P6_High_T2_10_Mid/1.png", "Image/P6_D1_P6_High_T2_10_Mid/2.png", "Image/P6_D1_P6_High_T2_10_Mid/3.png", "Image/P6_D1_P6_High_T2_10_Mid/4.png", "Image/P6_D1_P6_High_T2_10_Mid/5.png", "Image/P6_D1_P6_High_T2_10_Mid/6.png", "Image/P6_D1_P6_High_T2_10_Mid/7.png", "Image/P6_D1_P6_High_T2_10_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1971.6065000000003 seconds to 1984.7577 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P6_Low_T2_8_Left", "images": ["Image/P16_D1_P6_Low_T2_8_Left/1.png", "Image/P16_D1_P6_Low_T2_8_Left/2.png", "Image/P16_D1_P6_Low_T2_8_Left/3.png", "Image/P16_D1_P6_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into this mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1235.25206 seconds to 1245.32938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P5_High_T1_7_Mid", "images": ["Image/P25_D2_P5_High_T1_7_Mid/1.png", "Image/P25_D2_P5_High_T1_7_Mid/2.png", "Image/P25_D2_P5_High_T1_7_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 641.97224 seconds to 650.5733 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P5_Low_T1_9_Mid", "images": ["Image/P19_D1_P5_Low_T1_9_Mid/1.png", "Image/P19_D1_P5_Low_T1_9_Mid/2.png", "Image/P19_D1_P5_Low_T1_9_Mid/3.png", "Image/P19_D1_P5_Low_T1_9_Mid/4.png", "Image/P19_D1_P5_Low_T1_9_Mid/5.png", "Image/P19_D1_P5_Low_T1_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 830.2922 seconds to 847.3984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P8_High_T2_23_Left", "images": ["Image/P7_D1_P8_High_T2_23_Left/1.png", "Image/P7_D1_P8_High_T2_23_Left/2.png", "Image/P7_D1_P8_High_T2_23_Left/3.png", "Image/P7_D1_P8_High_T2_23_Left/4.png", "Image/P7_D1_P8_High_T2_23_Left/5.png", "Image/P7_D1_P8_High_T2_23_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 619.2086 seconds to 634.1438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P9_Medium_T2_23_Left", "images": ["Image/P18_D3_P9_Medium_T2_23_Left/1.png", "Image/P18_D3_P9_Medium_T2_23_Left/2.png", "Image/P18_D3_P9_Medium_T2_23_Left/3.png", "Image/P18_D3_P9_Medium_T2_23_Left/4.png", "Image/P18_D3_P9_Medium_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1582.8332 seconds to 1593.5106 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P2_High_T1_23_Mid", "images": ["Image/P23_D3_P2_High_T1_23_Mid/1.png", "Image/P23_D3_P2_High_T1_23_Mid/2.png", "Image/P23_D3_P2_High_T1_23_Mid/3.png", "Image/P23_D3_P2_High_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 800.0032 seconds to 809.4409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P3_Low_T2_12_Mid", "images": ["Image/P4_D2_P3_Low_T2_12_Mid/1.png", "Image/P4_D2_P3_Low_T2_12_Mid/2.png", "Image/P4_D2_P3_Low_T2_12_Mid/3.png", "Image/P4_D2_P3_Low_T2_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1991.3641 seconds to 2002.3603 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P3_Medium_T1_8_Mid", "images": ["Image/P2_D1_P3_Medium_T1_8_Mid/1.png", "Image/P2_D1_P3_Medium_T1_8_Mid/2.png", "Image/P2_D1_P3_Medium_T1_8_Mid/3.png", "Image/P2_D1_P3_Medium_T1_8_Mid/4.png", "Image/P2_D1_P3_Medium_T1_8_Mid/5.png", "Image/P2_D1_P3_Medium_T1_8_Mid/6.png", "Image/P2_D1_P3_Medium_T1_8_Mid/7.png", "Image/P2_D1_P3_Medium_T1_8_Mid/8.png", "Image/P2_D1_P3_Medium_T1_8_Mid/9.png", "Image/P2_D1_P3_Medium_T1_8_Mid/10.png", "Image/P2_D1_P3_Medium_T1_8_Mid/11.png", "Image/P2_D1_P3_Medium_T1_8_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"start. Start. Hello\"\nThe frames roughly correspond to the speech interval from 699.6238999999999 seconds to 713.4861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P5_High_T3_3_Right", "images": ["Image/P26_D2_P5_High_T3_3_Right/1.png", "Image/P26_D2_P5_High_T3_3_Right/2.png", "Image/P26_D2_P5_High_T3_3_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the green vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1646.3727 seconds to 1655.4887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P5_Low_T3_18_Mid", "images": ["Image/P25_D3_P5_Low_T3_18_Mid/1.png", "Image/P25_D3_P5_Low_T3_18_Mid/2.png", "Image/P25_D3_P5_Low_T3_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1740.37555 seconds to 1749.31316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P5_High_T3_26_Mid", "images": ["Image/P22_D2_P5_High_T3_26_Mid/1.png", "Image/P22_D2_P5_High_T3_26_Mid/2.png", "Image/P22_D2_P5_High_T3_26_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1001.4661 seconds to 1010.2487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P5_High_T1_17_Left", "images": ["Image/P11_D1_P5_High_T1_17_Left/1.png", "Image/P11_D1_P5_High_T1_17_Left/2.png", "Image/P11_D1_P5_High_T1_17_Left/3.png", "Image/P11_D1_P5_High_T1_17_Left/4.png", "Image/P11_D1_P5_High_T1_17_Left/5.png", "Image/P11_D1_P5_High_T1_17_Left/6.png", "Image/P11_D1_P5_High_T1_17_Left/7.png", "Image/P11_D1_P5_High_T1_17_Left/8.png", "Image/P11_D1_P5_High_T1_17_Left/9.png", "Image/P11_D1_P5_High_T1_17_Left/10.png", "Image/P11_D1_P5_High_T1_17_Left/11.png", "Image/P11_D1_P5_High_T1_17_Left/12.png", "Image/P11_D1_P5_High_T1_17_Left/13.png", "Image/P11_D1_P5_High_T1_17_Left/14.png", "Image/P11_D1_P5_High_T1_17_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please move the breath Bottle to the right side of the shelf\"\nThe frames roughly correspond to the speech interval from 2586.9228999999996 seconds to 2610.4185 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P6_Medium_T1_20_Left", "images": ["Image/P29_D1_P6_Medium_T1_20_Left/1.png", "Image/P29_D1_P6_Medium_T1_20_Left/2.png", "Image/P29_D1_P6_Medium_T1_20_Left/3.png", "Image/P29_D1_P6_Medium_T1_20_Left/4.png", "Image/P29_D1_P6_Medium_T1_20_Left/5.png", "Image/P29_D1_P6_Medium_T1_20_Left/6.png", "Image/P29_D1_P6_Medium_T1_20_Left/7.png", "Image/P29_D1_P6_Medium_T1_20_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2814.1643 seconds to 2827.6659999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P8_Medium_T2_21_Right", "images": ["Image/P5_D3_P8_Medium_T2_21_Right/1.png", "Image/P5_D3_P8_Medium_T2_21_Right/2.png", "Image/P5_D3_P8_Medium_T2_21_Right/3.png", "Image/P5_D3_P8_Medium_T2_21_Right/4.png", "Image/P5_D3_P8_Medium_T2_21_Right/5.png", "Image/P5_D3_P8_Medium_T2_21_Right/6.png", "Image/P5_D3_P8_Medium_T2_21_Right/7.png", "Image/P5_D3_P8_Medium_T2_21_Right/8.png", "Image/P5_D3_P8_Medium_T2_21_Right/9.png", "Image/P5_D3_P8_Medium_T2_21_Right/10.png", "Image/P5_D3_P8_Medium_T2_21_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water. into the bottle into the rightmost bottle on the ground. Finished.\"\nThe frames roughly correspond to the speech interval from 2811.0841 seconds to 2827.1346 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P9_Low_T3_23_Right", "images": ["Image/P21_D3_P9_Low_T3_23_Right/1.png", "Image/P21_D3_P9_Low_T3_23_Right/2.png", "Image/P21_D3_P9_Low_T3_23_Right/3.png", "Image/P21_D3_P9_Low_T3_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 402.0657 seconds to 414.2026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P1_High_T1_14_Mid", "images": ["Image/P1_D3_P1_High_T1_14_Mid/1.png", "Image/P1_D3_P1_High_T1_14_Mid/2.png", "Image/P1_D3_P1_High_T1_14_Mid/3.png", "Image/P1_D3_P1_High_T1_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1643.3775999999998 seconds to 1653.0526999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P5_High_T2_9_Mid", "images": ["Image/P4_D3_P5_High_T2_9_Mid/1.png", "Image/P4_D3_P5_High_T2_9_Mid/2.png", "Image/P4_D3_P5_High_T2_9_Mid/3.png", "Image/P4_D3_P5_High_T2_9_Mid/4.png", "Image/P4_D3_P5_High_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you fill in the green bottle? Finish\"\nThe frames roughly correspond to the speech interval from 748.4604999999999 seconds to 760.3897 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P1_Low_T2_14_Left", "images": ["Image/P16_D3_P1_Low_T2_14_Left/1.png", "Image/P16_D3_P1_Low_T2_14_Left/2.png", "Image/P16_D3_P1_Low_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 516.3806999999999 seconds to 524.5783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P3_Medium_T2_25_Mid", "images": ["Image/P20_D2_P3_Medium_T2_25_Mid/1.png", "Image/P20_D2_P3_Medium_T2_25_Mid/2.png", "Image/P20_D2_P3_Medium_T2_25_Mid/3.png", "Image/P20_D2_P3_Medium_T2_25_Mid/4.png", "Image/P20_D2_P3_Medium_T2_25_Mid/5.png", "Image/P20_D2_P3_Medium_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into the middle cup on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2916.2078 seconds to 2927.6346000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P3_High_T2_21_Right", "images": ["Image/P7_D3_P3_High_T2_21_Right/1.png", "Image/P7_D3_P3_High_T2_21_Right/2.png", "Image/P7_D3_P3_High_T2_21_Right/3.png", "Image/P7_D3_P3_High_T2_21_Right/4.png", "Image/P7_D3_P3_High_T2_21_Right/5.png", "Image/P7_D3_P3_High_T2_21_Right/6.png", "Image/P7_D3_P3_High_T2_21_Right/7.png", "Image/P7_D3_P3_High_T2_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1633.22261 seconds to 1648.1443800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P4_High_T3_25_Left", "images": ["Image/P8_D2_P4_High_T3_25_Left/1.png", "Image/P8_D2_P4_High_T3_25_Left/2.png", "Image/P8_D2_P4_High_T3_25_Left/3.png", "Image/P8_D2_P4_High_T3_25_Left/4.png", "Image/P8_D2_P4_High_T3_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean that white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 2239.8589 seconds to 2249.9779000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P7_Low_T3_14_Mid", "images": ["Image/P6_D3_P7_Low_T3_14_Mid/1.png", "Image/P6_D3_P7_Low_T3_14_Mid/2.png", "Image/P6_D3_P7_Low_T3_14_Mid/3.png", "Image/P6_D3_P7_Low_T3_14_Mid/4.png", "Image/P6_D3_P7_Low_T3_14_Mid/5.png", "Image/P6_D3_P7_Low_T3_14_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 3630.6472 seconds to 3645.2348 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P3_Low_T3_8_Right", "images": ["Image/P13_D2_P3_Low_T3_8_Right/1.png", "Image/P13_D2_P3_Low_T3_8_Right/2.png", "Image/P13_D2_P3_Low_T3_8_Right/3.png", "Image/P13_D2_P3_Low_T3_8_Right/4.png", "Image/P13_D2_P3_Low_T3_8_Right/5.png", "Image/P13_D2_P3_Low_T3_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the green glass over here\"\nThe frames roughly correspond to the speech interval from 859.92346 seconds to 870.47396 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P2_High_T3_7_Right", "images": ["Image/P19_D1_P2_High_T3_7_Right/1.png", "Image/P19_D1_P2_High_T3_7_Right/2.png", "Image/P19_D1_P2_High_T3_7_Right/3.png", "Image/P19_D1_P2_High_T3_7_Right/4.png", "Image/P19_D1_P2_High_T3_7_Right/5.png", "Image/P19_D1_P2_High_T3_7_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 777.8556 seconds to 790.6474000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P9_Low_T2_15_Mid", "images": ["Image/P29_D1_P9_Low_T2_15_Mid/1.png", "Image/P29_D1_P9_Low_T2_15_Mid/2.png", "Image/P29_D1_P9_Low_T2_15_Mid/3.png", "Image/P29_D1_P9_Low_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2755.2846999999997 seconds to 2764.9019999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P2_Low_T2_10_Left", "images": ["Image/P1_D1_P2_Low_T2_10_Left/1.png", "Image/P1_D1_P2_Low_T2_10_Left/2.png", "Image/P1_D1_P2_Low_T2_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this one? Finish.\"\nThe frames roughly correspond to the speech interval from 475.102 seconds to 483.246 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P4_Low_T1_9_Mid", "images": ["Image/P29_D2_P4_Low_T1_9_Mid/1.png", "Image/P29_D2_P4_Low_T1_9_Mid/2.png", "Image/P29_D2_P4_Low_T1_9_Mid/3.png", "Image/P29_D2_P4_Low_T1_9_Mid/4.png", "Image/P29_D2_P4_Low_T1_9_Mid/5.png", "Image/P29_D2_P4_Low_T1_9_Mid/6.png", "Image/P29_D2_P4_Low_T1_9_Mid/7.png", "Image/P29_D2_P4_Low_T1_9_Mid/8.png", "Image/P29_D2_P4_Low_T1_9_Mid/9.png", "Image/P29_D2_P4_Low_T1_9_Mid/10.png", "Image/P29_D2_P4_Low_T1_9_Mid/11.png", "Image/P29_D2_P4_Low_T1_9_Mid/12.png", "Image/P29_D2_P4_Low_T1_9_Mid/13.png", "Image/P29_D2_P4_Low_T1_9_Mid/14.png", "Image/P29_D2_P4_Low_T1_9_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 251.3296 seconds to 268.4932 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P4_High_T3_8_Mid", "images": ["Image/P29_D2_P4_High_T3_8_Mid/1.png", "Image/P29_D2_P4_High_T3_8_Mid/2.png", "Image/P29_D2_P4_High_T3_8_Mid/3.png", "Image/P29_D2_P4_High_T3_8_Mid/4.png", "Image/P29_D2_P4_High_T3_8_Mid/5.png", "Image/P29_D2_P4_High_T3_8_Mid/6.png", "Image/P29_D2_P4_High_T3_8_Mid/7.png", "Image/P29_D2_P4_High_T3_8_Mid/8.png", "Image/P29_D2_P4_High_T3_8_Mid/9.png", "Image/P29_D2_P4_High_T3_8_Mid/10.png", "Image/P29_D2_P4_High_T3_8_Mid/11.png", "Image/P29_D2_P4_High_T3_8_Mid/12.png", "Image/P29_D2_P4_High_T3_8_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 387.1607 seconds to 407.4842 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P9_Medium_T3_9_Left", "images": ["Image/P30_D1_P9_Medium_T3_9_Left/1.png", "Image/P30_D1_P9_Medium_T3_9_Left/2.png", "Image/P30_D1_P9_Medium_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 994.0402 seconds to 1003.0882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P2_Low_T2_14_Right", "images": ["Image/P4_D3_P2_Low_T2_14_Right/1.png", "Image/P4_D3_P2_Low_T2_14_Right/2.png", "Image/P4_D3_P2_Low_T2_14_Right/3.png", "Image/P4_D3_P2_Low_T2_14_Right/4.png", "Image/P4_D3_P2_Low_T2_14_Right/5.png", "Image/P4_D3_P2_Low_T2_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help me watering the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 839.5658999999998 seconds to 852.9696999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P1_Low_T1_11_Mid", "images": ["Image/P30_D1_P1_Low_T1_11_Mid/1.png", "Image/P30_D1_P1_Low_T1_11_Mid/2.png", "Image/P30_D1_P1_Low_T1_11_Mid/3.png", "Image/P30_D1_P1_Low_T1_11_Mid/4.png", "Image/P30_D1_P1_Low_T1_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this face down here. Finish.\"\nThe frames roughly correspond to the speech interval from 667.0613900000001 seconds to 676.6523400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P3_Medium_T2_24_Mid", "images": ["Image/P24_D3_P3_Medium_T2_24_Mid/1.png", "Image/P24_D3_P3_Medium_T2_24_Mid/2.png", "Image/P24_D3_P3_Medium_T2_24_Mid/3.png", "Image/P24_D3_P3_Medium_T2_24_Mid/4.png", "Image/P24_D3_P3_Medium_T2_24_Mid/5.png", "Image/P24_D3_P3_Medium_T2_24_Mid/6.png", "Image/P24_D3_P3_Medium_T2_24_Mid/7.png", "Image/P24_D3_P3_Medium_T2_24_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1650.5437 seconds to 1664.6297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P3_High_T2_7_Right", "images": ["Image/P21_D1_P3_High_T2_7_Right/1.png", "Image/P21_D1_P3_High_T2_7_Right/2.png", "Image/P21_D1_P3_High_T2_7_Right/3.png", "Image/P21_D1_P3_High_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1101.1237999999998 seconds to 1111.8135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P6_Medium_T3_25_Right", "images": ["Image/P12_D2_P6_Medium_T3_25_Right/1.png", "Image/P12_D2_P6_Medium_T3_25_Right/2.png", "Image/P12_D2_P6_Medium_T3_25_Right/3.png", "Image/P12_D2_P6_Medium_T3_25_Right/4.png", "Image/P12_D2_P6_Medium_T3_25_Right/5.png", "Image/P12_D2_P6_Medium_T3_25_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 618.69094 seconds to 630.5635 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P3_High_T2_24_Right", "images": ["Image/P13_D3_P3_High_T2_24_Right/1.png", "Image/P13_D3_P3_High_T2_24_Right/2.png", "Image/P13_D3_P3_High_T2_24_Right/3.png", "Image/P13_D3_P3_High_T2_24_Right/4.png", "Image/P13_D3_P3_High_T2_24_Right/5.png", "Image/P13_D3_P3_High_T2_24_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the plant over here\"\nThe frames roughly correspond to the speech interval from 1830.0965 seconds to 1840.9665 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P5_Low_T3_23_Mid", "images": ["Image/P5_D1_P5_Low_T3_23_Mid/1.png", "Image/P5_D1_P5_Low_T3_23_Mid/2.png", "Image/P5_D1_P5_Low_T3_23_Mid/3.png", "Image/P5_D1_P5_Low_T3_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1857.5929 seconds to 1868.0726 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P5_Medium_T2_26_Mid", "images": ["Image/P19_D1_P5_Medium_T2_26_Mid/1.png", "Image/P19_D1_P5_Medium_T2_26_Mid/2.png", "Image/P19_D1_P5_Medium_T2_26_Mid/3.png", "Image/P19_D1_P5_Medium_T2_26_Mid/4.png", "Image/P19_D1_P5_Medium_T2_26_Mid/5.png", "Image/P19_D1_P5_Medium_T2_26_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour from this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 1102.2703 seconds to 1116.62 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P7_Medium_T3_10_Left", "images": ["Image/P10_D1_P7_Medium_T3_10_Left/1.png", "Image/P10_D1_P7_Medium_T3_10_Left/2.png", "Image/P10_D1_P7_Medium_T3_10_Left/3.png", "Image/P10_D1_P7_Medium_T3_10_Left/4.png", "Image/P10_D1_P7_Medium_T3_10_Left/5.png", "Image/P10_D1_P7_Medium_T3_10_Left/6.png", "Image/P10_D1_P7_Medium_T3_10_Left/7.png", "Image/P10_D1_P7_Medium_T3_10_Left/8.png", "Image/P10_D1_P7_Medium_T3_10_Left/9.png", "Image/P10_D1_P7_Medium_T3_10_Left/10.png", "Image/P10_D1_P7_Medium_T3_10_Left/11.png", "Image/P10_D1_P7_Medium_T3_10_Left/12.png", "Image/P10_D1_P7_Medium_T3_10_Left/13.png", "Image/P10_D1_P7_Medium_T3_10_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1903.13049 seconds to 1926.60057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P3_High_T1_10_Left", "images": ["Image/P24_D3_P3_High_T1_10_Left/1.png", "Image/P24_D3_P3_High_T1_10_Left/2.png", "Image/P24_D3_P3_High_T1_10_Left/3.png", "Image/P24_D3_P3_High_T1_10_Left/4.png", "Image/P24_D3_P3_High_T1_10_Left/5.png", "Image/P24_D3_P3_High_T1_10_Left/6.png", "Image/P24_D3_P3_High_T1_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle down to this area? Finish.\"\nThe frames roughly correspond to the speech interval from 2043.781 seconds to 2056.085 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P3_Medium_T3_24_Left", "images": ["Image/P18_D1_P3_Medium_T3_24_Left/1.png", "Image/P18_D1_P3_Medium_T3_24_Left/2.png", "Image/P18_D1_P3_Medium_T3_24_Left/3.png", "Image/P18_D1_P3_Medium_T3_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up this plant on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 831.8625000000001 seconds to 841.9544000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P6_Medium_T3_6_Mid", "images": ["Image/P25_D2_P6_Medium_T3_6_Mid/1.png", "Image/P25_D2_P6_Medium_T3_6_Mid/2.png", "Image/P25_D2_P6_Medium_T3_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the second plant. Finish.\"\nThe frames roughly correspond to the speech interval from 886.2945 seconds to 894.6738 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P3_Low_T1_19_Right", "images": ["Image/P18_D1_P3_Low_T1_19_Right/1.png", "Image/P18_D1_P3_Low_T1_19_Right/2.png", "Image/P18_D1_P3_Low_T1_19_Right/3.png", "Image/P18_D1_P3_Low_T1_19_Right/4.png", "Image/P18_D1_P3_Low_T1_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down here to the bin in the back. Finish.\"\nThe frames roughly correspond to the speech interval from 611.2328 seconds to 623.14805 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P4_Medium_T3_8_Right", "images": ["Image/P15_D3_P4_Medium_T3_8_Right/1.png", "Image/P15_D3_P4_Medium_T3_8_Right/2.png", "Image/P15_D3_P4_Medium_T3_8_Right/3.png", "Image/P15_D3_P4_Medium_T3_8_Right/4.png", "Image/P15_D3_P4_Medium_T3_8_Right/5.png", "Image/P15_D3_P4_Medium_T3_8_Right/6.png", "Image/P15_D3_P4_Medium_T3_8_Right/7.png", "Image/P15_D3_P4_Medium_T3_8_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the cup at the right side on the table desk. Finish.\"\nThe frames roughly correspond to the speech interval from 377.2928 seconds to 391.8927000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P5_High_T2_16_Right", "images": ["Image/P2_D3_P5_High_T2_16_Right/1.png", "Image/P2_D3_P5_High_T2_16_Right/2.png", "Image/P2_D3_P5_High_T2_16_Right/3.png", "Image/P2_D3_P5_High_T2_16_Right/4.png", "Image/P2_D3_P5_High_T2_16_Right/5.png", "Image/P2_D3_P5_High_T2_16_Right/6.png", "Image/P2_D3_P5_High_T2_16_Right/7.png", "Image/P2_D3_P5_High_T2_16_Right/8.png", "Image/P2_D3_P5_High_T2_16_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that brown cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1833.3128 seconds to 1847.8412000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P1_High_T1_7_Mid", "images": ["Image/P22_D1_P1_High_T1_7_Mid/1.png", "Image/P22_D1_P1_High_T1_7_Mid/2.png", "Image/P22_D1_P1_High_T1_7_Mid/3.png", "Image/P22_D1_P1_High_T1_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Put the cup in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 1409.53175 seconds to 1419.31678 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P8_High_T1_24_Right", "images": ["Image/P14_D3_P8_High_T1_24_Right/1.png", "Image/P14_D3_P8_High_T1_24_Right/2.png", "Image/P14_D3_P8_High_T1_24_Right/3.png", "Image/P14_D3_P8_High_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start and move the plant up to the top. Finish.\"\nThe frames roughly correspond to the speech interval from 1420.50186 seconds to 1429.54206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P7_High_T1_18_Left", "images": ["Image/P27_D2_P7_High_T1_18_Left/1.png", "Image/P27_D2_P7_High_T1_18_Left/2.png", "Image/P27_D2_P7_High_T1_18_Left/3.png", "Image/P27_D2_P7_High_T1_18_Left/4.png", "Image/P27_D2_P7_High_T1_18_Left/5.png", "Image/P27_D2_P7_High_T1_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the red cup to the top of that shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1331.6473 seconds to 1342.8386 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P7_Medium_T3_22_Mid", "images": ["Image/P1_D3_P7_Medium_T3_22_Mid/1.png", "Image/P1_D3_P7_Medium_T3_22_Mid/2.png", "Image/P1_D3_P7_Medium_T3_22_Mid/3.png", "Image/P1_D3_P7_Medium_T3_22_Mid/4.png", "Image/P1_D3_P7_Medium_T3_22_Mid/5.png", "Image/P1_D3_P7_Medium_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the middle plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1590.9055999999998 seconds to 1601.5418999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P3_Low_T2_22_Mid", "images": ["Image/P27_D3_P3_Low_T2_22_Mid/1.png", "Image/P27_D3_P3_Low_T2_22_Mid/2.png", "Image/P27_D3_P3_Low_T2_22_Mid/3.png", "Image/P27_D3_P3_Low_T2_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 289.7903 seconds to 300.1741 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P7_High_T1_4_Right", "images": ["Image/P6_D2_P7_High_T1_4_Right/1.png", "Image/P6_D2_P7_High_T1_4_Right/2.png", "Image/P6_D2_P7_High_T1_4_Right/3.png", "Image/P6_D2_P7_High_T1_4_Right/4.png", "Image/P6_D2_P7_High_T1_4_Right/5.png", "Image/P6_D2_P7_High_T1_4_Right/6.png", "Image/P6_D2_P7_High_T1_4_Right/7.png", "Image/P6_D2_P7_High_T1_4_Right/8.png", "Image/P6_D2_P7_High_T1_4_Right/9.png", "Image/P6_D2_P7_High_T1_4_Right/10.png", "Image/P6_D2_P7_High_T1_4_Right/11.png", "Image/P6_D2_P7_High_T1_4_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi Friday\"\nThe frames roughly correspond to the speech interval from 878.8756999999999 seconds to 906.9969 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P6_Low_T1_15_Right", "images": ["Image/P23_D3_P6_Low_T1_15_Right/1.png", "Image/P23_D3_P6_Low_T1_15_Right/2.png", "Image/P23_D3_P6_Low_T1_15_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 784.0235 seconds to 793.0713000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P8_High_T2_22_Mid", "images": ["Image/P11_D1_P8_High_T2_22_Mid/1.png", "Image/P11_D1_P8_High_T2_22_Mid/2.png", "Image/P11_D1_P8_High_T2_22_Mid/3.png", "Image/P11_D1_P8_High_T2_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 2352.2475099999997 seconds to 2362.3814799999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P8_Low_T1_1_Right", "images": ["Image/P27_D1_P8_Low_T1_1_Right/1.png", "Image/P27_D1_P8_Low_T1_1_Right/2.png", "Image/P27_D1_P8_Low_T1_1_Right/3.png", "Image/P27_D1_P8_Low_T1_1_Right/4.png", "Image/P27_D1_P8_Low_T1_1_Right/5.png", "Image/P27_D1_P8_Low_T1_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take that glass and put it in the center of this cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 995.325 seconds to 1008.5754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P1_Medium_T2_21_Mid", "images": ["Image/P22_D1_P1_Medium_T2_21_Mid/1.png", "Image/P22_D1_P1_Medium_T2_21_Mid/2.png", "Image/P22_D1_P1_Medium_T2_21_Mid/3.png", "Image/P22_D1_P1_Medium_T2_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 1799.4146 seconds to 1808.6148 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P1_High_T1_19_Mid", "images": ["Image/P29_D2_P1_High_T1_19_Mid/1.png", "Image/P29_D2_P1_High_T1_19_Mid/2.png", "Image/P29_D2_P1_High_T1_19_Mid/3.png", "Image/P29_D2_P1_High_T1_19_Mid/4.png", "Image/P29_D2_P1_High_T1_19_Mid/5.png", "Image/P29_D2_P1_High_T1_19_Mid/6.png", "Image/P29_D2_P1_High_T1_19_Mid/7.png", "Image/P29_D2_P1_High_T1_19_Mid/8.png", "Image/P29_D2_P1_High_T1_19_Mid/9.png", "Image/P29_D2_P1_High_T1_19_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 552.7367 seconds to 568.1323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P1_Low_T1_20_Mid", "images": ["Image/P18_D1_P1_Low_T1_20_Mid/1.png", "Image/P18_D1_P1_Low_T1_20_Mid/2.png", "Image/P18_D1_P1_Low_T1_20_Mid/3.png", "Image/P18_D1_P1_Low_T1_20_Mid/4.png", "Image/P18_D1_P1_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 957.5458 seconds to 967.9987 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P7_Low_T1_13_Mid", "images": ["Image/P3_D2_P7_Low_T1_13_Mid/1.png", "Image/P3_D2_P7_Low_T1_13_Mid/2.png", "Image/P3_D2_P7_Low_T1_13_Mid/3.png", "Image/P3_D2_P7_Low_T1_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2387.9469 seconds to 2398.8042 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P2_Low_T1_20_Right", "images": ["Image/P19_D1_P2_Low_T1_20_Right/1.png", "Image/P19_D1_P2_Low_T1_20_Right/2.png", "Image/P19_D1_P2_Low_T1_20_Right/3.png", "Image/P19_D1_P2_Low_T1_20_Right/4.png", "Image/P19_D1_P2_Low_T1_20_Right/5.png", "Image/P19_D1_P2_Low_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container. Finish.\"\nThe frames roughly correspond to the speech interval from 426.25194 seconds to 440.77027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P3_High_T1_7_Right", "images": ["Image/P28_D1_P3_High_T1_7_Right/1.png", "Image/P28_D1_P3_High_T1_7_Right/2.png", "Image/P28_D1_P3_High_T1_7_Right/3.png", "Image/P28_D1_P3_High_T1_7_Right/4.png", "Image/P28_D1_P3_High_T1_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1528.4694 seconds to 1538.8546 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P3_Low_T2_25_Left", "images": ["Image/P24_D3_P3_Low_T2_25_Left/1.png", "Image/P24_D3_P3_Low_T2_25_Left/2.png", "Image/P24_D3_P3_Low_T2_25_Left/3.png", "Image/P24_D3_P3_Low_T2_25_Left/4.png", "Image/P24_D3_P3_Low_T2_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1530.73486 seconds to 1540.92345 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P1_Medium_T1_2_Right", "images": ["Image/P11_D3_P1_Medium_T1_2_Right/1.png", "Image/P11_D3_P1_Medium_T1_2_Right/2.png", "Image/P11_D3_P1_Medium_T1_2_Right/3.png", "Image/P11_D3_P1_Medium_T1_2_Right/4.png", "Image/P11_D3_P1_Medium_T1_2_Right/5.png", "Image/P11_D3_P1_Medium_T1_2_Right/6.png", "Image/P11_D3_P1_Medium_T1_2_Right/7.png", "Image/P11_D3_P1_Medium_T1_2_Right/8.png", "Image/P11_D3_P1_Medium_T1_2_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first green bottle from the right side to the under of the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 1949.9586 seconds to 1964.7135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P9_Medium_T2_5_Right", "images": ["Image/P24_D2_P9_Medium_T2_5_Right/1.png", "Image/P24_D2_P9_Medium_T2_5_Right/2.png", "Image/P24_D2_P9_Medium_T2_5_Right/3.png", "Image/P24_D2_P9_Medium_T2_5_Right/4.png", "Image/P24_D2_P9_Medium_T2_5_Right/5.png", "Image/P24_D2_P9_Medium_T2_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 640.8649 seconds to 651.8056 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P1_High_T3_7_Mid", "images": ["Image/P7_D2_P1_High_T3_7_Mid/1.png", "Image/P7_D2_P1_High_T3_7_Mid/2.png", "Image/P7_D2_P1_High_T3_7_Mid/3.png", "Image/P7_D2_P1_High_T3_7_Mid/4.png", "Image/P7_D2_P1_High_T3_7_Mid/5.png", "Image/P7_D2_P1_High_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1491.2588 seconds to 1504.3715000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P2_Low_T3_21_Left", "images": ["Image/P16_D1_P2_Low_T3_21_Left/1.png", "Image/P16_D1_P2_Low_T3_21_Left/2.png", "Image/P16_D1_P2_Low_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1484.2782 seconds to 1493.6186 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P3_High_T3_25_Left", "images": ["Image/P1_D3_P3_High_T3_25_Left/1.png", "Image/P1_D3_P3_High_T3_25_Left/2.png", "Image/P1_D3_P3_High_T3_25_Left/3.png", "Image/P1_D3_P3_High_T3_25_Left/4.png", "Image/P1_D3_P3_High_T3_25_Left/5.png", "Image/P1_D3_P3_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1751.3153999999995 seconds to 1762.6194999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P7_Medium_T2_17_Right", "images": ["Image/P10_D3_P7_Medium_T2_17_Right/1.png", "Image/P10_D3_P7_Medium_T2_17_Right/2.png", "Image/P10_D3_P7_Medium_T2_17_Right/3.png", "Image/P10_D3_P7_Medium_T2_17_Right/4.png", "Image/P10_D3_P7_Medium_T2_17_Right/5.png", "Image/P10_D3_P7_Medium_T2_17_Right/6.png", "Image/P10_D3_P7_Medium_T2_17_Right/7.png", "Image/P10_D3_P7_Medium_T2_17_Right/8.png", "Image/P10_D3_P7_Medium_T2_17_Right/9.png", "Image/P10_D3_P7_Medium_T2_17_Right/10.png", "Image/P10_D3_P7_Medium_T2_17_Right/11.png", "Image/P10_D3_P7_Medium_T2_17_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey\"\nThe frames roughly correspond to the speech interval from 620.37969 seconds to 638.72609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P1_High_T1_16_Mid", "images": ["Image/P12_D3_P1_High_T1_16_Mid/1.png", "Image/P12_D3_P1_High_T1_16_Mid/2.png", "Image/P12_D3_P1_High_T1_16_Mid/3.png", "Image/P12_D3_P1_High_T1_16_Mid/4.png", "Image/P12_D3_P1_High_T1_16_Mid/5.png", "Image/P12_D3_P1_High_T1_16_Mid/6.png", "Image/P12_D3_P1_High_T1_16_Mid/7.png", "Image/P12_D3_P1_High_T1_16_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2294.55408 seconds to 2312.21769 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P3_Medium_T1_8_Right", "images": ["Image/P12_D2_P3_Medium_T1_8_Right/1.png", "Image/P12_D2_P3_Medium_T1_8_Right/2.png", "Image/P12_D2_P3_Medium_T1_8_Right/3.png", "Image/P12_D2_P3_Medium_T1_8_Right/4.png", "Image/P12_D2_P3_Medium_T1_8_Right/5.png", "Image/P12_D2_P3_Medium_T1_8_Right/6.png", "Image/P12_D2_P3_Medium_T1_8_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the mug on the right to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 314.34090000000003 seconds to 329.66690000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P8_Low_T1_17_Left", "images": ["Image/P24_D1_P8_Low_T1_17_Left/1.png", "Image/P24_D1_P8_Low_T1_17_Left/2.png", "Image/P24_D1_P8_Low_T1_17_Left/3.png", "Image/P24_D1_P8_Low_T1_17_Left/4.png", "Image/P24_D1_P8_Low_T1_17_Left/5.png", "Image/P24_D1_P8_Low_T1_17_Left/6.png", "Image/P24_D1_P8_Low_T1_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you please move this cup over here\"\nThe frames roughly correspond to the speech interval from 1177.9992 seconds to 1190.1233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P5_High_T2_27_Right", "images": ["Image/P26_D1_P5_High_T2_27_Right/1.png", "Image/P26_D1_P5_High_T2_27_Right/2.png", "Image/P26_D1_P5_High_T2_27_Right/3.png", "Image/P26_D1_P5_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into this white jug? Finish\"\nThe frames roughly correspond to the speech interval from 347.2422 seconds to 359.1291 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P2_Low_T2_7_Left", "images": ["Image/P2_D2_P2_Low_T2_7_Left/1.png", "Image/P2_D2_P2_Low_T2_7_Left/2.png", "Image/P2_D2_P2_Low_T2_7_Left/3.png", "Image/P2_D2_P2_Low_T2_7_Left/4.png", "Image/P2_D2_P2_Low_T2_7_Left/5.png", "Image/P2_D2_P2_Low_T2_7_Left/6.png", "Image/P2_D2_P2_Low_T2_7_Left/7.png", "Image/P2_D2_P2_Low_T2_7_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2919.0701 seconds to 2932.6683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P4_Low_T2_11_Mid", "images": ["Image/P10_D1_P4_Low_T2_11_Mid/1.png", "Image/P10_D1_P4_Low_T2_11_Mid/2.png", "Image/P10_D1_P4_Low_T2_11_Mid/3.png", "Image/P10_D1_P4_Low_T2_11_Mid/4.png", "Image/P10_D1_P4_Low_T2_11_Mid/5.png", "Image/P10_D1_P4_Low_T2_11_Mid/6.png", "Image/P10_D1_P4_Low_T2_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2139.63685 seconds to 2152.79119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P7_High_T2_23_Mid", "images": ["Image/P29_D2_P7_High_T2_23_Mid/1.png", "Image/P29_D2_P7_High_T2_23_Mid/2.png", "Image/P29_D2_P7_High_T2_23_Mid/3.png", "Image/P29_D2_P7_High_T2_23_Mid/4.png", "Image/P29_D2_P7_High_T2_23_Mid/5.png", "Image/P29_D2_P7_High_T2_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 491.2397 seconds to 502.98800000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P3_High_T2_24_Mid", "images": ["Image/P17_D3_P3_High_T2_24_Mid/1.png", "Image/P17_D3_P3_High_T2_24_Mid/2.png", "Image/P17_D3_P3_High_T2_24_Mid/3.png", "Image/P17_D3_P3_High_T2_24_Mid/4.png", "Image/P17_D3_P3_High_T2_24_Mid/5.png", "Image/P17_D3_P3_High_T2_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the white plant on the shelf over there? Finish.\"\nThe frames roughly correspond to the speech interval from 996.0236 seconds to 1010.598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P1_Low_T1_25_Right", "images": ["Image/P1_D2_P1_Low_T1_25_Right/1.png", "Image/P1_D2_P1_Low_T1_25_Right/2.png", "Image/P1_D2_P1_Low_T1_25_Right/3.png", "Image/P1_D2_P1_Low_T1_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1124.7013000000002 seconds to 1134.8643000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P2_High_T3_13_Left", "images": ["Image/P20_D1_P2_High_T3_13_Left/1.png", "Image/P20_D1_P2_High_T3_13_Left/2.png", "Image/P20_D1_P2_High_T3_13_Left/3.png", "Image/P20_D1_P2_High_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 364.68839 seconds to 375.50474 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P5_Medium_T2_14_Mid", "images": ["Image/P7_D3_P5_Medium_T2_14_Mid/1.png", "Image/P7_D3_P5_Medium_T2_14_Mid/2.png", "Image/P7_D3_P5_Medium_T2_14_Mid/3.png", "Image/P7_D3_P5_Medium_T2_14_Mid/4.png", "Image/P7_D3_P5_Medium_T2_14_Mid/5.png", "Image/P7_D3_P5_Medium_T2_14_Mid/6.png", "Image/P7_D3_P5_Medium_T2_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1841.0196 seconds to 1853.7837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P1_Low_T3_26_Left", "images": ["Image/P28_D3_P1_Low_T3_26_Left/1.png", "Image/P28_D3_P1_Low_T3_26_Left/2.png", "Image/P28_D3_P1_Low_T3_26_Left/3.png", "Image/P28_D3_P1_Low_T3_26_Left/4.png", "Image/P28_D3_P1_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 215.6987 seconds to 226.2911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P6_Medium_T2_8_Left", "images": ["Image/P18_D1_P6_Medium_T2_8_Left/1.png", "Image/P18_D1_P6_Medium_T2_8_Left/2.png", "Image/P18_D1_P6_Medium_T2_8_Left/3.png", "Image/P18_D1_P6_Medium_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 976.003 seconds to 986.2336 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P4_Low_T3_21_Left", "images": ["Image/P23_D2_P4_Low_T3_21_Left/1.png", "Image/P23_D2_P4_Low_T3_21_Left/2.png", "Image/P23_D2_P4_Low_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 376.1667 seconds to 384.7541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P3_Low_T3_18_Left", "images": ["Image/P6_D1_P3_Low_T3_18_Left/1.png", "Image/P6_D1_P3_Low_T3_18_Left/2.png", "Image/P6_D1_P3_Low_T3_18_Left/3.png", "Image/P6_D1_P3_Low_T3_18_Left/4.png", "Image/P6_D1_P3_Low_T3_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2038.7280000000003 seconds to 2049.2493000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P4_Low_T2_18_Right", "images": ["Image/P18_D1_P4_Low_T2_18_Right/1.png", "Image/P18_D1_P4_Low_T2_18_Right/2.png", "Image/P18_D1_P4_Low_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 866.8706 seconds to 876.7536 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P5_Low_T3_12_Left", "images": ["Image/P24_D1_P5_Low_T3_12_Left/1.png", "Image/P24_D1_P5_Low_T3_12_Left/2.png", "Image/P24_D1_P5_Low_T3_12_Left/3.png", "Image/P24_D1_P5_Low_T3_12_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you clean this bottle\"\nThe frames roughly correspond to the speech interval from 835.1123299999999 seconds to 844.5475899999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D2_P9_Low_T1_9_Right", "images": ["Image/P12_D2_P9_Low_T1_9_Right/1.png", "Image/P12_D2_P9_Low_T1_9_Right/2.png", "Image/P12_D2_P9_Low_T1_9_Right/3.png", "Image/P12_D2_P9_Low_T1_9_Right/4.png", "Image/P12_D2_P9_Low_T1_9_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this green mug to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 676.0548 seconds to 687.078 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P7_High_T2_1_Mid", "images": ["Image/P25_D1_P7_High_T2_1_Mid/1.png", "Image/P25_D1_P7_High_T2_1_Mid/2.png", "Image/P25_D1_P7_High_T2_1_Mid/3.png", "Image/P25_D1_P7_High_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 404.3985 seconds to 413.9607 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P4_High_T3_7_Left", "images": ["Image/P3_D2_P4_High_T3_7_Left/1.png", "Image/P3_D2_P4_High_T3_7_Left/2.png", "Image/P3_D2_P4_High_T3_7_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2489.4387 seconds to 2498.8478 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P5_High_T3_24_Left", "images": ["Image/P19_D1_P5_High_T3_24_Left/1.png", "Image/P19_D1_P5_High_T3_24_Left/2.png", "Image/P19_D1_P5_High_T3_24_Left/3.png", "Image/P19_D1_P5_High_T3_24_Left/4.png", "Image/P19_D1_P5_High_T3_24_Left/5.png", "Image/P19_D1_P5_High_T3_24_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this flowerpot here. Okay.\"\nThe frames roughly correspond to the speech interval from 857.7129 seconds to 877.0337999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P8_High_T1_5_Right", "images": ["Image/P27_D2_P8_High_T1_5_Right/1.png", "Image/P27_D2_P8_High_T1_5_Right/2.png", "Image/P27_D2_P8_High_T1_5_Right/3.png", "Image/P27_D2_P8_High_T1_5_Right/4.png", "Image/P27_D2_P8_High_T1_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put that plant on top of that shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1481.1046 seconds to 1492.6574999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P6_High_T2_24_Right", "images": ["Image/P8_D2_P6_High_T2_24_Right/1.png", "Image/P8_D2_P6_High_T2_24_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"water the planet. Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2532.1912 seconds to 2542.755 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P3_High_T1_15_Mid", "images": ["Image/P19_D1_P3_High_T1_15_Mid/1.png", "Image/P19_D1_P3_High_T1_15_Mid/2.png", "Image/P19_D1_P3_High_T1_15_Mid/3.png", "Image/P19_D1_P3_High_T1_15_Mid/4.png", "Image/P19_D1_P3_High_T1_15_Mid/5.png", "Image/P19_D1_P3_High_T1_15_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1039.6724499999998 seconds to 1063.4887399999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P3_Low_T1_9_Right", "images": ["Image/P26_D3_P3_Low_T1_9_Right/1.png", "Image/P26_D3_P3_Low_T1_9_Right/2.png", "Image/P26_D3_P3_Low_T1_9_Right/3.png", "Image/P26_D3_P3_Low_T1_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move the green vase to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 865.1279000000001 seconds to 875.4409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P4_High_T3_11_Mid", "images": ["Image/P12_D3_P4_High_T3_11_Mid/1.png", "Image/P12_D3_P4_High_T3_11_Mid/2.png", "Image/P12_D3_P4_High_T3_11_Mid/3.png", "Image/P12_D3_P4_High_T3_11_Mid/4.png", "Image/P12_D3_P4_High_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2339.91899 seconds to 2350.74739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P2_Low_T1_24_Right", "images": ["Image/P5_D1_P2_Low_T1_24_Right/1.png", "Image/P5_D1_P2_Low_T1_24_Right/2.png", "Image/P5_D1_P2_Low_T1_24_Right/3.png", "Image/P5_D1_P2_Low_T1_24_Right/4.png", "Image/P5_D1_P2_Low_T1_24_Right/5.png", "Image/P5_D1_P2_Low_T1_24_Right/6.png", "Image/P5_D1_P2_Low_T1_24_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant into the bucket next to the wall. Finished.\"\nThe frames roughly correspond to the speech interval from 1937.9259 seconds to 1952.7235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P9_Medium_T2_15_Left", "images": ["Image/P25_D2_P9_Medium_T2_15_Left/1.png", "Image/P25_D2_P9_Medium_T2_15_Left/2.png", "Image/P25_D2_P9_Medium_T2_15_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the third plant. Finish.\"\nThe frames roughly correspond to the speech interval from 807.1451 seconds to 815.7206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P4_Medium_T2_16_Left", "images": ["Image/P3_D2_P4_Medium_T2_16_Left/1.png", "Image/P3_D2_P4_Medium_T2_16_Left/2.png", "Image/P3_D2_P4_Medium_T2_16_Left/3.png", "Image/P3_D2_P4_Medium_T2_16_Left/4.png", "Image/P3_D2_P4_Medium_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the item to this space? Finish.\"\nThe frames roughly correspond to the speech interval from 2340.1244 seconds to 2350.6203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P4_High_T1_13_Mid", "images": ["Image/P26_D2_P4_High_T1_13_Mid/1.png", "Image/P26_D2_P4_High_T1_13_Mid/2.png", "Image/P26_D2_P4_High_T1_13_Mid/3.png", "Image/P26_D2_P4_High_T1_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the plant to the middle of this shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1310.3031 seconds to 1320.2704 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P1_Medium_T3_15_Right", "images": ["Image/P27_D1_P1_Medium_T3_15_Right/1.png", "Image/P27_D1_P1_Medium_T3_15_Right/2.png", "Image/P27_D1_P1_Medium_T3_15_Right/3.png", "Image/P27_D1_P1_Medium_T3_15_Right/4.png", "Image/P27_D1_P1_Medium_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 901.3943 seconds to 912.0002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P5_Low_T3_3_Left", "images": ["Image/P8_D1_P5_Low_T3_3_Left/1.png", "Image/P8_D1_P5_Low_T3_3_Left/2.png", "Image/P8_D1_P5_Low_T3_3_Left/3.png", "Image/P8_D1_P5_Low_T3_3_Left/4.png", "Image/P8_D1_P5_Low_T3_3_Left/5.png", "Image/P8_D1_P5_Low_T3_3_Left/6.png", "Image/P8_D1_P5_Low_T3_3_Left/7.png", "Image/P8_D1_P5_Low_T3_3_Left/8.png", "Image/P8_D1_P5_Low_T3_3_Left/9.png", "Image/P8_D1_P5_Low_T3_3_Left/10.png", "Image/P8_D1_P5_Low_T3_3_Left/11.png", "Image/P8_D1_P5_Low_T3_3_Left/12.png", "Image/P8_D1_P5_Low_T3_3_Left/13.png", "Image/P8_D1_P5_Low_T3_3_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 422.3067999999999 seconds to 444.78899999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P3_High_T3_12_Right", "images": ["Image/P15_D2_P3_High_T3_12_Right/1.png", "Image/P15_D2_P3_High_T3_12_Right/2.png", "Image/P15_D2_P3_High_T3_12_Right/3.png", "Image/P15_D2_P3_High_T3_12_Right/4.png", "Image/P15_D2_P3_High_T3_12_Right/5.png", "Image/P15_D2_P3_High_T3_12_Right/6.png", "Image/P15_D2_P3_High_T3_12_Right/7.png", "Image/P15_D2_P3_High_T3_12_Right/8.png", "Image/P15_D2_P3_High_T3_12_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown. Put on the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2565.18374 seconds to 2579.6456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P9_Low_T3_19_Mid", "images": ["Image/P13_D1_P9_Low_T3_19_Mid/1.png", "Image/P13_D1_P9_Low_T3_19_Mid/2.png", "Image/P13_D1_P9_Low_T3_19_Mid/3.png", "Image/P13_D1_P9_Low_T3_19_Mid/4.png", "Image/P13_D1_P9_Low_T3_19_Mid/5.png", "Image/P13_D1_P9_Low_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 720.3335 seconds to 731.9432 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P5_Medium_T2_24_Left", "images": ["Image/P29_D2_P5_Medium_T2_24_Left/1.png", "Image/P29_D2_P5_Medium_T2_24_Left/2.png", "Image/P29_D2_P5_Medium_T2_24_Left/3.png", "Image/P29_D2_P5_Medium_T2_24_Left/4.png", "Image/P29_D2_P5_Medium_T2_24_Left/5.png", "Image/P29_D2_P5_Medium_T2_24_Left/6.png", "Image/P29_D2_P5_Medium_T2_24_Left/7.png", "Image/P29_D2_P5_Medium_T2_24_Left/8.png", "Image/P29_D2_P5_Medium_T2_24_Left/9.png", "Image/P29_D2_P5_Medium_T2_24_Left/10.png", "Image/P29_D2_P5_Medium_T2_24_Left/11.png", "Image/P29_D2_P5_Medium_T2_24_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 293.74559999999997 seconds to 311.58979999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P7_High_T2_4_Mid", "images": ["Image/P20_D2_P7_High_T2_4_Mid/1.png", "Image/P20_D2_P7_High_T2_4_Mid/2.png", "Image/P20_D2_P7_High_T2_4_Mid/3.png", "Image/P20_D2_P7_High_T2_4_Mid/4.png", "Image/P20_D2_P7_High_T2_4_Mid/5.png", "Image/P20_D2_P7_High_T2_4_Mid/6.png", "Image/P20_D2_P7_High_T2_4_Mid/7.png", "Image/P20_D2_P7_High_T2_4_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3169.6330000000003 seconds to 3183.1487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P9_Low_T3_27_Right", "images": ["Image/P22_D2_P9_Low_T3_27_Right/1.png", "Image/P22_D2_P9_Low_T3_27_Right/2.png", "Image/P22_D2_P9_Low_T3_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 899.2104999999999 seconds to 908.2451 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P1_High_T2_27_Right", "images": ["Image/P18_D1_P1_High_T2_27_Right/1.png", "Image/P18_D1_P1_High_T2_27_Right/2.png", "Image/P18_D1_P1_High_T2_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 770.0443 seconds to 780.6659 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P8_Medium_T3_4_Left", "images": ["Image/P12_D1_P8_Medium_T3_4_Left/1.png", "Image/P12_D1_P8_Medium_T3_4_Left/2.png", "Image/P12_D1_P8_Medium_T3_4_Left/3.png", "Image/P12_D1_P8_Medium_T3_4_Left/4.png", "Image/P12_D1_P8_Medium_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1528.7314 seconds to 1542.7773 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P1_High_T3_21_Right", "images": ["Image/P14_D1_P1_High_T3_21_Right/1.png", "Image/P14_D1_P1_High_T3_21_Right/2.png", "Image/P14_D1_P1_High_T3_21_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 463.4512 seconds to 472.2194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P3_High_T3_5_Right", "images": ["Image/P13_D2_P3_High_T3_5_Right/1.png", "Image/P13_D2_P3_High_T3_5_Right/2.png", "Image/P13_D2_P3_High_T3_5_Right/3.png", "Image/P13_D2_P3_High_T3_5_Right/4.png", "Image/P13_D2_P3_High_T3_5_Right/5.png", "Image/P13_D2_P3_High_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the plant over here\"\nThe frames roughly correspond to the speech interval from 1209.92616 seconds to 1220.54646 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P2_High_T1_6_Right", "images": ["Image/P23_D1_P2_High_T1_6_Right/1.png", "Image/P23_D1_P2_High_T1_6_Right/2.png", "Image/P23_D1_P2_High_T1_6_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1585.8765 seconds to 1595.3549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P5_High_T3_12_Right", "images": ["Image/P28_D1_P5_High_T3_12_Right/1.png", "Image/P28_D1_P5_High_T3_12_Right/2.png", "Image/P28_D1_P5_High_T3_12_Right/3.png", "Image/P28_D1_P5_High_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1363.99213 seconds to 1373.11006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P6_Medium_T1_21_Mid", "images": ["Image/P24_D2_P6_Medium_T1_21_Mid/1.png", "Image/P24_D2_P6_Medium_T1_21_Mid/2.png", "Image/P24_D2_P6_Medium_T1_21_Mid/3.png", "Image/P24_D2_P6_Medium_T1_21_Mid/4.png", "Image/P24_D2_P6_Medium_T1_21_Mid/5.png", "Image/P24_D2_P6_Medium_T1_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one here please? Finish.\"\nThe frames roughly correspond to the speech interval from 345.0526 seconds to 358.05279999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P4_High_T2_12_Right", "images": ["Image/P11_D3_P4_High_T2_12_Right/1.png", "Image/P11_D3_P4_High_T2_12_Right/2.png", "Image/P11_D3_P4_High_T2_12_Right/3.png", "Image/P11_D3_P4_High_T2_12_Right/4.png", "Image/P11_D3_P4_High_T2_12_Right/5.png", "Image/P11_D3_P4_High_T2_12_Right/6.png", "Image/P11_D3_P4_High_T2_12_Right/7.png", "Image/P11_D3_P4_High_T2_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the red bottle on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 1594.0261 seconds to 1608.0712 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P8_Low_T1_22_Right", "images": ["Image/P12_D1_P8_Low_T1_22_Right/1.png", "Image/P12_D1_P8_Low_T1_22_Right/2.png", "Image/P12_D1_P8_Low_T1_22_Right/3.png", "Image/P12_D1_P8_Low_T1_22_Right/4.png", "Image/P12_D1_P8_Low_T1_22_Right/5.png", "Image/P12_D1_P8_Low_T1_22_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1510.7615 seconds to 1522.3567 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P6_Low_T2_3_Mid", "images": ["Image/P18_D1_P6_Low_T2_3_Mid/1.png", "Image/P18_D1_P6_Low_T2_3_Mid/2.png", "Image/P18_D1_P6_Low_T2_3_Mid/3.png", "Image/P18_D1_P6_Low_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 720.349 seconds to 733.9908 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P7_High_T2_8_Mid", "images": ["Image/P1_D3_P7_High_T2_8_Mid/1.png", "Image/P1_D3_P7_High_T2_8_Mid/2.png", "Image/P1_D3_P7_High_T2_8_Mid/3.png", "Image/P1_D3_P7_High_T2_8_Mid/4.png", "Image/P1_D3_P7_High_T2_8_Mid/5.png", "Image/P1_D3_P7_High_T2_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1607.9327999999998 seconds to 1620.0432999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P7_Medium_T1_23_Mid", "images": ["Image/P26_D1_P7_Medium_T1_23_Mid/1.png", "Image/P26_D1_P7_Medium_T1_23_Mid/2.png", "Image/P26_D1_P7_Medium_T1_23_Mid/3.png", "Image/P26_D1_P7_Medium_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move one of those plants up here? Finish.\"\nThe frames roughly correspond to the speech interval from 86.02247999999999 seconds to 98.63324 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P6_High_T2_22_Mid", "images": ["Image/P16_D1_P6_High_T2_22_Mid/1.png", "Image/P16_D1_P6_High_T2_22_Mid/2.png", "Image/P16_D1_P6_High_T2_22_Mid/3.png", "Image/P16_D1_P6_High_T2_22_Mid/4.png", "Image/P16_D1_P6_High_T2_22_Mid/5.png", "Image/P16_D1_P6_High_T2_22_Mid/6.png", "Image/P16_D1_P6_High_T2_22_Mid/7.png", "Image/P16_D1_P6_High_T2_22_Mid/8.png", "Image/P16_D1_P6_High_T2_22_Mid/9.png", "Image/P16_D1_P6_High_T2_22_Mid/10.png", "Image/P16_D1_P6_High_T2_22_Mid/11.png", "Image/P16_D1_P6_High_T2_22_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please water this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1648.1562 seconds to 1666.3922 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P3_Medium_T1_1_Left", "images": ["Image/P10_D3_P3_Medium_T1_1_Left/1.png", "Image/P10_D3_P3_Medium_T1_1_Left/2.png", "Image/P10_D3_P3_Medium_T1_1_Left/3.png", "Image/P10_D3_P3_Medium_T1_1_Left/4.png", "Image/P10_D3_P3_Medium_T1_1_Left/5.png", "Image/P10_D3_P3_Medium_T1_1_Left/6.png", "Image/P10_D3_P3_Medium_T1_1_Left/7.png", "Image/P10_D3_P3_Medium_T1_1_Left/8.png", "Image/P10_D3_P3_Medium_T1_1_Left/9.png", "Image/P10_D3_P3_Medium_T1_1_Left/10.png", "Image/P10_D3_P3_Medium_T1_1_Left/11.png", "Image/P10_D3_P3_Medium_T1_1_Left/12.png", "Image/P10_D3_P3_Medium_T1_1_Left/13.png", "Image/P10_D3_P3_Medium_T1_1_Left/14.png", "Image/P10_D3_P3_Medium_T1_1_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 130.95145 seconds to 157.09174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P7_Medium_T3_20_Mid", "images": ["Image/P11_D3_P7_Medium_T3_20_Mid/1.png", "Image/P11_D3_P7_Medium_T3_20_Mid/2.png", "Image/P11_D3_P7_Medium_T3_20_Mid/3.png", "Image/P11_D3_P7_Medium_T3_20_Mid/4.png", "Image/P11_D3_P7_Medium_T3_20_Mid/5.png", "Image/P11_D3_P7_Medium_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white glass in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 1930.9699 seconds to 1942.8181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P9_High_T1_6_Left", "images": ["Image/P6_D1_P9_High_T1_6_Left/1.png", "Image/P6_D1_P9_High_T1_6_Left/2.png", "Image/P6_D1_P9_High_T1_6_Left/3.png", "Image/P6_D1_P9_High_T1_6_Left/4.png", "Image/P6_D1_P9_High_T1_6_Left/5.png", "Image/P6_D1_P9_High_T1_6_Left/6.png", "Image/P6_D1_P9_High_T1_6_Left/7.png", "Image/P6_D1_P9_High_T1_6_Left/8.png", "Image/P6_D1_P9_High_T1_6_Left/9.png", "Image/P6_D1_P9_High_T1_6_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2584.208 seconds to 2600.1159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P9_Low_T3_15_Right", "images": ["Image/P15_D1_P9_Low_T3_15_Right/1.png", "Image/P15_D1_P9_Low_T3_15_Right/2.png", "Image/P15_D1_P9_Low_T3_15_Right/3.png", "Image/P15_D1_P9_Low_T3_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1856.48097 seconds to 1865.73727 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P2_High_T3_13_Right", "images": ["Image/P7_D2_P2_High_T3_13_Right/1.png", "Image/P7_D2_P2_High_T3_13_Right/2.png", "Image/P7_D2_P2_High_T3_13_Right/3.png", "Image/P7_D2_P2_High_T3_13_Right/4.png", "Image/P7_D2_P2_High_T3_13_Right/5.png", "Image/P7_D2_P2_High_T3_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1057.6537 seconds to 1070.5151 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P1_High_T3_13_Left", "images": ["Image/P3_D1_P1_High_T3_13_Left/1.png", "Image/P3_D1_P1_High_T3_13_Left/2.png", "Image/P3_D1_P1_High_T3_13_Left/3.png", "Image/P3_D1_P1_High_T3_13_Left/4.png", "Image/P3_D1_P1_High_T3_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1248.72419 seconds to 1260.20935 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P5_High_T1_3_Mid", "images": ["Image/P18_D2_P5_High_T1_3_Mid/1.png", "Image/P18_D2_P5_High_T1_3_Mid/2.png", "Image/P18_D2_P5_High_T1_3_Mid/3.png", "Image/P18_D2_P5_High_T1_3_Mid/4.png", "Image/P18_D2_P5_High_T1_3_Mid/5.png", "Image/P18_D2_P5_High_T1_3_Mid/6.png", "Image/P18_D2_P5_High_T1_3_Mid/7.png", "Image/P18_D2_P5_High_T1_3_Mid/8.png", "Image/P18_D2_P5_High_T1_3_Mid/9.png", "Image/P18_D2_P5_High_T1_3_Mid/10.png", "Image/P18_D2_P5_High_T1_3_Mid/11.png", "Image/P18_D2_P5_High_T1_3_Mid/12.png", "Image/P18_D2_P5_High_T1_3_Mid/13.png", "Image/P18_D2_P5_High_T1_3_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green vase to the top of the table. Finish.\"\nThe frames roughly correspond to the speech interval from 165.46930000000003 seconds to 184.4414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P5_High_T2_11_Left", "images": ["Image/P14_D2_P5_High_T2_11_Left/1.png", "Image/P14_D2_P5_High_T2_11_Left/2.png", "Image/P14_D2_P5_High_T2_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Fill up that bottle\"\nThe frames roughly correspond to the speech interval from 1897.2055 seconds to 1905.7754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P1_High_T2_7_Right", "images": ["Image/P16_D2_P1_High_T2_7_Right/1.png", "Image/P16_D2_P1_High_T2_7_Right/2.png", "Image/P16_D2_P1_High_T2_7_Right/3.png", "Image/P16_D2_P1_High_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 723.7436 seconds to 733.0563000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P9_High_T2_8_Mid", "images": ["Image/P20_D2_P9_High_T2_8_Mid/1.png", "Image/P20_D2_P9_High_T2_8_Mid/2.png", "Image/P20_D2_P9_High_T2_8_Mid/3.png", "Image/P20_D2_P9_High_T2_8_Mid/4.png", "Image/P20_D2_P9_High_T2_8_Mid/5.png", "Image/P20_D2_P9_High_T2_8_Mid/6.png", "Image/P20_D2_P9_High_T2_8_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into the green cup next to the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2937.226 seconds to 2950.2003000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P8_Low_T1_13_Left", "images": ["Image/P10_D3_P8_Low_T1_13_Left/1.png", "Image/P10_D3_P8_Low_T1_13_Left/2.png", "Image/P10_D3_P8_Low_T1_13_Left/3.png", "Image/P10_D3_P8_Low_T1_13_Left/4.png", "Image/P10_D3_P8_Low_T1_13_Left/5.png", "Image/P10_D3_P8_Low_T1_13_Left/6.png", "Image/P10_D3_P8_Low_T1_13_Left/7.png", "Image/P10_D3_P8_Low_T1_13_Left/8.png", "Image/P10_D3_P8_Low_T1_13_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 959.17709 seconds to 972.98589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P3_Low_T3_21_Right", "images": ["Image/P1_D1_P3_Low_T3_21_Right/1.png", "Image/P1_D1_P3_Low_T3_21_Right/2.png", "Image/P1_D1_P3_Low_T3_21_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 396.9572 seconds to 405.6861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P6_Low_T3_26_Mid", "images": ["Image/P29_D1_P6_Low_T3_26_Mid/1.png", "Image/P29_D1_P6_Low_T3_26_Mid/2.png", "Image/P29_D1_P6_Low_T3_26_Mid/3.png", "Image/P29_D1_P6_Low_T3_26_Mid/4.png", "Image/P29_D1_P6_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2670.8880999999997 seconds to 2681.6607999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P6_Low_T2_24_Left", "images": ["Image/P19_D1_P6_Low_T2_24_Left/1.png", "Image/P19_D1_P6_Low_T2_24_Left/2.png", "Image/P19_D1_P6_Low_T2_24_Left/3.png", "Image/P19_D1_P6_Low_T2_24_Left/4.png", "Image/P19_D1_P6_Low_T2_24_Left/5.png", "Image/P19_D1_P6_Low_T2_24_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 920.3395 seconds to 938.2104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P3_Low_T2_1_Right", "images": ["Image/P1_D2_P3_Low_T2_1_Right/1.png", "Image/P1_D2_P3_Low_T2_1_Right/2.png", "Image/P1_D2_P3_Low_T2_1_Right/3.png", "Image/P1_D2_P3_Low_T2_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1004.8466 seconds to 1015.6367 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P3_Medium_T3_20_Left", "images": ["Image/P16_D2_P3_Medium_T3_20_Left/1.png", "Image/P16_D2_P3_Medium_T3_20_Left/2.png", "Image/P16_D2_P3_Medium_T3_20_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1044.3609 seconds to 1052.9638 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P3_Low_T1_9_Right", "images": ["Image/P30_D1_P3_Low_T1_9_Right/1.png", "Image/P30_D1_P3_Low_T1_9_Right/2.png", "Image/P30_D1_P3_Low_T1_9_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 796.4333 seconds to 804.5761 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P9_Low_T2_22_Right", "images": ["Image/P18_D2_P9_Low_T2_22_Right/1.png", "Image/P18_D2_P9_Low_T2_22_Right/2.png", "Image/P18_D2_P9_Low_T2_22_Right/3.png", "Image/P18_D2_P9_Low_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water that bottom plant. Finish.\"\nThe frames roughly correspond to the speech interval from 442.7396 seconds to 454.4321 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P6_Medium_T3_15_Left", "images": ["Image/P18_D3_P6_Medium_T3_15_Left/1.png", "Image/P18_D3_P6_Medium_T3_15_Left/2.png", "Image/P18_D3_P6_Medium_T3_15_Left/3.png", "Image/P18_D3_P6_Medium_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1406.538 seconds to 1419.4696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P3_Low_T3_7_Mid", "images": ["Image/P15_D3_P3_Low_T3_7_Mid/1.png", "Image/P15_D3_P3_Low_T3_7_Mid/2.png", "Image/P15_D3_P3_Low_T3_7_Mid/3.png", "Image/P15_D3_P3_Low_T3_7_Mid/4.png", "Image/P15_D3_P3_Low_T3_7_Mid/5.png", "Image/P15_D3_P3_Low_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the cup at the top of the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 431.1483 seconds to 442.4086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P2_Low_T1_27_Mid", "images": ["Image/P3_D1_P2_Low_T1_27_Mid/1.png", "Image/P3_D1_P2_Low_T1_27_Mid/2.png", "Image/P3_D1_P2_Low_T1_27_Mid/3.png", "Image/P3_D1_P2_Low_T1_27_Mid/4.png", "Image/P3_D1_P2_Low_T1_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1304.4687 seconds to 1316.1211 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P4_High_T3_5_Mid", "images": ["Image/P20_D3_P4_High_T3_5_Mid/1.png", "Image/P20_D3_P4_High_T3_5_Mid/2.png", "Image/P20_D3_P4_High_T3_5_Mid/3.png", "Image/P20_D3_P4_High_T3_5_Mid/4.png", "Image/P20_D3_P4_High_T3_5_Mid/5.png", "Image/P20_D3_P4_High_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant in the middle of the table? Finish.\"\nThe frames roughly correspond to the speech interval from 1347.1731 seconds to 1363.2092 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P4_High_T3_25_Left", "images": ["Image/P9_D1_P4_High_T3_25_Left/1.png", "Image/P9_D1_P4_High_T3_25_Left/2.png", "Image/P9_D1_P4_High_T3_25_Left/3.png", "Image/P9_D1_P4_High_T3_25_Left/4.png", "Image/P9_D1_P4_High_T3_25_Left/5.png", "Image/P9_D1_P4_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Would you help me to clean this white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1358.3673 seconds to 1370.9331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P1_High_T1_24_Left", "images": ["Image/P3_D2_P1_High_T1_24_Left/1.png", "Image/P3_D2_P1_High_T1_24_Left/2.png", "Image/P3_D2_P1_High_T1_24_Left/3.png", "Image/P3_D2_P1_High_T1_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 2322.9151 seconds to 2333.8571 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P1_High_T3_23_Right", "images": ["Image/P16_D1_P1_High_T3_23_Right/1.png", "Image/P16_D1_P1_High_T3_23_Right/2.png", "Image/P16_D1_P1_High_T3_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1396.862 seconds to 1407.3436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P6_Low_T1_1_Left", "images": ["Image/P23_D2_P6_Low_T1_1_Left/1.png", "Image/P23_D2_P6_Low_T1_1_Left/2.png", "Image/P23_D2_P6_Low_T1_1_Left/3.png", "Image/P23_D2_P6_Low_T1_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 120.84685000000002 seconds to 133.47090000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P8_High_T2_5_Right", "images": ["Image/P24_D3_P8_High_T2_5_Right/1.png", "Image/P24_D3_P8_High_T2_5_Right/2.png", "Image/P24_D3_P8_High_T2_5_Right/3.png", "Image/P24_D3_P8_High_T2_5_Right/4.png", "Image/P24_D3_P8_High_T2_5_Right/5.png", "Image/P24_D3_P8_High_T2_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1690.9249 seconds to 1702.6039 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P7_High_T1_6_Left", "images": ["Image/P19_D3_P7_High_T1_6_Left/1.png", "Image/P19_D3_P7_High_T1_6_Left/2.png", "Image/P19_D3_P7_High_T1_6_Left/3.png", "Image/P19_D3_P7_High_T1_6_Left/4.png", "Image/P19_D3_P7_High_T1_6_Left/5.png", "Image/P19_D3_P7_High_T1_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that flowerpot up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2449.3273999999997 seconds to 2462.2875 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P4_High_T3_21_Right", "images": ["Image/P16_D1_P4_High_T3_21_Right/1.png", "Image/P16_D1_P4_High_T3_21_Right/2.png", "Image/P16_D1_P4_High_T3_21_Right/3.png", "Image/P16_D1_P4_High_T3_21_Right/4.png", "Image/P16_D1_P4_High_T3_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1273.5311 seconds to 1285.5022 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P9_High_T2_19_Right", "images": ["Image/P5_D1_P9_High_T2_19_Right/1.png", "Image/P5_D1_P9_High_T2_19_Right/2.png", "Image/P5_D1_P9_High_T2_19_Right/3.png", "Image/P5_D1_P9_High_T2_19_Right/4.png", "Image/P5_D1_P9_High_T2_19_Right/5.png", "Image/P5_D1_P9_High_T2_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"water the white bottle in the bucket. Finished.\"\nThe frames roughly correspond to the speech interval from 1916.7535 seconds to 1929.1549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P9_Low_T3_4_Left", "images": ["Image/P30_D2_P9_Low_T3_4_Left/1.png", "Image/P30_D2_P9_Low_T3_4_Left/2.png", "Image/P30_D2_P9_Low_T3_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 242.2665 seconds to 250.8685 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P8_Medium_T3_24_Right", "images": ["Image/P25_D3_P8_Medium_T3_24_Right/1.png", "Image/P25_D3_P8_Medium_T3_24_Right/2.png", "Image/P25_D3_P8_Medium_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Clean the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2024.0501 seconds to 2032.6124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P7_Low_T1_16_Mid", "images": ["Image/P13_D1_P7_Low_T1_16_Mid/1.png", "Image/P13_D1_P7_Low_T1_16_Mid/2.png", "Image/P13_D1_P7_Low_T1_16_Mid/3.png", "Image/P13_D1_P7_Low_T1_16_Mid/4.png", "Image/P13_D1_P7_Low_T1_16_Mid/5.png", "Image/P13_D1_P7_Low_T1_16_Mid/6.png", "Image/P13_D1_P7_Low_T1_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you please put the brown cup up here please? Finish.\"\nThe frames roughly correspond to the speech interval from 444.776 seconds to 456.9684 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P4_Medium_T3_16_Left", "images": ["Image/P22_D3_P4_Medium_T3_16_Left/1.png", "Image/P22_D3_P4_Medium_T3_16_Left/2.png", "Image/P22_D3_P4_Medium_T3_16_Left/3.png", "Image/P22_D3_P4_Medium_T3_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 143.3322 seconds to 153.53502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P6_Low_T3_20_Left", "images": ["Image/P24_D2_P6_Low_T3_20_Left/1.png", "Image/P24_D2_P6_Low_T3_20_Left/2.png", "Image/P24_D2_P6_Low_T3_20_Left/3.png", "Image/P24_D2_P6_Low_T3_20_Left/4.png", "Image/P24_D2_P6_Low_T3_20_Left/5.png", "Image/P24_D2_P6_Low_T3_20_Left/6.png", "Image/P24_D2_P6_Low_T3_20_Left/7.png", "Image/P24_D2_P6_Low_T3_20_Left/8.png", "Image/P24_D2_P6_Low_T3_20_Left/9.png", "Image/P24_D2_P6_Low_T3_20_Left/10.png", "Image/P24_D2_P6_Low_T3_20_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 124.83929999999998 seconds to 138.3412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P2_High_T3_9_Mid", "images": ["Image/P27_D1_P2_High_T3_9_Mid/1.png", "Image/P27_D1_P2_High_T3_9_Mid/2.png", "Image/P27_D1_P2_High_T3_9_Mid/3.png", "Image/P27_D1_P2_High_T3_9_Mid/4.png", "Image/P27_D1_P2_High_T3_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this glass? Finish\"\nThe frames roughly correspond to the speech interval from 862.2921000000001 seconds to 874.2493000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P4_Medium_T1_7_Left", "images": ["Image/P5_D1_P4_Medium_T1_7_Left/1.png", "Image/P5_D1_P4_Medium_T1_7_Left/2.png", "Image/P5_D1_P4_Medium_T1_7_Left/3.png", "Image/P5_D1_P4_Medium_T1_7_Left/4.png", "Image/P5_D1_P4_Medium_T1_7_Left/5.png", "Image/P5_D1_P4_Medium_T1_7_Left/6.png", "Image/P5_D1_P4_Medium_T1_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the curve over here to here\"\nThe frames roughly correspond to the speech interval from 1982.6646 seconds to 1997.5001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P4_Low_T2_6_Mid", "images": ["Image/P15_D1_P4_Low_T2_6_Mid/1.png", "Image/P15_D1_P4_Low_T2_6_Mid/2.png", "Image/P15_D1_P4_Low_T2_6_Mid/3.png", "Image/P15_D1_P4_Low_T2_6_Mid/4.png", "Image/P15_D1_P4_Low_T2_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2088.1073699999997 seconds to 2099.24387 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P7_High_T1_25_Right", "images": ["Image/P5_D1_P7_High_T1_25_Right/1.png", "Image/P5_D1_P7_High_T1_25_Right/2.png", "Image/P5_D1_P7_High_T1_25_Right/3.png", "Image/P5_D1_P7_High_T1_25_Right/4.png", "Image/P5_D1_P7_High_T1_25_Right/5.png", "Image/P5_D1_P7_High_T1_25_Right/6.png", "Image/P5_D1_P7_High_T1_25_Right/7.png", "Image/P5_D1_P7_High_T1_25_Right/8.png", "Image/P5_D1_P7_High_T1_25_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup on the ground to here on the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 1727.1209 seconds to 1741.8057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P1_High_T2_4_Right", "images": ["Image/P15_D1_P1_High_T2_4_Right/1.png", "Image/P15_D1_P1_High_T2_4_Right/2.png", "Image/P15_D1_P1_High_T2_4_Right/3.png", "Image/P15_D1_P1_High_T2_4_Right/4.png", "Image/P15_D1_P1_High_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2031.65007 seconds to 2042.90887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P2_Medium_T2_14_Mid", "images": ["Image/P2_D2_P2_Medium_T2_14_Mid/1.png", "Image/P2_D2_P2_Medium_T2_14_Mid/2.png", "Image/P2_D2_P2_Medium_T2_14_Mid/3.png", "Image/P2_D2_P2_Medium_T2_14_Mid/4.png", "Image/P2_D2_P2_Medium_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please water that plant in the middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2775.5407 seconds to 2788.0672999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P1_Low_T1_9_Left", "images": ["Image/P15_D3_P1_Low_T1_9_Left/1.png", "Image/P15_D3_P1_Low_T1_9_Left/2.png", "Image/P15_D3_P1_Low_T1_9_Left/3.png", "Image/P15_D3_P1_Low_T1_9_Left/4.png", "Image/P15_D3_P1_Low_T1_9_Left/5.png", "Image/P15_D3_P1_Low_T1_9_Left/6.png", "Image/P15_D3_P1_Low_T1_9_Left/7.png", "Image/P15_D3_P1_Low_T1_9_Left/8.png", "Image/P15_D3_P1_Low_T1_9_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 137.77320000000003 seconds to 153.7797 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P9_Medium_T3_17_Right", "images": ["Image/P12_D3_P9_Medium_T3_17_Right/1.png", "Image/P12_D3_P9_Medium_T3_17_Right/2.png", "Image/P12_D3_P9_Medium_T3_17_Right/3.png", "Image/P12_D3_P9_Medium_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2436.38429 seconds to 2448.11839 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_High_T2_22_Mid", "images": ["Image/P20_D2_P4_High_T2_22_Mid/1.png", "Image/P20_D2_P4_High_T2_22_Mid/2.png", "Image/P20_D2_P4_High_T2_22_Mid/3.png", "Image/P20_D2_P4_High_T2_22_Mid/4.png", "Image/P20_D2_P4_High_T2_22_Mid/5.png", "Image/P20_D2_P4_High_T2_22_Mid/6.png", "Image/P20_D2_P4_High_T2_22_Mid/7.png", "Image/P20_D2_P4_High_T2_22_Mid/8.png", "Image/P20_D2_P4_High_T2_22_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that plant on the table next to those thingy. Finish.\"\nThe frames roughly correspond to the speech interval from 2987.7698 seconds to 3003.1088000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P5_Medium_T2_2_Mid", "images": ["Image/P15_D3_P5_Medium_T2_2_Mid/1.png", "Image/P15_D3_P5_Medium_T2_2_Mid/2.png", "Image/P15_D3_P5_Medium_T2_2_Mid/3.png", "Image/P15_D3_P5_Medium_T2_2_Mid/4.png", "Image/P15_D3_P5_Medium_T2_2_Mid/5.png", "Image/P15_D3_P5_Medium_T2_2_Mid/6.png", "Image/P15_D3_P5_Medium_T2_2_Mid/7.png", "Image/P15_D3_P5_Medium_T2_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the middle pot in the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 670.8897 seconds to 684.3507 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P5_Medium_T3_16_Left", "images": ["Image/P4_D2_P5_Medium_T3_16_Left/1.png", "Image/P4_D2_P5_Medium_T3_16_Left/2.png", "Image/P4_D2_P5_Medium_T3_16_Left/3.png", "Image/P4_D2_P5_Medium_T3_16_Left/4.png", "Image/P4_D2_P5_Medium_T3_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left cap? Finish.\"\nThe frames roughly correspond to the speech interval from 2008.1373 seconds to 2021.9583 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P2_Low_T2_25_Mid", "images": ["Image/P11_D2_P2_Low_T2_25_Mid/1.png", "Image/P11_D2_P2_Low_T2_25_Mid/2.png", "Image/P11_D2_P2_Low_T2_25_Mid/3.png", "Image/P11_D2_P2_Low_T2_25_Mid/4.png", "Image/P11_D2_P2_Low_T2_25_Mid/5.png", "Image/P11_D2_P2_Low_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour the water in a white glass\"\nThe frames roughly correspond to the speech interval from 807.4186 seconds to 819.8605 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P9_Low_T2_25_Right", "images": ["Image/P9_D3_P9_Low_T2_25_Right/1.png", "Image/P9_D3_P9_Low_T2_25_Right/2.png", "Image/P9_D3_P9_Low_T2_25_Right/3.png", "Image/P9_D3_P9_Low_T2_25_Right/4.png", "Image/P9_D3_P9_Low_T2_25_Right/5.png", "Image/P9_D3_P9_Low_T2_25_Right/6.png", "Image/P9_D3_P9_Low_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the cup on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 363.2854 seconds to 376.2972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P7_Low_T1_11_Left", "images": ["Image/P13_D2_P7_Low_T1_11_Left/1.png", "Image/P13_D2_P7_Low_T1_11_Left/2.png", "Image/P13_D2_P7_Low_T1_11_Left/3.png", "Image/P13_D2_P7_Low_T1_11_Left/4.png", "Image/P13_D2_P7_Low_T1_11_Left/5.png", "Image/P13_D2_P7_Low_T1_11_Left/6.png", "Image/P13_D2_P7_Low_T1_11_Left/7.png", "Image/P13_D2_P7_Low_T1_11_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1001.69796 seconds to 1014.71286 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P7_Low_T1_1_Right", "images": ["Image/P13_D3_P7_Low_T1_1_Right/1.png", "Image/P13_D3_P7_Low_T1_1_Right/2.png", "Image/P13_D3_P7_Low_T1_1_Right/3.png", "Image/P13_D3_P7_Low_T1_1_Right/4.png", "Image/P13_D3_P7_Low_T1_1_Right/5.png", "Image/P13_D3_P7_Low_T1_1_Right/6.png", "Image/P13_D3_P7_Low_T1_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the green bottle from the floor to the top shelf\"\nThe frames roughly correspond to the speech interval from 1426.64116 seconds to 1438.7232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P7_Low_T3_2_Left", "images": ["Image/P24_D1_P7_Low_T3_2_Left/1.png", "Image/P24_D1_P7_Low_T3_2_Left/2.png", "Image/P24_D1_P7_Low_T3_2_Left/3.png", "Image/P24_D1_P7_Low_T3_2_Left/4.png", "Image/P24_D1_P7_Low_T3_2_Left/5.png", "Image/P24_D1_P7_Low_T3_2_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1031.0055 seconds to 1042.7256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P8_Low_T3_24_Left", "images": ["Image/P9_D3_P8_Low_T3_24_Left/1.png", "Image/P9_D3_P8_Low_T3_24_Left/2.png", "Image/P9_D3_P8_Low_T3_24_Left/3.png", "Image/P9_D3_P8_Low_T3_24_Left/4.png", "Image/P9_D3_P8_Low_T3_24_Left/5.png", "Image/P9_D3_P8_Low_T3_24_Left/6.png", "Image/P9_D3_P8_Low_T3_24_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the land on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 907.0107 seconds to 921.328 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P2_Low_T3_25_Left", "images": ["Image/P15_D1_P2_Low_T3_25_Left/1.png", "Image/P15_D1_P2_Low_T3_25_Left/2.png", "Image/P15_D1_P2_Low_T3_25_Left/3.png", "Image/P15_D1_P2_Low_T3_25_Left/4.png", "Image/P15_D1_P2_Low_T3_25_Left/5.png", "Image/P15_D1_P2_Low_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1467.0160099999998 seconds to 1479.98743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P1_Low_T2_3_Mid", "images": ["Image/P12_D3_P1_Low_T2_3_Mid/1.png", "Image/P12_D3_P1_Low_T2_3_Mid/2.png", "Image/P12_D3_P1_Low_T2_3_Mid/3.png", "Image/P12_D3_P1_Low_T2_3_Mid/4.png", "Image/P12_D3_P1_Low_T2_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2532.11109 seconds to 2543.37139 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P6_Medium_T3_17_Right", "images": ["Image/P2_D1_P6_Medium_T3_17_Right/1.png", "Image/P2_D1_P6_Medium_T3_17_Right/2.png", "Image/P2_D1_P6_Medium_T3_17_Right/3.png", "Image/P2_D1_P6_Medium_T3_17_Right/4.png", "Image/P2_D1_P6_Medium_T3_17_Right/5.png", "Image/P2_D1_P6_Medium_T3_17_Right/6.png", "Image/P2_D1_P6_Medium_T3_17_Right/7.png", "Image/P2_D1_P6_Medium_T3_17_Right/8.png", "Image/P2_D1_P6_Medium_T3_17_Right/9.png", "Image/P2_D1_P6_Medium_T3_17_Right/10.png", "Image/P2_D1_P6_Medium_T3_17_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 488.3615 seconds to 506.3167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P8_Low_T2_16_Left", "images": ["Image/P18_D1_P8_Low_T2_16_Left/1.png", "Image/P18_D1_P8_Low_T2_16_Left/2.png", "Image/P18_D1_P8_Low_T2_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1098.0952 seconds to 1106.1129 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P5_Medium_T2_18_Mid", "images": ["Image/P18_D2_P5_Medium_T2_18_Mid/1.png", "Image/P18_D2_P5_Medium_T2_18_Mid/2.png", "Image/P18_D2_P5_Medium_T2_18_Mid/3.png", "Image/P18_D2_P5_Medium_T2_18_Mid/4.png", "Image/P18_D2_P5_Medium_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour water into the middle cup. Finish.\"\nThe frames roughly correspond to the speech interval from 424.4485 seconds to 434.9167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P7_High_T2_5_Mid", "images": ["Image/P16_D2_P7_High_T2_5_Mid/1.png", "Image/P16_D2_P7_High_T2_5_Mid/2.png", "Image/P16_D2_P7_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 974.0649 seconds to 982.7926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P6_Medium_T2_23_Left", "images": ["Image/P10_D2_P6_Medium_T2_23_Left/1.png", "Image/P10_D2_P6_Medium_T2_23_Left/2.png", "Image/P10_D2_P6_Medium_T2_23_Left/3.png", "Image/P10_D2_P6_Medium_T2_23_Left/4.png", "Image/P10_D2_P6_Medium_T2_23_Left/5.png", "Image/P10_D2_P6_Medium_T2_23_Left/6.png", "Image/P10_D2_P6_Medium_T2_23_Left/7.png", "Image/P10_D2_P6_Medium_T2_23_Left/8.png", "Image/P10_D2_P6_Medium_T2_23_Left/9.png", "Image/P10_D2_P6_Medium_T2_23_Left/10.png", "Image/P10_D2_P6_Medium_T2_23_Left/11.png", "Image/P10_D2_P6_Medium_T2_23_Left/12.png", "Image/P10_D2_P6_Medium_T2_23_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1532.51618 seconds to 1551.0434800000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P8_Low_T2_11_Mid", "images": ["Image/P17_D3_P8_Low_T2_11_Mid/1.png", "Image/P17_D3_P8_Low_T2_11_Mid/2.png", "Image/P17_D3_P8_Low_T2_11_Mid/3.png", "Image/P17_D3_P8_Low_T2_11_Mid/4.png", "Image/P17_D3_P8_Low_T2_11_Mid/5.png", "Image/P17_D3_P8_Low_T2_11_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that brown vase on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 831.4213500000001 seconds to 845.5496 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P3_Low_T2_26_Left", "images": ["Image/P3_D2_P3_Low_T2_26_Left/1.png", "Image/P3_D2_P3_Low_T2_26_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2270.3621000000003 seconds to 2280.6759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P8_High_T2_15_Right", "images": ["Image/P3_D2_P8_High_T2_15_Right/1.png", "Image/P3_D2_P8_High_T2_15_Right/2.png", "Image/P3_D2_P8_High_T2_15_Right/3.png", "Image/P3_D2_P8_High_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2439.8758000000003 seconds to 2449.6872000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P7_Low_T3_5_Left", "images": ["Image/P16_D1_P7_Low_T3_5_Left/1.png", "Image/P16_D1_P7_Low_T3_5_Left/2.png", "Image/P16_D1_P7_Low_T3_5_Left/3.png", "Image/P16_D1_P7_Low_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1299.0193 seconds to 1308.234 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P7_Medium_T2_7_Mid", "images": ["Image/P13_D2_P7_Medium_T2_7_Mid/1.png", "Image/P13_D2_P7_Medium_T2_7_Mid/2.png", "Image/P13_D2_P7_Medium_T2_7_Mid/3.png", "Image/P13_D2_P7_Medium_T2_7_Mid/4.png", "Image/P13_D2_P7_Medium_T2_7_Mid/5.png", "Image/P13_D2_P7_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you fill up the middle green glass with water\"\nThe frames roughly correspond to the speech interval from 840.8384900000001 seconds to 852.1985500000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P4_High_T3_13_Left", "images": ["Image/P2_D3_P4_High_T3_13_Left/1.png", "Image/P2_D3_P4_High_T3_13_Left/2.png", "Image/P2_D3_P4_High_T3_13_Left/3.png", "Image/P2_D3_P4_High_T3_13_Left/4.png", "Image/P2_D3_P4_High_T3_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1780.9569 seconds to 1796.3927000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P3_High_T1_12_Mid", "images": ["Image/P23_D2_P3_High_T1_12_Mid/1.png", "Image/P23_D2_P3_High_T1_12_Mid/2.png", "Image/P23_D2_P3_High_T1_12_Mid/3.png", "Image/P23_D2_P3_High_T1_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 194.1759 seconds to 203.7667 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P9_Low_T3_24_Right", "images": ["Image/P11_D3_P9_Low_T3_24_Right/1.png", "Image/P11_D3_P9_Low_T3_24_Right/2.png", "Image/P11_D3_P9_Low_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2003.3274 seconds to 2011.9852 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P5_Medium_T2_22_Mid", "images": ["Image/P2_D1_P5_Medium_T2_22_Mid/1.png", "Image/P2_D1_P5_Medium_T2_22_Mid/2.png", "Image/P2_D1_P5_Medium_T2_22_Mid/3.png", "Image/P2_D1_P5_Medium_T2_22_Mid/4.png", "Image/P2_D1_P5_Medium_T2_22_Mid/5.png", "Image/P2_D1_P5_Medium_T2_22_Mid/6.png", "Image/P2_D1_P5_Medium_T2_22_Mid/7.png", "Image/P2_D1_P5_Medium_T2_22_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Hello. Could you please water the plant on the very right side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 650.7243 seconds to 665.8384 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P3_Medium_T3_18_Right", "images": ["Image/P11_D3_P3_Medium_T3_18_Right/1.png", "Image/P11_D3_P3_Medium_T3_18_Right/2.png", "Image/P11_D3_P3_Medium_T3_18_Right/3.png", "Image/P11_D3_P3_Medium_T3_18_Right/4.png", "Image/P11_D3_P3_Medium_T3_18_Right/5.png", "Image/P11_D3_P3_Medium_T3_18_Right/6.png", "Image/P11_D3_P3_Medium_T3_18_Right/7.png", "Image/P11_D3_P3_Medium_T3_18_Right/8.png", "Image/P11_D3_P3_Medium_T3_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first glass from the right side on the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1618.9003 seconds to 1633.9885 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P3_Low_T1_1_Left", "images": ["Image/P26_D1_P3_Low_T1_1_Left/1.png", "Image/P26_D1_P3_Low_T1_1_Left/2.png", "Image/P26_D1_P3_Low_T1_1_Left/3.png", "Image/P26_D1_P3_Low_T1_1_Left/4.png", "Image/P26_D1_P3_Low_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this mug down here? Finish\"\nThe frames roughly correspond to the speech interval from 574.6936999999999 seconds to 585.6207999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P7_Medium_T1_19_Mid", "images": ["Image/P16_D2_P7_Medium_T1_19_Mid/1.png", "Image/P16_D2_P7_Medium_T1_19_Mid/2.png", "Image/P16_D2_P7_Medium_T1_19_Mid/3.png", "Image/P16_D2_P7_Medium_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 761.2017000000001 seconds to 770.9187000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P3_Low_T3_18_Mid", "images": ["Image/P29_D1_P3_Low_T3_18_Mid/1.png", "Image/P29_D1_P3_Low_T3_18_Mid/2.png", "Image/P29_D1_P3_Low_T3_18_Mid/3.png", "Image/P29_D1_P3_Low_T3_18_Mid/4.png", "Image/P29_D1_P3_Low_T3_18_Mid/5.png", "Image/P29_D1_P3_Low_T3_18_Mid/6.png", "Image/P29_D1_P3_Low_T3_18_Mid/7.png", "Image/P29_D1_P3_Low_T3_18_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2564.0809 seconds to 2578.8621 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P7_Low_T1_20_Mid", "images": ["Image/P28_D2_P7_Low_T1_20_Mid/1.png", "Image/P28_D2_P7_Low_T1_20_Mid/2.png", "Image/P28_D2_P7_Low_T1_20_Mid/3.png", "Image/P28_D2_P7_Low_T1_20_Mid/4.png", "Image/P28_D2_P7_Low_T1_20_Mid/5.png", "Image/P28_D2_P7_Low_T1_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that white base the very top of the bookshelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1028.9339 seconds to 1040.111 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P3_High_T2_1_Mid", "images": ["Image/P5_D1_P3_High_T2_1_Mid/1.png", "Image/P5_D1_P3_High_T2_1_Mid/2.png", "Image/P5_D1_P3_High_T2_1_Mid/3.png", "Image/P5_D1_P3_High_T2_1_Mid/4.png", "Image/P5_D1_P3_High_T2_1_Mid/5.png", "Image/P5_D1_P3_High_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle which is next to the plant. Finished.\"\nThe frames roughly correspond to the speech interval from 1748.8781 seconds to 1761.1119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P5_Low_T2_2_Mid", "images": ["Image/P21_D2_P5_Low_T2_2_Mid/1.png", "Image/P21_D2_P5_Low_T2_2_Mid/2.png", "Image/P21_D2_P5_Low_T2_2_Mid/3.png", "Image/P21_D2_P5_Low_T2_2_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1724.6729 seconds to 1735.3074 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P6_High_T3_24_Mid", "images": ["Image/P29_D2_P6_High_T3_24_Mid/1.png", "Image/P29_D2_P6_High_T3_24_Mid/2.png", "Image/P29_D2_P6_High_T3_24_Mid/3.png", "Image/P29_D2_P6_High_T3_24_Mid/4.png", "Image/P29_D2_P6_High_T3_24_Mid/5.png", "Image/P29_D2_P6_High_T3_24_Mid/6.png", "Image/P29_D2_P6_High_T3_24_Mid/7.png", "Image/P29_D2_P6_High_T3_24_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start\"\nThe frames roughly correspond to the speech interval from 343.77139999999997 seconds to 357.00919999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P7_Low_T3_26_Left", "images": ["Image/P17_D2_P7_Low_T3_26_Left/1.png", "Image/P17_D2_P7_Low_T3_26_Left/2.png", "Image/P17_D2_P7_Low_T3_26_Left/3.png", "Image/P17_D2_P7_Low_T3_26_Left/4.png", "Image/P17_D2_P7_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that white mug down there? Finish.\"\nThe frames roughly correspond to the speech interval from 402.5454 seconds to 414.7528999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P3_Low_T3_21_Right", "images": ["Image/P18_D3_P3_Low_T3_21_Right/1.png", "Image/P18_D3_P3_Low_T3_21_Right/2.png", "Image/P18_D3_P3_Low_T3_21_Right/3.png", "Image/P18_D3_P3_Low_T3_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1319.3857 seconds to 1328.8932 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P5_Low_T3_6_Right", "images": ["Image/P8_D3_P5_Low_T3_6_Right/1.png", "Image/P8_D3_P5_Low_T3_6_Right/2.png", "Image/P8_D3_P5_Low_T3_6_Right/3.png", "Image/P8_D3_P5_Low_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you clean that planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1748.957 seconds to 1758.4501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P5_High_T1_27_Left", "images": ["Image/P4_D2_P5_High_T1_27_Left/1.png", "Image/P4_D2_P5_High_T1_27_Left/2.png", "Image/P4_D2_P5_High_T1_27_Left/3.png", "Image/P4_D2_P5_High_T1_27_Left/4.png", "Image/P4_D2_P5_High_T1_27_Left/5.png", "Image/P4_D2_P5_High_T1_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white cap to the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1553.0678999999998 seconds to 1566.3331999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P4_High_T3_16_Mid", "images": ["Image/P24_D3_P4_High_T3_16_Mid/1.png", "Image/P24_D3_P4_High_T3_16_Mid/2.png", "Image/P24_D3_P4_High_T3_16_Mid/3.png", "Image/P24_D3_P4_High_T3_16_Mid/4.png", "Image/P24_D3_P4_High_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1812.9051 seconds to 1823.1087 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P8_High_T2_9_Mid", "images": ["Image/P8_D1_P8_High_T2_9_Mid/1.png", "Image/P8_D1_P8_High_T2_9_Mid/2.png", "Image/P8_D1_P8_High_T2_9_Mid/3.png", "Image/P8_D1_P8_High_T2_9_Mid/4.png", "Image/P8_D1_P8_High_T2_9_Mid/5.png", "Image/P8_D1_P8_High_T2_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 863.6338999999999 seconds to 876.9366999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P1_Medium_T2_22_Mid", "images": ["Image/P17_D1_P1_Medium_T2_22_Mid/1.png", "Image/P17_D1_P1_Medium_T2_22_Mid/2.png", "Image/P17_D1_P1_Medium_T2_22_Mid/3.png", "Image/P17_D1_P1_Medium_T2_22_Mid/4.png", "Image/P17_D1_P1_Medium_T2_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1583.8461000000002 seconds to 1595.9425 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P5_Low_T3_13_Mid", "images": ["Image/P6_D2_P5_Low_T3_13_Mid/1.png", "Image/P6_D2_P5_Low_T3_13_Mid/2.png", "Image/P6_D2_P5_Low_T3_13_Mid/3.png", "Image/P6_D2_P5_Low_T3_13_Mid/4.png", "Image/P6_D2_P5_Low_T3_13_Mid/5.png", "Image/P6_D2_P5_Low_T3_13_Mid/6.png", "Image/P6_D2_P5_Low_T3_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 1060.1826 seconds to 1075.4541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P9_High_T2_6_Mid", "images": ["Image/P26_D1_P9_High_T2_6_Mid/1.png", "Image/P26_D1_P9_High_T2_6_Mid/2.png", "Image/P26_D1_P9_High_T2_6_Mid/3.png", "Image/P26_D1_P9_High_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish\"\nThe frames roughly correspond to the speech interval from 132.221 seconds to 142.38289999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P9_Low_T1_24_Left", "images": ["Image/P8_D1_P9_Low_T1_24_Left/1.png", "Image/P8_D1_P9_Low_T1_24_Left/2.png", "Image/P8_D1_P9_Low_T1_24_Left/3.png", "Image/P8_D1_P9_Low_T1_24_Left/4.png", "Image/P8_D1_P9_Low_T1_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 767.9563999999999 seconds to 779.3004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P4_High_T3_20_Mid", "images": ["Image/P5_D2_P4_High_T3_20_Mid/1.png", "Image/P5_D2_P4_High_T3_20_Mid/2.png", "Image/P5_D2_P4_High_T3_20_Mid/3.png", "Image/P5_D2_P4_High_T3_20_Mid/4.png", "Image/P5_D2_P4_High_T3_20_Mid/5.png", "Image/P5_D2_P4_High_T3_20_Mid/6.png", "Image/P5_D2_P4_High_T3_20_Mid/7.png", "Image/P5_D2_P4_High_T3_20_Mid/8.png", "Image/P5_D2_P4_High_T3_20_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean the bottle in white color\"\nThe frames roughly correspond to the speech interval from 167.70989999999998 seconds to 183.4535 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P1_Low_T1_10_Mid", "images": ["Image/P19_D2_P1_Low_T1_10_Mid/1.png", "Image/P19_D2_P1_Low_T1_10_Mid/2.png", "Image/P19_D2_P1_Low_T1_10_Mid/3.png", "Image/P19_D2_P1_Low_T1_10_Mid/4.png", "Image/P19_D2_P1_Low_T1_10_Mid/5.png", "Image/P19_D2_P1_Low_T1_10_Mid/6.png", "Image/P19_D2_P1_Low_T1_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container down here.\"\nThe frames roughly correspond to the speech interval from 1729.9913 seconds to 1747.2225999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P9_Low_T1_15_Mid", "images": ["Image/P12_D3_P9_Low_T1_15_Mid/1.png", "Image/P12_D3_P9_Low_T1_15_Mid/2.png", "Image/P12_D3_P9_Low_T1_15_Mid/3.png", "Image/P12_D3_P9_Low_T1_15_Mid/4.png", "Image/P12_D3_P9_Low_T1_15_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2474.26009 seconds to 2486.8283899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P5_High_T1_6_Right", "images": ["Image/P1_D2_P5_High_T1_6_Right/1.png", "Image/P1_D2_P5_High_T1_6_Right/2.png", "Image/P1_D2_P5_High_T1_6_Right/3.png", "Image/P1_D2_P5_High_T1_6_Right/4.png", "Image/P1_D2_P5_High_T1_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 771.02713 seconds to 781.31479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P7_High_T2_22_Left", "images": ["Image/P7_D1_P7_High_T2_22_Left/1.png", "Image/P7_D1_P7_High_T2_22_Left/2.png", "Image/P7_D1_P7_High_T2_22_Left/3.png", "Image/P7_D1_P7_High_T2_22_Left/4.png", "Image/P7_D1_P7_High_T2_22_Left/5.png", "Image/P7_D1_P7_High_T2_22_Left/6.png", "Image/P7_D1_P7_High_T2_22_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 519.5008 seconds to 534.2692999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P7_Medium_T1_15_Right", "images": ["Image/P21_D1_P7_Medium_T1_15_Right/1.png", "Image/P21_D1_P7_Medium_T1_15_Right/2.png", "Image/P21_D1_P7_Medium_T1_15_Right/3.png", "Image/P21_D1_P7_Medium_T1_15_Right/4.png", "Image/P21_D1_P7_Medium_T1_15_Right/5.png", "Image/P21_D1_P7_Medium_T1_15_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1138.346 seconds to 1151.1799 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P5_Medium_T2_5_Mid", "images": ["Image/P11_D1_P5_Medium_T2_5_Mid/1.png", "Image/P11_D1_P5_Medium_T2_5_Mid/2.png", "Image/P11_D1_P5_Medium_T2_5_Mid/3.png", "Image/P11_D1_P5_Medium_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the second plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2394.21386 seconds to 2406.21251 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P7_High_T3_4_Mid", "images": ["Image/P2_D1_P7_High_T3_4_Mid/1.png", "Image/P2_D1_P7_High_T3_4_Mid/2.png", "Image/P2_D1_P7_High_T3_4_Mid/3.png", "Image/P2_D1_P7_High_T3_4_Mid/4.png", "Image/P2_D1_P7_High_T3_4_Mid/5.png", "Image/P2_D1_P7_High_T3_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 972.3164 seconds to 986.4286 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P9_Medium_T3_2_Left", "images": ["Image/P26_D3_P9_Medium_T3_2_Left/1.png", "Image/P26_D3_P9_Medium_T3_2_Left/2.png", "Image/P26_D3_P9_Medium_T3_2_Left/3.png", "Image/P26_D3_P9_Medium_T3_2_Left/4.png", "Image/P26_D3_P9_Medium_T3_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the leftmost face down there? Finish.\"\nThe frames roughly correspond to the speech interval from 671.51556 seconds to 686.22879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P7_High_T3_11_Mid", "images": ["Image/P17_D3_P7_High_T3_11_Mid/1.png", "Image/P17_D3_P7_High_T3_11_Mid/2.png", "Image/P17_D3_P7_High_T3_11_Mid/3.png", "Image/P17_D3_P7_High_T3_11_Mid/4.png", "Image/P17_D3_P7_High_T3_11_Mid/5.png", "Image/P17_D3_P7_High_T3_11_Mid/6.png", "Image/P17_D3_P7_High_T3_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the brown vase on the floor under the desk? Finish\"\nThe frames roughly correspond to the speech interval from 1177.7473 seconds to 1190.5949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P4_Low_T2_14_Mid", "images": ["Image/P5_D3_P4_Low_T2_14_Mid/1.png", "Image/P5_D3_P4_Low_T2_14_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Water the plant on the table.\"\nThe frames roughly correspond to the speech interval from 2617.7093999999997 seconds to 2628.4822999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P8_High_T3_17_Mid", "images": ["Image/P26_D1_P8_High_T3_17_Mid/1.png", "Image/P26_D1_P8_High_T3_17_Mid/2.png", "Image/P26_D1_P8_High_T3_17_Mid/3.png", "Image/P26_D1_P8_High_T3_17_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this face in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 278.841 seconds to 295.0004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P9_Medium_T2_1_Mid", "images": ["Image/P24_D1_P9_Medium_T2_1_Mid/1.png", "Image/P24_D1_P9_Medium_T2_1_Mid/2.png", "Image/P24_D1_P9_Medium_T2_1_Mid/3.png", "Image/P24_D1_P9_Medium_T2_1_Mid/4.png", "Image/P24_D1_P9_Medium_T2_1_Mid/5.png", "Image/P24_D1_P9_Medium_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in the middle bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 903.6796 seconds to 916.1513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P4_High_T1_23_Right", "images": ["Image/P24_D1_P4_High_T1_23_Right/1.png", "Image/P24_D1_P4_High_T1_23_Right/2.png", "Image/P24_D1_P4_High_T1_23_Right/3.png", "Image/P24_D1_P4_High_T1_23_Right/4.png", "Image/P24_D1_P4_High_T1_23_Right/5.png", "Image/P24_D1_P4_High_T1_23_Right/6.png", "Image/P24_D1_P4_High_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this planet over here please? Finish.\"\nThe frames roughly correspond to the speech interval from 1009.9036 seconds to 1022.3464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P5_High_T2_17_Right", "images": ["Image/P13_D1_P5_High_T2_17_Right/1.png", "Image/P13_D1_P5_High_T2_17_Right/2.png", "Image/P13_D1_P5_High_T2_17_Right/3.png", "Image/P13_D1_P5_High_T2_17_Right/4.png", "Image/P13_D1_P5_High_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you pour water into this cup right here? Finish.\"\nThe frames roughly correspond to the speech interval from 176.4421 seconds to 188.09081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P4_Medium_T2_21_Left", "images": ["Image/P18_D3_P4_Medium_T2_21_Left/1.png", "Image/P18_D3_P4_Medium_T2_21_Left/2.png", "Image/P18_D3_P4_Medium_T2_21_Left/3.png", "Image/P18_D3_P4_Medium_T2_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1549.928 seconds to 1561.0076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P8_Low_T2_27_Left", "images": ["Image/P18_D3_P8_Low_T2_27_Left/1.png", "Image/P18_D3_P8_Low_T2_27_Left/2.png", "Image/P18_D3_P8_Low_T2_27_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1387.0647 seconds to 1396.6691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P3_High_T3_16_Right", "images": ["Image/P7_D1_P3_High_T3_16_Right/1.png", "Image/P7_D1_P3_High_T3_16_Right/2.png", "Image/P7_D1_P3_High_T3_16_Right/3.png", "Image/P7_D1_P3_High_T3_16_Right/4.png", "Image/P7_D1_P3_High_T3_16_Right/5.png", "Image/P7_D1_P3_High_T3_16_Right/6.png", "Image/P7_D1_P3_High_T3_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 355.566 seconds to 370.1391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P6_High_T2_10_Left", "images": ["Image/P17_D2_P6_High_T2_10_Left/1.png", "Image/P17_D2_P6_High_T2_10_Left/2.png", "Image/P17_D2_P6_High_T2_10_Left/3.png", "Image/P17_D2_P6_High_T2_10_Left/4.png", "Image/P17_D2_P6_High_T2_10_Left/5.png", "Image/P17_D2_P6_High_T2_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that brown cup right there? Finish.\"\nThe frames roughly correspond to the speech interval from 464.7547 seconds to 477.4629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P7_Low_T1_20_Mid", "images": ["Image/P10_D3_P7_Low_T1_20_Mid/1.png", "Image/P10_D3_P7_Low_T1_20_Mid/2.png", "Image/P10_D3_P7_Low_T1_20_Mid/3.png", "Image/P10_D3_P7_Low_T1_20_Mid/4.png", "Image/P10_D3_P7_Low_T1_20_Mid/5.png", "Image/P10_D3_P7_Low_T1_20_Mid/6.png", "Image/P10_D3_P7_Low_T1_20_Mid/7.png", "Image/P10_D3_P7_Low_T1_20_Mid/8.png", "Image/P10_D3_P7_Low_T1_20_Mid/9.png", "Image/P10_D3_P7_Low_T1_20_Mid/10.png", "Image/P10_D3_P7_Low_T1_20_Mid/11.png", "Image/P10_D3_P7_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 521.46539 seconds to 538.75329 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P1_Medium_T3_5_Right", "images": ["Image/P19_D2_P1_Medium_T3_5_Right/1.png", "Image/P19_D2_P1_Medium_T3_5_Right/2.png", "Image/P19_D2_P1_Medium_T3_5_Right/3.png", "Image/P19_D2_P1_Medium_T3_5_Right/4.png", "Image/P19_D2_P1_Medium_T3_5_Right/5.png", "Image/P19_D2_P1_Medium_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1779.3505 seconds to 1792.0587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P9_Low_T1_5_Mid", "images": ["Image/P25_D1_P9_Low_T1_5_Mid/1.png", "Image/P25_D1_P9_Low_T1_5_Mid/2.png", "Image/P25_D1_P9_Low_T1_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 423.0776 seconds to 431.821 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P5_High_T1_5_Right", "images": ["Image/P29_D3_P5_High_T1_5_Right/1.png", "Image/P29_D3_P5_High_T1_5_Right/2.png", "Image/P29_D3_P5_High_T1_5_Right/3.png", "Image/P29_D3_P5_High_T1_5_Right/4.png", "Image/P29_D3_P5_High_T1_5_Right/5.png", "Image/P29_D3_P5_High_T1_5_Right/6.png", "Image/P29_D3_P5_High_T1_5_Right/7.png", "Image/P29_D3_P5_High_T1_5_Right/8.png", "Image/P29_D3_P5_High_T1_5_Right/9.png", "Image/P29_D3_P5_High_T1_5_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1739.9511999999995 seconds to 1755.0679999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P3_High_T2_14_Left", "images": ["Image/P22_D1_P3_High_T2_14_Left/1.png", "Image/P22_D1_P3_High_T2_14_Left/2.png", "Image/P22_D1_P3_High_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1326.8468 seconds to 1338.46773 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P7_Low_T2_12_Right", "images": ["Image/P29_D1_P7_Low_T2_12_Right/1.png", "Image/P29_D1_P7_Low_T2_12_Right/2.png", "Image/P29_D1_P7_Low_T2_12_Right/3.png", "Image/P29_D1_P7_Low_T2_12_Right/4.png", "Image/P29_D1_P7_Low_T2_12_Right/5.png", "Image/P29_D1_P7_Low_T2_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2689.2765999999997 seconds to 2700.6065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P6_Medium_T3_2_Mid", "images": ["Image/P8_D1_P6_Medium_T3_2_Mid/1.png", "Image/P8_D1_P6_Medium_T3_2_Mid/2.png", "Image/P8_D1_P6_Medium_T3_2_Mid/3.png", "Image/P8_D1_P6_Medium_T3_2_Mid/4.png", "Image/P8_D1_P6_Medium_T3_2_Mid/5.png", "Image/P8_D1_P6_Medium_T3_2_Mid/6.png", "Image/P8_D1_P6_Medium_T3_2_Mid/7.png", "Image/P8_D1_P6_Medium_T3_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the green bottle that is in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 739.7114999999999 seconds to 754.8883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P8_Low_T1_14_Mid", "images": ["Image/P7_D3_P8_Low_T1_14_Mid/1.png", "Image/P7_D3_P8_Low_T1_14_Mid/2.png", "Image/P7_D3_P8_Low_T1_14_Mid/3.png", "Image/P7_D3_P8_Low_T1_14_Mid/4.png", "Image/P7_D3_P8_Low_T1_14_Mid/5.png", "Image/P7_D3_P8_Low_T1_14_Mid/6.png", "Image/P7_D3_P8_Low_T1_14_Mid/7.png", "Image/P7_D3_P8_Low_T1_14_Mid/8.png", "Image/P7_D3_P8_Low_T1_14_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1957.8357 seconds to 1974.5677 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P4_Medium_T3_1_Right", "images": ["Image/P3_D1_P4_Medium_T3_1_Right/1.png", "Image/P3_D1_P4_Medium_T3_1_Right/2.png", "Image/P3_D1_P4_Medium_T3_1_Right/3.png", "Image/P3_D1_P4_Medium_T3_1_Right/4.png", "Image/P3_D1_P4_Medium_T3_1_Right/5.png", "Image/P3_D1_P4_Medium_T3_1_Right/6.png", "Image/P3_D1_P4_Medium_T3_1_Right/7.png", "Image/P3_D1_P4_Medium_T3_1_Right/8.png", "Image/P3_D1_P4_Medium_T3_1_Right/9.png", "Image/P3_D1_P4_Medium_T3_1_Right/10.png", "Image/P3_D1_P4_Medium_T3_1_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1649.6369 seconds to 1666.0914 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P3_High_T1_14_Right", "images": ["Image/P18_D3_P3_High_T1_14_Right/1.png", "Image/P18_D3_P3_High_T1_14_Right/2.png", "Image/P18_D3_P3_High_T1_14_Right/3.png", "Image/P18_D3_P3_High_T1_14_Right/4.png", "Image/P18_D3_P3_High_T1_14_Right/5.png", "Image/P18_D3_P3_High_T1_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1212.1046700000002 seconds to 1222.90558 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P3_Medium_T2_14_Mid", "images": ["Image/P21_D2_P3_Medium_T2_14_Mid/1.png", "Image/P21_D2_P3_Medium_T2_14_Mid/2.png", "Image/P21_D2_P3_Medium_T2_14_Mid/3.png", "Image/P21_D2_P3_Medium_T2_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1796.9160000000002 seconds to 1807.6067 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P8_Medium_T1_9_Mid", "images": ["Image/P8_D3_P8_Medium_T1_9_Mid/1.png", "Image/P8_D3_P8_Medium_T1_9_Mid/2.png", "Image/P8_D3_P8_Medium_T1_9_Mid/3.png", "Image/P8_D3_P8_Medium_T1_9_Mid/4.png", "Image/P8_D3_P8_Medium_T1_9_Mid/5.png", "Image/P8_D3_P8_Medium_T1_9_Mid/6.png", "Image/P8_D3_P8_Medium_T1_9_Mid/7.png", "Image/P8_D3_P8_Medium_T1_9_Mid/8.png", "Image/P8_D3_P8_Medium_T1_9_Mid/9.png", "Image/P8_D3_P8_Medium_T1_9_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the green glass in the middle? Move it to the top floor of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1842.9176 seconds to 1861.4296 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P4_Low_T1_11_Left", "images": ["Image/P4_D1_P4_Low_T1_11_Left/1.png", "Image/P4_D1_P4_Low_T1_11_Left/2.png", "Image/P4_D1_P4_Low_T1_11_Left/3.png", "Image/P4_D1_P4_Low_T1_11_Left/4.png", "Image/P4_D1_P4_Low_T1_11_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the bottle to the middle of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2370.6358 seconds to 2382.8981 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P4_Low_T3_27_Right", "images": ["Image/P17_D1_P4_Low_T3_27_Right/1.png", "Image/P17_D1_P4_Low_T3_27_Right/2.png", "Image/P17_D1_P4_Low_T3_27_Right/3.png", "Image/P17_D1_P4_Low_T3_27_Right/4.png", "Image/P17_D1_P4_Low_T3_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white mug next to me? Finish\"\nThe frames roughly correspond to the speech interval from 1858.0685 seconds to 1869.4258 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P6_High_T3_10_Mid", "images": ["Image/P26_D3_P6_High_T3_10_Mid/1.png", "Image/P26_D3_P6_High_T3_10_Mid/2.png", "Image/P26_D3_P6_High_T3_10_Mid/3.png", "Image/P26_D3_P6_High_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 789.5983 seconds to 799.5228 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P3_High_T2_25_Mid", "images": ["Image/P21_D2_P3_High_T2_25_Mid/1.png", "Image/P21_D2_P3_High_T2_25_Mid/2.png", "Image/P21_D2_P3_High_T2_25_Mid/3.png", "Image/P21_D2_P3_High_T2_25_Mid/4.png", "Image/P21_D2_P3_High_T2_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1640.0946 seconds to 1651.2435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P8_Medium_T1_4_Left", "images": ["Image/P9_D2_P8_Medium_T1_4_Left/1.png", "Image/P9_D2_P8_Medium_T1_4_Left/2.png", "Image/P9_D2_P8_Medium_T1_4_Left/3.png", "Image/P9_D2_P8_Medium_T1_4_Left/4.png", "Image/P9_D2_P8_Medium_T1_4_Left/5.png", "Image/P9_D2_P8_Medium_T1_4_Left/6.png", "Image/P9_D2_P8_Medium_T1_4_Left/7.png", "Image/P9_D2_P8_Medium_T1_4_Left/8.png", "Image/P9_D2_P8_Medium_T1_4_Left/9.png", "Image/P9_D2_P8_Medium_T1_4_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the left plant from the floor to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 3198.9429 seconds to 3215.759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P2_Medium_T2_5_Mid", "images": ["Image/P12_D3_P2_Medium_T2_5_Mid/1.png", "Image/P12_D3_P2_Medium_T2_5_Mid/2.png", "Image/P12_D3_P2_Medium_T2_5_Mid/3.png", "Image/P12_D3_P2_Medium_T2_5_Mid/4.png", "Image/P12_D3_P2_Medium_T2_5_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2646.01779 seconds to 2661.03639 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P9_Medium_T1_9_Right", "images": ["Image/P9_D3_P9_Medium_T1_9_Right/1.png", "Image/P9_D3_P9_Medium_T1_9_Right/2.png", "Image/P9_D3_P9_Medium_T1_9_Right/3.png", "Image/P9_D3_P9_Medium_T1_9_Right/4.png", "Image/P9_D3_P9_Medium_T1_9_Right/5.png", "Image/P9_D3_P9_Medium_T1_9_Right/6.png", "Image/P9_D3_P9_Medium_T1_9_Right/7.png", "Image/P9_D3_P9_Medium_T1_9_Right/8.png", "Image/P9_D3_P9_Medium_T1_9_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the most right cup to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 613.7278 seconds to 633.0442 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P8_High_T2_25_Right", "images": ["Image/P30_D2_P8_High_T2_25_Right/1.png", "Image/P30_D2_P8_High_T2_25_Right/2.png", "Image/P30_D2_P8_High_T2_25_Right/3.png", "Image/P30_D2_P8_High_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 340.3751 seconds to 350.50739999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P5_High_T1_13_Right", "images": ["Image/P24_D3_P5_High_T1_13_Right/1.png", "Image/P24_D3_P5_High_T1_13_Right/2.png", "Image/P24_D3_P5_High_T1_13_Right/3.png", "Image/P24_D3_P5_High_T1_13_Right/4.png", "Image/P24_D3_P5_High_T1_13_Right/5.png", "Image/P24_D3_P5_High_T1_13_Right/6.png", "Image/P24_D3_P5_High_T1_13_Right/7.png", "Image/P24_D3_P5_High_T1_13_Right/8.png", "Image/P24_D3_P5_High_T1_13_Right/9.png", "Image/P24_D3_P5_High_T1_13_Right/10.png", "Image/P24_D3_P5_High_T1_13_Right/11.png", "Image/P24_D3_P5_High_T1_13_Right/12.png", "Image/P24_D3_P5_High_T1_13_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1595.54646 seconds to 1608.3801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P3_High_T2_7_Right", "images": ["Image/P29_D2_P3_High_T2_7_Right/1.png", "Image/P29_D2_P3_High_T2_7_Right/2.png", "Image/P29_D2_P3_High_T2_7_Right/3.png", "Image/P29_D2_P3_High_T2_7_Right/4.png", "Image/P29_D2_P3_High_T2_7_Right/5.png", "Image/P29_D2_P3_High_T2_7_Right/6.png", "Image/P29_D2_P3_High_T2_7_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi Friday\"\nThe frames roughly correspond to the speech interval from 621.0014 seconds to 634.9894 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P9_Low_T1_7_Mid", "images": ["Image/P9_D1_P9_Low_T1_7_Mid/1.png", "Image/P9_D1_P9_Low_T1_7_Mid/2.png", "Image/P9_D1_P9_Low_T1_7_Mid/3.png", "Image/P9_D1_P9_Low_T1_7_Mid/4.png", "Image/P9_D1_P9_Low_T1_7_Mid/5.png", "Image/P9_D1_P9_Low_T1_7_Mid/6.png", "Image/P9_D1_P9_Low_T1_7_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the green cup in the box on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1885.1361 seconds to 1903.4104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P2_Medium_T2_27_Mid", "images": ["Image/P4_D1_P2_Medium_T2_27_Mid/1.png", "Image/P4_D1_P2_Medium_T2_27_Mid/2.png", "Image/P4_D1_P2_Medium_T2_27_Mid/3.png", "Image/P4_D1_P2_Medium_T2_27_Mid/4.png", "Image/P4_D1_P2_Medium_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the middle bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2346.7362 seconds to 2359.861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P6_Low_T2_10_Mid", "images": ["Image/P15_D2_P6_Low_T2_10_Mid/1.png", "Image/P15_D2_P6_Low_T2_10_Mid/2.png", "Image/P15_D2_P6_Low_T2_10_Mid/3.png", "Image/P15_D2_P6_Low_T2_10_Mid/4.png", "Image/P15_D2_P6_Low_T2_10_Mid/5.png", "Image/P15_D2_P6_Low_T2_10_Mid/6.png", "Image/P15_D2_P6_Low_T2_10_Mid/7.png", "Image/P15_D2_P6_Low_T2_10_Mid/8.png", "Image/P15_D2_P6_Low_T2_10_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the brown Container at the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3099.6387 seconds to 3113.4182 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P4_High_T2_16_Right", "images": ["Image/P21_D3_P4_High_T2_16_Right/1.png", "Image/P21_D3_P4_High_T2_16_Right/2.png", "Image/P21_D3_P4_High_T2_16_Right/3.png", "Image/P21_D3_P4_High_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 287.04415 seconds to 300.65797999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P2_Low_T2_1_Mid", "images": ["Image/P17_D1_P2_Low_T2_1_Mid/1.png", "Image/P17_D1_P2_Low_T2_1_Mid/2.png", "Image/P17_D1_P2_Low_T2_1_Mid/3.png", "Image/P17_D1_P2_Low_T2_1_Mid/4.png", "Image/P17_D1_P2_Low_T2_1_Mid/5.png", "Image/P17_D1_P2_Low_T2_1_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this green vase right in front of me? Finish\"\nThe frames roughly correspond to the speech interval from 1644.663 seconds to 1656.3967000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P7_Medium_T2_7_Left", "images": ["Image/P29_D3_P7_Medium_T2_7_Left/1.png", "Image/P29_D3_P7_Medium_T2_7_Left/2.png", "Image/P29_D3_P7_Medium_T2_7_Left/3.png", "Image/P29_D3_P7_Medium_T2_7_Left/4.png", "Image/P29_D3_P7_Medium_T2_7_Left/5.png", "Image/P29_D3_P7_Medium_T2_7_Left/6.png", "Image/P29_D3_P7_Medium_T2_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1634.7036999999998 seconds to 1646.8548999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P4_Low_T2_17_Left", "images": ["Image/P27_D3_P4_Low_T2_17_Left/1.png", "Image/P27_D3_P4_Low_T2_17_Left/2.png", "Image/P27_D3_P4_Low_T2_17_Left/3.png", "Image/P27_D3_P4_Low_T2_17_Left/4.png", "Image/P27_D3_P4_Low_T2_17_Left/5.png", "Image/P27_D3_P4_Low_T2_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that glass on the table? Finish\"\nThe frames roughly correspond to the speech interval from 228.98800000000003 seconds to 240.123 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P3_High_T2_7_Mid", "images": ["Image/P8_D1_P3_High_T2_7_Mid/1.png", "Image/P8_D1_P3_High_T2_7_Mid/2.png", "Image/P8_D1_P3_High_T2_7_Mid/3.png", "Image/P8_D1_P3_High_T2_7_Mid/4.png", "Image/P8_D1_P3_High_T2_7_Mid/5.png", "Image/P8_D1_P3_High_T2_7_Mid/6.png", "Image/P8_D1_P3_High_T2_7_Mid/7.png", "Image/P8_D1_P3_High_T2_7_Mid/8.png", "Image/P8_D1_P3_High_T2_7_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 101.12759999999992 seconds to 119.26345999999992 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P3_Low_T1_2_Left", "images": ["Image/P12_D1_P3_Low_T1_2_Left/1.png", "Image/P12_D1_P3_Low_T1_2_Left/2.png", "Image/P12_D1_P3_Low_T1_2_Left/3.png", "Image/P12_D1_P3_Low_T1_2_Left/4.png", "Image/P12_D1_P3_Low_T1_2_Left/5.png", "Image/P12_D1_P3_Low_T1_2_Left/6.png", "Image/P12_D1_P3_Low_T1_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1723.4441000000002 seconds to 1737.8918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P4_Low_T3_11_Mid", "images": ["Image/P21_D3_P4_Low_T3_11_Mid/1.png", "Image/P21_D3_P4_Low_T3_11_Mid/2.png", "Image/P21_D3_P4_Low_T3_11_Mid/3.png", "Image/P21_D3_P4_Low_T3_11_Mid/4.png", "Image/P21_D3_P4_Low_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 310.8621 seconds to 322.652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P2_Medium_T3_21_Left", "images": ["Image/P23_D3_P2_Medium_T3_21_Left/1.png", "Image/P23_D3_P2_Medium_T3_21_Left/2.png", "Image/P23_D3_P2_Medium_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 698.9872 seconds to 708.8133 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P4_Medium_T2_3_Mid", "images": ["Image/P13_D3_P4_Medium_T2_3_Mid/1.png", "Image/P13_D3_P4_Medium_T2_3_Mid/2.png", "Image/P13_D3_P4_Medium_T2_3_Mid/3.png", "Image/P13_D3_P4_Medium_T2_3_Mid/4.png", "Image/P13_D3_P4_Medium_T2_3_Mid/5.png", "Image/P13_D3_P4_Medium_T2_3_Mid/6.png", "Image/P13_D3_P4_Medium_T2_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday Can you fill the vase in the middle with water\"\nThe frames roughly correspond to the speech interval from 1868.0001 seconds to 1880.7093 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P1_Low_T2_15_Mid", "images": ["Image/P18_D3_P1_Low_T2_15_Mid/1.png", "Image/P18_D3_P1_Low_T2_15_Mid/2.png", "Image/P18_D3_P1_Low_T2_15_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1471.1689 seconds to 1478.8524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P8_Medium_T3_12_Mid", "images": ["Image/P12_D3_P8_Medium_T3_12_Mid/1.png", "Image/P12_D3_P8_Medium_T3_12_Mid/2.png", "Image/P12_D3_P8_Medium_T3_12_Mid/3.png", "Image/P12_D3_P8_Medium_T3_12_Mid/4.png", "Image/P12_D3_P8_Medium_T3_12_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2493.0255899999997 seconds to 2505.66269 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P6_Medium_T1_3_Right", "images": ["Image/P23_D1_P6_Medium_T1_3_Right/1.png", "Image/P23_D1_P6_Medium_T1_3_Right/2.png", "Image/P23_D1_P6_Medium_T1_3_Right/3.png", "Image/P23_D1_P6_Medium_T1_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1815.8466 seconds to 1825.5341 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P4_Medium_T2_7_Mid", "images": ["Image/P2_D2_P4_Medium_T2_7_Mid/1.png", "Image/P2_D2_P4_Medium_T2_7_Mid/2.png", "Image/P2_D2_P4_Medium_T2_7_Mid/3.png", "Image/P2_D2_P4_Medium_T2_7_Mid/4.png", "Image/P2_D2_P4_Medium_T2_7_Mid/5.png", "Image/P2_D2_P4_Medium_T2_7_Mid/6.png", "Image/P2_D2_P4_Medium_T2_7_Mid/7.png", "Image/P2_D2_P4_Medium_T2_7_Mid/8.png", "Image/P2_D2_P4_Medium_T2_7_Mid/9.png", "Image/P2_D2_P4_Medium_T2_7_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the mug in the very middle? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2563.0672 seconds to 2579.9505999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P5_Medium_T2_11_Left", "images": ["Image/P27_D3_P5_Medium_T2_11_Left/1.png", "Image/P27_D3_P5_Medium_T2_11_Left/2.png", "Image/P27_D3_P5_Medium_T2_11_Left/3.png", "Image/P27_D3_P5_Medium_T2_11_Left/4.png", "Image/P27_D3_P5_Medium_T2_11_Left/5.png", "Image/P27_D3_P5_Medium_T2_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in the left red glass? Finish\"\nThe frames roughly correspond to the speech interval from 623.3806 seconds to 634.6422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P5_Medium_T2_21_Mid", "images": ["Image/P20_D1_P5_Medium_T2_21_Mid/1.png", "Image/P20_D1_P5_Medium_T2_21_Mid/2.png", "Image/P20_D1_P5_Medium_T2_21_Mid/3.png", "Image/P20_D1_P5_Medium_T2_21_Mid/4.png", "Image/P20_D1_P5_Medium_T2_21_Mid/5.png", "Image/P20_D1_P5_Medium_T2_21_Mid/6.png", "Image/P20_D1_P5_Medium_T2_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Pour some water into this thing. Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 409.81843 seconds to 422.2867700000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P7_High_T1_3_Right", "images": ["Image/P20_D3_P7_High_T1_3_Right/1.png", "Image/P20_D3_P7_High_T1_3_Right/2.png", "Image/P20_D3_P7_High_T1_3_Right/3.png", "Image/P20_D3_P7_High_T1_3_Right/4.png", "Image/P20_D3_P7_High_T1_3_Right/5.png", "Image/P20_D3_P7_High_T1_3_Right/6.png", "Image/P20_D3_P7_High_T1_3_Right/7.png", "Image/P20_D3_P7_High_T1_3_Right/8.png", "Image/P20_D3_P7_High_T1_3_Right/9.png", "Image/P20_D3_P7_High_T1_3_Right/10.png", "Image/P20_D3_P7_High_T1_3_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move the green bottle on the floor into the top of the shelf\"\nThe frames roughly correspond to the speech interval from 1317.3297 seconds to 1336.6357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P5_High_T2_23_Right", "images": ["Image/P19_D3_P5_High_T2_23_Right/1.png", "Image/P19_D3_P5_High_T2_23_Right/2.png", "Image/P19_D3_P5_High_T2_23_Right/3.png", "Image/P19_D3_P5_High_T2_23_Right/4.png", "Image/P19_D3_P5_High_T2_23_Right/5.png", "Image/P19_D3_P5_High_T2_23_Right/6.png", "Image/P19_D3_P5_High_T2_23_Right/7.png", "Image/P19_D3_P5_High_T2_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this flower. There. Finish.\"\nThe frames roughly correspond to the speech interval from 2064.8044 seconds to 2078.59766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P3_Medium_T3_6_Mid", "images": ["Image/P16_D3_P3_Medium_T3_6_Mid/1.png", "Image/P16_D3_P3_Medium_T3_6_Mid/2.png", "Image/P16_D3_P3_Medium_T3_6_Mid/3.png", "Image/P16_D3_P3_Medium_T3_6_Mid/4.png", "Image/P16_D3_P3_Medium_T3_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase? Finish.\"\nThe frames roughly correspond to the speech interval from 198.1018 seconds to 207.9288 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P1_Low_T3_21_Left", "images": ["Image/P21_D3_P1_Low_T3_21_Left/1.png", "Image/P21_D3_P1_Low_T3_21_Left/2.png", "Image/P21_D3_P1_Low_T3_21_Left/3.png", "Image/P21_D3_P1_Low_T3_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 347.05409999999995 seconds to 357.33989999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P2_Medium_T3_9_Mid", "images": ["Image/P16_D2_P2_Medium_T3_9_Mid/1.png", "Image/P16_D2_P2_Medium_T3_9_Mid/2.png", "Image/P16_D2_P2_Medium_T3_9_Mid/3.png", "Image/P16_D2_P2_Medium_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 919.9443 seconds to 929.4652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P3_High_T2_12_Right", "images": ["Image/P6_D1_P3_High_T2_12_Right/1.png", "Image/P6_D1_P3_High_T2_12_Right/2.png", "Image/P6_D1_P3_High_T2_12_Right/3.png", "Image/P6_D1_P3_High_T2_12_Right/4.png", "Image/P6_D1_P3_High_T2_12_Right/5.png", "Image/P6_D1_P3_High_T2_12_Right/6.png", "Image/P6_D1_P3_High_T2_12_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2150.5609000000004 seconds to 2163.4194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P8_High_T1_11_Left", "images": ["Image/P13_D1_P8_High_T1_11_Left/1.png", "Image/P13_D1_P8_High_T1_11_Left/2.png", "Image/P13_D1_P8_High_T1_11_Left/3.png", "Image/P13_D1_P8_High_T1_11_Left/4.png", "Image/P13_D1_P8_High_T1_11_Left/5.png", "Image/P13_D1_P8_High_T1_11_Left/6.png", "Image/P13_D1_P8_High_T1_11_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move that round cup up to here\"\nThe frames roughly correspond to the speech interval from 264.3307 seconds to 277.22130000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P3_Medium_T3_26_Right", "images": ["Image/P7_D1_P3_Medium_T3_26_Right/1.png", "Image/P7_D1_P3_Medium_T3_26_Right/2.png", "Image/P7_D1_P3_Medium_T3_26_Right/3.png", "Image/P7_D1_P3_Medium_T3_26_Right/4.png", "Image/P7_D1_P3_Medium_T3_26_Right/5.png", "Image/P7_D1_P3_Medium_T3_26_Right/6.png", "Image/P7_D1_P3_Medium_T3_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 186.0076 seconds to 200.3719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P4_Medium_T2_27_Left", "images": ["Image/P7_D1_P4_Medium_T2_27_Left/1.png", "Image/P7_D1_P4_Medium_T2_27_Left/2.png", "Image/P7_D1_P4_Medium_T2_27_Left/3.png", "Image/P7_D1_P4_Medium_T2_27_Left/4.png", "Image/P7_D1_P4_Medium_T2_27_Left/5.png", "Image/P7_D1_P4_Medium_T2_27_Left/6.png", "Image/P7_D1_P4_Medium_T2_27_Left/7.png", "Image/P7_D1_P4_Medium_T2_27_Left/8.png", "Image/P7_D1_P4_Medium_T2_27_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 306.8465 seconds to 324.6349 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P2_Low_T1_6_Mid", "images": ["Image/P6_D3_P2_Low_T1_6_Mid/1.png", "Image/P6_D3_P2_Low_T1_6_Mid/2.png", "Image/P6_D3_P2_Low_T1_6_Mid/3.png", "Image/P6_D3_P2_Low_T1_6_Mid/4.png", "Image/P6_D3_P2_Low_T1_6_Mid/5.png", "Image/P6_D3_P2_Low_T1_6_Mid/6.png", "Image/P6_D3_P2_Low_T1_6_Mid/7.png", "Image/P6_D3_P2_Low_T1_6_Mid/8.png", "Image/P6_D3_P2_Low_T1_6_Mid/9.png", "Image/P6_D3_P2_Low_T1_6_Mid/10.png", "Image/P6_D3_P2_Low_T1_6_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3411.3958 seconds to 3431.2867 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P3_High_T3_11_Mid", "images": ["Image/P5_D3_P3_High_T3_11_Mid/1.png", "Image/P5_D3_P3_High_T3_11_Mid/2.png", "Image/P5_D3_P3_High_T3_11_Mid/3.png", "Image/P5_D3_P3_High_T3_11_Mid/4.png", "Image/P5_D3_P3_High_T3_11_Mid/5.png", "Image/P5_D3_P3_High_T3_11_Mid/6.png", "Image/P5_D3_P3_High_T3_11_Mid/7.png", "Image/P5_D3_P3_High_T3_11_Mid/8.png", "Image/P5_D3_P3_High_T3_11_Mid/9.png", "Image/P5_D3_P3_High_T3_11_Mid/10.png", "Image/P5_D3_P3_High_T3_11_Mid/11.png", "Image/P5_D3_P3_High_T3_11_Mid/12.png", "Image/P5_D3_P3_High_T3_11_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle bottle place on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2346.6882 seconds to 2365.5780999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P8_High_T2_4_Mid", "images": ["Image/P26_D2_P8_High_T2_4_Mid/1.png", "Image/P26_D2_P8_High_T2_4_Mid/2.png", "Image/P26_D2_P8_High_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1453.2480999999998 seconds to 1465.8449999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P9_Low_T2_8_Left", "images": ["Image/P19_D2_P9_Low_T2_8_Left/1.png", "Image/P19_D2_P9_Low_T2_8_Left/2.png", "Image/P19_D2_P9_Low_T2_8_Left/3.png", "Image/P19_D2_P9_Low_T2_8_Left/4.png", "Image/P19_D2_P9_Low_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here.\"\nThe frames roughly correspond to the speech interval from 1635.4603 seconds to 1650.8826 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P8_High_T1_12_Left", "images": ["Image/P5_D1_P8_High_T1_12_Left/1.png", "Image/P5_D1_P8_High_T1_12_Left/2.png", "Image/P5_D1_P8_High_T1_12_Left/3.png", "Image/P5_D1_P8_High_T1_12_Left/4.png", "Image/P5_D1_P8_High_T1_12_Left/5.png", "Image/P5_D1_P8_High_T1_12_Left/6.png", "Image/P5_D1_P8_High_T1_12_Left/7.png", "Image/P5_D1_P8_High_T1_12_Left/8.png", "Image/P5_D1_P8_High_T1_12_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottle in the bucket which is in front of me to the shelf in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1640.9145 seconds to 1657.1463 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P9_Low_T3_27_Left", "images": ["Image/P26_D1_P9_Low_T3_27_Left/1.png", "Image/P26_D1_P9_Low_T3_27_Left/2.png", "Image/P26_D1_P9_Low_T3_27_Left/3.png", "Image/P26_D1_P9_Low_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that cup on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 494.6681 seconds to 505.0254 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P5_Medium_T2_8_Left", "images": ["Image/P22_D3_P5_Medium_T2_8_Left/1.png", "Image/P22_D3_P5_Medium_T2_8_Left/2.png", "Image/P22_D3_P5_Medium_T2_8_Left/3.png", "Image/P22_D3_P5_Medium_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the cup on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 300.3342 seconds to 310.6213 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P7_High_T2_15_Left", "images": ["Image/P5_D3_P7_High_T2_15_Left/1.png", "Image/P5_D3_P7_High_T2_15_Left/2.png", "Image/P5_D3_P7_High_T2_15_Left/3.png", "Image/P5_D3_P7_High_T2_15_Left/4.png", "Image/P5_D3_P7_High_T2_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant underneath the table. Finished.\"\nThe frames roughly correspond to the speech interval from 2861.7532 seconds to 2872.8480999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P1_Low_T1_10_Left", "images": ["Image/P13_D1_P1_Low_T1_10_Left/1.png", "Image/P13_D1_P1_Low_T1_10_Left/2.png", "Image/P13_D1_P1_Low_T1_10_Left/3.png", "Image/P13_D1_P1_Low_T1_10_Left/4.png", "Image/P13_D1_P1_Low_T1_10_Left/5.png", "Image/P13_D1_P1_Low_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this brown jug down here\"\nThe frames roughly correspond to the speech interval from 467.1866 seconds to 478.8235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P8_Low_T1_20_Mid", "images": ["Image/P15_D3_P8_Low_T1_20_Mid/1.png", "Image/P15_D3_P8_Low_T1_20_Mid/2.png", "Image/P15_D3_P8_Low_T1_20_Mid/3.png", "Image/P15_D3_P8_Low_T1_20_Mid/4.png", "Image/P15_D3_P8_Low_T1_20_Mid/5.png", "Image/P15_D3_P8_Low_T1_20_Mid/6.png", "Image/P15_D3_P8_Low_T1_20_Mid/7.png", "Image/P15_D3_P8_Low_T1_20_Mid/8.png", "Image/P15_D3_P8_Low_T1_20_Mid/9.png", "Image/P15_D3_P8_Low_T1_20_Mid/10.png", "Image/P15_D3_P8_Low_T1_20_Mid/11.png", "Image/P15_D3_P8_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white container from the under shelf to the top of the same shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 579.3399999999999 seconds to 597.0317 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P8_Low_T3_20_Right", "images": ["Image/P23_D3_P8_Low_T3_20_Right/1.png", "Image/P23_D3_P8_Low_T3_20_Right/2.png", "Image/P23_D3_P8_Low_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 1013.4522 seconds to 1022.1651 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P9_Low_T3_22_Right", "images": ["Image/P29_D2_P9_Low_T3_22_Right/1.png", "Image/P29_D2_P9_Low_T3_22_Right/2.png", "Image/P29_D2_P9_Low_T3_22_Right/3.png", "Image/P29_D2_P9_Low_T3_22_Right/4.png", "Image/P29_D2_P9_Low_T3_22_Right/5.png", "Image/P29_D2_P9_Low_T3_22_Right/6.png", "Image/P29_D2_P9_Low_T3_22_Right/7.png", "Image/P29_D2_P9_Low_T3_22_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 228.6263 seconds to 242.8391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P2_Medium_T1_16_Left", "images": ["Image/P8_D3_P2_Medium_T1_16_Left/1.png", "Image/P8_D3_P2_Medium_T1_16_Left/2.png", "Image/P8_D3_P2_Medium_T1_16_Left/3.png", "Image/P8_D3_P2_Medium_T1_16_Left/4.png", "Image/P8_D3_P2_Medium_T1_16_Left/5.png", "Image/P8_D3_P2_Medium_T1_16_Left/6.png", "Image/P8_D3_P2_Medium_T1_16_Left/7.png", "Image/P8_D3_P2_Medium_T1_16_Left/8.png", "Image/P8_D3_P2_Medium_T1_16_Left/9.png", "Image/P8_D3_P2_Medium_T1_16_Left/10.png", "Image/P8_D3_P2_Medium_T1_16_Left/11.png", "Image/P8_D3_P2_Medium_T1_16_Left/12.png", "Image/P8_D3_P2_Medium_T1_16_Left/13.png", "Image/P8_D3_P2_Medium_T1_16_Left/14.png", "Image/P8_D3_P2_Medium_T1_16_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Hi. Could you move the leftmost brown glass under the wooden shelf? You can see the black strip there. Just move it on there. Finish.\"\nThe frames roughly correspond to the speech interval from 1659.5358 seconds to 1680.1236 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P4_Medium_T2_14_Left", "images": ["Image/P29_D1_P4_Medium_T2_14_Left/1.png", "Image/P29_D1_P4_Medium_T2_14_Left/2.png", "Image/P29_D1_P4_Medium_T2_14_Left/3.png", "Image/P29_D1_P4_Medium_T2_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2735.2821 seconds to 2747.2374999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P1_Low_T3_5_Mid", "images": ["Image/P2_D1_P1_Low_T3_5_Mid/1.png", "Image/P2_D1_P1_Low_T3_5_Mid/2.png", "Image/P2_D1_P1_Low_T3_5_Mid/3.png", "Image/P2_D1_P1_Low_T3_5_Mid/4.png", "Image/P2_D1_P1_Low_T3_5_Mid/5.png", "Image/P2_D1_P1_Low_T3_5_Mid/6.png", "Image/P2_D1_P1_Low_T3_5_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please claim this plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 874.8627999999999 seconds to 891.2438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P5_Medium_T2_25_Right", "images": ["Image/P16_D2_P5_Medium_T2_25_Right/1.png", "Image/P16_D2_P5_Medium_T2_25_Right/2.png", "Image/P16_D2_P5_Medium_T2_25_Right/3.png", "Image/P16_D2_P5_Medium_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 777.4479000000001 seconds to 787.6911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P6_High_T3_17_Right", "images": ["Image/P12_D1_P6_High_T3_17_Right/1.png", "Image/P12_D1_P6_High_T3_17_Right/2.png", "Image/P12_D1_P6_High_T3_17_Right/3.png", "Image/P12_D1_P6_High_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1492.7203 seconds to 1502.5893 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P4_Low_T1_15_Left", "images": ["Image/P22_D3_P4_Low_T1_15_Left/1.png", "Image/P22_D3_P4_Low_T1_15_Left/2.png", "Image/P22_D3_P4_Low_T1_15_Left/3.png", "Image/P22_D3_P4_Low_T1_15_Left/4.png", "Image/P22_D3_P4_Low_T1_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant to the shelf right there. Finish.\"\nThe frames roughly correspond to the speech interval from 231.125 seconds to 241.3414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P4_Medium_T3_3_Mid", "images": ["Image/P4_D1_P4_Medium_T3_3_Mid/1.png", "Image/P4_D1_P4_Medium_T3_3_Mid/2.png", "Image/P4_D1_P4_Medium_T3_3_Mid/3.png", "Image/P4_D1_P4_Medium_T3_3_Mid/4.png", "Image/P4_D1_P4_Medium_T3_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the bottle in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2165.0921 seconds to 2179.9404999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P8_Low_T2_25_Right", "images": ["Image/P14_D1_P8_Low_T2_25_Right/1.png", "Image/P14_D1_P8_Low_T2_25_Right/2.png", "Image/P14_D1_P8_Low_T2_25_Right/3.png", "Image/P14_D1_P8_Low_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill that up. Finished.\"\nThe frames roughly correspond to the speech interval from 552.6773000000001 seconds to 561.9049 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P7_High_T3_14_Mid", "images": ["Image/P25_D3_P7_High_T3_14_Mid/1.png", "Image/P25_D3_P7_High_T3_14_Mid/2.png", "Image/P25_D3_P7_High_T3_14_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2134.7858 seconds to 2142.9036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P3_Medium_T3_19_Right", "images": ["Image/P24_D1_P3_Medium_T3_19_Right/1.png", "Image/P24_D1_P3_Medium_T3_19_Right/2.png", "Image/P24_D1_P3_Medium_T3_19_Right/3.png", "Image/P24_D1_P3_Medium_T3_19_Right/4.png", "Image/P24_D1_P3_Medium_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1378.6408 seconds to 1389.4290999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P8_High_T3_8_Right", "images": ["Image/P30_D3_P8_High_T3_8_Right/1.png", "Image/P30_D3_P8_High_T3_8_Right/2.png", "Image/P30_D3_P8_High_T3_8_Right/3.png", "Image/P30_D3_P8_High_T3_8_Right/4.png", "Image/P30_D3_P8_High_T3_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the green cup over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1250.1744 seconds to 1262.2001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P9_Medium_T3_11_Right", "images": ["Image/P20_D2_P9_Medium_T3_11_Right/1.png", "Image/P20_D2_P9_Medium_T3_11_Right/2.png", "Image/P20_D2_P9_Medium_T3_11_Right/3.png", "Image/P20_D2_P9_Medium_T3_11_Right/4.png", "Image/P20_D2_P9_Medium_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the right bottle on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3010.5298000000003 seconds to 3022.277 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P9_Low_T1_6_Mid", "images": ["Image/P23_D1_P9_Low_T1_6_Mid/1.png", "Image/P23_D1_P9_Low_T1_6_Mid/2.png", "Image/P23_D1_P9_Low_T1_6_Mid/3.png", "Image/P23_D1_P9_Low_T1_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1912.6607 seconds to 1922.2783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P5_Low_T3_13_Mid", "images": ["Image/P18_D2_P5_Low_T3_13_Mid/1.png", "Image/P18_D2_P5_Low_T3_13_Mid/2.png", "Image/P18_D2_P5_Low_T3_13_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 525.8558 seconds to 534.1238000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P5_High_T1_5_Mid", "images": ["Image/P21_D3_P5_High_T1_5_Mid/1.png", "Image/P21_D3_P5_High_T1_5_Mid/2.png", "Image/P21_D3_P5_High_T1_5_Mid/3.png", "Image/P21_D3_P5_High_T1_5_Mid/4.png", "Image/P21_D3_P5_High_T1_5_Mid/5.png", "Image/P21_D3_P5_High_T1_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 714.1098999999999 seconds to 726.0523000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P6_Low_T2_23_Mid", "images": ["Image/P13_D3_P6_Low_T2_23_Mid/1.png", "Image/P13_D3_P6_Low_T2_23_Mid/2.png", "Image/P13_D3_P6_Low_T2_23_Mid/3.png", "Image/P13_D3_P6_Low_T2_23_Mid/4.png", "Image/P13_D3_P6_Low_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday\"\nThe frames roughly correspond to the speech interval from 1749.7639 seconds to 1760.1905 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P4_High_T1_18_Mid", "images": ["Image/P26_D1_P4_High_T1_18_Mid/1.png", "Image/P26_D1_P4_High_T1_18_Mid/2.png", "Image/P26_D1_P4_High_T1_18_Mid/3.png", "Image/P26_D1_P4_High_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the vase here in the middle? Finish\"\nThe frames roughly correspond to the speech interval from 367.03649999999993 seconds to 380.8448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P7_Medium_T2_13_Right", "images": ["Image/P19_D2_P7_Medium_T2_13_Right/1.png", "Image/P19_D2_P7_Medium_T2_13_Right/2.png", "Image/P19_D2_P7_Medium_T2_13_Right/3.png", "Image/P19_D2_P7_Medium_T2_13_Right/4.png", "Image/P19_D2_P7_Medium_T2_13_Right/5.png", "Image/P19_D2_P7_Medium_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1888.0508 seconds to 1900.7031 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P8_Medium_T3_1_Mid", "images": ["Image/P2_D2_P8_Medium_T3_1_Mid/1.png", "Image/P2_D2_P8_Medium_T3_1_Mid/2.png", "Image/P2_D2_P8_Medium_T3_1_Mid/3.png", "Image/P2_D2_P8_Medium_T3_1_Mid/4.png", "Image/P2_D2_P8_Medium_T3_1_Mid/5.png", "Image/P2_D2_P8_Medium_T3_1_Mid/6.png", "Image/P2_D2_P8_Medium_T3_1_Mid/7.png", "Image/P2_D2_P8_Medium_T3_1_Mid/8.png", "Image/P2_D2_P8_Medium_T3_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2819.4438 seconds to 2834.0193 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P3_Low_T2_14_Left", "images": ["Image/P29_D3_P3_Low_T2_14_Left/1.png", "Image/P29_D3_P3_Low_T2_14_Left/2.png", "Image/P29_D3_P3_Low_T2_14_Left/3.png", "Image/P29_D3_P3_Low_T2_14_Left/4.png", "Image/P29_D3_P3_Low_T2_14_Left/5.png", "Image/P29_D3_P3_Low_T2_14_Left/6.png", "Image/P29_D3_P3_Low_T2_14_Left/7.png", "Image/P29_D3_P3_Low_T2_14_Left/8.png", "Image/P29_D3_P3_Low_T2_14_Left/9.png", "Image/P29_D3_P3_Low_T2_14_Left/10.png", "Image/P29_D3_P3_Low_T2_14_Left/11.png", "Image/P29_D3_P3_Low_T2_14_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1572.0489999999998 seconds to 1589.6991999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P5_High_T2_21_Left", "images": ["Image/P8_D1_P5_High_T2_21_Left/1.png", "Image/P8_D1_P5_High_T2_21_Left/2.png", "Image/P8_D1_P5_High_T2_21_Left/3.png", "Image/P8_D1_P5_High_T2_21_Left/4.png", "Image/P8_D1_P5_High_T2_21_Left/5.png", "Image/P8_D1_P5_High_T2_21_Left/6.png", "Image/P8_D1_P5_High_T2_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Start. So\"\nThe frames roughly correspond to the speech interval from 718.4091999999999 seconds to 732.4069999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P6_Medium_T3_1_Right", "images": ["Image/P21_D3_P6_Medium_T3_1_Right/1.png", "Image/P21_D3_P6_Medium_T3_1_Right/2.png", "Image/P21_D3_P6_Medium_T3_1_Right/3.png", "Image/P21_D3_P6_Medium_T3_1_Right/4.png", "Image/P21_D3_P6_Medium_T3_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 566.6143 seconds to 577.2071 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P5_Low_T3_19_Right", "images": ["Image/P19_D2_P5_Low_T3_19_Right/1.png", "Image/P19_D2_P5_Low_T3_19_Right/2.png", "Image/P19_D2_P5_Low_T3_19_Right/3.png", "Image/P19_D2_P5_Low_T3_19_Right/4.png", "Image/P19_D2_P5_Low_T3_19_Right/5.png", "Image/P19_D2_P5_Low_T3_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1453.165 seconds to 1466.0407 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P4_Low_T1_17_Right", "images": ["Image/P8_D1_P4_Low_T1_17_Right/1.png", "Image/P8_D1_P4_Low_T1_17_Right/2.png", "Image/P8_D1_P4_Low_T1_17_Right/3.png", "Image/P8_D1_P4_Low_T1_17_Right/4.png", "Image/P8_D1_P4_Low_T1_17_Right/5.png", "Image/P8_D1_P4_Low_T1_17_Right/6.png", "Image/P8_D1_P4_Low_T1_17_Right/7.png", "Image/P8_D1_P4_Low_T1_17_Right/8.png", "Image/P8_D1_P4_Low_T1_17_Right/9.png", "Image/P8_D1_P4_Low_T1_17_Right/10.png", "Image/P8_D1_P4_Low_T1_17_Right/11.png", "Image/P8_D1_P4_Low_T1_17_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 492.4615999999999 seconds to 512.3041999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P2_Low_T1_4_Left", "images": ["Image/P17_D3_P2_Low_T1_4_Left/1.png", "Image/P17_D3_P2_Low_T1_4_Left/2.png", "Image/P17_D3_P2_Low_T1_4_Left/3.png", "Image/P17_D3_P2_Low_T1_4_Left/4.png", "Image/P17_D3_P2_Low_T1_4_Left/5.png", "Image/P17_D3_P2_Low_T1_4_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant on the top shelf down to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 786.16787 seconds to 798.44538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P3_Low_T1_23_Left", "images": ["Image/P7_D2_P3_Low_T1_23_Left/1.png", "Image/P7_D2_P3_Low_T1_23_Left/2.png", "Image/P7_D2_P3_Low_T1_23_Left/3.png", "Image/P7_D2_P3_Low_T1_23_Left/4.png", "Image/P7_D2_P3_Low_T1_23_Left/5.png", "Image/P7_D2_P3_Low_T1_23_Left/6.png", "Image/P7_D2_P3_Low_T1_23_Left/7.png", "Image/P7_D2_P3_Low_T1_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Done. Hey Friday\"\nThe frames roughly correspond to the speech interval from 967.85631 seconds to 983.5989000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P7_Medium_T3_16_Left", "images": ["Image/P6_D3_P7_Medium_T3_16_Left/1.png", "Image/P6_D3_P7_Medium_T3_16_Left/2.png", "Image/P6_D3_P7_Medium_T3_16_Left/3.png", "Image/P6_D3_P7_Medium_T3_16_Left/4.png", "Image/P6_D3_P7_Medium_T3_16_Left/5.png", "Image/P6_D3_P7_Medium_T3_16_Left/6.png", "Image/P6_D3_P7_Medium_T3_16_Left/7.png", "Image/P6_D3_P7_Medium_T3_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3441.5597 seconds to 3455.7434 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P7_Low_T1_5_Mid", "images": ["Image/P5_D3_P7_Low_T1_5_Mid/1.png", "Image/P5_D3_P7_Low_T1_5_Mid/2.png", "Image/P5_D3_P7_Low_T1_5_Mid/3.png", "Image/P5_D3_P7_Low_T1_5_Mid/4.png", "Image/P5_D3_P7_Low_T1_5_Mid/5.png", "Image/P5_D3_P7_Low_T1_5_Mid/6.png", "Image/P5_D3_P7_Low_T1_5_Mid/7.png", "Image/P5_D3_P7_Low_T1_5_Mid/8.png", "Image/P5_D3_P7_Low_T1_5_Mid/9.png", "Image/P5_D3_P7_Low_T1_5_Mid/10.png", "Image/P5_D3_P7_Low_T1_5_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are two levels of bookshelves on top of the table. Move the plant to the top level of the bookshelf. Finished.\"\nThe frames roughly correspond to the speech interval from 2438.7826 seconds to 2456.3775 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P8_Low_T2_8_Left", "images": ["Image/P28_D1_P8_Low_T2_8_Left/1.png", "Image/P28_D1_P8_Low_T2_8_Left/2.png", "Image/P28_D1_P8_Low_T2_8_Left/3.png", "Image/P28_D1_P8_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1385.51204 seconds to 1394.978 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P2_Medium_T2_17_Right", "images": ["Image/P1_D3_P2_Medium_T2_17_Right/1.png", "Image/P1_D3_P2_Medium_T2_17_Right/2.png", "Image/P1_D3_P2_Medium_T2_17_Right/3.png", "Image/P1_D3_P2_Medium_T2_17_Right/4.png", "Image/P1_D3_P2_Medium_T2_17_Right/5.png", "Image/P1_D3_P2_Medium_T2_17_Right/6.png", "Image/P1_D3_P2_Medium_T2_17_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the red cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1731.2254999999998 seconds to 1743.4620999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P8_Low_T3_18_Mid", "images": ["Image/P9_D2_P8_Low_T3_18_Mid/1.png", "Image/P9_D2_P8_Low_T3_18_Mid/2.png", "Image/P9_D2_P8_Low_T3_18_Mid/3.png", "Image/P9_D2_P8_Low_T3_18_Mid/4.png", "Image/P9_D2_P8_Low_T3_18_Mid/5.png", "Image/P9_D2_P8_Low_T3_18_Mid/6.png", "Image/P9_D2_P8_Low_T3_18_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the red cup under the desk on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 3250.0588 seconds to 3266.7087 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P9_Medium_T3_11_Right", "images": ["Image/P8_D2_P9_Medium_T3_11_Right/1.png", "Image/P8_D2_P9_Medium_T3_11_Right/2.png", "Image/P8_D2_P9_Medium_T3_11_Right/3.png", "Image/P8_D2_P9_Medium_T3_11_Right/4.png", "Image/P8_D2_P9_Medium_T3_11_Right/5.png", "Image/P8_D2_P9_Medium_T3_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me with cleaning the brown bottle on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 2411.9648 seconds to 2424.8396 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P2_Medium_T1_21_Left", "images": ["Image/P25_D1_P2_Medium_T1_21_Left/1.png", "Image/P25_D1_P2_Medium_T1_21_Left/2.png", "Image/P25_D1_P2_Medium_T1_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this down. Finish.\"\nThe frames roughly correspond to the speech interval from 495.1426 seconds to 503.2429 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P8_High_T3_16_Mid", "images": ["Image/P20_D2_P8_High_T3_16_Mid/1.png", "Image/P20_D2_P8_High_T3_16_Mid/2.png", "Image/P20_D2_P8_High_T3_16_Mid/3.png", "Image/P20_D2_P8_High_T3_16_Mid/4.png", "Image/P20_D2_P8_High_T3_16_Mid/5.png", "Image/P20_D2_P8_High_T3_16_Mid/6.png", "Image/P20_D2_P8_High_T3_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that cup next to the plants? Finish.\"\nThe frames roughly correspond to the speech interval from 2876.2169000000004 seconds to 2888.5217 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P7_High_T1_26_Right", "images": ["Image/P4_D1_P7_High_T1_26_Right/1.png", "Image/P4_D1_P7_High_T1_26_Right/2.png", "Image/P4_D1_P7_High_T1_26_Right/3.png", "Image/P4_D1_P7_High_T1_26_Right/4.png", "Image/P4_D1_P7_High_T1_26_Right/5.png", "Image/P4_D1_P7_High_T1_26_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right bottle up to the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 2304.7106 seconds to 2319.1310999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P1_Low_T1_23_Right", "images": ["Image/P17_D3_P1_Low_T1_23_Right/1.png", "Image/P17_D3_P1_Low_T1_23_Right/2.png", "Image/P17_D3_P1_Low_T1_23_Right/3.png", "Image/P17_D3_P1_Low_T1_23_Right/4.png", "Image/P17_D3_P1_Low_T1_23_Right/5.png", "Image/P17_D3_P1_Low_T1_23_Right/6.png", "Image/P17_D3_P1_Low_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that white plant down to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1088.8825000000002 seconds to 1102.316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P8_Medium_T2_26_Mid", "images": ["Image/P13_D1_P8_Medium_T2_26_Mid/1.png", "Image/P13_D1_P8_Medium_T2_26_Mid/2.png", "Image/P13_D1_P8_Medium_T2_26_Mid/3.png", "Image/P13_D1_P8_Medium_T2_26_Mid/4.png", "Image/P13_D1_P8_Medium_T2_26_Mid/5.png", "Image/P13_D1_P8_Medium_T2_26_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the middle cup down there\"\nThe frames roughly correspond to the speech interval from 571.7241 seconds to 583.1942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P6_Medium_T3_3_Left", "images": ["Image/P11_D1_P6_Medium_T3_3_Left/1.png", "Image/P11_D1_P6_Medium_T3_3_Left/2.png", "Image/P11_D1_P6_Medium_T3_3_Left/3.png", "Image/P11_D1_P6_Medium_T3_3_Left/4.png", "Image/P11_D1_P6_Medium_T3_3_Left/5.png", "Image/P11_D1_P6_Medium_T3_3_Left/6.png", "Image/P11_D1_P6_Medium_T3_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the first green glass from the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 2644.8565999999996 seconds to 2662.2002999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P1_Low_T2_10_Mid", "images": ["Image/P17_D2_P1_Low_T2_10_Mid/1.png", "Image/P17_D2_P1_Low_T2_10_Mid/2.png", "Image/P17_D2_P1_Low_T2_10_Mid/3.png", "Image/P17_D2_P1_Low_T2_10_Mid/4.png", "Image/P17_D2_P1_Low_T2_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in this brown cup over here? Finish.\"\nThe frames roughly correspond to the speech interval from 605.9712000000001 seconds to 621.3936 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P1_High_T2_17_Mid", "images": ["Image/P17_D3_P1_High_T2_17_Mid/1.png", "Image/P17_D3_P1_High_T2_17_Mid/2.png", "Image/P17_D3_P1_High_T2_17_Mid/3.png", "Image/P17_D3_P1_High_T2_17_Mid/4.png", "Image/P17_D3_P1_High_T2_17_Mid/5.png", "Image/P17_D3_P1_High_T2_17_Mid/6.png", "Image/P17_D3_P1_High_T2_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in that brown mug on the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1239.4830000000002 seconds to 1252.3585 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P8_Medium_T3_17_Right", "images": ["Image/P25_D2_P8_Medium_T3_17_Right/1.png", "Image/P25_D2_P8_Medium_T3_17_Right/2.png", "Image/P25_D2_P8_Medium_T3_17_Right/3.png", "Image/P25_D2_P8_Medium_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the first orange cup. Finish.\"\nThe frames roughly correspond to the speech interval from 658.63392 seconds to 667.77928 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P7_Low_T1_15_Right", "images": ["Image/P17_D1_P7_Low_T1_15_Right/1.png", "Image/P17_D1_P7_Low_T1_15_Right/2.png", "Image/P17_D1_P7_Low_T1_15_Right/3.png", "Image/P17_D1_P7_Low_T1_15_Right/4.png", "Image/P17_D1_P7_Low_T1_15_Right/5.png", "Image/P17_D1_P7_Low_T1_15_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant in the basket to the shelf right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1993.3279 seconds to 2005.8701 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P3_Low_T2_6_Mid", "images": ["Image/P30_D2_P3_Low_T2_6_Mid/1.png", "Image/P30_D2_P3_Low_T2_6_Mid/2.png", "Image/P30_D2_P3_Low_T2_6_Mid/3.png", "Image/P30_D2_P3_Low_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 203.9178 seconds to 214.1478 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P9_Medium_T3_16_Mid", "images": ["Image/P11_D3_P9_Medium_T3_16_Mid/1.png", "Image/P11_D3_P9_Medium_T3_16_Mid/2.png", "Image/P11_D3_P9_Medium_T3_16_Mid/3.png", "Image/P11_D3_P9_Medium_T3_16_Mid/4.png", "Image/P11_D3_P9_Medium_T3_16_Mid/5.png", "Image/P11_D3_P9_Medium_T3_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the second bottle in the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1642.7041 seconds to 1654.3673 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P6_High_T1_17_Right", "images": ["Image/P25_D3_P6_High_T1_17_Right/1.png", "Image/P25_D3_P6_High_T1_17_Right/2.png", "Image/P25_D3_P6_High_T1_17_Right/3.png", "Image/P25_D3_P6_High_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this onto the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 2041.3704 seconds to 2050.3355 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P4_Low_T1_10_Left", "images": ["Image/P24_D2_P4_Low_T1_10_Left/1.png", "Image/P24_D2_P4_Low_T1_10_Left/2.png", "Image/P24_D2_P4_Low_T1_10_Left/3.png", "Image/P24_D2_P4_Low_T1_10_Left/4.png", "Image/P24_D2_P4_Low_T1_10_Left/5.png", "Image/P24_D2_P4_Low_T1_10_Left/6.png", "Image/P24_D2_P4_Low_T1_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this bottle to this area? Please finish.\"\nThe frames roughly correspond to the speech interval from 681.5385 seconds to 694.0656 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P2_High_T2_15_Right", "images": ["Image/P9_D3_P2_High_T2_15_Right/1.png", "Image/P9_D3_P2_High_T2_15_Right/2.png", "Image/P9_D3_P2_High_T2_15_Right/3.png", "Image/P9_D3_P2_High_T2_15_Right/4.png", "Image/P9_D3_P2_High_T2_15_Right/5.png", "Image/P9_D3_P2_High_T2_15_Right/6.png", "Image/P9_D3_P2_High_T2_15_Right/7.png", "Image/P9_D3_P2_High_T2_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 501.4281 seconds to 516.3026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P1_Low_T3_25_Right", "images": ["Image/P23_D2_P1_Low_T3_25_Right/1.png", "Image/P23_D2_P1_Low_T3_25_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 102.09522 seconds to 110.80877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P8_Medium_T2_16_Right", "images": ["Image/P13_D3_P8_Medium_T2_16_Right/1.png", "Image/P13_D3_P8_Medium_T2_16_Right/2.png", "Image/P13_D3_P8_Medium_T2_16_Right/3.png", "Image/P13_D3_P8_Medium_T2_16_Right/4.png", "Image/P13_D3_P8_Medium_T2_16_Right/5.png", "Image/P13_D3_P8_Medium_T2_16_Right/6.png", "Image/P13_D3_P8_Medium_T2_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the right brown cup on the floor with water\"\nThe frames roughly correspond to the speech interval from 1488.8351 seconds to 1501.2938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P9_Medium_T1_24_Mid", "images": ["Image/P28_D2_P9_Medium_T1_24_Mid/1.png", "Image/P28_D2_P9_Medium_T1_24_Mid/2.png", "Image/P28_D2_P9_Medium_T1_24_Mid/3.png", "Image/P28_D2_P9_Medium_T1_24_Mid/4.png", "Image/P28_D2_P9_Medium_T1_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that middle plant to the very top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1245.6672 seconds to 1256.6084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P9_High_T1_12_Mid", "images": ["Image/P30_D2_P9_High_T1_12_Mid/1.png", "Image/P30_D2_P9_High_T1_12_Mid/2.png", "Image/P30_D2_P9_High_T1_12_Mid/3.png", "Image/P30_D2_P9_High_T1_12_Mid/4.png", "Image/P30_D2_P9_High_T1_12_Mid/5.png", "Image/P30_D2_P9_High_T1_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that red object up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 222.0685 seconds to 233.66350000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P6_Low_T2_14_Mid", "images": ["Image/P21_D1_P6_Low_T2_14_Mid/1.png", "Image/P21_D1_P6_Low_T2_14_Mid/2.png", "Image/P21_D1_P6_Low_T2_14_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 828.25119 seconds to 841.4867199999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P8_High_T2_14_Left", "images": ["Image/P28_D2_P8_High_T2_14_Left/1.png", "Image/P28_D2_P8_High_T2_14_Left/2.png", "Image/P28_D2_P8_High_T2_14_Left/3.png", "Image/P28_D2_P8_High_T2_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1165.7936 seconds to 1174.883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P2_Low_T2_24_Left", "images": ["Image/P13_D2_P2_Low_T2_24_Left/1.png", "Image/P13_D2_P2_Low_T2_24_Left/2.png", "Image/P13_D2_P2_Low_T2_24_Left/3.png", "Image/P13_D2_P2_Low_T2_24_Left/4.png", "Image/P13_D2_P2_Low_T2_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you water the plant here please? Finish.\"\nThe frames roughly correspond to the speech interval from 1192.48466 seconds to 1202.49326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P2_Medium_T2_7_Right", "images": ["Image/P28_D2_P2_Medium_T2_7_Right/1.png", "Image/P28_D2_P2_Medium_T2_7_Right/2.png", "Image/P28_D2_P2_Medium_T2_7_Right/3.png", "Image/P28_D2_P2_Medium_T2_7_Right/4.png", "Image/P28_D2_P2_Medium_T2_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 975.6202 seconds to 986.825 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P4_High_T3_20_Mid", "images": ["Image/P30_D3_P4_High_T3_20_Mid/1.png", "Image/P30_D3_P4_High_T3_20_Mid/2.png", "Image/P30_D3_P4_High_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1529.2952 seconds to 1537.4806 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P9_Low_T2_13_Right", "images": ["Image/P15_D2_P9_Low_T2_13_Right/1.png", "Image/P15_D2_P9_Low_T2_13_Right/2.png", "Image/P15_D2_P9_Low_T2_13_Right/3.png", "Image/P15_D2_P9_Low_T2_13_Right/4.png", "Image/P15_D2_P9_Low_T2_13_Right/5.png", "Image/P15_D2_P9_Low_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the floor. Finished.\"\nThe frames roughly correspond to the speech interval from 3199.3739 seconds to 3210.1198999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P6_Low_T1_19_Mid", "images": ["Image/P1_D2_P6_Low_T1_19_Mid/1.png", "Image/P1_D2_P6_Low_T1_19_Mid/2.png", "Image/P1_D2_P6_Low_T1_19_Mid/3.png", "Image/P1_D2_P6_Low_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 970.6816 seconds to 980.4135 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P1_High_T3_20_Right", "images": ["Image/P8_D3_P1_High_T3_20_Right/1.png", "Image/P8_D3_P1_High_T3_20_Right/2.png", "Image/P8_D3_P1_High_T3_20_Right/3.png", "Image/P8_D3_P1_High_T3_20_Right/4.png", "Image/P8_D3_P1_High_T3_20_Right/5.png", "Image/P8_D3_P1_High_T3_20_Right/6.png", "Image/P8_D3_P1_High_T3_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1687.7238 seconds to 1700.9338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P9_Medium_T3_2_Left", "images": ["Image/P5_D1_P9_Medium_T3_2_Left/1.png", "Image/P5_D1_P9_Medium_T3_2_Left/2.png", "Image/P5_D1_P9_Medium_T3_2_Left/3.png", "Image/P5_D1_P9_Medium_T3_2_Left/4.png", "Image/P5_D1_P9_Medium_T3_2_Left/5.png", "Image/P5_D1_P9_Medium_T3_2_Left/6.png", "Image/P5_D1_P9_Medium_T3_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the leftmost bottle in the bucket. Finished.\"\nThe frames roughly correspond to the speech interval from 1895.2874 seconds to 1909.0123 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P8_High_T3_12_Mid", "images": ["Image/P14_D2_P8_High_T3_12_Mid/1.png", "Image/P14_D2_P8_High_T3_12_Mid/2.png", "Image/P14_D2_P8_High_T3_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1853.6167 seconds to 1862.3771 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P6_Medium_T2_19_Mid", "images": ["Image/P9_D1_P6_Medium_T2_19_Mid/1.png", "Image/P9_D1_P6_Medium_T2_19_Mid/2.png", "Image/P9_D1_P6_Medium_T2_19_Mid/3.png", "Image/P9_D1_P6_Medium_T2_19_Mid/4.png", "Image/P9_D1_P6_Medium_T2_19_Mid/5.png", "Image/P9_D1_P6_Medium_T2_19_Mid/6.png", "Image/P9_D1_P6_Medium_T2_19_Mid/7.png", "Image/P9_D1_P6_Medium_T2_19_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the middle pot? Finish.\"\nThe frames roughly correspond to the speech interval from 1309.2493 seconds to 1323.6065999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P9_Low_T3_11_Left", "images": ["Image/P28_D3_P9_Low_T3_11_Left/1.png", "Image/P28_D3_P9_Low_T3_11_Left/2.png", "Image/P28_D3_P9_Low_T3_11_Left/3.png", "Image/P28_D3_P9_Low_T3_11_Left/4.png", "Image/P28_D3_P9_Low_T3_11_Left/5.png", "Image/P28_D3_P9_Low_T3_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 171.36370000000002 seconds to 184.8794 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P8_Medium_T3_24_Mid", "images": ["Image/P15_D1_P8_Medium_T3_24_Mid/1.png", "Image/P15_D1_P8_Medium_T3_24_Mid/2.png", "Image/P15_D1_P8_Medium_T3_24_Mid/3.png", "Image/P15_D1_P8_Medium_T3_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1835.74057 seconds to 1845.31657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P3_Medium_T1_24_Mid", "images": ["Image/P23_D3_P3_Medium_T1_24_Mid/1.png", "Image/P23_D3_P3_Medium_T1_24_Mid/2.png", "Image/P23_D3_P3_Medium_T1_24_Mid/3.png", "Image/P23_D3_P3_Medium_T1_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 849.4466 seconds to 860.1088 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P3_Medium_T3_14_Left", "images": ["Image/P9_D1_P3_Medium_T3_14_Left/1.png", "Image/P9_D1_P3_Medium_T3_14_Left/2.png", "Image/P9_D1_P3_Medium_T3_14_Left/3.png", "Image/P9_D1_P3_Medium_T3_14_Left/4.png", "Image/P9_D1_P3_Medium_T3_14_Left/5.png", "Image/P9_D1_P3_Medium_T3_14_Left/6.png", "Image/P9_D1_P3_Medium_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1515.5477999999998 seconds to 1536.3629999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P8_Medium_T1_6_Right", "images": ["Image/P2_D1_P8_Medium_T1_6_Right/1.png", "Image/P2_D1_P8_Medium_T1_6_Right/2.png", "Image/P2_D1_P8_Medium_T1_6_Right/3.png", "Image/P2_D1_P8_Medium_T1_6_Right/4.png", "Image/P2_D1_P8_Medium_T1_6_Right/5.png", "Image/P2_D1_P8_Medium_T1_6_Right/6.png", "Image/P2_D1_P8_Medium_T1_6_Right/7.png", "Image/P2_D1_P8_Medium_T1_6_Right/8.png", "Image/P2_D1_P8_Medium_T1_6_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 541.2863 seconds to 556.4976999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P9_High_T3_14_Left", "images": ["Image/P5_D2_P9_High_T3_14_Left/1.png", "Image/P5_D2_P9_High_T3_14_Left/2.png", "Image/P5_D2_P9_High_T3_14_Left/3.png", "Image/P5_D2_P9_High_T3_14_Left/4.png", "Image/P5_D2_P9_High_T3_14_Left/5.png", "Image/P5_D2_P9_High_T3_14_Left/6.png", "Image/P5_D2_P9_High_T3_14_Left/7.png", "Image/P5_D2_P9_High_T3_14_Left/8.png", "Image/P5_D2_P9_High_T3_14_Left/9.png", "Image/P5_D2_P9_High_T3_14_Left/10.png", "Image/P5_D2_P9_High_T3_14_Left/11.png", "Image/P5_D2_P9_High_T3_14_Left/12.png", "Image/P5_D2_P9_High_T3_14_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. There's a plant to the left side\"\nThe frames roughly correspond to the speech interval from 844.2481 seconds to 863.5291 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P4_Low_T2_13_Mid", "images": ["Image/P23_D3_P4_Low_T2_13_Mid/1.png", "Image/P23_D3_P4_Low_T2_13_Mid/2.png", "Image/P23_D3_P4_Low_T2_13_Mid/3.png", "Image/P23_D3_P4_Low_T2_13_Mid/4.png", "Image/P23_D3_P4_Low_T2_13_Mid/5.png", "Image/P23_D3_P4_Low_T2_13_Mid/6.png", "Image/P23_D3_P4_Low_T2_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 648.86076 seconds to 658.2293900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P1_High_T3_6_Left", "images": ["Image/P11_D2_P1_High_T3_6_Left/1.png", "Image/P11_D2_P1_High_T3_6_Left/2.png", "Image/P11_D2_P1_High_T3_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Clean the plant\"\nThe frames roughly correspond to the speech interval from 879.6965 seconds to 888.0846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P9_Medium_T2_26_Right", "images": ["Image/P2_D1_P9_Medium_T2_26_Right/1.png", "Image/P2_D1_P9_Medium_T2_26_Right/2.png", "Image/P2_D1_P9_Medium_T2_26_Right/3.png", "Image/P2_D1_P9_Medium_T2_26_Right/4.png", "Image/P2_D1_P9_Medium_T2_26_Right/5.png", "Image/P2_D1_P9_Medium_T2_26_Right/6.png", "Image/P2_D1_P9_Medium_T2_26_Right/7.png", "Image/P2_D1_P9_Medium_T2_26_Right/8.png", "Image/P2_D1_P9_Medium_T2_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 849.4170999999999 seconds to 867.0503999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P5_High_T1_18_Right", "images": ["Image/P20_D2_P5_High_T1_18_Right/1.png", "Image/P20_D2_P5_High_T1_18_Right/2.png", "Image/P20_D2_P5_High_T1_18_Right/3.png", "Image/P20_D2_P5_High_T1_18_Right/4.png", "Image/P20_D2_P5_High_T1_18_Right/5.png", "Image/P20_D2_P5_High_T1_18_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the orange bottle over here? Finish.\"\nThe frames roughly correspond to the speech interval from 2815.17772 seconds to 2826.5085500000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P1_High_T3_12_Left", "images": ["Image/P9_D3_P1_High_T3_12_Left/1.png", "Image/P9_D3_P1_High_T3_12_Left/2.png", "Image/P9_D3_P1_High_T3_12_Left/3.png", "Image/P9_D3_P1_High_T3_12_Left/4.png", "Image/P9_D3_P1_High_T3_12_Left/5.png", "Image/P9_D3_P1_High_T3_12_Left/6.png", "Image/P9_D3_P1_High_T3_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to clean the right bottle\"\nThe frames roughly correspond to the speech interval from 471.9754 seconds to 485.9734 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_Low_T3_12_Right", "images": ["Image/P20_D2_P4_Low_T3_12_Right/1.png", "Image/P20_D2_P4_Low_T3_12_Right/2.png", "Image/P20_D2_P4_Low_T3_12_Right/3.png", "Image/P20_D2_P4_Low_T3_12_Right/4.png", "Image/P20_D2_P4_Low_T3_12_Right/5.png", "Image/P20_D2_P4_Low_T3_12_Right/6.png", "Image/P20_D2_P4_Low_T3_12_Right/7.png", "Image/P20_D2_P4_Low_T3_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that bottle over there? Finish.\"\nThe frames roughly correspond to the speech interval from 3107.8441000000003 seconds to 3121.3589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P6_High_T2_2_Left", "images": ["Image/P16_D3_P6_High_T2_2_Left/1.png", "Image/P16_D3_P6_High_T2_2_Left/2.png", "Image/P16_D3_P6_High_T2_2_Left/3.png", "Image/P16_D3_P6_High_T2_2_Left/4.png", "Image/P16_D3_P6_High_T2_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water into that vase? Finish\"\nThe frames roughly correspond to the speech interval from 160.2114 seconds to 170.51129999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P8_Low_T1_4_Left", "images": ["Image/P6_D2_P8_Low_T1_4_Left/1.png", "Image/P6_D2_P8_Low_T1_4_Left/2.png", "Image/P6_D2_P8_Low_T1_4_Left/3.png", "Image/P6_D2_P8_Low_T1_4_Left/4.png", "Image/P6_D2_P8_Low_T1_4_Left/5.png", "Image/P6_D2_P8_Low_T1_4_Left/6.png", "Image/P6_D2_P8_Low_T1_4_Left/7.png", "Image/P6_D2_P8_Low_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1229.6978 seconds to 1251.0238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P6_Medium_T2_26_Left", "images": ["Image/P25_D3_P6_Medium_T2_26_Left/1.png", "Image/P25_D3_P6_Medium_T2_26_Left/2.png", "Image/P25_D3_P6_Medium_T2_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1901.3268 seconds to 1909.9867 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P8_Medium_T2_4_Right", "images": ["Image/P20_D1_P8_Medium_T2_4_Right/1.png", "Image/P20_D1_P8_Medium_T2_4_Right/2.png", "Image/P20_D1_P8_Medium_T2_4_Right/3.png", "Image/P20_D1_P8_Medium_T2_4_Right/4.png", "Image/P20_D1_P8_Medium_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start.\"\nThe frames roughly correspond to the speech interval from 180.80629999999996 seconds to 192.37395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P8_Medium_T2_1_Mid", "images": ["Image/P12_D2_P8_Medium_T2_1_Mid/1.png", "Image/P12_D2_P8_Medium_T2_1_Mid/2.png", "Image/P12_D2_P8_Medium_T2_1_Mid/3.png", "Image/P12_D2_P8_Medium_T2_1_Mid/4.png", "Image/P12_D2_P8_Medium_T2_1_Mid/5.png", "Image/P12_D2_P8_Medium_T2_1_Mid/6.png", "Image/P12_D2_P8_Medium_T2_1_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 693.4811 seconds to 706.0504999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P7_Medium_T2_20_Mid", "images": ["Image/P9_D3_P7_Medium_T2_20_Mid/1.png", "Image/P9_D3_P7_Medium_T2_20_Mid/2.png", "Image/P9_D3_P7_Medium_T2_20_Mid/3.png", "Image/P9_D3_P7_Medium_T2_20_Mid/4.png", "Image/P9_D3_P7_Medium_T2_20_Mid/5.png", "Image/P9_D3_P7_Medium_T2_20_Mid/6.png", "Image/P9_D3_P7_Medium_T2_20_Mid/7.png", "Image/P9_D3_P7_Medium_T2_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the middle of the cup? Finish.\"\nThe frames roughly correspond to the speech interval from 404.5974 seconds to 420.1926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P8_Low_T3_18_Right", "images": ["Image/P16_D2_P8_Low_T3_18_Right/1.png", "Image/P16_D2_P8_Low_T3_18_Right/2.png", "Image/P16_D2_P8_Low_T3_18_Right/3.png", "Image/P16_D2_P8_Low_T3_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 814.1958000000001 seconds to 823.7595000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P4_Low_T2_10_Left", "images": ["Image/P6_D3_P4_Low_T2_10_Left/1.png", "Image/P6_D3_P4_Low_T2_10_Left/2.png", "Image/P6_D3_P4_Low_T2_10_Left/3.png", "Image/P6_D3_P4_Low_T2_10_Left/4.png", "Image/P6_D3_P4_Low_T2_10_Left/5.png", "Image/P6_D3_P4_Low_T2_10_Left/6.png", "Image/P6_D3_P4_Low_T2_10_Left/7.png", "Image/P6_D3_P4_Low_T2_10_Left/8.png", "Image/P6_D3_P4_Low_T2_10_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4721.1064 seconds to 4738.2851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P9_Medium_T2_9_Left", "images": ["Image/P23_D2_P9_Medium_T2_9_Left/1.png", "Image/P23_D2_P9_Medium_T2_9_Left/2.png", "Image/P23_D2_P9_Medium_T2_9_Left/3.png", "Image/P23_D2_P9_Medium_T2_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 501.2937 seconds to 511.3285 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P9_Medium_T1_16_Left", "images": ["Image/P16_D1_P9_Medium_T1_16_Left/1.png", "Image/P16_D1_P9_Medium_T1_16_Left/2.png", "Image/P16_D1_P9_Medium_T1_16_Left/3.png", "Image/P16_D1_P9_Medium_T1_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this mug up here? Finish.\"\nThe frames roughly correspond to the speech interval from 1589.904 seconds to 1601.2616 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P2_High_T1_14_Mid", "images": ["Image/P13_D2_P2_High_T1_14_Mid/1.png", "Image/P13_D2_P2_High_T1_14_Mid/2.png", "Image/P13_D2_P2_High_T1_14_Mid/3.png", "Image/P13_D2_P2_High_T1_14_Mid/4.png", "Image/P13_D2_P2_High_T1_14_Mid/5.png", "Image/P13_D2_P2_High_T1_14_Mid/6.png", "Image/P13_D2_P2_High_T1_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from the top shelf all the way to the bottom shelf down here\"\nThe frames roughly correspond to the speech interval from 819.55596 seconds to 832.25058 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P9_Low_T2_23_Right", "images": ["Image/P9_D2_P9_Low_T2_23_Right/1.png", "Image/P9_D2_P9_Low_T2_23_Right/2.png", "Image/P9_D2_P9_Low_T2_23_Right/3.png", "Image/P9_D2_P9_Low_T2_23_Right/4.png", "Image/P9_D2_P9_Low_T2_23_Right/5.png", "Image/P9_D2_P9_Low_T2_23_Right/6.png", "Image/P9_D2_P9_Low_T2_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3033.2634 seconds to 3046.2188 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P6_Low_T3_19_Mid", "images": ["Image/P10_D1_P6_Low_T3_19_Mid/1.png", "Image/P10_D1_P6_Low_T3_19_Mid/2.png", "Image/P10_D1_P6_Low_T3_19_Mid/3.png", "Image/P10_D1_P6_Low_T3_19_Mid/4.png", "Image/P10_D1_P6_Low_T3_19_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2570.4865 seconds to 2582.3860999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P2_High_T1_4_Left", "images": ["Image/P12_D3_P2_High_T1_4_Left/1.png", "Image/P12_D3_P2_High_T1_4_Left/2.png", "Image/P12_D3_P2_High_T1_4_Left/3.png", "Image/P12_D3_P2_High_T1_4_Left/4.png", "Image/P12_D3_P2_High_T1_4_Left/5.png", "Image/P12_D3_P2_High_T1_4_Left/6.png", "Image/P12_D3_P2_High_T1_4_Left/7.png", "Image/P12_D3_P2_High_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2396.84119 seconds to 2410.29749 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P3_Low_T3_11_Right", "images": ["Image/P15_D2_P3_Low_T3_11_Right/1.png", "Image/P15_D2_P3_Low_T3_11_Right/2.png", "Image/P15_D2_P3_Low_T3_11_Right/3.png", "Image/P15_D2_P3_Low_T3_11_Right/4.png", "Image/P15_D2_P3_Low_T3_11_Right/5.png", "Image/P15_D2_P3_Low_T3_11_Right/6.png", "Image/P15_D2_P3_Low_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the brown container at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3020.4614 seconds to 3033.8241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P2_High_T1_15_Mid", "images": ["Image/P29_D3_P2_High_T1_15_Mid/1.png", "Image/P29_D3_P2_High_T1_15_Mid/2.png", "Image/P29_D3_P2_High_T1_15_Mid/3.png", "Image/P29_D3_P2_High_T1_15_Mid/4.png", "Image/P29_D3_P2_High_T1_15_Mid/5.png", "Image/P29_D3_P2_High_T1_15_Mid/6.png", "Image/P29_D3_P2_High_T1_15_Mid/7.png", "Image/P29_D3_P2_High_T1_15_Mid/8.png", "Image/P29_D3_P2_High_T1_15_Mid/9.png", "Image/P29_D3_P2_High_T1_15_Mid/10.png", "Image/P29_D3_P2_High_T1_15_Mid/11.png", "Image/P29_D3_P2_High_T1_15_Mid/12.png", "Image/P29_D3_P2_High_T1_15_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1378.8839999999998 seconds to 1398.245 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P1_Low_T2_23_Mid", "images": ["Image/P29_D3_P1_Low_T2_23_Mid/1.png", "Image/P29_D3_P1_Low_T2_23_Mid/2.png", "Image/P29_D3_P1_Low_T2_23_Mid/3.png", "Image/P29_D3_P1_Low_T2_23_Mid/4.png", "Image/P29_D3_P1_Low_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1422.5633999999998 seconds to 1432.6549999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P5_High_T3_5_Right", "images": ["Image/P3_D2_P5_High_T3_5_Right/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you move the item to this space?\"\nThe frames roughly correspond to the speech interval from 2356.2034000000003 seconds to 2364.8359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P9_Low_T1_20_Mid", "images": ["Image/P5_D3_P9_Low_T1_20_Mid/1.png", "Image/P5_D3_P9_Low_T1_20_Mid/2.png", "Image/P5_D3_P9_Low_T1_20_Mid/3.png", "Image/P5_D3_P9_Low_T1_20_Mid/4.png", "Image/P5_D3_P9_Low_T1_20_Mid/5.png", "Image/P5_D3_P9_Low_T1_20_Mid/6.png", "Image/P5_D3_P9_Low_T1_20_Mid/7.png", "Image/P5_D3_P9_Low_T1_20_Mid/8.png", "Image/P5_D3_P9_Low_T1_20_Mid/9.png", "Image/P5_D3_P9_Low_T1_20_Mid/10.png", "Image/P5_D3_P9_Low_T1_20_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottle From the bottom level of the shell to the top level of the shell in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2836.5306 seconds to 2854.1241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P9_High_T3_20_Left", "images": ["Image/P12_D2_P9_High_T3_20_Left/1.png", "Image/P12_D2_P9_High_T3_20_Left/2.png", "Image/P12_D2_P9_High_T3_20_Left/3.png", "Image/P12_D2_P9_High_T3_20_Left/4.png", "Image/P12_D2_P9_High_T3_20_Left/5.png", "Image/P12_D2_P9_High_T3_20_Left/6.png", "Image/P12_D2_P9_High_T3_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white vase on the bottom shelf that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 242.6957 seconds to 259.10630000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P2_High_T3_8_Right", "images": ["Image/P5_D2_P2_High_T3_8_Right/1.png", "Image/P5_D2_P2_High_T3_8_Right/2.png", "Image/P5_D2_P2_High_T3_8_Right/3.png", "Image/P5_D2_P2_High_T3_8_Right/4.png", "Image/P5_D2_P2_High_T3_8_Right/5.png", "Image/P5_D2_P2_High_T3_8_Right/6.png", "Image/P5_D2_P2_High_T3_8_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle next to the plant over there. Finished.\"\nThe frames roughly correspond to the speech interval from 381.8999 seconds to 399.1739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P6_Low_T1_8_Left", "images": ["Image/P14_D1_P6_Low_T1_8_Left/1.png", "Image/P14_D1_P6_Low_T1_8_Left/2.png", "Image/P14_D1_P6_Low_T1_8_Left/3.png", "Image/P14_D1_P6_Low_T1_8_Left/4.png", "Image/P14_D1_P6_Low_T1_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup to the end of the table. Finish.\"\nThe frames roughly correspond to the speech interval from 347.7901 seconds to 357.5899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P3_Medium_T2_27_Left", "images": ["Image/P27_D2_P3_Medium_T2_27_Left/1.png", "Image/P27_D2_P3_Medium_T2_27_Left/2.png", "Image/P27_D2_P3_Medium_T2_27_Left/3.png", "Image/P27_D2_P3_Medium_T2_27_Left/4.png", "Image/P27_D2_P3_Medium_T2_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in that first cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1351.2044999999998 seconds to 1362.172 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P7_Low_T3_5_Right", "images": ["Image/P9_D1_P7_Low_T3_5_Right/1.png", "Image/P9_D1_P7_Low_T3_5_Right/2.png", "Image/P9_D1_P7_Low_T3_5_Right/3.png", "Image/P9_D1_P7_Low_T3_5_Right/4.png", "Image/P9_D1_P7_Low_T3_5_Right/5.png", "Image/P9_D1_P7_Low_T3_5_Right/6.png", "Image/P9_D1_P7_Low_T3_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the plant in this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1715.6515 seconds to 1729.9683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P1_Low_T2_4_Left", "images": ["Image/P24_D2_P1_Low_T2_4_Left/1.png", "Image/P24_D2_P1_Low_T2_4_Left/2.png", "Image/P24_D2_P1_Low_T2_4_Left/3.png", "Image/P24_D2_P1_Low_T2_4_Left/4.png", "Image/P24_D2_P1_Low_T2_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you pour water to this planet\"\nThe frames roughly correspond to the speech interval from 215.7488 seconds to 226.4122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P1_High_T1_27_Mid", "images": ["Image/P5_D2_P1_High_T1_27_Mid/1.png", "Image/P5_D2_P1_High_T1_27_Mid/2.png", "Image/P5_D2_P1_High_T1_27_Mid/3.png", "Image/P5_D2_P1_High_T1_27_Mid/4.png", "Image/P5_D2_P1_High_T1_27_Mid/5.png", "Image/P5_D2_P1_High_T1_27_Mid/6.png", "Image/P5_D2_P1_High_T1_27_Mid/7.png", "Image/P5_D2_P1_High_T1_27_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottom cup to the bottom level of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 706.6569 seconds to 720.8704 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P9_Medium_T2_25_Right", "images": ["Image/P15_D3_P9_Medium_T2_25_Right/1.png", "Image/P15_D3_P9_Medium_T2_25_Right/2.png", "Image/P15_D3_P9_Medium_T2_25_Right/3.png", "Image/P15_D3_P9_Medium_T2_25_Right/4.png", "Image/P15_D3_P9_Medium_T2_25_Right/5.png", "Image/P15_D3_P9_Medium_T2_25_Right/6.png", "Image/P15_D3_P9_Medium_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 227.2212 seconds to 241.04260000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P9_High_T1_24_Right", "images": ["Image/P10_D1_P9_High_T1_24_Right/1.png", "Image/P10_D1_P9_High_T1_24_Right/2.png", "Image/P10_D1_P9_High_T1_24_Right/3.png", "Image/P10_D1_P9_High_T1_24_Right/4.png", "Image/P10_D1_P9_High_T1_24_Right/5.png", "Image/P10_D1_P9_High_T1_24_Right/6.png", "Image/P10_D1_P9_High_T1_24_Right/7.png", "Image/P10_D1_P9_High_T1_24_Right/8.png", "Image/P10_D1_P9_High_T1_24_Right/9.png", "Image/P10_D1_P9_High_T1_24_Right/10.png", "Image/P10_D1_P9_High_T1_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2546.3765 seconds to 2562.4125 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P1_Medium_T2_22_Right", "images": ["Image/P26_D3_P1_Medium_T2_22_Right/1.png", "Image/P26_D3_P1_Medium_T2_22_Right/2.png", "Image/P26_D3_P1_Medium_T2_22_Right/3.png", "Image/P26_D3_P1_Medium_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1070.1694 seconds to 1080.9424 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P8_Medium_T3_20_Mid", "images": ["Image/P26_D3_P8_Medium_T3_20_Mid/1.png", "Image/P26_D3_P8_Medium_T3_20_Mid/2.png", "Image/P26_D3_P8_Medium_T3_20_Mid/3.png", "Image/P26_D3_P8_Medium_T3_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the middle base on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 694.16425 seconds to 705.1319900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P1_Low_T2_17_Left", "images": ["Image/P16_D2_P1_Low_T2_17_Left/1.png", "Image/P16_D2_P1_Low_T2_17_Left/2.png", "Image/P16_D2_P1_Low_T2_17_Left/3.png", "Image/P16_D2_P1_Low_T2_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 796.7382 seconds to 807.5561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P7_Medium_T2_3_Left", "images": ["Image/P15_D3_P7_Medium_T2_3_Left/1.png", "Image/P15_D3_P7_Medium_T2_3_Left/2.png", "Image/P15_D3_P7_Medium_T2_3_Left/3.png", "Image/P15_D3_P7_Medium_T2_3_Left/4.png", "Image/P15_D3_P7_Medium_T2_3_Left/5.png", "Image/P15_D3_P7_Medium_T2_3_Left/6.png", "Image/P15_D3_P7_Medium_T2_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 103.41720000000002 seconds to 116.2368 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P7_Low_T1_17_Left", "images": ["Image/P1_D3_P7_Low_T1_17_Left/1.png", "Image/P1_D3_P7_Low_T1_17_Left/2.png", "Image/P1_D3_P7_Low_T1_17_Left/3.png", "Image/P1_D3_P7_Low_T1_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1573.2522 seconds to 1583.2612999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P5_High_T2_27_Left", "images": ["Image/P16_D2_P5_High_T2_27_Left/1.png", "Image/P16_D2_P5_High_T2_27_Left/2.png", "Image/P16_D2_P5_High_T2_27_Left/3.png", "Image/P16_D2_P5_High_T2_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 904.2711 seconds to 914.307 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P6_Medium_T1_10_Left", "images": ["Image/P20_D1_P6_Medium_T1_10_Left/1.png", "Image/P20_D1_P6_Medium_T1_10_Left/2.png", "Image/P20_D1_P6_Medium_T1_10_Left/3.png", "Image/P20_D1_P6_Medium_T1_10_Left/4.png", "Image/P20_D1_P6_Medium_T1_10_Left/5.png", "Image/P20_D1_P6_Medium_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this bottle over to here. Finish.\"\nThe frames roughly correspond to the speech interval from 261.89 seconds to 274.529 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P7_Low_T3_8_Mid", "images": ["Image/P23_D2_P7_Low_T3_8_Mid/1.png", "Image/P23_D2_P7_Low_T3_8_Mid/2.png", "Image/P23_D2_P7_Low_T3_8_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 467.0769 seconds to 478.7422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P4_Low_T2_24_Mid", "images": ["Image/P25_D2_P4_Low_T2_24_Mid/1.png", "Image/P25_D2_P4_Low_T2_24_Mid/2.png", "Image/P25_D2_P4_Low_T2_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 1061.0176 seconds to 1069.5487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P7_Medium_T2_20_Right", "images": ["Image/P8_D2_P7_Medium_T2_20_Right/1.png", "Image/P8_D2_P7_Medium_T2_20_Right/2.png", "Image/P8_D2_P7_Medium_T2_20_Right/3.png", "Image/P8_D2_P7_Medium_T2_20_Right/4.png", "Image/P8_D2_P7_Medium_T2_20_Right/5.png", "Image/P8_D2_P7_Medium_T2_20_Right/6.png", "Image/P8_D2_P7_Medium_T2_20_Right/7.png", "Image/P8_D2_P7_Medium_T2_20_Right/8.png", "Image/P8_D2_P7_Medium_T2_20_Right/9.png", "Image/P8_D2_P7_Medium_T2_20_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Done. Could you help me with pouring some water into the white boat\"\nThe frames roughly correspond to the speech interval from 2387.8962 seconds to 2404.6421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P5_Medium_T2_21_Left", "images": ["Image/P1_D1_P5_Medium_T2_21_Left/1.png", "Image/P1_D1_P5_Medium_T2_21_Left/2.png", "Image/P1_D1_P5_Medium_T2_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 207.5755 seconds to 216.2765 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P4_Low_T1_25_Mid", "images": ["Image/P13_D1_P4_Low_T1_25_Mid/1.png", "Image/P13_D1_P4_Low_T1_25_Mid/2.png", "Image/P13_D1_P4_Low_T1_25_Mid/3.png", "Image/P13_D1_P4_Low_T1_25_Mid/4.png", "Image/P13_D1_P4_Low_T1_25_Mid/5.png", "Image/P13_D1_P4_Low_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this white cup to the middle here\"\nThe frames roughly correspond to the speech interval from 379.1857 seconds to 390.9883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P9_Medium_T3_4_Mid", "images": ["Image/P11_D2_P9_Medium_T3_4_Mid/1.png", "Image/P11_D2_P9_Medium_T3_4_Mid/2.png", "Image/P11_D2_P9_Medium_T3_4_Mid/3.png", "Image/P11_D2_P9_Medium_T3_4_Mid/4.png", "Image/P11_D2_P9_Medium_T3_4_Mid/5.png", "Image/P11_D2_P9_Medium_T3_4_Mid/6.png", "Image/P11_D2_P9_Medium_T3_4_Mid/7.png", "Image/P11_D2_P9_Medium_T3_4_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the second plant on the bottom of the shelf. Please finish.\"\nThe frames roughly correspond to the speech interval from 616.2707 seconds to 630.0062 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P8_High_T2_10_Left", "images": ["Image/P22_D3_P8_High_T2_10_Left/1.png", "Image/P22_D3_P8_High_T2_10_Left/2.png", "Image/P22_D3_P8_High_T2_10_Left/3.png", "Image/P22_D3_P8_High_T2_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the orange bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 336.4013 seconds to 346.8133 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P1_Medium_T3_20_Right", "images": ["Image/P13_D2_P1_Medium_T3_20_Right/1.png", "Image/P13_D2_P1_Medium_T3_20_Right/2.png", "Image/P13_D2_P1_Medium_T3_20_Right/3.png", "Image/P13_D2_P1_Medium_T3_20_Right/4.png", "Image/P13_D2_P1_Medium_T3_20_Right/5.png", "Image/P13_D2_P1_Medium_T3_20_Right/6.png", "Image/P13_D2_P1_Medium_T3_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the white bottle on the right\"\nThe frames roughly correspond to the speech interval from 1139.53386 seconds to 1151.54516 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P9_Low_T3_21_Right", "images": ["Image/P6_D1_P9_Low_T3_21_Right/1.png", "Image/P6_D1_P9_Low_T3_21_Right/2.png", "Image/P6_D1_P9_Low_T3_21_Right/3.png", "Image/P6_D1_P9_Low_T3_21_Right/4.png", "Image/P6_D1_P9_Low_T3_21_Right/5.png", "Image/P6_D1_P9_Low_T3_21_Right/6.png", "Image/P6_D1_P9_Low_T3_21_Right/7.png", "Image/P6_D1_P9_Low_T3_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey buddy\"\nThe frames roughly correspond to the speech interval from 2414.0519000000004 seconds to 2426.9665000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P2_Low_T3_12_Mid", "images": ["Image/P13_D1_P2_Low_T3_12_Mid/1.png", "Image/P13_D1_P2_Low_T3_12_Mid/2.png", "Image/P13_D1_P2_Low_T3_12_Mid/3.png", "Image/P13_D1_P2_Low_T3_12_Mid/4.png", "Image/P13_D1_P2_Low_T3_12_Mid/5.png", "Image/P13_D1_P2_Low_T3_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean off the brown vase here\"\nThe frames roughly correspond to the speech interval from 399.9532 seconds to 411.3669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P5_Low_T1_3_Left", "images": ["Image/P11_D2_P5_Low_T1_3_Left/1.png", "Image/P11_D2_P5_Low_T1_3_Left/2.png", "Image/P11_D2_P5_Low_T1_3_Left/3.png", "Image/P11_D2_P5_Low_T1_3_Left/4.png", "Image/P11_D2_P5_Low_T1_3_Left/5.png", "Image/P11_D2_P5_Low_T1_3_Left/6.png", "Image/P11_D2_P5_Low_T1_3_Left/7.png", "Image/P11_D2_P5_Low_T1_3_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green glass to the top of the right shelf please. Finish.\"\nThe frames roughly correspond to the speech interval from 924.7037 seconds to 942.4502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P9_Medium_T1_22_Mid", "images": ["Image/P24_D3_P9_Medium_T1_22_Mid/1.png", "Image/P24_D3_P9_Medium_T1_22_Mid/2.png", "Image/P24_D3_P9_Medium_T1_22_Mid/3.png", "Image/P24_D3_P9_Medium_T1_22_Mid/4.png", "Image/P24_D3_P9_Medium_T1_22_Mid/5.png", "Image/P24_D3_P9_Medium_T1_22_Mid/6.png", "Image/P24_D3_P9_Medium_T1_22_Mid/7.png", "Image/P24_D3_P9_Medium_T1_22_Mid/8.png", "Image/P24_D3_P9_Medium_T1_22_Mid/9.png", "Image/P24_D3_P9_Medium_T1_22_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet up to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1960.2205 seconds to 1975.224 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P7_High_T1_26_Left", "images": ["Image/P7_D2_P7_High_T1_26_Left/1.png", "Image/P7_D2_P7_High_T1_26_Left/2.png", "Image/P7_D2_P7_High_T1_26_Left/3.png", "Image/P7_D2_P7_High_T1_26_Left/4.png", "Image/P7_D2_P7_High_T1_26_Left/5.png", "Image/P7_D2_P7_High_T1_26_Left/6.png", "Image/P7_D2_P7_High_T1_26_Left/7.png", "Image/P7_D2_P7_High_T1_26_Left/8.png", "Image/P7_D2_P7_High_T1_26_Left/9.png", "Image/P7_D2_P7_High_T1_26_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1368.5263000000002 seconds to 1387.9585000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P7_Medium_T2_10_Left", "images": ["Image/P14_D3_P7_Medium_T2_10_Left/1.png", "Image/P14_D3_P7_Medium_T2_10_Left/2.png", "Image/P14_D3_P7_Medium_T2_10_Left/3.png", "Image/P14_D3_P7_Medium_T2_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water that first bottle\"\nThe frames roughly correspond to the speech interval from 1226.39576 seconds to 1235.58766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P7_Medium_T1_2_Mid", "images": ["Image/P24_D2_P7_Medium_T1_2_Mid/1.png", "Image/P24_D2_P7_Medium_T1_2_Mid/2.png", "Image/P24_D2_P7_Medium_T1_2_Mid/3.png", "Image/P24_D2_P7_Medium_T1_2_Mid/4.png", "Image/P24_D2_P7_Medium_T1_2_Mid/5.png", "Image/P24_D2_P7_Medium_T1_2_Mid/6.png", "Image/P24_D2_P7_Medium_T1_2_Mid/7.png", "Image/P24_D2_P7_Medium_T1_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this bottle up here\"\nThe frames roughly correspond to the speech interval from 234.5132 seconds to 248.7263 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P9_High_T3_17_Right", "images": ["Image/P14_D3_P9_High_T3_17_Right/1.png", "Image/P14_D3_P9_High_T3_17_Right/2.png", "Image/P14_D3_P9_High_T3_17_Right/3.png", "Image/P14_D3_P9_High_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1114.7650600000002 seconds to 1124.17686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P7_Medium_T3_22_Left", "images": ["Image/P11_D1_P7_Medium_T3_22_Left/1.png", "Image/P11_D1_P7_Medium_T3_22_Left/2.png", "Image/P11_D1_P7_Medium_T3_22_Left/3.png", "Image/P11_D1_P7_Medium_T3_22_Left/4.png", "Image/P11_D1_P7_Medium_T3_22_Left/5.png", "Image/P11_D1_P7_Medium_T3_22_Left/6.png", "Image/P11_D1_P7_Medium_T3_22_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first plant from the left side in the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 2374.1717099999996 seconds to 2386.8095999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P5_Medium_T1_1_Mid", "images": ["Image/P23_D3_P5_Medium_T1_1_Mid/1.png", "Image/P23_D3_P5_Medium_T1_1_Mid/2.png", "Image/P23_D3_P5_Medium_T1_1_Mid/3.png", "Image/P23_D3_P5_Medium_T1_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put the middle one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1047.4857 seconds to 1056.6164 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P3_Medium_T2_7_Left", "images": ["Image/P10_D2_P3_Medium_T2_7_Left/1.png", "Image/P10_D2_P3_Medium_T2_7_Left/2.png", "Image/P10_D2_P3_Medium_T2_7_Left/3.png", "Image/P10_D2_P3_Medium_T2_7_Left/4.png", "Image/P10_D2_P3_Medium_T2_7_Left/5.png", "Image/P10_D2_P3_Medium_T2_7_Left/6.png", "Image/P10_D2_P3_Medium_T2_7_Left/7.png", "Image/P10_D2_P3_Medium_T2_7_Left/8.png", "Image/P10_D2_P3_Medium_T2_7_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1798.5799800000002 seconds to 1813.5025800000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P5_High_T2_21_Left", "images": ["Image/P5_D2_P5_High_T2_21_Left/1.png", "Image/P5_D2_P5_High_T2_21_Left/2.png", "Image/P5_D2_P5_High_T2_21_Left/3.png", "Image/P5_D2_P5_High_T2_21_Left/4.png", "Image/P5_D2_P5_High_T2_21_Left/5.png", "Image/P5_D2_P5_High_T2_21_Left/6.png", "Image/P5_D2_P5_High_T2_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle that is in front of me in white color. Finish.\"\nThe frames roughly correspond to the speech interval from 293.9136 seconds to 308.3055 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P6_Medium_T2_23_Right", "images": ["Image/P22_D2_P6_Medium_T2_23_Right/1.png", "Image/P22_D2_P6_Medium_T2_23_Right/2.png", "Image/P22_D2_P6_Medium_T2_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Water the plant on the right. Finish\"\nThe frames roughly correspond to the speech interval from 969.2414 seconds to 979.347 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P7_High_T3_22_Right", "images": ["Image/P4_D2_P7_High_T3_22_Right/1.png", "Image/P4_D2_P7_High_T3_22_Right/2.png", "Image/P4_D2_P7_High_T3_22_Right/3.png", "Image/P4_D2_P7_High_T3_22_Right/4.png", "Image/P4_D2_P7_High_T3_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the plant? Finish\"\nThe frames roughly correspond to the speech interval from 1784.6112999999998 seconds to 1798.1123999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P5_High_T1_23_Mid", "images": ["Image/P2_D1_P5_High_T1_23_Mid/1.png", "Image/P2_D1_P5_High_T1_23_Mid/2.png", "Image/P2_D1_P5_High_T1_23_Mid/3.png", "Image/P2_D1_P5_High_T1_23_Mid/4.png", "Image/P2_D1_P5_High_T1_23_Mid/5.png", "Image/P2_D1_P5_High_T1_23_Mid/6.png", "Image/P2_D1_P5_High_T1_23_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 385.1732999999999 seconds to 398.36639999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P7_Medium_T1_7_Left", "images": ["Image/P23_D2_P7_Medium_T1_7_Left/1.png", "Image/P23_D2_P7_Medium_T1_7_Left/2.png", "Image/P23_D2_P7_Medium_T1_7_Left/3.png", "Image/P23_D2_P7_Medium_T1_7_Left/4.png", "Image/P23_D2_P7_Medium_T1_7_Left/5.png", "Image/P23_D2_P7_Medium_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 268.09182999999996 seconds to 280.24372999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P7_Medium_T1_14_Right", "images": ["Image/P16_D1_P7_Medium_T1_14_Right/1.png", "Image/P16_D1_P7_Medium_T1_14_Right/2.png", "Image/P16_D1_P7_Medium_T1_14_Right/3.png", "Image/P16_D1_P7_Medium_T1_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move that plant up here? Finish.\"\nThe frames roughly correspond to the speech interval from 1194.48158 seconds to 1211.15609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P8_Medium_T3_22_Left", "images": ["Image/P4_D1_P8_Medium_T3_22_Left/1.png", "Image/P4_D1_P8_Medium_T3_22_Left/2.png", "Image/P4_D1_P8_Medium_T3_22_Left/3.png", "Image/P4_D1_P8_Medium_T3_22_Left/4.png", "Image/P4_D1_P8_Medium_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean this plant? Finish\"\nThe frames roughly correspond to the speech interval from 2247.2346 seconds to 2259.1614 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P9_High_T2_21_Left", "images": ["Image/P2_D3_P9_High_T2_21_Left/1.png", "Image/P2_D3_P9_High_T2_21_Left/2.png", "Image/P2_D3_P9_High_T2_21_Left/3.png", "Image/P2_D3_P9_High_T2_21_Left/4.png", "Image/P2_D3_P9_High_T2_21_Left/5.png", "Image/P2_D3_P9_High_T2_21_Left/6.png", "Image/P2_D3_P9_High_T2_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that white vessel there? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2157.1991000000003 seconds to 2169.3515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P4_Medium_T3_27_Left", "images": ["Image/P30_D2_P4_Medium_T3_27_Left/1.png", "Image/P30_D2_P4_Medium_T3_27_Left/2.png", "Image/P30_D2_P4_Medium_T3_27_Left/3.png", "Image/P30_D2_P4_Medium_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup closest to me. Finish.\"\nThe frames roughly correspond to the speech interval from 465.4032 seconds to 474.298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P8_Low_T2_8_Left", "images": ["Image/P21_D1_P8_Low_T2_8_Left/1.png", "Image/P21_D1_P8_Low_T2_8_Left/2.png", "Image/P21_D1_P8_Low_T2_8_Left/3.png", "Image/P21_D1_P8_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 992.3122 seconds to 1002.25 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P9_Medium_T2_13_Right", "images": ["Image/P14_D2_P9_Medium_T2_13_Right/1.png", "Image/P14_D2_P9_Medium_T2_13_Right/2.png", "Image/P14_D2_P9_Medium_T2_13_Right/3.png", "Image/P14_D2_P9_Medium_T2_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1760.909 seconds to 1769.9614 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P2_Medium_T3_5_Mid", "images": ["Image/P23_D1_P2_Medium_T3_5_Mid/1.png", "Image/P23_D1_P2_Medium_T3_5_Mid/2.png", "Image/P23_D1_P2_Medium_T3_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start cleaning up this one\"\nThe frames roughly correspond to the speech interval from 1769.6332 seconds to 1778.2904 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P1_Low_T1_7_Mid", "images": ["Image/P27_D3_P1_Low_T1_7_Mid/1.png", "Image/P27_D3_P1_Low_T1_7_Mid/2.png", "Image/P27_D3_P1_Low_T1_7_Mid/3.png", "Image/P27_D3_P1_Low_T1_7_Mid/4.png", "Image/P27_D3_P1_Low_T1_7_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that cup and put it on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 269.88620000000003 seconds to 280.8815 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P6_High_T3_21_Mid", "images": ["Image/P30_D3_P6_High_T3_21_Mid/1.png", "Image/P30_D3_P6_High_T3_21_Mid/2.png", "Image/P30_D3_P6_High_T3_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1620.1230999999998 seconds to 1628.1117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P4_Low_T3_26_Right", "images": ["Image/P19_D3_P4_Low_T3_26_Right/1.png", "Image/P19_D3_P4_Low_T3_26_Right/2.png", "Image/P19_D3_P4_Low_T3_26_Right/3.png", "Image/P19_D3_P4_Low_T3_26_Right/4.png", "Image/P19_D3_P4_Low_T3_26_Right/5.png", "Image/P19_D3_P4_Low_T3_26_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this glass over here. Finish.\"\nThe frames roughly correspond to the speech interval from 2108.83272 seconds to 2120.85916 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P7_Medium_T1_27_Right", "images": ["Image/P22_D2_P7_Medium_T1_27_Right/1.png", "Image/P22_D2_P7_Medium_T1_27_Right/2.png", "Image/P22_D2_P7_Medium_T1_27_Right/3.png", "Image/P22_D2_P7_Medium_T1_27_Right/4.png", "Image/P22_D2_P7_Medium_T1_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup on top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 585.2745499999999 seconds to 596.3825999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
