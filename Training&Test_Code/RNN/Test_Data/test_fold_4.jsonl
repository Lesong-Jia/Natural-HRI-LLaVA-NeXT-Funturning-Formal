{"id": "P12_D2_P9_Medium_T3_9_Right", "images": ["Image/P12_D2_P9_Medium_T3_9_Right/1.png", "Image/P12_D2_P9_Medium_T3_9_Right/2.png", "Image/P12_D2_P9_Medium_T3_9_Right/3.png", "Image/P12_D2_P9_Medium_T3_9_Right/4.png", "Image/P12_D2_P9_Medium_T3_9_Right/5.png", "Image/P12_D2_P9_Medium_T3_9_Right/6.png", "Image/P12_D2_P9_Medium_T3_9_Right/7.png", "Image/P12_D2_P9_Medium_T3_9_Right/8.png", "Image/P12_D2_P9_Medium_T3_9_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 404.2652000000001 seconds to 420.4103 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P8_Low_T3_19_Right", "images": ["Image/P4_D1_P8_Low_T3_19_Right/1.png", "Image/P4_D1_P8_Low_T3_19_Right/2.png", "Image/P4_D1_P8_Low_T3_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start and clean the bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 2186.4853 seconds to 2198.1231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P1_Medium_T1_26_Left", "images": ["Image/P11_D2_P1_Medium_T1_26_Left/1.png", "Image/P11_D2_P1_Medium_T1_26_Left/2.png", "Image/P11_D2_P1_Medium_T1_26_Left/3.png", "Image/P11_D2_P1_Medium_T1_26_Left/4.png", "Image/P11_D2_P1_Medium_T1_26_Left/5.png", "Image/P11_D2_P1_Medium_T1_26_Left/6.png", "Image/P11_D2_P1_Medium_T1_26_Left/7.png", "Image/P11_D2_P1_Medium_T1_26_Left/8.png", "Image/P11_D2_P1_Medium_T1_26_Left/9.png", "Image/P11_D2_P1_Medium_T1_26_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please move the first white glass from the left side to the bottom of the shelf\"\nThe frames roughly correspond to the speech interval from 1097.7424 seconds to 1116.5029 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P8_Medium_T3_17_Mid", "images": ["Image/P9_D3_P8_Medium_T3_17_Mid/1.png", "Image/P9_D3_P8_Medium_T3_17_Mid/2.png", "Image/P9_D3_P8_Medium_T3_17_Mid/3.png", "Image/P9_D3_P8_Medium_T3_17_Mid/4.png", "Image/P9_D3_P8_Medium_T3_17_Mid/5.png", "Image/P9_D3_P8_Medium_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the middle cup on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 930.9663 seconds to 944.2399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P7_High_T2_25_Right", "images": ["Image/P22_D2_P7_High_T2_25_Right/1.png", "Image/P22_D2_P7_High_T2_25_Right/2.png", "Image/P22_D2_P7_High_T2_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water in the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 756.4613999999999 seconds to 765.2717999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P5_High_T2_5_Mid", "images": ["Image/P5_D1_P5_High_T2_5_Mid/1.png", "Image/P5_D1_P5_High_T2_5_Mid/2.png", "Image/P5_D1_P5_High_T2_5_Mid/3.png", "Image/P5_D1_P5_High_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1840.959 seconds to 1851.5492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P1_Medium_T2_24_Mid", "images": ["Image/P13_D1_P1_Medium_T2_24_Mid/1.png", "Image/P13_D1_P1_Medium_T2_24_Mid/2.png", "Image/P13_D1_P1_Medium_T2_24_Mid/3.png", "Image/P13_D1_P1_Medium_T2_24_Mid/4.png", "Image/P13_D1_P1_Medium_T2_24_Mid/5.png", "Image/P13_D1_P1_Medium_T2_24_Mid/6.png", "Image/P13_D1_P1_Medium_T2_24_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday Can you water the middle plant right here? Finish\"\nThe frames roughly correspond to the speech interval from 151.44225 seconds to 164.97059000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P5_High_T1_11_Left", "images": ["Image/P10_D2_P5_High_T1_11_Left/1.png", "Image/P10_D2_P5_High_T1_11_Left/2.png", "Image/P10_D2_P5_High_T1_11_Left/3.png", "Image/P10_D2_P5_High_T1_11_Left/4.png", "Image/P10_D2_P5_High_T1_11_Left/5.png", "Image/P10_D2_P5_High_T1_11_Left/6.png", "Image/P10_D2_P5_High_T1_11_Left/7.png", "Image/P10_D2_P5_High_T1_11_Left/8.png", "Image/P10_D2_P5_High_T1_11_Left/9.png", "Image/P10_D2_P5_High_T1_11_Left/10.png", "Image/P10_D2_P5_High_T1_11_Left/11.png", "Image/P10_D2_P5_High_T1_11_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 1403.70278 seconds to 1423.5926800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P8_Low_T1_16_Left", "images": ["Image/P25_D2_P8_Low_T1_16_Left/1.png", "Image/P25_D2_P8_Low_T1_16_Left/2.png", "Image/P25_D2_P8_Low_T1_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Move the cup over here. Finish.\"\nThe frames roughly correspond to the speech interval from 968.7136 seconds to 977.803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P4_High_T1_3_Right", "images": ["Image/P5_D3_P4_High_T1_3_Right/1.png", "Image/P5_D3_P4_High_T1_3_Right/2.png", "Image/P5_D3_P4_High_T1_3_Right/3.png", "Image/P5_D3_P4_High_T1_3_Right/4.png", "Image/P5_D3_P4_High_T1_3_Right/5.png", "Image/P5_D3_P4_High_T1_3_Right/6.png", "Image/P5_D3_P4_High_T1_3_Right/7.png", "Image/P5_D3_P4_High_T1_3_Right/8.png", "Image/P5_D3_P4_High_T1_3_Right/9.png", "Image/P5_D3_P4_High_T1_3_Right/10.png", "Image/P5_D3_P4_High_T1_3_Right/11.png", "Image/P5_D3_P4_High_T1_3_Right/12.png", "Image/P5_D3_P4_High_T1_3_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the bottle on the table to the shelf in front of me. To the second level of the shelf in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 2175.9484199999997 seconds to 2196.00556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P9_Medium_T2_2_Mid", "images": ["Image/P22_D1_P9_Medium_T2_2_Mid/1.png", "Image/P22_D1_P9_Medium_T2_2_Mid/2.png", "Image/P22_D1_P9_Medium_T2_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 1551.4035 seconds to 1560.8818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P6_Low_T2_10_Right", "images": ["Image/P11_D2_P6_Low_T2_10_Right/1.png", "Image/P11_D2_P6_Low_T2_10_Right/2.png", "Image/P11_D2_P6_Low_T2_10_Right/3.png", "Image/P11_D2_P6_Low_T2_10_Right/4.png", "Image/P11_D2_P6_Low_T2_10_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a glass on the shelf. Please finish.\"\nThe frames roughly correspond to the speech interval from 571.0703000000001 seconds to 582.9991 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P2_High_T2_21_Left", "images": ["Image/P20_D3_P2_High_T2_21_Left/1.png", "Image/P20_D3_P2_High_T2_21_Left/2.png", "Image/P20_D3_P2_High_T2_21_Left/3.png", "Image/P20_D3_P2_High_T2_21_Left/4.png", "Image/P20_D3_P2_High_T2_21_Left/5.png", "Image/P20_D3_P2_High_T2_21_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that white thingy. Finished.\"\nThe frames roughly correspond to the speech interval from 1815.1759 seconds to 1827.8412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P8_Medium_T3_22_Right", "images": ["Image/P19_D1_P8_Medium_T3_22_Right/1.png", "Image/P19_D1_P8_Medium_T3_22_Right/2.png", "Image/P19_D1_P8_Medium_T3_22_Right/3.png", "Image/P19_D1_P8_Medium_T3_22_Right/4.png", "Image/P19_D1_P8_Medium_T3_22_Right/5.png", "Image/P19_D1_P8_Medium_T3_22_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1230.0562 seconds to 1243.1111999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D1_P5_Low_T1_4_Mid", "images": ["Image/P10_D1_P5_Low_T1_4_Mid/1.png", "Image/P10_D1_P5_Low_T1_4_Mid/2.png", "Image/P10_D1_P5_Low_T1_4_Mid/3.png", "Image/P10_D1_P5_Low_T1_4_Mid/4.png", "Image/P10_D1_P5_Low_T1_4_Mid/5.png", "Image/P10_D1_P5_Low_T1_4_Mid/6.png", "Image/P10_D1_P5_Low_T1_4_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2271.8477 seconds to 2286.1431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P5_Low_T2_21_Left", "images": ["Image/P12_D2_P5_Low_T2_21_Left/1.png", "Image/P12_D2_P5_Low_T2_21_Left/2.png", "Image/P12_D2_P5_Low_T2_21_Left/3.png", "Image/P12_D2_P5_Low_T2_21_Left/4.png", "Image/P12_D2_P5_Low_T2_21_Left/5.png", "Image/P12_D2_P5_Low_T2_21_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the white vase in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 191.8328 seconds to 204.611 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P3_High_T2_14_Right", "images": ["Image/P20_D1_P3_High_T2_14_Right/1.png", "Image/P20_D1_P3_High_T2_14_Right/2.png", "Image/P20_D1_P3_High_T2_14_Right/3.png", "Image/P20_D1_P3_High_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this plant over here. Finished.\"\nThe frames roughly correspond to the speech interval from 705.8667 seconds to 717.6803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P6_Low_T2_21_Left", "images": ["Image/P15_D1_P6_Low_T2_21_Left/1.png", "Image/P15_D1_P6_Low_T2_21_Left/2.png", "Image/P15_D1_P6_Low_T2_21_Left/3.png", "Image/P15_D1_P6_Low_T2_21_Left/4.png", "Image/P15_D1_P6_Low_T2_21_Left/5.png", "Image/P15_D1_P6_Low_T2_21_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start please pour water into the white jar. Finish.\"\nThe frames roughly correspond to the speech interval from 1950.59237 seconds to 1962.36847 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P2_Medium_T3_12_Right", "images": ["Image/P10_D2_P2_Medium_T3_12_Right/1.png", "Image/P10_D2_P2_Medium_T3_12_Right/2.png", "Image/P10_D2_P2_Medium_T3_12_Right/3.png", "Image/P10_D2_P2_Medium_T3_12_Right/4.png", "Image/P10_D2_P2_Medium_T3_12_Right/5.png", "Image/P10_D2_P2_Medium_T3_12_Right/6.png", "Image/P10_D2_P2_Medium_T3_12_Right/7.png", "Image/P10_D2_P2_Medium_T3_12_Right/8.png", "Image/P10_D2_P2_Medium_T3_12_Right/9.png", "Image/P10_D2_P2_Medium_T3_12_Right/10.png", "Image/P10_D2_P2_Medium_T3_12_Right/11.png", "Image/P10_D2_P2_Medium_T3_12_Right/12.png", "Image/P10_D2_P2_Medium_T3_12_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1265.2817800000005 seconds to 1283.64108 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P1_Medium_T3_14_Left", "images": ["Image/P13_D3_P1_Medium_T3_14_Left/1.png", "Image/P13_D3_P1_Medium_T3_14_Left/2.png", "Image/P13_D3_P1_Medium_T3_14_Left/3.png", "Image/P13_D3_P1_Medium_T3_14_Left/4.png", "Image/P13_D3_P1_Medium_T3_14_Left/5.png", "Image/P13_D3_P1_Medium_T3_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday Can you clean the left plant over there\"\nThe frames roughly correspond to the speech interval from 1614.0444 seconds to 1625.2502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P3_Medium_T2_1_Left", "images": ["Image/P7_D3_P3_Medium_T2_1_Left/1.png", "Image/P7_D3_P3_Medium_T2_1_Left/2.png", "Image/P7_D3_P3_Medium_T2_1_Left/3.png", "Image/P7_D3_P3_Medium_T2_1_Left/4.png", "Image/P7_D3_P3_Medium_T2_1_Left/5.png", "Image/P7_D3_P3_Medium_T2_1_Left/6.png", "Image/P7_D3_P3_Medium_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2032.752 seconds to 2046.5464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P5_High_T1_17_Right", "images": ["Image/P7_D3_P5_High_T1_17_Right/1.png", "Image/P7_D3_P5_High_T1_17_Right/2.png", "Image/P7_D3_P5_High_T1_17_Right/3.png", "Image/P7_D3_P5_High_T1_17_Right/4.png", "Image/P7_D3_P5_High_T1_17_Right/5.png", "Image/P7_D3_P5_High_T1_17_Right/6.png", "Image/P7_D3_P5_High_T1_17_Right/7.png", "Image/P7_D3_P5_High_T1_17_Right/8.png", "Image/P7_D3_P5_High_T1_17_Right/9.png", "Image/P7_D3_P5_High_T1_17_Right/10.png", "Image/P7_D3_P5_High_T1_17_Right/11.png", "Image/P7_D3_P5_High_T1_17_Right/12.png", "Image/P7_D3_P5_High_T1_17_Right/13.png", "Image/P7_D3_P5_High_T1_17_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Before they move the brown mug to the second level of the shelf\"\nThe frames roughly correspond to the speech interval from 1769.6521 seconds to 1789.947 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D1_P8_Medium_T2_24_Mid", "images": ["Image/P8_D1_P8_Medium_T2_24_Mid/1.png", "Image/P8_D1_P8_Medium_T2_24_Mid/2.png", "Image/P8_D1_P8_Medium_T2_24_Mid/3.png", "Image/P8_D1_P8_Medium_T2_24_Mid/4.png", "Image/P8_D1_P8_Medium_T2_24_Mid/5.png", "Image/P8_D1_P8_Medium_T2_24_Mid/6.png", "Image/P8_D1_P8_Medium_T2_24_Mid/7.png", "Image/P8_D1_P8_Medium_T2_24_Mid/8.png", "Image/P8_D1_P8_Medium_T2_24_Mid/9.png", "Image/P8_D1_P8_Medium_T2_24_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 523.1375999999999 seconds to 539.6767 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P4_Low_T3_17_Right", "images": ["Image/P20_D1_P4_Low_T3_17_Right/1.png", "Image/P20_D1_P4_Low_T3_17_Right/2.png", "Image/P20_D1_P4_Low_T3_17_Right/3.png", "Image/P20_D1_P4_Low_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 239.4653 seconds to 248.8325 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P9_Medium_T1_1_Right", "images": ["Image/P16_D2_P9_Medium_T1_1_Right/1.png", "Image/P16_D2_P9_Medium_T1_1_Right/2.png", "Image/P16_D2_P9_Medium_T1_1_Right/3.png", "Image/P16_D2_P9_Medium_T1_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 644.08049 seconds to 653.81005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P8_Medium_T3_9_Right", "images": ["Image/P22_D1_P8_Medium_T3_9_Right/1.png", "Image/P22_D1_P8_Medium_T3_9_Right/2.png", "Image/P22_D1_P8_Medium_T3_9_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1443.25897 seconds to 1453.05795 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P9_High_T2_21_Right", "images": ["Image/P27_D2_P9_High_T2_21_Right/1.png", "Image/P27_D2_P9_High_T2_21_Right/2.png", "Image/P27_D2_P9_High_T2_21_Right/3.png", "Image/P27_D2_P9_High_T2_21_Right/4.png", "Image/P27_D2_P9_High_T2_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the white vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1245.09379 seconds to 1255.50399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P7_Medium_T3_23_Right", "images": ["Image/P5_D2_P7_Medium_T3_23_Right/1.png", "Image/P5_D2_P7_Medium_T3_23_Right/2.png", "Image/P5_D2_P7_Medium_T3_23_Right/3.png", "Image/P5_D2_P7_Medium_T3_23_Right/4.png", "Image/P5_D2_P7_Medium_T3_23_Right/5.png", "Image/P5_D2_P7_Medium_T3_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Clean the plant as dry most of free plants. Finish.\"\nThe frames roughly correspond to the speech interval from 622.1772000000001 seconds to 635.8200999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P9_High_T3_23_Left", "images": ["Image/P30_D3_P9_High_T3_23_Left/1.png", "Image/P30_D3_P9_High_T3_23_Left/2.png", "Image/P30_D3_P9_High_T3_23_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1268.6031 seconds to 1276.9277 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P6_Low_T1_19_Right", "images": ["Image/P24_D1_P6_Low_T1_19_Right/1.png", "Image/P24_D1_P6_Low_T1_19_Right/2.png", "Image/P24_D1_P6_Low_T1_19_Right/3.png", "Image/P24_D1_P6_Low_T1_19_Right/4.png", "Image/P24_D1_P6_Low_T1_19_Right/5.png", "Image/P24_D1_P6_Low_T1_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 956.3389 seconds to 968.1423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P1_Medium_T3_10_Mid", "images": ["Image/P28_D2_P1_Medium_T3_10_Mid/1.png", "Image/P28_D2_P1_Medium_T3_10_Mid/2.png", "Image/P28_D2_P1_Medium_T3_10_Mid/3.png", "Image/P28_D2_P1_Medium_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that middle vase. Finish.\"\nThe frames roughly correspond to the speech interval from 818.07752 seconds to 827.83338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P9_High_T1_8_Mid", "images": ["Image/P25_D3_P9_High_T1_8_Mid/1.png", "Image/P25_D3_P9_High_T1_8_Mid/2.png", "Image/P25_D3_P9_High_T1_8_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Move this up there. Finish.\"\nThe frames roughly correspond to the speech interval from 1995.0788 seconds to 2003.4033 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P2_High_T3_4_Right", "images": ["Image/P16_D2_P2_High_T3_4_Right/1.png", "Image/P16_D2_P2_High_T3_4_Right/2.png", "Image/P16_D2_P2_High_T3_4_Right/3.png", "Image/P16_D2_P2_High_T3_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 938.1783 seconds to 949.4141 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P1_High_T1_19_Mid", "images": ["Image/P26_D2_P1_High_T1_19_Mid/1.png", "Image/P26_D2_P1_High_T1_19_Mid/2.png", "Image/P26_D2_P1_High_T1_19_Mid/3.png", "Image/P26_D2_P1_High_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move the white base to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 1256.9915 seconds to 1268.39086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P2_Low_T1_3_Left", "images": ["Image/P10_D2_P2_Low_T1_3_Left/1.png", "Image/P10_D2_P2_Low_T1_3_Left/2.png", "Image/P10_D2_P2_Low_T1_3_Left/3.png", "Image/P10_D2_P2_Low_T1_3_Left/4.png", "Image/P10_D2_P2_Low_T1_3_Left/5.png", "Image/P10_D2_P2_Low_T1_3_Left/6.png", "Image/P10_D2_P2_Low_T1_3_Left/7.png", "Image/P10_D2_P2_Low_T1_3_Left/8.png", "Image/P10_D2_P2_Low_T1_3_Left/9.png", "Image/P10_D2_P2_Low_T1_3_Left/10.png", "Image/P10_D2_P2_Low_T1_3_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1454.48148 seconds to 1472.1725800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P4_Medium_T2_2_Mid", "images": ["Image/P10_D3_P4_Medium_T2_2_Mid/1.png", "Image/P10_D3_P4_Medium_T2_2_Mid/2.png", "Image/P10_D3_P4_Medium_T2_2_Mid/3.png", "Image/P10_D3_P4_Medium_T2_2_Mid/4.png", "Image/P10_D3_P4_Medium_T2_2_Mid/5.png", "Image/P10_D3_P4_Medium_T2_2_Mid/6.png", "Image/P10_D3_P4_Medium_T2_2_Mid/7.png", "Image/P10_D3_P4_Medium_T2_2_Mid/8.png", "Image/P10_D3_P4_Medium_T2_2_Mid/9.png", "Image/P10_D3_P4_Medium_T2_2_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 648.90279 seconds to 665.52269 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P8_Medium_T2_4_Mid", "images": ["Image/P16_D3_P8_Medium_T2_4_Mid/1.png", "Image/P16_D3_P8_Medium_T2_4_Mid/2.png", "Image/P16_D3_P8_Medium_T2_4_Mid/3.png", "Image/P16_D3_P8_Medium_T2_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 231.9544 seconds to 241.2953 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P2_Medium_T3_19_Right", "images": ["Image/P12_D1_P2_Medium_T3_19_Right/1.png", "Image/P12_D1_P2_Medium_T3_19_Right/2.png", "Image/P12_D1_P2_Medium_T3_19_Right/3.png", "Image/P12_D1_P2_Medium_T3_19_Right/4.png", "Image/P12_D1_P2_Medium_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1309.4099 seconds to 1320.8092 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P3_Low_T2_22_Mid", "images": ["Image/P28_D1_P3_Low_T2_22_Mid/1.png", "Image/P28_D1_P3_Low_T2_22_Mid/2.png", "Image/P28_D1_P3_Low_T2_22_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Please pour water into this lid. Finish.\"\nThe frames roughly correspond to the speech interval from 1716.3889 seconds to 1725.1459 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P1_Medium_T1_17_Left", "images": ["Image/P26_D1_P1_Medium_T1_17_Left/1.png", "Image/P26_D1_P1_Medium_T1_17_Left/2.png", "Image/P26_D1_P1_Medium_T1_17_Left/3.png", "Image/P26_D1_P1_Medium_T1_17_Left/4.png", "Image/P26_D1_P1_Medium_T1_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the left canister down to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 306.44370000000004 seconds to 317.20269999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P5_Low_T3_13_Right", "images": ["Image/P1_D2_P5_Low_T3_13_Right/1.png", "Image/P1_D2_P5_Low_T3_13_Right/2.png", "Image/P1_D2_P5_Low_T3_13_Right/3.png", "Image/P1_D2_P5_Low_T3_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean that one? Finish.\"\nThe frames roughly correspond to the speech interval from 719.62669 seconds to 728.9128300000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P1_Low_T1_1_Right", "images": ["Image/P16_D1_P1_Low_T1_1_Right/1.png", "Image/P16_D1_P1_Low_T1_1_Right/2.png", "Image/P16_D1_P1_Low_T1_1_Right/3.png", "Image/P16_D1_P1_Low_T1_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this vase down there? Finish.\"\nThe frames roughly correspond to the speech interval from 1414.415 seconds to 1424.1857 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P2_Low_T1_22_Mid", "images": ["Image/P7_D2_P2_Low_T1_22_Mid/1.png", "Image/P7_D2_P2_Low_T1_22_Mid/2.png", "Image/P7_D2_P2_Low_T1_22_Mid/3.png", "Image/P7_D2_P2_Low_T1_22_Mid/4.png", "Image/P7_D2_P2_Low_T1_22_Mid/5.png", "Image/P7_D2_P2_Low_T1_22_Mid/6.png", "Image/P7_D2_P2_Low_T1_22_Mid/7.png", "Image/P7_D2_P2_Low_T1_22_Mid/8.png", "Image/P7_D2_P2_Low_T1_22_Mid/9.png", "Image/P7_D2_P2_Low_T1_22_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. If I remove that plant to the bottom of the shelf\"\nThe frames roughly correspond to the speech interval from 1417.2047000000002 seconds to 1435.3981 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P2_High_T3_4_Mid", "images": ["Image/P18_D2_P2_High_T3_4_Mid/1.png", "Image/P18_D2_P2_High_T3_4_Mid/2.png", "Image/P18_D2_P2_High_T3_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 510.8497 seconds to 519.0349 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P7_Medium_T1_9_Left", "images": ["Image/P14_D2_P7_Medium_T1_9_Left/1.png", "Image/P14_D2_P7_Medium_T1_9_Left/2.png", "Image/P14_D2_P7_Medium_T1_9_Left/3.png", "Image/P14_D2_P7_Medium_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1808.8176 seconds to 1817.8716 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P6_Medium_T2_15_Right", "images": ["Image/P16_D2_P6_Medium_T2_15_Right/1.png", "Image/P16_D2_P6_Medium_T2_15_Right/2.png", "Image/P16_D2_P6_Medium_T2_15_Right/3.png", "Image/P16_D2_P6_Medium_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1078.6879 seconds to 1088.2231000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P7_Low_T2_12_Mid", "images": ["Image/P7_D2_P7_Low_T2_12_Mid/1.png", "Image/P7_D2_P7_Low_T2_12_Mid/2.png", "Image/P7_D2_P7_Low_T2_12_Mid/3.png", "Image/P7_D2_P7_Low_T2_12_Mid/4.png", "Image/P7_D2_P7_Low_T2_12_Mid/5.png", "Image/P7_D2_P7_Low_T2_12_Mid/6.png", "Image/P7_D2_P7_Low_T2_12_Mid/7.png", "Image/P7_D2_P7_Low_T2_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. If ready\"\nThe frames roughly correspond to the speech interval from 1207.3196000000005 seconds to 1222.7981000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P1_High_T3_21_Right", "images": ["Image/P6_D3_P1_High_T3_21_Right/1.png", "Image/P6_D3_P1_High_T3_21_Right/2.png", "Image/P6_D3_P1_High_T3_21_Right/3.png", "Image/P6_D3_P1_High_T3_21_Right/4.png", "Image/P6_D3_P1_High_T3_21_Right/5.png", "Image/P6_D3_P1_High_T3_21_Right/6.png", "Image/P6_D3_P1_High_T3_21_Right/7.png", "Image/P6_D3_P1_High_T3_21_Right/8.png", "Image/P6_D3_P1_High_T3_21_Right/9.png", "Image/P6_D3_P1_High_T3_21_Right/10.png", "Image/P6_D3_P1_High_T3_21_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3516.1292 seconds to 3532.7766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P3_Low_T3_13_Left", "images": ["Image/P16_D3_P3_Low_T3_13_Left/1.png", "Image/P16_D3_P3_Low_T3_13_Left/2.png", "Image/P16_D3_P3_Low_T3_13_Left/3.png", "Image/P16_D3_P3_Low_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 532.1795 seconds to 541.3527 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P5_High_T3_27_Right", "images": ["Image/P8_D3_P5_High_T3_27_Right/1.png", "Image/P8_D3_P5_High_T3_27_Right/2.png", "Image/P8_D3_P5_High_T3_27_Right/3.png", "Image/P8_D3_P5_High_T3_27_Right/4.png", "Image/P8_D3_P5_High_T3_27_Right/5.png", "Image/P8_D3_P5_High_T3_27_Right/6.png", "Image/P8_D3_P5_High_T3_27_Right/7.png", "Image/P8_D3_P5_High_T3_27_Right/8.png", "Image/P8_D3_P5_High_T3_27_Right/9.png", "Image/P8_D3_P5_High_T3_27_Right/10.png", "Image/P8_D3_P5_High_T3_27_Right/11.png", "Image/P8_D3_P5_High_T3_27_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1013.35462 seconds to 1030.55957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P1_Low_T2_19_Left", "images": ["Image/P21_D1_P1_Low_T2_19_Left/1.png", "Image/P21_D1_P1_Low_T2_19_Left/2.png", "Image/P21_D1_P1_Low_T2_19_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1084.1694 seconds to 1094.6649 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P8_Medium_T1_26_Left", "images": ["Image/P23_D2_P8_Medium_T1_26_Left/1.png", "Image/P23_D2_P8_Medium_T1_26_Left/2.png", "Image/P23_D2_P8_Medium_T1_26_Left/3.png", "Image/P23_D2_P8_Medium_T1_26_Left/4.png", "Image/P23_D2_P8_Medium_T1_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move to that one. Left one. Over there. Finish.\"\nThe frames roughly correspond to the speech interval from 305.87145999999996 seconds to 318.21754999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P6_Medium_T2_22_Right", "images": ["Image/P8_D2_P6_Medium_T2_22_Right/1.png", "Image/P8_D2_P6_Medium_T2_22_Right/2.png", "Image/P8_D2_P6_Medium_T2_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you water the planet that is on the right?\"\nThe frames roughly correspond to the speech interval from 2175.1028 seconds to 2186.7268000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P7_Low_T3_20_Right", "images": ["Image/P20_D3_P7_Low_T3_20_Right/1.png", "Image/P20_D3_P7_Low_T3_20_Right/2.png", "Image/P20_D3_P7_Low_T3_20_Right/3.png", "Image/P20_D3_P7_Low_T3_20_Right/4.png", "Image/P20_D3_P7_Low_T3_20_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Okay\"\nThe frames roughly correspond to the speech interval from 1400.6122 seconds to 1417.5236 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P1_Low_T1_4_Mid", "images": ["Image/P14_D3_P1_Low_T1_4_Mid/1.png", "Image/P14_D3_P1_Low_T1_4_Mid/2.png", "Image/P14_D3_P1_Low_T1_4_Mid/3.png", "Image/P14_D3_P1_Low_T1_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1404.10186 seconds to 1413.61276 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P5_High_T3_9_Mid", "images": ["Image/P6_D1_P5_High_T3_9_Mid/1.png", "Image/P6_D1_P5_High_T3_9_Mid/2.png", "Image/P6_D1_P5_High_T3_9_Mid/3.png", "Image/P6_D1_P5_High_T3_9_Mid/4.png", "Image/P6_D1_P5_High_T3_9_Mid/5.png", "Image/P6_D1_P5_High_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1992.346 seconds to 2005.9578 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P5_Medium_T3_6_Mid", "images": ["Image/P9_D1_P5_Medium_T3_6_Mid/1.png", "Image/P9_D1_P5_Medium_T3_6_Mid/2.png", "Image/P9_D1_P5_Medium_T3_6_Mid/3.png", "Image/P9_D1_P5_Medium_T3_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you help me to clean this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1826.535 seconds to 1844.2397 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P2_High_T1_12_Right", "images": ["Image/P25_D3_P2_High_T1_12_Right/1.png", "Image/P25_D3_P2_High_T1_12_Right/2.png", "Image/P25_D3_P2_High_T1_12_Right/3.png", "Image/P25_D3_P2_High_T1_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1754.88296 seconds to 1763.6807 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P8_Medium_T1_9_Left", "images": ["Image/P21_D2_P8_Medium_T1_9_Left/1.png", "Image/P21_D2_P8_Medium_T1_9_Left/2.png", "Image/P21_D2_P8_Medium_T1_9_Left/3.png", "Image/P21_D2_P8_Medium_T1_9_Left/4.png", "Image/P21_D2_P8_Medium_T1_9_Left/5.png", "Image/P21_D2_P8_Medium_T1_9_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1529.1252000000002 seconds to 1541.8754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P5_Medium_T1_18_Left", "images": ["Image/P12_D2_P5_Medium_T1_18_Left/1.png", "Image/P12_D2_P5_Medium_T1_18_Left/2.png", "Image/P12_D2_P5_Medium_T1_18_Left/3.png", "Image/P12_D2_P5_Medium_T1_18_Left/4.png", "Image/P12_D2_P5_Medium_T1_18_Left/5.png", "Image/P12_D2_P5_Medium_T1_18_Left/6.png", "Image/P12_D2_P5_Medium_T1_18_Left/7.png", "Image/P12_D2_P5_Medium_T1_18_Left/8.png", "Image/P12_D2_P5_Medium_T1_18_Left/9.png", "Image/P12_D2_P5_Medium_T1_18_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 448.2088 seconds to 468.5316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P4_Low_T2_15_Right", "images": ["Image/P6_D2_P4_Low_T2_15_Right/1.png", "Image/P6_D2_P4_Low_T2_15_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Is this a cup? I'm a little hungry.\"\nThe frames roughly correspond to the speech interval from 807.8870999999999 seconds to 824.9408 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P2_Medium_T2_15_Mid", "images": ["Image/P14_D1_P2_Medium_T2_15_Mid/1.png", "Image/P14_D1_P2_Medium_T2_15_Mid/2.png", "Image/P14_D1_P2_Medium_T2_15_Mid/3.png", "Image/P14_D1_P2_Medium_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Water this plant\"\nThe frames roughly correspond to the speech interval from 134.56470000000002 seconds to 144.0014 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P1_Low_T3_16_Left", "images": ["Image/P26_D3_P1_Low_T3_16_Left/1.png", "Image/P26_D3_P1_Low_T3_16_Left/2.png", "Image/P26_D3_P1_Low_T3_16_Left/3.png", "Image/P26_D3_P1_Low_T3_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the vase on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 883.2097 seconds to 896.3074 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P8_Low_T2_13_Mid", "images": ["Image/P27_D2_P8_Low_T2_13_Mid/1.png", "Image/P27_D2_P8_Low_T2_13_Mid/2.png", "Image/P27_D2_P8_Low_T2_13_Mid/3.png", "Image/P27_D2_P8_Low_T2_13_Mid/4.png", "Image/P27_D2_P8_Low_T2_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1465.3192 seconds to 1475.23 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P1_Medium_T2_8_Right", "images": ["Image/P5_D3_P1_Medium_T2_8_Right/1.png", "Image/P5_D3_P1_Medium_T2_8_Right/2.png", "Image/P5_D3_P1_Medium_T2_8_Right/3.png", "Image/P5_D3_P1_Medium_T2_8_Right/4.png", "Image/P5_D3_P1_Medium_T2_8_Right/5.png", "Image/P5_D3_P1_Medium_T2_8_Right/6.png", "Image/P5_D3_P1_Medium_T2_8_Right/7.png", "Image/P5_D3_P1_Medium_T2_8_Right/8.png", "Image/P5_D3_P1_Medium_T2_8_Right/9.png", "Image/P5_D3_P1_Medium_T2_8_Right/10.png", "Image/P5_D3_P1_Medium_T2_8_Right/11.png", "Image/P5_D3_P1_Medium_T2_8_Right/12.png", "Image/P5_D3_P1_Medium_T2_8_Right/13.png", "Image/P5_D3_P1_Medium_T2_8_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup that's on top of the table and far away from the middle cup. Finish.\"\nThe frames roughly correspond to the speech interval from 2905.7956 seconds to 2925.6744 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P5_High_T2_21_Mid", "images": ["Image/P15_D1_P5_High_T2_21_Mid/1.png", "Image/P15_D1_P5_High_T2_21_Mid/2.png", "Image/P15_D1_P5_High_T2_21_Mid/3.png", "Image/P15_D1_P5_High_T2_21_Mid/4.png", "Image/P15_D1_P5_High_T2_21_Mid/5.png", "Image/P15_D1_P5_High_T2_21_Mid/6.png", "Image/P15_D1_P5_High_T2_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the white jar. Finish.\"\nThe frames roughly correspond to the speech interval from 1877.29057 seconds to 1888.99617 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P7_High_T1_9_Left", "images": ["Image/P26_D2_P7_High_T1_9_Left/1.png", "Image/P26_D2_P7_High_T1_9_Left/2.png", "Image/P26_D2_P7_High_T1_9_Left/3.png", "Image/P26_D2_P7_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this green vase to the top shelf\"\nThe frames roughly correspond to the speech interval from 1345.7166 seconds to 1355.7804999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P9_Low_T2_24_Left", "images": ["Image/P16_D3_P9_Low_T2_24_Left/1.png", "Image/P16_D3_P9_Low_T2_24_Left/2.png", "Image/P16_D3_P9_Low_T2_24_Left/3.png", "Image/P16_D3_P9_Low_T2_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 88.55171999999999 seconds to 97.65524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P1_High_T1_26_Right", "images": ["Image/P2_D2_P1_High_T1_26_Right/1.png", "Image/P2_D2_P1_High_T1_26_Right/2.png", "Image/P2_D2_P1_High_T1_26_Right/3.png", "Image/P2_D2_P1_High_T1_26_Right/4.png", "Image/P2_D2_P1_High_T1_26_Right/5.png", "Image/P2_D2_P1_High_T1_26_Right/6.png", "Image/P2_D2_P1_High_T1_26_Right/7.png", "Image/P2_D2_P1_High_T1_26_Right/8.png", "Image/P2_D2_P1_High_T1_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2421.22311 seconds to 2437.43808 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P2_Low_T3_11_Left", "images": ["Image/P26_D1_P2_Low_T3_11_Left/1.png", "Image/P26_D1_P2_Low_T3_11_Left/2.png", "Image/P26_D1_P2_Low_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this? Finish.\"\nThe frames roughly correspond to the speech interval from 63.207129999999985 seconds to 73.61927999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P7_Medium_T1_3_Left", "images": ["Image/P29_D2_P7_Medium_T1_3_Left/1.png", "Image/P29_D2_P7_Medium_T1_3_Left/2.png", "Image/P29_D2_P7_Medium_T1_3_Left/3.png", "Image/P29_D2_P7_Medium_T1_3_Left/4.png", "Image/P29_D2_P7_Medium_T1_3_Left/5.png", "Image/P29_D2_P7_Medium_T1_3_Left/6.png", "Image/P29_D2_P7_Medium_T1_3_Left/7.png", "Image/P29_D2_P7_Medium_T1_3_Left/8.png", "Image/P29_D2_P7_Medium_T1_3_Left/9.png", "Image/P29_D2_P7_Medium_T1_3_Left/10.png", "Image/P29_D2_P7_Medium_T1_3_Left/11.png", "Image/P29_D2_P7_Medium_T1_3_Left/12.png", "Image/P29_D2_P7_Medium_T1_3_Left/13.png", "Image/P29_D2_P7_Medium_T1_3_Left/14.png", "Image/P29_D2_P7_Medium_T1_3_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 642.7435 seconds to 663.8185 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P9_High_T1_25_Mid", "images": ["Image/P29_D3_P9_High_T1_25_Mid/1.png", "Image/P29_D3_P9_High_T1_25_Mid/2.png", "Image/P29_D3_P9_High_T1_25_Mid/3.png", "Image/P29_D3_P9_High_T1_25_Mid/4.png", "Image/P29_D3_P9_High_T1_25_Mid/5.png", "Image/P29_D3_P9_High_T1_25_Mid/6.png", "Image/P29_D3_P9_High_T1_25_Mid/7.png", "Image/P29_D3_P9_High_T1_25_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1596.4099 seconds to 1610.2594 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P5_Low_T1_3_Left", "images": ["Image/P22_D3_P5_Low_T1_3_Left/1.png", "Image/P22_D3_P5_Low_T1_3_Left/2.png", "Image/P22_D3_P5_Low_T1_3_Left/3.png", "Image/P22_D3_P5_Low_T1_3_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase to that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 283.2975 seconds to 293.6382 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P9_Medium_T3_10_Right", "images": ["Image/P19_D3_P9_Medium_T3_10_Right/1.png", "Image/P19_D3_P9_Medium_T3_10_Right/2.png", "Image/P19_D3_P9_Medium_T3_10_Right/3.png", "Image/P19_D3_P9_Medium_T3_10_Right/4.png", "Image/P19_D3_P9_Medium_T3_10_Right/5.png", "Image/P19_D3_P9_Medium_T3_10_Right/6.png", "Image/P19_D3_P9_Medium_T3_10_Right/7.png", "Image/P19_D3_P9_Medium_T3_10_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that container right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2516.1841999999997 seconds to 2535.5321 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P4_Medium_T1_17_Left", "images": ["Image/P17_D1_P4_Medium_T1_17_Left/1.png", "Image/P17_D1_P4_Medium_T1_17_Left/2.png", "Image/P17_D1_P4_Medium_T1_17_Left/3.png", "Image/P17_D1_P4_Medium_T1_17_Left/4.png", "Image/P17_D1_P4_Medium_T1_17_Left/5.png", "Image/P17_D1_P4_Medium_T1_17_Left/6.png", "Image/P17_D1_P4_Medium_T1_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this brown mug to the back center of the table? Finish.\"\nThe frames roughly correspond to the speech interval from 1813.1489 seconds to 1825.7458 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P9_Low_T1_3_Right", "images": ["Image/P3_D3_P9_Low_T1_3_Right/1.png", "Image/P3_D3_P9_Low_T1_3_Right/2.png", "Image/P3_D3_P9_Low_T1_3_Right/3.png", "Image/P3_D3_P9_Low_T1_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you move the item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 886.9884999999999 seconds to 896.9839 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P7_Medium_T2_22_Mid", "images": ["Image/P5_D3_P7_Medium_T2_22_Mid/1.png", "Image/P5_D3_P7_Medium_T2_22_Mid/2.png", "Image/P5_D3_P7_Medium_T2_22_Mid/3.png", "Image/P5_D3_P7_Medium_T2_22_Mid/4.png", "Image/P5_D3_P7_Medium_T2_22_Mid/5.png", "Image/P5_D3_P7_Medium_T2_22_Mid/6.png", "Image/P5_D3_P7_Medium_T2_22_Mid/7.png", "Image/P5_D3_P7_Medium_T2_22_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. There are three plants on the ground. Water\"\nThe frames roughly correspond to the speech interval from 2464.5901999999996 seconds to 2478.7347999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P8_Low_T3_27_Left", "images": ["Image/P28_D2_P8_Low_T3_27_Left/1.png", "Image/P28_D2_P8_Low_T3_27_Left/2.png", "Image/P28_D2_P8_Low_T3_27_Left/3.png", "Image/P28_D2_P8_Low_T3_27_Left/4.png", "Image/P28_D2_P8_Low_T3_27_Left/5.png", "Image/P28_D2_P8_Low_T3_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that object over there. Finish.\"\nThe frames roughly correspond to the speech interval from 796.0539200000001 seconds to 808.92925 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P8_Low_T1_7_Left", "images": ["Image/P28_D3_P8_Low_T1_7_Left/1.png", "Image/P28_D3_P8_Low_T1_7_Left/2.png", "Image/P28_D3_P8_Low_T1_7_Left/3.png", "Image/P28_D3_P8_Low_T1_7_Left/4.png", "Image/P28_D3_P8_Low_T1_7_Left/5.png", "Image/P28_D3_P8_Low_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 415.4357 seconds to 426.6124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P9_High_T2_11_Right", "images": ["Image/P22_D1_P9_High_T2_11_Right/1.png", "Image/P22_D1_P9_High_T2_11_Right/2.png", "Image/P22_D1_P9_High_T2_11_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1727.6013 seconds to 1736.3967 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P2_Medium_T3_15_Mid", "images": ["Image/P3_D2_P2_Medium_T3_15_Mid/1.png", "Image/P3_D2_P2_Medium_T3_15_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2520.8314 seconds to 2529.9479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P2_Medium_T1_22_Right", "images": ["Image/P16_D1_P2_Medium_T1_22_Right/1.png", "Image/P16_D1_P2_Medium_T1_22_Right/2.png", "Image/P16_D1_P2_Medium_T1_22_Right/3.png", "Image/P16_D1_P2_Medium_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move this plant down there. Finish.\"\nThe frames roughly correspond to the speech interval from 1554.1436 seconds to 1563.9547 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P5_Low_T2_4_Right", "images": ["Image/P15_D3_P5_Low_T2_4_Right/1.png", "Image/P15_D3_P5_Low_T2_4_Right/2.png", "Image/P15_D3_P5_Low_T2_4_Right/3.png", "Image/P15_D3_P5_Low_T2_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 802.1971 seconds to 812.7622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P4_High_T3_15_Right", "images": ["Image/P25_D3_P4_High_T3_15_Right/1.png", "Image/P25_D3_P4_High_T3_15_Right/2.png", "Image/P25_D3_P4_High_T3_15_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1802.2038 seconds to 1813.7868 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P3_Low_T2_6_Right", "images": ["Image/P29_D2_P3_Low_T2_6_Right/1.png", "Image/P29_D2_P3_Low_T2_6_Right/2.png", "Image/P29_D2_P3_Low_T2_6_Right/3.png", "Image/P29_D2_P3_Low_T2_6_Right/4.png", "Image/P29_D2_P3_Low_T2_6_Right/5.png", "Image/P29_D2_P3_Low_T2_6_Right/6.png", "Image/P29_D2_P3_Low_T2_6_Right/7.png", "Image/P29_D2_P3_Low_T2_6_Right/8.png", "Image/P29_D2_P3_Low_T2_6_Right/9.png", "Image/P29_D2_P3_Low_T2_6_Right/10.png", "Image/P29_D2_P3_Low_T2_6_Right/11.png", "Image/P29_D2_P3_Low_T2_6_Right/12.png", "Image/P29_D2_P3_Low_T2_6_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Just know for the\"\nThe frames roughly correspond to the speech interval from 88.67669000000001 seconds to 102.13573 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P1_High_T3_18_Right", "images": ["Image/P14_D2_P1_High_T3_18_Right/1.png", "Image/P14_D2_P1_High_T3_18_Right/2.png", "Image/P14_D2_P1_High_T3_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1617.3080999999995 seconds to 1625.9883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P3_Low_T1_26_Left", "images": ["Image/P16_D2_P3_Low_T1_26_Left/1.png", "Image/P16_D2_P3_Low_T1_26_Left/2.png", "Image/P16_D2_P3_Low_T1_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that mug down there? Finish.\"\nThe frames roughly correspond to the speech interval from 1007.863 seconds to 1016.5767 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P6_Low_T3_15_Left", "images": ["Image/P11_D3_P6_Low_T3_15_Left/1.png", "Image/P11_D3_P6_Low_T3_15_Left/2.png", "Image/P11_D3_P6_Low_T3_15_Left/3.png", "Image/P11_D3_P6_Low_T3_15_Left/4.png", "Image/P11_D3_P6_Low_T3_15_Left/5.png", "Image/P11_D3_P6_Low_T3_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1742.258 seconds to 1753.5885 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P9_High_T1_15_Mid", "images": ["Image/P1_D2_P9_High_T1_15_Mid/1.png", "Image/P1_D2_P9_High_T1_15_Mid/2.png", "Image/P1_D2_P9_High_T1_15_Mid/3.png", "Image/P1_D2_P9_High_T1_15_Mid/4.png", "Image/P1_D2_P9_High_T1_15_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 845.5245 seconds to 856.4383 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P3_High_T2_5_Mid", "images": ["Image/P2_D3_P3_High_T2_5_Mid/1.png", "Image/P2_D3_P3_High_T2_5_Mid/2.png", "Image/P2_D3_P3_High_T2_5_Mid/3.png", "Image/P2_D3_P3_High_T2_5_Mid/4.png", "Image/P2_D3_P3_High_T2_5_Mid/5.png", "Image/P2_D3_P3_High_T2_5_Mid/6.png", "Image/P2_D3_P3_High_T2_5_Mid/7.png", "Image/P2_D3_P3_High_T2_5_Mid/8.png", "Image/P2_D3_P3_High_T2_5_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 2008.6875 seconds to 2025.5288000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P2_Medium_T1_27_Left", "images": ["Image/P20_D3_P2_Medium_T1_27_Left/1.png", "Image/P20_D3_P2_Medium_T1_27_Left/2.png", "Image/P20_D3_P2_Medium_T1_27_Left/3.png", "Image/P20_D3_P2_Medium_T1_27_Left/4.png", "Image/P20_D3_P2_Medium_T1_27_Left/5.png", "Image/P20_D3_P2_Medium_T1_27_Left/6.png", "Image/P20_D3_P2_Medium_T1_27_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the laptop to the bottom of the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1768.6403 seconds to 1783.2143999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P4_Medium_T2_27_Left", "images": ["Image/P26_D1_P4_Medium_T2_27_Left/1.png", "Image/P26_D1_P4_Medium_T2_27_Left/2.png", "Image/P26_D1_P4_Medium_T2_27_Left/3.png", "Image/P26_D1_P4_Medium_T2_27_Left/4.png", "Image/P26_D1_P4_Medium_T2_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into just the left cup? Finish\"\nThe frames roughly correspond to the speech interval from 455.03829999999994 seconds to 467.8569 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P2_High_T1_22_Left", "images": ["Image/P24_D1_P2_High_T1_22_Left/1.png", "Image/P24_D1_P2_High_T1_22_Left/2.png", "Image/P24_D1_P2_High_T1_22_Left/3.png", "Image/P24_D1_P2_High_T1_22_Left/4.png", "Image/P24_D1_P2_High_T1_22_Left/5.png", "Image/P24_D1_P2_High_T1_22_Left/6.png", "Image/P24_D1_P2_High_T1_22_Left/7.png", "Image/P24_D1_P2_High_T1_22_Left/8.png", "Image/P24_D1_P2_High_T1_22_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this area? Please finish.\"\nThe frames roughly correspond to the speech interval from 1315.9305 seconds to 1330.4636999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P9_High_T1_1_Right", "images": ["Image/P20_D1_P9_High_T1_1_Right/1.png", "Image/P20_D1_P9_High_T1_1_Right/2.png", "Image/P20_D1_P9_High_T1_1_Right/3.png", "Image/P20_D1_P9_High_T1_1_Right/4.png", "Image/P20_D1_P9_High_T1_1_Right/5.png", "Image/P20_D1_P9_High_T1_1_Right/6.png", "Image/P20_D1_P9_High_T1_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey robot\"\nThe frames roughly correspond to the speech interval from 628.2604 seconds to 641.1217 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P5_Low_T2_25_Right", "images": ["Image/P27_D2_P5_Low_T2_25_Right/1.png", "Image/P27_D2_P5_Low_T2_25_Right/2.png", "Image/P27_D2_P5_Low_T2_25_Right/3.png", "Image/P27_D2_P5_Low_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put water in that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1617.4096 seconds to 1626.9157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P9_Low_T1_19_Right", "images": ["Image/P15_D3_P9_Low_T1_19_Right/1.png", "Image/P15_D3_P9_Low_T1_19_Right/2.png", "Image/P15_D3_P9_Low_T1_19_Right/3.png", "Image/P15_D3_P9_Low_T1_19_Right/4.png", "Image/P15_D3_P9_Low_T1_19_Right/5.png", "Image/P15_D3_P9_Low_T1_19_Right/6.png", "Image/P15_D3_P9_Low_T1_19_Right/7.png", "Image/P15_D3_P9_Low_T1_19_Right/8.png", "Image/P15_D3_P9_Low_T1_19_Right/9.png", "Image/P15_D3_P9_Low_T1_19_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white container to the top of the brown shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 641.4639999999999 seconds to 656.4689 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P9_High_T1_3_Mid", "images": ["Image/P22_D3_P9_High_T1_3_Mid/1.png", "Image/P22_D3_P9_High_T1_3_Mid/2.png", "Image/P22_D3_P9_High_T1_3_Mid/3.png", "Image/P22_D3_P9_High_T1_3_Mid/4.png", "Image/P22_D3_P9_High_T1_3_Mid/5.png", "Image/P22_D3_P9_High_T1_3_Mid/6.png", "Image/P22_D3_P9_High_T1_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 453.8854 seconds to 466.2317000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P6_Low_T3_2_Left", "images": ["Image/P8_D3_P6_Low_T3_2_Left/1.png", "Image/P8_D3_P6_Low_T3_2_Left/2.png", "Image/P8_D3_P6_Low_T3_2_Left/3.png", "Image/P8_D3_P6_Low_T3_2_Left/4.png", "Image/P8_D3_P6_Low_T3_2_Left/5.png", "Image/P8_D3_P6_Low_T3_2_Left/6.png", "Image/P8_D3_P6_Low_T3_2_Left/7.png", "Image/P8_D3_P6_Low_T3_2_Left/8.png", "Image/P8_D3_P6_Low_T3_2_Left/9.png", "Image/P8_D3_P6_Low_T3_2_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 995.84375 seconds to 1006.18841 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P8_Low_T3_15_Left", "images": ["Image/P5_D2_P8_Low_T3_15_Left/1.png", "Image/P5_D2_P8_Low_T3_15_Left/2.png", "Image/P5_D2_P8_Low_T3_15_Left/3.png", "Image/P5_D2_P8_Low_T3_15_Left/4.png", "Image/P5_D2_P8_Low_T3_15_Left/5.png", "Image/P5_D2_P8_Low_T3_15_Left/6.png", "Image/P5_D2_P8_Low_T3_15_Left/7.png", "Image/P5_D2_P8_Low_T3_15_Left/8.png", "Image/P5_D2_P8_Low_T3_15_Left/9.png", "Image/P5_D2_P8_Low_T3_15_Left/10.png", "Image/P5_D2_P8_Low_T3_15_Left/11.png", "Image/P5_D2_P8_Low_T3_15_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the pan in front of me. It is in a shelf and at the bottom level of the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 351.2421 seconds to 369.0616 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P6_Medium_T1_24_Left", "images": ["Image/P30_D3_P6_Medium_T1_24_Left/1.png", "Image/P30_D3_P6_Medium_T1_24_Left/2.png", "Image/P30_D3_P6_Medium_T1_24_Left/3.png", "Image/P30_D3_P6_Medium_T1_24_Left/4.png", "Image/P30_D3_P6_Medium_T1_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the plant furthest away from me\"\nThe frames roughly correspond to the speech interval from 1126.10652 seconds to 1136.6282 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P4_Low_T1_8_Mid", "images": ["Image/P15_D3_P4_Low_T1_8_Mid/1.png", "Image/P15_D3_P4_Low_T1_8_Mid/2.png", "Image/P15_D3_P4_Low_T1_8_Mid/3.png", "Image/P15_D3_P4_Low_T1_8_Mid/4.png", "Image/P15_D3_P4_Low_T1_8_Mid/5.png", "Image/P15_D3_P4_Low_T1_8_Mid/6.png", "Image/P15_D3_P4_Low_T1_8_Mid/7.png", "Image/P15_D3_P4_Low_T1_8_Mid/8.png", "Image/P15_D3_P4_Low_T1_8_Mid/9.png", "Image/P15_D3_P4_Low_T1_8_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the cup on the table disk to the second floor of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 702.0436 seconds to 717.6612 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P1_Low_T2_7_Right", "images": ["Image/P11_D3_P1_Low_T2_7_Right/1.png", "Image/P11_D3_P1_Low_T2_7_Right/2.png", "Image/P11_D3_P1_Low_T2_7_Right/3.png", "Image/P11_D3_P1_Low_T2_7_Right/4.png", "Image/P11_D3_P1_Low_T2_7_Right/5.png", "Image/P11_D3_P1_Low_T2_7_Right/6.png", "Image/P11_D3_P1_Low_T2_7_Right/7.png", "Image/P11_D3_P1_Low_T2_7_Right/8.png", "Image/P11_D3_P1_Low_T2_7_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please pour the water in a green glass on the shelf\"\nThe frames roughly correspond to the speech interval from 1401.33197 seconds to 1417.98022 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P9_Low_T1_17_Mid", "images": ["Image/P14_D1_P9_Low_T1_17_Mid/1.png", "Image/P14_D1_P9_Low_T1_17_Mid/2.png", "Image/P14_D1_P9_Low_T1_17_Mid/3.png", "Image/P14_D1_P9_Low_T1_17_Mid/4.png", "Image/P14_D1_P9_Low_T1_17_Mid/5.png", "Image/P14_D1_P9_Low_T1_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move that cup up to the top of the shelf\"\nThe frames roughly correspond to the speech interval from 66.25971000000001 seconds to 79.5108 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P7_Low_T2_4_Right", "images": ["Image/P21_D3_P7_Low_T2_4_Right/1.png", "Image/P21_D3_P7_Low_T2_4_Right/2.png", "Image/P21_D3_P7_Low_T2_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 661.2546 seconds to 671.4038 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P2_High_T2_19_Mid", "images": ["Image/P1_D1_P2_High_T2_19_Mid/1.png", "Image/P1_D1_P2_High_T2_19_Mid/2.png", "Image/P1_D1_P2_High_T2_19_Mid/3.png", "Image/P1_D1_P2_High_T2_19_Mid/4.png", "Image/P1_D1_P2_High_T2_19_Mid/5.png", "Image/P1_D1_P2_High_T2_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this one? Finish.\"\nThe frames roughly correspond to the speech interval from 458.6877 seconds to 469.0313 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P2_Medium_T1_21_Mid", "images": ["Image/P9_D3_P2_Medium_T1_21_Mid/1.png", "Image/P9_D3_P2_Medium_T1_21_Mid/2.png", "Image/P9_D3_P2_Medium_T1_21_Mid/3.png", "Image/P9_D3_P2_Medium_T1_21_Mid/4.png", "Image/P9_D3_P2_Medium_T1_21_Mid/5.png", "Image/P9_D3_P2_Medium_T1_21_Mid/6.png", "Image/P9_D3_P2_Medium_T1_21_Mid/7.png", "Image/P9_D3_P2_Medium_T1_21_Mid/8.png", "Image/P9_D3_P2_Medium_T1_21_Mid/9.png", "Image/P9_D3_P2_Medium_T1_21_Mid/10.png", "Image/P9_D3_P2_Medium_T1_21_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the middle ball totes from the top layer to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 968.0003 seconds to 985.8439 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P2_High_T2_17_Mid", "images": ["Image/P16_D3_P2_High_T2_17_Mid/1.png", "Image/P16_D3_P2_High_T2_17_Mid/2.png", "Image/P16_D3_P2_High_T2_17_Mid/3.png", "Image/P16_D3_P2_High_T2_17_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water into that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 298.7138 seconds to 307.98490000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P1_Low_T3_8_Mid", "images": ["Image/P4_D1_P1_Low_T3_8_Mid/1.png", "Image/P4_D1_P1_Low_T3_8_Mid/2.png", "Image/P4_D1_P1_Low_T3_8_Mid/3.png", "Image/P4_D1_P1_Low_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2086.24246 seconds to 2100.23024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P3_Medium_T3_16_Right", "images": ["Image/P23_D2_P3_Medium_T3_16_Right/1.png", "Image/P23_D2_P3_Medium_T3_16_Right/2.png", "Image/P23_D2_P3_Medium_T3_16_Right/3.png", "Image/P23_D2_P3_Medium_T3_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the one on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 540.3931 seconds to 550.6107 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P7_Medium_T3_5_Mid", "images": ["Image/P16_D3_P7_Medium_T3_5_Mid/1.png", "Image/P16_D3_P7_Medium_T3_5_Mid/2.png", "Image/P16_D3_P7_Medium_T3_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 417.5344 seconds to 426.37339999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P3_Low_T2_16_Left", "images": ["Image/P17_D1_P3_Low_T2_16_Left/1.png", "Image/P17_D1_P3_Low_T2_16_Left/2.png", "Image/P17_D1_P3_Low_T2_16_Left/3.png", "Image/P17_D1_P3_Low_T2_16_Left/4.png", "Image/P17_D1_P3_Low_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this brown mug right here? Finish\"\nThe frames roughly correspond to the speech interval from 1664.6232000000002 seconds to 1675.9396000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P1_Medium_T1_9_Left", "images": ["Image/P15_D2_P1_Medium_T1_9_Left/1.png", "Image/P15_D2_P1_Medium_T1_9_Left/2.png", "Image/P15_D2_P1_Medium_T1_9_Left/3.png", "Image/P15_D2_P1_Medium_T1_9_Left/4.png", "Image/P15_D2_P1_Medium_T1_9_Left/5.png", "Image/P15_D2_P1_Medium_T1_9_Left/6.png", "Image/P15_D2_P1_Medium_T1_9_Left/7.png", "Image/P15_D2_P1_Medium_T1_9_Left/8.png", "Image/P15_D2_P1_Medium_T1_9_Left/9.png", "Image/P15_D2_P1_Medium_T1_9_Left/10.png", "Image/P15_D2_P1_Medium_T1_9_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the left-sided cup from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2650.7894 seconds to 2666.9789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P1_Medium_T3_20_Right", "images": ["Image/P18_D2_P1_Medium_T3_20_Right/1.png", "Image/P18_D2_P1_Medium_T3_20_Right/2.png", "Image/P18_D2_P1_Medium_T3_20_Right/3.png", "Image/P18_D2_P1_Medium_T3_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Clean up that first bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 278.19210000000004 seconds to 288.78560000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P5_Low_T3_24_Left", "images": ["Image/P14_D3_P5_Low_T3_24_Left/1.png", "Image/P14_D3_P5_Low_T3_24_Left/2.png", "Image/P14_D3_P5_Low_T3_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that plant up. Finish.\"\nThe frames roughly correspond to the speech interval from 1256.97316 seconds to 1265.29166 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P2_Low_T3_27_Left", "images": ["Image/P20_D3_P2_Low_T3_27_Left/1.png", "Image/P20_D3_P2_Low_T3_27_Left/2.png", "Image/P20_D3_P2_Low_T3_27_Left/3.png", "Image/P20_D3_P2_Low_T3_27_Left/4.png", "Image/P20_D3_P2_Low_T3_27_Left/5.png", "Image/P20_D3_P2_Low_T3_27_Left/6.png", "Image/P20_D3_P2_Low_T3_27_Left/7.png", "Image/P20_D3_P2_Low_T3_27_Left/8.png", "Image/P20_D3_P2_Low_T3_27_Left/9.png", "Image/P20_D3_P2_Low_T3_27_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1587.3055 seconds to 1608.6999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P4_Low_T1_1_Mid", "images": ["Image/P28_D1_P4_Low_T1_1_Mid/1.png", "Image/P28_D1_P4_Low_T1_1_Mid/2.png", "Image/P28_D1_P4_Low_T1_1_Mid/3.png", "Image/P28_D1_P4_Low_T1_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1490.7339 seconds to 1500.6443 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P6_Low_T2_8_Right", "images": ["Image/P17_D3_P6_Low_T2_8_Right/1.png", "Image/P17_D3_P6_Low_T2_8_Right/2.png", "Image/P17_D3_P6_Low_T2_8_Right/3.png", "Image/P17_D3_P6_Low_T2_8_Right/4.png", "Image/P17_D3_P6_Low_T2_8_Right/5.png", "Image/P17_D3_P6_Low_T2_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this green mug over here? Finish\"\nThe frames roughly correspond to the speech interval from 853.4843000000001 seconds to 865.1201000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P9_Low_T3_25_Right", "images": ["Image/P3_D1_P9_Low_T3_25_Right/1.png", "Image/P3_D1_P9_Low_T3_25_Right/2.png", "Image/P3_D1_P9_Low_T3_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1504.8055 seconds to 1518.3239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P8_Medium_T3_11_Right", "images": ["Image/P1_D3_P8_Medium_T3_11_Right/1.png", "Image/P1_D3_P8_Medium_T3_11_Right/2.png", "Image/P1_D3_P8_Medium_T3_11_Right/3.png", "Image/P1_D3_P8_Medium_T3_11_Right/4.png", "Image/P1_D3_P8_Medium_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the red bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1422.0313999999998 seconds to 1432.8077999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P1_Low_T2_1_Left", "images": ["Image/P29_D2_P1_Low_T2_1_Left/1.png", "Image/P29_D2_P1_Low_T2_1_Left/2.png", "Image/P29_D2_P1_Low_T2_1_Left/3.png", "Image/P29_D2_P1_Low_T2_1_Left/4.png", "Image/P29_D2_P1_Low_T2_1_Left/5.png", "Image/P29_D2_P1_Low_T2_1_Left/6.png", "Image/P29_D2_P1_Low_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 185.5173 seconds to 197.8214 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P2_High_T2_22_Right", "images": ["Image/P6_D1_P2_High_T2_22_Right/1.png", "Image/P6_D1_P2_High_T2_22_Right/2.png", "Image/P6_D1_P2_High_T2_22_Right/3.png", "Image/P6_D1_P2_High_T2_22_Right/4.png", "Image/P6_D1_P2_High_T2_22_Right/5.png", "Image/P6_D1_P2_High_T2_22_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2506.9977000000003 seconds to 2519.8818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P5_Low_T1_2_Right", "images": ["Image/P25_D1_P5_Low_T1_2_Right/1.png", "Image/P25_D1_P5_Low_T1_2_Right/2.png", "Image/P25_D1_P5_Low_T1_2_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 280.4133 seconds to 288.7503 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P3_Low_T1_9_Right", "images": ["Image/P23_D1_P3_Low_T1_9_Right/1.png", "Image/P23_D1_P3_Low_T1_9_Right/2.png", "Image/P23_D1_P3_Low_T1_9_Right/3.png", "Image/P23_D1_P3_Low_T1_9_Right/4.png", "Image/P23_D1_P3_Low_T1_9_Right/5.png", "Image/P23_D1_P3_Low_T1_9_Right/6.png", "Image/P23_D1_P3_Low_T1_9_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1488.22901 seconds to 1501.22925 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P7_Medium_T2_21_Right", "images": ["Image/P6_D2_P7_Medium_T2_21_Right/1.png", "Image/P6_D2_P7_Medium_T2_21_Right/2.png", "Image/P6_D2_P7_Medium_T2_21_Right/3.png", "Image/P6_D2_P7_Medium_T2_21_Right/4.png", "Image/P6_D2_P7_Medium_T2_21_Right/5.png", "Image/P6_D2_P7_Medium_T2_21_Right/6.png", "Image/P6_D2_P7_Medium_T2_21_Right/7.png", "Image/P6_D2_P7_Medium_T2_21_Right/8.png", "Image/P6_D2_P7_Medium_T2_21_Right/9.png", "Image/P6_D2_P7_Medium_T2_21_Right/10.png", "Image/P6_D2_P7_Medium_T2_21_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 843.347 seconds to 861.3063 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P5_Low_T3_5_Right", "images": ["Image/P3_D3_P5_Low_T3_5_Right/1.png", "Image/P3_D3_P5_Low_T3_5_Right/2.png", "Image/P3_D3_P5_Low_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 923.0888 seconds to 932.0819 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P6_High_T1_17_Left", "images": ["Image/P27_D3_P6_High_T1_17_Left/1.png", "Image/P27_D3_P6_High_T1_17_Left/2.png", "Image/P27_D3_P6_High_T1_17_Left/3.png", "Image/P27_D3_P6_High_T1_17_Left/4.png", "Image/P27_D3_P6_High_T1_17_Left/5.png", "Image/P27_D3_P6_High_T1_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take the red cup and put it on the counter on that table? Finished.\"\nThe frames roughly correspond to the speech interval from 453.739 seconds to 466.7957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P8_High_T1_9_Left", "images": ["Image/P3_D1_P8_High_T1_9_Left/1.png", "Image/P3_D1_P8_High_T1_9_Left/2.png", "Image/P3_D1_P8_High_T1_9_Left/3.png", "Image/P3_D1_P8_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1464.4036999999998 seconds to 1474.5801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P2_Medium_T1_15_Left", "images": ["Image/P2_D3_P2_Medium_T1_15_Left/1.png", "Image/P2_D3_P2_Medium_T1_15_Left/2.png", "Image/P2_D3_P2_Medium_T1_15_Left/3.png", "Image/P2_D3_P2_Medium_T1_15_Left/4.png", "Image/P2_D3_P2_Medium_T1_15_Left/5.png", "Image/P2_D3_P2_Medium_T1_15_Left/6.png", "Image/P2_D3_P2_Medium_T1_15_Left/7.png", "Image/P2_D3_P2_Medium_T1_15_Left/8.png", "Image/P2_D3_P2_Medium_T1_15_Left/9.png", "Image/P2_D3_P2_Medium_T1_15_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move the plant on the very left side all the way to the bottom of the shelf? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2175.5736 seconds to 2191.1904000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P5_Medium_T1_11_Right", "images": ["Image/P4_D3_P5_Medium_T1_11_Right/1.png", "Image/P4_D3_P5_Medium_T1_11_Right/2.png", "Image/P4_D3_P5_Medium_T1_11_Right/3.png", "Image/P4_D3_P5_Medium_T1_11_Right/4.png", "Image/P4_D3_P5_Medium_T1_11_Right/5.png", "Image/P4_D3_P5_Medium_T1_11_Right/6.png", "Image/P4_D3_P5_Medium_T1_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that bottle to the middle of the flag shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 887.3319999999999 seconds to 901.4458 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P1_High_T2_24_Mid", "images": ["Image/P9_D1_P1_High_T2_24_Mid/1.png", "Image/P9_D1_P1_High_T2_24_Mid/2.png", "Image/P9_D1_P1_High_T2_24_Mid/3.png", "Image/P9_D1_P1_High_T2_24_Mid/4.png", "Image/P9_D1_P1_High_T2_24_Mid/5.png", "Image/P9_D1_P1_High_T2_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1631.8193 seconds to 1648.3155 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P4_High_T1_20_Left", "images": ["Image/P15_D1_P4_High_T1_20_Left/1.png", "Image/P15_D1_P4_High_T1_20_Left/2.png", "Image/P15_D1_P4_High_T1_20_Left/3.png", "Image/P15_D1_P4_High_T1_20_Left/4.png", "Image/P15_D1_P4_High_T1_20_Left/5.png", "Image/P15_D1_P4_High_T1_20_Left/6.png", "Image/P15_D1_P4_High_T1_20_Left/7.png", "Image/P15_D1_P4_High_T1_20_Left/8.png", "Image/P15_D1_P4_High_T1_20_Left/9.png", "Image/P15_D1_P4_High_T1_20_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white container from the front to the back of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1545.98317 seconds to 1562.4501699999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P4_Low_T3_21_Mid", "images": ["Image/P17_D2_P4_Low_T3_21_Mid/1.png", "Image/P17_D2_P4_Low_T3_21_Mid/2.png", "Image/P17_D2_P4_Low_T3_21_Mid/3.png", "Image/P17_D2_P4_Low_T3_21_Mid/4.png", "Image/P17_D2_P4_Low_T3_21_Mid/5.png", "Image/P17_D2_P4_Low_T3_21_Mid/6.png", "Image/P17_D2_P4_Low_T3_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this white object on the shelf over here? Finish.\"\nThe frames roughly correspond to the speech interval from 680.4577 seconds to 696.618 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P4_Low_T2_26_Mid", "images": ["Image/P15_D2_P4_Low_T2_26_Mid/1.png", "Image/P15_D2_P4_Low_T2_26_Mid/2.png", "Image/P15_D2_P4_Low_T2_26_Mid/3.png", "Image/P15_D2_P4_Low_T2_26_Mid/4.png", "Image/P15_D2_P4_Low_T2_26_Mid/5.png", "Image/P15_D2_P4_Low_T2_26_Mid/6.png", "Image/P15_D2_P4_Low_T2_26_Mid/7.png", "Image/P15_D2_P4_Low_T2_26_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 3150.0135 seconds to 3162.9035 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P2_Medium_T2_12_Mid", "images": ["Image/P23_D2_P2_Medium_T2_12_Mid/1.png", "Image/P23_D2_P2_Medium_T2_12_Mid/2.png", "Image/P23_D2_P2_Medium_T2_12_Mid/3.png", "Image/P23_D2_P2_Medium_T2_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 409.7281 seconds to 420.5014 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P7_Low_T3_4_Mid", "images": ["Image/P26_D2_P7_Low_T3_4_Mid/1.png", "Image/P26_D2_P7_Low_T3_4_Mid/2.png", "Image/P26_D2_P7_Low_T3_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant down there? Finish.\"\nThe frames roughly correspond to the speech interval from 1293.2460999999998 seconds to 1302.7168 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P2_Low_T2_17_Mid", "images": ["Image/P29_D2_P2_Low_T2_17_Mid/1.png", "Image/P29_D2_P2_Low_T2_17_Mid/2.png", "Image/P29_D2_P2_Low_T2_17_Mid/3.png", "Image/P29_D2_P2_Low_T2_17_Mid/4.png", "Image/P29_D2_P2_Low_T2_17_Mid/5.png", "Image/P29_D2_P2_Low_T2_17_Mid/6.png", "Image/P29_D2_P2_Low_T2_17_Mid/7.png", "Image/P29_D2_P2_Low_T2_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 274.47929999999997 seconds to 288.0369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P4_Medium_T2_5_Mid", "images": ["Image/P6_D1_P4_Medium_T2_5_Mid/1.png", "Image/P6_D1_P4_Medium_T2_5_Mid/2.png", "Image/P6_D1_P4_Medium_T2_5_Mid/3.png", "Image/P6_D1_P4_Medium_T2_5_Mid/4.png", "Image/P6_D1_P4_Medium_T2_5_Mid/5.png", "Image/P6_D1_P4_Medium_T2_5_Mid/6.png", "Image/P6_D1_P4_Medium_T2_5_Mid/7.png", "Image/P6_D1_P4_Medium_T2_5_Mid/8.png", "Image/P6_D1_P4_Medium_T2_5_Mid/9.png", "Image/P6_D1_P4_Medium_T2_5_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2059.8017 seconds to 2075.7650000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P9_Low_T1_11_Mid", "images": ["Image/P20_D3_P9_Low_T1_11_Mid/1.png", "Image/P20_D3_P9_Low_T1_11_Mid/2.png", "Image/P20_D3_P9_Low_T1_11_Mid/3.png", "Image/P20_D3_P9_Low_T1_11_Mid/4.png", "Image/P20_D3_P9_Low_T1_11_Mid/5.png", "Image/P20_D3_P9_Low_T1_11_Mid/6.png", "Image/P20_D3_P9_Low_T1_11_Mid/7.png", "Image/P20_D3_P9_Low_T1_11_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the orange bottle on the floor to the top of that shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1373.1359 seconds to 1388.3614 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P8_Low_T3_10_Right", "images": ["Image/P29_D2_P8_Low_T3_10_Right/1.png", "Image/P29_D2_P8_Low_T3_10_Right/2.png", "Image/P29_D2_P8_Low_T3_10_Right/3.png", "Image/P29_D2_P8_Low_T3_10_Right/4.png", "Image/P29_D2_P8_Low_T3_10_Right/5.png", "Image/P29_D2_P8_Low_T3_10_Right/6.png", "Image/P29_D2_P8_Low_T3_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 531.2323 seconds to 545.4159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P5_Low_T1_12_Mid", "images": ["Image/P23_D1_P5_Low_T1_12_Mid/1.png", "Image/P23_D1_P5_Low_T1_12_Mid/2.png", "Image/P23_D1_P5_Low_T1_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1686.2662 seconds to 1695.2441 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P4_Low_T1_25_Left", "images": ["Image/P16_D3_P4_Low_T1_25_Left/1.png", "Image/P16_D3_P4_Low_T1_25_Left/2.png", "Image/P16_D3_P4_Low_T1_25_Left/3.png", "Image/P16_D3_P4_Low_T1_25_Left/4.png", "Image/P16_D3_P4_Low_T1_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that mug over there. Finished.\"\nThe frames roughly correspond to the speech interval from 68.17382999999998 seconds to 79.35116999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P1_Medium_T2_16_Left", "images": ["Image/P24_D3_P1_Medium_T2_16_Left/1.png", "Image/P24_D3_P1_Medium_T2_16_Left/2.png", "Image/P24_D3_P1_Medium_T2_16_Left/3.png", "Image/P24_D3_P1_Medium_T2_16_Left/4.png", "Image/P24_D3_P1_Medium_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1572.46695 seconds to 1584.50765 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P6_Low_T3_9_Mid", "images": ["Image/P6_D1_P6_Low_T3_9_Mid/1.png", "Image/P6_D1_P6_Low_T3_9_Mid/2.png", "Image/P6_D1_P6_Low_T3_9_Mid/3.png", "Image/P6_D1_P6_Low_T3_9_Mid/4.png", "Image/P6_D1_P6_Low_T3_9_Mid/5.png", "Image/P6_D1_P6_Low_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2192.7933000000003 seconds to 2205.8619000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P6_High_T1_26_Mid", "images": ["Image/P26_D2_P6_High_T1_26_Mid/1.png", "Image/P26_D2_P6_High_T1_26_Mid/2.png", "Image/P26_D2_P6_High_T1_26_Mid/3.png", "Image/P26_D2_P6_High_T1_26_Mid/4.png", "Image/P26_D2_P6_High_T1_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please move this white vase to this side\"\nThe frames roughly correspond to the speech interval from 1383.9673 seconds to 1395.6759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P3_Medium_T3_19_Mid", "images": ["Image/P3_D1_P3_Medium_T3_19_Mid/1.png", "Image/P3_D1_P3_Medium_T3_19_Mid/2.png", "Image/P3_D1_P3_Medium_T3_19_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 1226.74158 seconds to 1241.90171 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P9_High_T3_11_Left", "images": ["Image/P6_D3_P9_High_T3_11_Left/1.png", "Image/P6_D3_P9_High_T3_11_Left/2.png", "Image/P6_D3_P9_High_T3_11_Left/3.png", "Image/P6_D3_P9_High_T3_11_Left/4.png", "Image/P6_D3_P9_High_T3_11_Left/5.png", "Image/P6_D3_P9_High_T3_11_Left/6.png", "Image/P6_D3_P9_High_T3_11_Left/7.png", "Image/P6_D3_P9_High_T3_11_Left/8.png", "Image/P6_D3_P9_High_T3_11_Left/9.png", "Image/P6_D3_P9_High_T3_11_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4417.564969999999 seconds to 4433.83858 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P2_High_T3_13_Right", "images": ["Image/P13_D1_P2_High_T3_13_Right/1.png", "Image/P13_D1_P2_High_T3_13_Right/2.png", "Image/P13_D1_P2_High_T3_13_Right/3.png", "Image/P13_D1_P2_High_T3_13_Right/4.png", "Image/P13_D1_P2_High_T3_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 221.77973000000003 seconds to 231.3688 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P1_Low_T3_14_Right", "images": ["Image/P26_D1_P1_Low_T3_14_Right/1.png", "Image/P26_D1_P1_Low_T3_14_Right/2.png", "Image/P26_D1_P1_Low_T3_14_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this pot? Finish.\"\nThe frames roughly correspond to the speech interval from 257.2781 seconds to 269.2223 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P9_Low_T2_23_Mid", "images": ["Image/P2_D2_P9_Low_T2_23_Mid/1.png", "Image/P2_D2_P9_Low_T2_23_Mid/2.png", "Image/P2_D2_P9_Low_T2_23_Mid/3.png", "Image/P2_D2_P9_Low_T2_23_Mid/4.png", "Image/P2_D2_P9_Low_T2_23_Mid/5.png", "Image/P2_D2_P9_Low_T2_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2707.1792 seconds to 2721.6138 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P3_Low_T1_11_Left", "images": ["Image/P24_D2_P3_Low_T1_11_Left/1.png", "Image/P24_D2_P3_Low_T1_11_Left/2.png", "Image/P24_D2_P3_Low_T1_11_Left/3.png", "Image/P24_D2_P3_Low_T1_11_Left/4.png", "Image/P24_D2_P3_Low_T1_11_Left/5.png", "Image/P24_D2_P3_Low_T1_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this bottle down to this area? Please finish.\"\nThe frames roughly correspond to the speech interval from 706.2872 seconds to 717.4369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P9_High_T3_12_Mid", "images": ["Image/P18_D2_P9_High_T3_12_Mid/1.png", "Image/P18_D2_P9_High_T3_12_Mid/2.png", "Image/P18_D2_P9_High_T3_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 544.1889000000001 seconds to 552.596 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P1_Medium_T3_27_Right", "images": ["Image/P2_D1_P1_Medium_T3_27_Right/1.png", "Image/P2_D1_P1_Medium_T3_27_Right/2.png", "Image/P2_D1_P1_Medium_T3_27_Right/3.png", "Image/P2_D1_P1_Medium_T3_27_Right/4.png", "Image/P2_D1_P1_Medium_T3_27_Right/5.png", "Image/P2_D1_P1_Medium_T3_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 624.6936 seconds to 640.5320999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P4_Low_T2_14_Mid", "images": ["Image/P11_D1_P4_Low_T2_14_Mid/1.png", "Image/P11_D1_P4_Low_T2_14_Mid/2.png", "Image/P11_D1_P4_Low_T2_14_Mid/3.png", "Image/P11_D1_P4_Low_T2_14_Mid/4.png", "Image/P11_D1_P4_Low_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water the plant on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2685.3226999999997 seconds to 2695.8610999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P4_Low_T1_12_Right", "images": ["Image/P9_D2_P4_Low_T1_12_Right/1.png", "Image/P9_D2_P4_Low_T1_12_Right/2.png", "Image/P9_D2_P4_Low_T1_12_Right/3.png", "Image/P9_D2_P4_Low_T1_12_Right/4.png", "Image/P9_D2_P4_Low_T1_12_Right/5.png", "Image/P9_D2_P4_Low_T1_12_Right/6.png", "Image/P9_D2_P4_Low_T1_12_Right/7.png", "Image/P9_D2_P4_Low_T1_12_Right/8.png", "Image/P9_D2_P4_Low_T1_12_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this red bottle on this desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3224.4245 seconds to 3241.81 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P5_Medium_T3_11_Mid", "images": ["Image/P29_D3_P5_Medium_T3_11_Mid/1.png", "Image/P29_D3_P5_Medium_T3_11_Mid/2.png", "Image/P29_D3_P5_Medium_T3_11_Mid/3.png", "Image/P29_D3_P5_Medium_T3_11_Mid/4.png", "Image/P29_D3_P5_Medium_T3_11_Mid/5.png", "Image/P29_D3_P5_Medium_T3_11_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1457.4331999999995 seconds to 1468.8195999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P6_Medium_T3_9_Right", "images": ["Image/P1_D1_P6_Medium_T3_9_Right/1.png", "Image/P1_D1_P6_Medium_T3_9_Right/2.png", "Image/P1_D1_P6_Medium_T3_9_Right/3.png", "Image/P1_D1_P6_Medium_T3_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 539.9222 seconds to 548.9572000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P3_High_T3_6_Right", "images": ["Image/P17_D1_P3_High_T3_6_Right/1.png", "Image/P17_D1_P3_High_T3_6_Right/2.png", "Image/P17_D1_P3_High_T3_6_Right/3.png", "Image/P17_D1_P3_High_T3_6_Right/4.png", "Image/P17_D1_P3_High_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this green plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1682.3294 seconds to 1699.7571000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P9_Low_T3_6_Mid", "images": ["Image/P24_D1_P9_Low_T3_6_Mid/1.png", "Image/P24_D1_P9_Low_T3_6_Mid/2.png", "Image/P24_D1_P9_Low_T3_6_Mid/3.png", "Image/P24_D1_P9_Low_T3_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1401.8588 seconds to 1411.5068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P9_Medium_T2_11_Right", "images": ["Image/P21_D3_P9_Medium_T2_11_Right/1.png", "Image/P21_D3_P9_Medium_T2_11_Right/2.png", "Image/P21_D3_P9_Medium_T2_11_Right/3.png", "Image/P21_D3_P9_Medium_T2_11_Right/4.png", "Image/P21_D3_P9_Medium_T2_11_Right/5.png", "Image/P21_D3_P9_Medium_T2_11_Right/6.png", "Image/P21_D3_P9_Medium_T2_11_Right/7.png", "Image/P21_D3_P9_Medium_T2_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 156.33711 seconds to 169.63018 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P4_High_T3_13_Left", "images": ["Image/P10_D1_P4_High_T3_13_Left/1.png", "Image/P10_D1_P4_High_T3_13_Left/2.png", "Image/P10_D1_P4_High_T3_13_Left/3.png", "Image/P10_D1_P4_High_T3_13_Left/4.png", "Image/P10_D1_P4_High_T3_13_Left/5.png", "Image/P10_D1_P4_High_T3_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2448.6034 seconds to 2459.725 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P2_Medium_T3_26_Right", "images": ["Image/P18_D1_P2_Medium_T3_26_Right/1.png", "Image/P18_D1_P2_Medium_T3_26_Right/2.png", "Image/P18_D1_P2_Medium_T3_26_Right/3.png", "Image/P18_D1_P2_Medium_T3_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up this cup in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 935.9979 seconds to 947.4112 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P2_High_T1_4_Right", "images": ["Image/P24_D3_P2_High_T1_4_Right/1.png", "Image/P24_D3_P2_High_T1_4_Right/2.png", "Image/P24_D3_P2_High_T1_4_Right/3.png", "Image/P24_D3_P2_High_T1_4_Right/4.png", "Image/P24_D3_P2_High_T1_4_Right/5.png", "Image/P24_D3_P2_High_T1_4_Right/6.png", "Image/P24_D3_P2_High_T1_4_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2005.5436 seconds to 2017.6667 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P8_High_T1_10_Right", "images": ["Image/P28_D1_P8_High_T1_10_Right/1.png", "Image/P28_D1_P8_High_T1_10_Right/2.png", "Image/P28_D1_P8_High_T1_10_Right/3.png", "Image/P28_D1_P8_High_T1_10_Right/4.png", "Image/P28_D1_P8_High_T1_10_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this red vase to the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 1547.2486 seconds to 1560.124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P9_Medium_T1_7_Right", "images": ["Image/P21_D2_P9_Medium_T1_7_Right/1.png", "Image/P21_D2_P9_Medium_T1_7_Right/2.png", "Image/P21_D2_P9_Medium_T1_7_Right/3.png", "Image/P21_D2_P9_Medium_T1_7_Right/4.png", "Image/P21_D2_P9_Medium_T1_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1425.15755 seconds to 1436.6967100000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P7_Medium_T2_10_Left", "images": ["Image/P17_D1_P7_Medium_T2_10_Left/1.png", "Image/P17_D1_P7_Medium_T2_10_Left/2.png", "Image/P17_D1_P7_Medium_T2_10_Left/3.png", "Image/P17_D1_P7_Medium_T2_10_Left/4.png", "Image/P17_D1_P7_Medium_T2_10_Left/5.png", "Image/P17_D1_P7_Medium_T2_10_Left/6.png", "Image/P17_D1_P7_Medium_T2_10_Left/7.png", "Image/P17_D1_P7_Medium_T2_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in this leftmost vase on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1971.9752 seconds to 1985.9223 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P8_Medium_T3_20_Mid", "images": ["Image/P10_D3_P8_Medium_T3_20_Mid/1.png", "Image/P10_D3_P8_Medium_T3_20_Mid/2.png", "Image/P10_D3_P8_Medium_T3_20_Mid/3.png", "Image/P10_D3_P8_Medium_T3_20_Mid/4.png", "Image/P10_D3_P8_Medium_T3_20_Mid/5.png", "Image/P10_D3_P8_Medium_T3_20_Mid/6.png", "Image/P10_D3_P8_Medium_T3_20_Mid/7.png", "Image/P10_D3_P8_Medium_T3_20_Mid/8.png", "Image/P10_D3_P8_Medium_T3_20_Mid/9.png", "Image/P10_D3_P8_Medium_T3_20_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 324.76365999999996 seconds to 340.45081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P9_High_T1_21_Left", "images": ["Image/P22_D2_P9_High_T1_21_Left/1.png", "Image/P22_D2_P9_High_T1_21_Left/2.png", "Image/P22_D2_P9_High_T1_21_Left/3.png", "Image/P22_D2_P9_High_T1_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put the vase on top. Finish.\"\nThe frames roughly correspond to the speech interval from 879.2478999999998 seconds to 892.2089999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P7_High_T2_21_Right", "images": ["Image/P12_D2_P7_High_T2_21_Right/1.png", "Image/P12_D2_P7_High_T2_21_Right/2.png", "Image/P12_D2_P7_High_T2_21_Right/3.png", "Image/P12_D2_P7_High_T2_21_Right/4.png", "Image/P12_D2_P7_High_T2_21_Right/5.png", "Image/P12_D2_P7_High_T2_21_Right/6.png", "Image/P12_D2_P7_High_T2_21_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the white vase that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 561.1875 seconds to 573.3115399999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P7_Low_T1_4_Left", "images": ["Image/P23_D3_P7_Low_T1_4_Left/1.png", "Image/P23_D3_P7_Low_T1_4_Left/2.png", "Image/P23_D3_P7_Low_T1_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 734.4126 seconds to 743.3069 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P6_High_T1_18_Mid", "images": ["Image/P25_D2_P6_High_T1_18_Mid/1.png", "Image/P25_D2_P6_High_T1_18_Mid/2.png", "Image/P25_D2_P6_High_T1_18_Mid/3.png", "Image/P25_D2_P6_High_T1_18_Mid/4.png", "Image/P25_D2_P6_High_T1_18_Mid/5.png", "Image/P25_D2_P6_High_T1_18_Mid/6.png", "Image/P25_D2_P6_High_T1_18_Mid/7.png", "Image/P25_D2_P6_High_T1_18_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup over here. Finish.\"\nThe frames roughly correspond to the speech interval from 904.2641 seconds to 913.9671 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P8_Low_T3_10_Left", "images": ["Image/P16_D1_P8_Low_T3_10_Left/1.png", "Image/P16_D1_P8_Low_T3_10_Left/2.png", "Image/P16_D1_P8_Low_T3_10_Left/3.png", "Image/P16_D1_P8_Low_T3_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1376.9836 seconds to 1386.8116 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P6_Medium_T1_2_Right", "images": ["Image/P27_D2_P6_Medium_T1_2_Right/1.png", "Image/P27_D2_P6_Medium_T1_2_Right/2.png", "Image/P27_D2_P6_Medium_T1_2_Right/3.png", "Image/P27_D2_P6_Medium_T1_2_Right/4.png", "Image/P27_D2_P6_Medium_T1_2_Right/5.png", "Image/P27_D2_P6_Medium_T1_2_Right/6.png", "Image/P27_D2_P6_Medium_T1_2_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that vase to the center of that shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1388.0924 seconds to 1400.6055 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P4_Low_T1_3_Right", "images": ["Image/P30_D1_P4_Low_T1_3_Right/1.png", "Image/P30_D1_P4_Low_T1_3_Right/2.png", "Image/P30_D1_P4_Low_T1_3_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 811.8419000000001 seconds to 820.2918000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P1_Low_T3_22_Mid", "images": ["Image/P5_D3_P1_Low_T3_22_Mid/1.png", "Image/P5_D3_P1_Low_T3_22_Mid/2.png", "Image/P5_D3_P1_Low_T3_22_Mid/3.png", "Image/P5_D3_P1_Low_T3_22_Mid/4.png", "Image/P5_D3_P1_Low_T3_22_Mid/5.png", "Image/P5_D3_P1_Low_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant on the table over there. Finished.\"\nThe frames roughly correspond to the speech interval from 2565.9001 seconds to 2577.5773999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P7_Low_T1_24_Right", "images": ["Image/P15_D3_P7_Low_T1_24_Right/1.png", "Image/P15_D3_P7_Low_T1_24_Right/2.png", "Image/P15_D3_P7_Low_T1_24_Right/3.png", "Image/P15_D3_P7_Low_T1_24_Right/4.png", "Image/P15_D3_P7_Low_T1_24_Right/5.png", "Image/P15_D3_P7_Low_T1_24_Right/6.png", "Image/P15_D3_P7_Low_T1_24_Right/7.png", "Image/P15_D3_P7_Low_T1_24_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the under table to the top of the table. Finish.\"\nThe frames roughly correspond to the speech interval from 776.4449 seconds to 789.4325 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P6_Medium_T2_7_Right", "images": ["Image/P6_D3_P6_Medium_T2_7_Right/1.png", "Image/P6_D3_P6_Medium_T2_7_Right/2.png", "Image/P6_D3_P6_Medium_T2_7_Right/3.png", "Image/P6_D3_P6_Medium_T2_7_Right/4.png", "Image/P6_D3_P6_Medium_T2_7_Right/5.png", "Image/P6_D3_P6_Medium_T2_7_Right/6.png", "Image/P6_D3_P6_Medium_T2_7_Right/7.png", "Image/P6_D3_P6_Medium_T2_7_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3602.4737 seconds to 3618.6898 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P3_Medium_T3_1_Mid", "images": ["Image/P25_D3_P3_Medium_T3_1_Mid/1.png", "Image/P25_D3_P3_Medium_T3_1_Mid/2.png", "Image/P25_D3_P3_Medium_T3_1_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the second vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1770.38025 seconds to 1778.36789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P2_Medium_T2_2_Mid", "images": ["Image/P4_D2_P2_Medium_T2_2_Mid/1.png", "Image/P4_D2_P2_Medium_T2_2_Mid/2.png", "Image/P4_D2_P2_Medium_T2_2_Mid/3.png", "Image/P4_D2_P2_Medium_T2_2_Mid/4.png", "Image/P4_D2_P2_Medium_T2_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the green bottle in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 1867.9784999999997 seconds to 1880.5315 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P4_High_T1_26_Right", "images": ["Image/P11_D2_P4_High_T1_26_Right/1.png", "Image/P11_D2_P4_High_T1_26_Right/2.png", "Image/P11_D2_P4_High_T1_26_Right/3.png", "Image/P11_D2_P4_High_T1_26_Right/4.png", "Image/P11_D2_P4_High_T1_26_Right/5.png", "Image/P11_D2_P4_High_T1_26_Right/6.png", "Image/P11_D2_P4_High_T1_26_Right/7.png", "Image/P11_D2_P4_High_T1_26_Right/8.png", "Image/P11_D2_P4_High_T1_26_Right/9.png", "Image/P11_D2_P4_High_T1_26_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white glass to the top of the shelf in the front. Finish.\"\nThe frames roughly correspond to the speech interval from 673.6782000000001 seconds to 689.1472 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P4_High_T1_23_Left", "images": ["Image/P14_D1_P4_High_T1_23_Left/1.png", "Image/P14_D1_P4_High_T1_23_Left/2.png", "Image/P14_D1_P4_High_T1_23_Left/3.png", "Image/P14_D1_P4_High_T1_23_Left/4.png", "Image/P14_D1_P4_High_T1_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move that plant to the center of the table\"\nThe frames roughly correspond to the speech interval from 368.3066 seconds to 382.5887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P5_Low_T1_24_Right", "images": ["Image/P26_D1_P5_Low_T1_24_Right/1.png", "Image/P26_D1_P5_Low_T1_24_Right/2.png", "Image/P26_D1_P5_Low_T1_24_Right/3.png", "Image/P26_D1_P5_Low_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant over here? Finish\"\nThe frames roughly correspond to the speech interval from 592.8035 seconds to 606.973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P7_Low_T1_4_Left", "images": ["Image/P19_D1_P7_Low_T1_4_Left/1.png", "Image/P19_D1_P7_Low_T1_4_Left/2.png", "Image/P19_D1_P7_Low_T1_4_Left/3.png", "Image/P19_D1_P7_Low_T1_4_Left/4.png", "Image/P19_D1_P7_Low_T1_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1016.7886999999998 seconds to 1033.18589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P5_High_T3_11_Mid", "images": ["Image/P18_D1_P5_High_T3_11_Mid/1.png", "Image/P18_D1_P5_High_T3_11_Mid/2.png", "Image/P18_D1_P5_High_T3_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1112.0002 seconds to 1121.1595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P5_High_T2_14_Left", "images": ["Image/P30_D1_P5_High_T2_14_Left/1.png", "Image/P30_D1_P5_High_T2_14_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Water this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 683.7094800000001 seconds to 691.1143400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P7_Low_T1_7_Right", "images": ["Image/P29_D3_P7_Low_T1_7_Right/1.png", "Image/P29_D3_P7_Low_T1_7_Right/2.png", "Image/P29_D3_P7_Low_T1_7_Right/3.png", "Image/P29_D3_P7_Low_T1_7_Right/4.png", "Image/P29_D3_P7_Low_T1_7_Right/5.png", "Image/P29_D3_P7_Low_T1_7_Right/6.png", "Image/P29_D3_P7_Low_T1_7_Right/7.png", "Image/P29_D3_P7_Low_T1_7_Right/8.png", "Image/P29_D3_P7_Low_T1_7_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 1315.21258 seconds to 1331.08156 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P8_Low_T1_25_Mid", "images": ["Image/P17_D2_P8_Low_T1_25_Mid/1.png", "Image/P17_D2_P8_Low_T1_25_Mid/2.png", "Image/P17_D2_P8_Low_T1_25_Mid/3.png", "Image/P17_D2_P8_Low_T1_25_Mid/4.png", "Image/P17_D2_P8_Low_T1_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this candle up to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 158.3635 seconds to 170.7094 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P7_High_T2_10_Right", "images": ["Image/P1_D2_P7_High_T2_10_Right/1.png", "Image/P1_D2_P7_High_T2_10_Right/2.png", "Image/P1_D2_P7_High_T2_10_Right/3.png", "Image/P1_D2_P7_High_T2_10_Right/4.png", "Image/P1_D2_P7_High_T2_10_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1141.6443 seconds to 1152.044 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P3_High_T2_27_Left", "images": ["Image/P9_D1_P3_High_T2_27_Left/1.png", "Image/P9_D1_P3_High_T2_27_Left/2.png", "Image/P9_D1_P3_High_T2_27_Left/3.png", "Image/P9_D1_P3_High_T2_27_Left/4.png", "Image/P9_D1_P3_High_T2_27_Left/5.png", "Image/P9_D1_P3_High_T2_27_Left/6.png", "Image/P9_D1_P3_High_T2_27_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1240.77567 seconds to 1253.60596 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P9_Low_T2_6_Mid", "images": ["Image/P7_D1_P9_Low_T2_6_Mid/1.png", "Image/P7_D1_P9_Low_T2_6_Mid/2.png", "Image/P7_D1_P9_Low_T2_6_Mid/3.png", "Image/P7_D1_P9_Low_T2_6_Mid/4.png", "Image/P7_D1_P9_Low_T2_6_Mid/5.png", "Image/P7_D1_P9_Low_T2_6_Mid/6.png", "Image/P7_D1_P9_Low_T2_6_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Done. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 259.337 seconds to 274.37 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P4_Medium_T1_21_Left", "images": ["Image/P17_D2_P4_Medium_T1_21_Left/1.png", "Image/P17_D2_P4_Medium_T1_21_Left/2.png", "Image/P17_D2_P4_Medium_T1_21_Left/3.png", "Image/P17_D2_P4_Medium_T1_21_Left/4.png", "Image/P17_D2_P4_Medium_T1_21_Left/5.png", "Image/P17_D2_P4_Medium_T1_21_Left/6.png", "Image/P17_D2_P4_Medium_T1_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this white thing on the left to this table right here? Finish.\"\nThe frames roughly correspond to the speech interval from 131.414 seconds to 145.0273 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P5_Low_T1_25_Right", "images": ["Image/P6_D1_P5_Low_T1_25_Right/1.png", "Image/P6_D1_P5_Low_T1_25_Right/2.png", "Image/P6_D1_P5_Low_T1_25_Right/3.png", "Image/P6_D1_P5_Low_T1_25_Right/4.png", "Image/P6_D1_P5_Low_T1_25_Right/5.png", "Image/P6_D1_P5_Low_T1_25_Right/6.png", "Image/P6_D1_P5_Low_T1_25_Right/7.png", "Image/P6_D1_P5_Low_T1_25_Right/8.png", "Image/P6_D1_P5_Low_T1_25_Right/9.png", "Image/P6_D1_P5_Low_T1_25_Right/10.png", "Image/P6_D1_P5_Low_T1_25_Right/11.png", "Image/P6_D1_P5_Low_T1_25_Right/12.png", "Image/P6_D1_P5_Low_T1_25_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2282.0621 seconds to 2300.5039 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P1_Low_T3_24_Left", "images": ["Image/P29_D1_P1_Low_T3_24_Left/1.png", "Image/P29_D1_P1_Low_T3_24_Left/2.png", "Image/P29_D1_P1_Low_T3_24_Left/3.png", "Image/P29_D1_P1_Low_T3_24_Left/4.png", "Image/P29_D1_P1_Low_T3_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2772.7263999999996 seconds to 2783.6938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P3_Low_T1_9_Right", "images": ["Image/P28_D3_P3_Low_T1_9_Right/1.png", "Image/P28_D3_P3_Low_T1_9_Right/2.png", "Image/P28_D3_P3_Low_T1_9_Right/3.png", "Image/P28_D3_P3_Low_T1_9_Right/4.png", "Image/P28_D3_P3_Low_T1_9_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase to the bottom shelf. Finish\"\nThe frames roughly correspond to the speech interval from 546.2156 seconds to 557.5158 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P7_Low_T3_8_Mid", "images": ["Image/P3_D1_P7_Low_T3_8_Mid/1.png", "Image/P3_D1_P7_Low_T3_8_Mid/2.png", "Image/P3_D1_P7_Low_T3_8_Mid/3.png", "Image/P3_D1_P7_Low_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1560.3124 seconds to 1570.3639 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P1_High_T3_19_Right", "images": ["Image/P9_D2_P1_High_T3_19_Right/1.png", "Image/P9_D2_P1_High_T3_19_Right/2.png", "Image/P9_D2_P1_High_T3_19_Right/3.png", "Image/P9_D2_P1_High_T3_19_Right/4.png", "Image/P9_D2_P1_High_T3_19_Right/5.png", "Image/P9_D2_P1_High_T3_19_Right/6.png", "Image/P9_D2_P1_High_T3_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the white bottle on the top of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3129.9552 seconds to 3145.5912 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P7_Medium_T2_22_Mid", "images": ["Image/P23_D3_P7_Medium_T2_22_Mid/1.png", "Image/P23_D3_P7_Medium_T2_22_Mid/2.png", "Image/P23_D3_P7_Medium_T2_22_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the mellow plant. Finish.\"\nThe frames roughly correspond to the speech interval from 963.0333 seconds to 971.7468 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P5_Low_T3_14_Right", "images": ["Image/P23_D3_P5_Low_T3_14_Right/1.png", "Image/P23_D3_P5_Low_T3_14_Right/2.png", "Image/P23_D3_P5_Low_T3_14_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 915.3578 seconds to 924.2942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P6_Medium_T3_5_Left", "images": ["Image/P13_D2_P6_Medium_T3_5_Left/1.png", "Image/P13_D2_P6_Medium_T3_5_Left/2.png", "Image/P13_D2_P6_Medium_T3_5_Left/3.png", "Image/P13_D2_P6_Medium_T3_5_Left/4.png", "Image/P13_D2_P6_Medium_T3_5_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the left plant over here\"\nThe frames roughly correspond to the speech interval from 900.50016 seconds to 911.06506 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P8_High_T1_13_Left", "images": ["Image/P18_D1_P8_High_T1_13_Left/1.png", "Image/P18_D1_P8_High_T1_13_Left/2.png", "Image/P18_D1_P8_High_T1_13_Left/3.png", "Image/P18_D1_P8_High_T1_13_Left/4.png", "Image/P18_D1_P8_High_T1_13_Left/5.png", "Image/P18_D1_P8_High_T1_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 883.7422 seconds to 895.8389999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P2_Low_T3_15_Mid", "images": ["Image/P27_D1_P2_Low_T3_15_Mid/1.png", "Image/P27_D1_P2_Low_T3_15_Mid/2.png", "Image/P27_D1_P2_Low_T3_15_Mid/3.png", "Image/P27_D1_P2_Low_T3_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1143.4311 seconds to 1153.0226 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P5_Medium_T3_4_Right", "images": ["Image/P15_D1_P5_Medium_T3_4_Right/1.png", "Image/P15_D1_P5_Medium_T3_4_Right/2.png", "Image/P15_D1_P5_Medium_T3_4_Right/3.png", "Image/P15_D1_P5_Medium_T3_4_Right/4.png", "Image/P15_D1_P5_Medium_T3_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the right-sided plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2132.8604699999996 seconds to 2144.5389699999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P8_High_T3_5_Left", "images": ["Image/P25_D3_P8_High_T3_5_Left/1.png", "Image/P25_D3_P8_High_T3_5_Left/2.png", "Image/P25_D3_P8_High_T3_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2056.6152 seconds to 2064.6189999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P4_Medium_T2_15_Right", "images": ["Image/P30_D1_P4_Medium_T2_15_Right/1.png", "Image/P30_D1_P4_Medium_T2_15_Right/2.png", "Image/P30_D1_P4_Medium_T2_15_Right/3.png", "Image/P30_D1_P4_Medium_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 646.93367 seconds to 656.66245 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P2_Low_T1_22_Right", "images": ["Image/P14_D2_P2_Low_T1_22_Right/1.png", "Image/P14_D2_P2_Low_T1_22_Right/2.png", "Image/P14_D2_P2_Low_T1_22_Right/3.png", "Image/P14_D2_P2_Low_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant down to the bottom. Finish.\"\nThe frames roughly correspond to the speech interval from 1507.50715 seconds to 1517.13089 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P3_Medium_T2_11_Right", "images": ["Image/P28_D1_P3_Medium_T2_11_Right/1.png", "Image/P28_D1_P3_Medium_T2_11_Right/2.png", "Image/P28_D1_P3_Medium_T2_11_Right/3.png", "Image/P28_D1_P3_Medium_T2_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1641.9312 seconds to 1652.7609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P9_Low_T2_1_Left", "images": ["Image/P7_D3_P9_Low_T2_1_Left/1.png", "Image/P7_D3_P9_Low_T2_1_Left/2.png", "Image/P7_D3_P9_Low_T2_1_Left/3.png", "Image/P7_D3_P9_Low_T2_1_Left/4.png", "Image/P7_D3_P9_Low_T2_1_Left/5.png", "Image/P7_D3_P9_Low_T2_1_Left/6.png", "Image/P7_D3_P9_Low_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1982.2929 seconds to 1996.464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P5_High_T3_2_Left", "images": ["Image/P4_D1_P5_High_T3_2_Left/1.png", "Image/P4_D1_P5_High_T3_2_Left/2.png", "Image/P4_D1_P5_High_T3_2_Left/3.png", "Image/P4_D1_P5_High_T3_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2265.0951 seconds to 2276.2419999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P3_High_T1_6_Left", "images": ["Image/P30_D3_P3_High_T1_6_Left/1.png", "Image/P30_D3_P3_High_T1_6_Left/2.png", "Image/P30_D3_P3_High_T1_6_Left/3.png", "Image/P30_D3_P3_High_T1_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1545.4709 seconds to 1554.6987 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P4_High_T1_7_Mid", "images": ["Image/P12_D1_P4_High_T1_7_Mid/1.png", "Image/P12_D1_P4_High_T1_7_Mid/2.png", "Image/P12_D1_P4_High_T1_7_Mid/3.png", "Image/P12_D1_P4_High_T1_7_Mid/4.png", "Image/P12_D1_P4_High_T1_7_Mid/5.png", "Image/P12_D1_P4_High_T1_7_Mid/6.png", "Image/P12_D1_P4_High_T1_7_Mid/7.png", "Image/P12_D1_P4_High_T1_7_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1288.52947 seconds to 1302.9641 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P1_High_T3_27_Mid", "images": ["Image/P15_D3_P1_High_T3_27_Mid/1.png", "Image/P15_D3_P1_High_T3_27_Mid/2.png", "Image/P15_D3_P1_High_T3_27_Mid/3.png", "Image/P15_D3_P1_High_T3_27_Mid/4.png", "Image/P15_D3_P1_High_T3_27_Mid/5.png", "Image/P15_D3_P1_High_T3_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 520.4572999999999 seconds to 532.7902 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P9_High_T2_1_Mid", "images": ["Image/P7_D1_P9_High_T2_1_Mid/1.png", "Image/P7_D1_P9_High_T2_1_Mid/2.png", "Image/P7_D1_P9_High_T2_1_Mid/3.png", "Image/P7_D1_P9_High_T2_1_Mid/4.png", "Image/P7_D1_P9_High_T2_1_Mid/5.png", "Image/P7_D1_P9_High_T2_1_Mid/6.png", "Image/P7_D1_P9_High_T2_1_Mid/7.png", "Image/P7_D1_P9_High_T2_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 691.5225 seconds to 707.697 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P1_Low_T3_10_Mid", "images": ["Image/P23_D1_P1_Low_T3_10_Mid/1.png", "Image/P23_D1_P1_Low_T3_10_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1862.0057 seconds to 1871.4984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P7_Low_T2_27_Mid", "images": ["Image/P1_D2_P7_Low_T2_27_Mid/1.png", "Image/P1_D2_P7_Low_T2_27_Mid/2.png", "Image/P1_D2_P7_Low_T2_27_Mid/3.png", "Image/P1_D2_P7_Low_T2_27_Mid/4.png", "Image/P1_D2_P7_Low_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water to this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 951.3728 seconds to 961.4657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P3_High_T2_7_Right", "images": ["Image/P4_D2_P3_High_T2_7_Right/1.png", "Image/P4_D2_P3_High_T2_7_Right/2.png", "Image/P4_D2_P3_High_T2_7_Right/3.png", "Image/P4_D2_P3_High_T2_7_Right/4.png", "Image/P4_D2_P3_High_T2_7_Right/5.png", "Image/P4_D2_P3_High_T2_7_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the green bottle in the right hand? Finish.\"\nThe frames roughly correspond to the speech interval from 1889.6103 seconds to 1904.2792999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P7_Low_T3_10_Left", "images": ["Image/P8_D2_P7_Low_T3_10_Left/1.png", "Image/P8_D2_P7_Low_T3_10_Left/2.png", "Image/P8_D2_P7_Low_T3_10_Left/3.png", "Image/P8_D2_P7_Low_T3_10_Left/4.png", "Image/P8_D2_P7_Low_T3_10_Left/5.png", "Image/P8_D2_P7_Low_T3_10_Left/6.png", "Image/P8_D2_P7_Low_T3_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Could you clean that brown\"\nThe frames roughly correspond to the speech interval from 2257.4119 seconds to 2270.5677 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P1_Low_T3_23_Left", "images": ["Image/P1_D3_P1_Low_T3_23_Left/1.png", "Image/P1_D3_P1_Low_T3_23_Left/2.png", "Image/P1_D3_P1_Low_T3_23_Left/3.png", "Image/P1_D3_P1_Low_T3_23_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 1517.5912999999998 seconds to 1529.0214999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P8_High_T2_7_Left", "images": ["Image/P27_D3_P8_High_T2_7_Left/1.png", "Image/P27_D3_P8_High_T2_7_Left/2.png", "Image/P27_D3_P8_High_T2_7_Left/3.png", "Image/P27_D3_P8_High_T2_7_Left/4.png", "Image/P27_D3_P8_High_T2_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in the green glass? Finish\"\nThe frames roughly correspond to the speech interval from 606.6502 seconds to 617.4643000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P2_High_T1_1_Left", "images": ["Image/P10_D3_P2_High_T1_1_Left/1.png", "Image/P10_D3_P2_High_T1_1_Left/2.png", "Image/P10_D3_P2_High_T1_1_Left/3.png", "Image/P10_D3_P2_High_T1_1_Left/4.png", "Image/P10_D3_P2_High_T1_1_Left/5.png", "Image/P10_D3_P2_High_T1_1_Left/6.png", "Image/P10_D3_P2_High_T1_1_Left/7.png", "Image/P10_D3_P2_High_T1_1_Left/8.png", "Image/P10_D3_P2_High_T1_1_Left/9.png", "Image/P10_D3_P2_High_T1_1_Left/10.png", "Image/P10_D3_P2_High_T1_1_Left/11.png", "Image/P10_D3_P2_High_T1_1_Left/12.png", "Image/P10_D3_P2_High_T1_1_Left/13.png", "Image/P10_D3_P2_High_T1_1_Left/14.png", "Image/P10_D3_P2_High_T1_1_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 222.80807 seconds to 244.21637 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P2_Low_T2_7_Left", "images": ["Image/P24_D3_P2_Low_T2_7_Left/1.png", "Image/P24_D3_P2_Low_T2_7_Left/2.png", "Image/P24_D3_P2_Low_T2_7_Left/3.png", "Image/P24_D3_P2_Low_T2_7_Left/4.png", "Image/P24_D3_P2_Low_T2_7_Left/5.png", "Image/P24_D3_P2_Low_T2_7_Left/6.png", "Image/P24_D3_P2_Low_T2_7_Left/7.png", "Image/P24_D3_P2_Low_T2_7_Left/8.png", "Image/P24_D3_P2_Low_T2_7_Left/9.png", "Image/P24_D3_P2_Low_T2_7_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1614.8663 seconds to 1624.9449 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P6_Low_T1_5_Mid", "images": ["Image/P6_D3_P6_Low_T1_5_Mid/1.png", "Image/P6_D3_P6_Low_T1_5_Mid/2.png", "Image/P6_D3_P6_Low_T1_5_Mid/3.png", "Image/P6_D3_P6_Low_T1_5_Mid/4.png", "Image/P6_D3_P6_Low_T1_5_Mid/5.png", "Image/P6_D3_P6_Low_T1_5_Mid/6.png", "Image/P6_D3_P6_Low_T1_5_Mid/7.png", "Image/P6_D3_P6_Low_T1_5_Mid/8.png", "Image/P6_D3_P6_Low_T1_5_Mid/9.png", "Image/P6_D3_P6_Low_T1_5_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 4775.3732 seconds to 4794.0145999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P2_Medium_T3_4_Right", "images": ["Image/P10_D1_P2_Medium_T3_4_Right/1.png", "Image/P10_D1_P2_Medium_T3_4_Right/2.png", "Image/P10_D1_P2_Medium_T3_4_Right/3.png", "Image/P10_D1_P2_Medium_T3_4_Right/4.png", "Image/P10_D1_P2_Medium_T3_4_Right/5.png", "Image/P10_D1_P2_Medium_T3_4_Right/6.png", "Image/P10_D1_P2_Medium_T3_4_Right/7.png", "Image/P10_D1_P2_Medium_T3_4_Right/8.png", "Image/P10_D1_P2_Medium_T3_4_Right/9.png", "Image/P10_D1_P2_Medium_T3_4_Right/10.png", "Image/P10_D1_P2_Medium_T3_4_Right/11.png", "Image/P10_D1_P2_Medium_T3_4_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2422.3091 seconds to 2440.6009 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P9_Low_T2_23_Mid", "images": ["Image/P24_D3_P9_Low_T2_23_Mid/1.png", "Image/P24_D3_P9_Low_T2_23_Mid/2.png", "Image/P24_D3_P9_Low_T2_23_Mid/3.png", "Image/P24_D3_P9_Low_T2_23_Mid/4.png", "Image/P24_D3_P9_Low_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1551.42002 seconds to 1561.30232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P2_High_T3_16_Left", "images": ["Image/P14_D2_P2_High_T3_16_Left/1.png", "Image/P14_D2_P2_High_T3_16_Left/2.png", "Image/P14_D2_P2_High_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1713.6662 seconds to 1722.1261 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P7_Low_T2_1_Right", "images": ["Image/P4_D2_P7_Low_T2_1_Right/1.png", "Image/P4_D2_P7_Low_T2_1_Right/2.png", "Image/P4_D2_P7_Low_T2_1_Right/3.png", "Image/P4_D2_P7_Low_T2_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you fill in the bottle? Finish\"\nThe frames roughly correspond to the speech interval from 1806.0759 seconds to 1819.4216 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P9_Low_T2_27_Left", "images": ["Image/P10_D2_P9_Low_T2_27_Left/1.png", "Image/P10_D2_P9_Low_T2_27_Left/2.png", "Image/P10_D2_P9_Low_T2_27_Left/3.png", "Image/P10_D2_P9_Low_T2_27_Left/4.png", "Image/P10_D2_P9_Low_T2_27_Left/5.png", "Image/P10_D2_P9_Low_T2_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1325.4423800000002 seconds to 1337.1362800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P5_Medium_T3_13_Left", "images": ["Image/P21_D2_P5_Medium_T3_13_Left/1.png", "Image/P21_D2_P5_Medium_T3_13_Left/2.png", "Image/P21_D2_P5_Medium_T3_13_Left/3.png", "Image/P21_D2_P5_Medium_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1656.5758 seconds to 1666.7374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P6_High_T3_18_Right", "images": ["Image/P6_D3_P6_High_T3_18_Right/1.png", "Image/P6_D3_P6_High_T3_18_Right/2.png", "Image/P6_D3_P6_High_T3_18_Right/3.png", "Image/P6_D3_P6_High_T3_18_Right/4.png", "Image/P6_D3_P6_High_T3_18_Right/5.png", "Image/P6_D3_P6_High_T3_18_Right/6.png", "Image/P6_D3_P6_High_T3_18_Right/7.png", "Image/P6_D3_P6_High_T3_18_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3464.6803 seconds to 3478.4609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P9_Medium_T2_18_Right", "images": ["Image/P1_D2_P9_Medium_T2_18_Right/1.png", "Image/P1_D2_P9_Medium_T2_18_Right/2.png", "Image/P1_D2_P9_Medium_T2_18_Right/3.png", "Image/P1_D2_P9_Medium_T2_18_Right/4.png", "Image/P1_D2_P9_Medium_T2_18_Right/5.png", "Image/P1_D2_P9_Medium_T2_18_Right/6.png", "Image/P1_D2_P9_Medium_T2_18_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help pour some water in this cup? The right one? Finish.\"\nThe frames roughly correspond to the speech interval from 917.4442 seconds to 930.2939 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P2_Low_T3_2_Right", "images": ["Image/P4_D2_P2_Low_T3_2_Right/1.png", "Image/P4_D2_P2_Low_T3_2_Right/2.png", "Image/P4_D2_P2_Low_T3_2_Right/3.png", "Image/P4_D2_P2_Low_T3_2_Right/4.png", "Image/P4_D2_P2_Low_T3_2_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1929.4213999999995 seconds to 1941.0548 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P4_Medium_T2_14_Right", "images": ["Image/P11_D2_P4_Medium_T2_14_Right/1.png", "Image/P11_D2_P4_Medium_T2_14_Right/2.png", "Image/P11_D2_P4_Medium_T2_14_Right/3.png", "Image/P11_D2_P4_Medium_T2_14_Right/4.png", "Image/P11_D2_P4_Medium_T2_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the first plant from the right side please. Finish.\"\nThe frames roughly correspond to the speech interval from 950.9214 seconds to 963.3485 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P1_Medium_T1_18_Mid", "images": ["Image/P15_D1_P1_Medium_T1_18_Mid/1.png", "Image/P15_D1_P1_Medium_T1_18_Mid/2.png", "Image/P15_D1_P1_Medium_T1_18_Mid/3.png", "Image/P15_D1_P1_Medium_T1_18_Mid/4.png", "Image/P15_D1_P1_Medium_T1_18_Mid/5.png", "Image/P15_D1_P1_Medium_T1_18_Mid/6.png", "Image/P15_D1_P1_Medium_T1_18_Mid/7.png", "Image/P15_D1_P1_Medium_T1_18_Mid/8.png", "Image/P15_D1_P1_Medium_T1_18_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the middle cup from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 1925.25897 seconds to 1939.83207 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P7_Low_T3_13_Left", "images": ["Image/P19_D3_P7_Low_T3_13_Left/1.png", "Image/P19_D3_P7_Low_T3_13_Left/2.png", "Image/P19_D3_P7_Low_T3_13_Left/3.png", "Image/P19_D3_P7_Low_T3_13_Left/4.png", "Image/P19_D3_P7_Low_T3_13_Left/5.png", "Image/P19_D3_P7_Low_T3_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2343.2430999999997 seconds to 2358.2349 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P9_High_T3_13_Right", "images": ["Image/P27_D3_P9_High_T3_13_Right/1.png", "Image/P27_D3_P9_High_T3_13_Right/2.png", "Image/P27_D3_P9_High_T3_13_Right/3.png", "Image/P27_D3_P9_High_T3_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean that plant? Finish\"\nThe frames roughly correspond to the speech interval from 530.1456000000001 seconds to 540.2369000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P8_Low_T3_9_Mid", "images": ["Image/P16_D3_P8_Low_T3_9_Mid/1.png", "Image/P16_D3_P8_Low_T3_9_Mid/2.png", "Image/P16_D3_P8_Low_T3_9_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 370.7356 seconds to 379.5751 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P6_Medium_T3_15_Right", "images": ["Image/P4_D1_P6_Medium_T3_15_Right/1.png", "Image/P4_D1_P6_Medium_T3_15_Right/2.png", "Image/P4_D1_P6_Medium_T3_15_Right/3.png", "Image/P4_D1_P6_Medium_T3_15_Right/4.png", "Image/P4_D1_P6_Medium_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start and clean this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2205.3234 seconds to 2218.4060999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P6_Low_T1_5_Left", "images": ["Image/P27_D1_P6_Low_T1_5_Left/1.png", "Image/P27_D1_P6_Low_T1_5_Left/2.png", "Image/P27_D1_P6_Low_T1_5_Left/3.png", "Image/P27_D1_P6_Low_T1_5_Left/4.png", "Image/P27_D1_P6_Low_T1_5_Left/5.png", "Image/P27_D1_P6_Low_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 918.3753 seconds to 930.3181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P6_High_T3_26_Left", "images": ["Image/P17_D1_P6_High_T3_26_Left/1.png", "Image/P17_D1_P6_High_T3_26_Left/2.png", "Image/P17_D1_P6_High_T3_26_Left/3.png", "Image/P17_D1_P6_High_T3_26_Left/4.png", "Image/P17_D1_P6_High_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white mug right here? Finish\"\nThe frames roughly correspond to the speech interval from 1538.6767000000002 seconds to 1550.3402 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P7_Low_T2_9_Mid", "images": ["Image/P9_D2_P7_Low_T2_9_Mid/1.png", "Image/P9_D2_P7_Low_T2_9_Mid/2.png", "Image/P9_D2_P7_Low_T2_9_Mid/3.png", "Image/P9_D2_P7_Low_T2_9_Mid/4.png", "Image/P9_D2_P7_Low_T2_9_Mid/5.png", "Image/P9_D2_P7_Low_T2_9_Mid/6.png", "Image/P9_D2_P7_Low_T2_9_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in that cup under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2898.8284 seconds to 2912.2425 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P4_Low_T3_18_Left", "images": ["Image/P2_D2_P4_Low_T3_18_Left/1.png", "Image/P2_D2_P4_Low_T3_18_Left/2.png", "Image/P2_D2_P4_Low_T3_18_Left/3.png", "Image/P2_D2_P4_Low_T3_18_Left/4.png", "Image/P2_D2_P4_Low_T3_18_Left/5.png", "Image/P2_D2_P4_Low_T3_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 2467.63158 seconds to 2479.3783799999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P5_High_T3_22_Mid", "images": ["Image/P14_D3_P5_High_T3_22_Mid/1.png", "Image/P14_D3_P5_High_T3_22_Mid/2.png", "Image/P14_D3_P5_High_T3_22_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that plant up. Finish.\"\nThe frames roughly correspond to the speech interval from 1242.39256 seconds to 1250.64146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P2_High_T2_14_Mid", "images": ["Image/P17_D3_P2_High_T2_14_Mid/1.png", "Image/P17_D3_P2_High_T2_14_Mid/2.png", "Image/P17_D3_P2_High_T2_14_Mid/3.png", "Image/P17_D3_P2_High_T2_14_Mid/4.png", "Image/P17_D3_P2_High_T2_14_Mid/5.png", "Image/P17_D3_P2_High_T2_14_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the plant on the top shelf in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 972.026 seconds to 985.333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P5_Medium_T2_5_Left", "images": ["Image/P30_D1_P5_Medium_T2_5_Left/1.png", "Image/P30_D1_P5_Medium_T2_5_Left/2.png", "Image/P30_D1_P5_Medium_T2_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 700.35783 seconds to 709.5999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P7_High_T2_18_Left", "images": ["Image/P15_D2_P7_High_T2_18_Left/1.png", "Image/P15_D2_P7_High_T2_18_Left/2.png", "Image/P15_D2_P7_High_T2_18_Left/3.png", "Image/P15_D2_P7_High_T2_18_Left/4.png", "Image/P15_D2_P7_High_T2_18_Left/5.png", "Image/P15_D2_P7_High_T2_18_Left/6.png", "Image/P15_D2_P7_High_T2_18_Left/7.png", "Image/P15_D2_P7_High_T2_18_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2720.6129 seconds to 2734.4361 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P1_Medium_T3_19_Right", "images": ["Image/P4_D2_P1_Medium_T3_19_Right/1.png", "Image/P4_D2_P1_Medium_T3_19_Right/2.png", "Image/P4_D2_P1_Medium_T3_19_Right/3.png", "Image/P4_D2_P1_Medium_T3_19_Right/4.png", "Image/P4_D2_P1_Medium_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the right bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 1719.5070999999998 seconds to 1732.6601999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P3_Medium_T3_27_Mid", "images": ["Image/P8_D2_P3_Medium_T3_27_Mid/1.png", "Image/P8_D2_P3_Medium_T3_27_Mid/2.png", "Image/P8_D2_P3_Medium_T3_27_Mid/3.png", "Image/P8_D2_P3_Medium_T3_27_Mid/4.png", "Image/P8_D2_P3_Medium_T3_27_Mid/5.png", "Image/P8_D2_P3_Medium_T3_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Dar\"\nThe frames roughly correspond to the speech interval from 2119.0341000000003 seconds to 2130.7258 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P9_Medium_T3_23_Left", "images": ["Image/P2_D2_P9_Medium_T3_23_Left/1.png", "Image/P2_D2_P9_Medium_T3_23_Left/2.png", "Image/P2_D2_P9_Medium_T3_23_Left/3.png", "Image/P2_D2_P9_Medium_T3_23_Left/4.png", "Image/P2_D2_P9_Medium_T3_23_Left/5.png", "Image/P2_D2_P9_Medium_T3_23_Left/6.png", "Image/P2_D2_P9_Medium_T3_23_Left/7.png", "Image/P2_D2_P9_Medium_T3_23_Left/8.png", "Image/P2_D2_P9_Medium_T3_23_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the plant on the very left side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2794.1648 seconds to 2809.9627 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P5_High_T2_18_Right", "images": ["Image/P12_D1_P5_High_T2_18_Right/1.png", "Image/P12_D1_P5_High_T2_18_Right/2.png", "Image/P12_D1_P5_High_T2_18_Right/3.png", "Image/P12_D1_P5_High_T2_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1683.7998 seconds to 1694.2386 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P4_Medium_T3_2_Mid", "images": ["Image/P7_D2_P4_Medium_T3_2_Mid/1.png", "Image/P7_D2_P4_Medium_T3_2_Mid/2.png", "Image/P7_D2_P4_Medium_T3_2_Mid/3.png", "Image/P7_D2_P4_Medium_T3_2_Mid/4.png", "Image/P7_D2_P4_Medium_T3_2_Mid/5.png", "Image/P7_D2_P4_Medium_T3_2_Mid/6.png", "Image/P7_D2_P4_Medium_T3_2_Mid/7.png", "Image/P7_D2_P4_Medium_T3_2_Mid/8.png", "Image/P7_D2_P4_Medium_T3_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1395.1272 seconds to 1411.6087000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P8_High_T3_20_Right", "images": ["Image/P24_D1_P8_High_T3_20_Right/1.png", "Image/P24_D1_P8_High_T3_20_Right/2.png", "Image/P24_D1_P8_High_T3_20_Right/3.png", "Image/P24_D1_P8_High_T3_20_Right/4.png", "Image/P24_D1_P8_High_T3_20_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1198.0306999999998 seconds to 1209.0268 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P6_High_T1_2_Mid", "images": ["Image/P11_D2_P6_High_T1_2_Mid/1.png", "Image/P11_D2_P6_High_T1_2_Mid/2.png", "Image/P11_D2_P6_High_T1_2_Mid/3.png", "Image/P11_D2_P6_High_T1_2_Mid/4.png", "Image/P11_D2_P6_High_T1_2_Mid/5.png", "Image/P11_D2_P6_High_T1_2_Mid/6.png", "Image/P11_D2_P6_High_T1_2_Mid/7.png", "Image/P11_D2_P6_High_T1_2_Mid/8.png", "Image/P11_D2_P6_High_T1_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green glass to the middle layer of the shelf on my side. Finish.\"\nThe frames roughly correspond to the speech interval from 1033.4616 seconds to 1048.9587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P1_High_T3_11_Mid", "images": ["Image/P28_D1_P1_High_T3_11_Mid/1.png", "Image/P28_D1_P1_High_T3_11_Mid/2.png", "Image/P28_D1_P1_High_T3_11_Mid/3.png", "Image/P28_D1_P1_High_T3_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1701.1611 seconds to 1710.2644 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P5_High_T3_26_Right", "images": ["Image/P25_D1_P5_High_T3_26_Right/1.png", "Image/P25_D1_P5_High_T3_26_Right/2.png", "Image/P25_D1_P5_High_T3_26_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"start. Clean the finish.\"\nThe frames roughly correspond to the speech interval from 81.13678 seconds to 90.15524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P2_Low_T1_24_Mid", "images": ["Image/P22_D2_P2_Low_T1_24_Mid/1.png", "Image/P22_D2_P2_Low_T1_24_Mid/2.png", "Image/P22_D2_P2_Low_T1_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move the plant down there. Finish.\"\nThe frames roughly correspond to the speech interval from 809.6500999999998 seconds to 818.3068999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P9_Low_T1_26_Mid", "images": ["Image/P23_D2_P9_Low_T1_26_Mid/1.png", "Image/P23_D2_P9_Low_T1_26_Mid/2.png", "Image/P23_D2_P9_Low_T1_26_Mid/3.png", "Image/P23_D2_P9_Low_T1_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put that to the top. Finish.\"\nThe frames roughly correspond to the speech interval from 391.4231 seconds to 401.9312 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P6_Medium_T1_18_Right", "images": ["Image/P19_D1_P6_Medium_T1_18_Right/1.png", "Image/P19_D1_P6_Medium_T1_18_Right/2.png", "Image/P19_D1_P6_Medium_T1_18_Right/3.png", "Image/P19_D1_P6_Medium_T1_18_Right/4.png", "Image/P19_D1_P6_Medium_T1_18_Right/5.png", "Image/P19_D1_P6_Medium_T1_18_Right/6.png", "Image/P19_D1_P6_Medium_T1_18_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 805.1106 seconds to 820.0455999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P6_High_T1_12_Right", "images": ["Image/P1_D3_P6_High_T1_12_Right/1.png", "Image/P1_D3_P6_High_T1_12_Right/2.png", "Image/P1_D3_P6_High_T1_12_Right/3.png", "Image/P1_D3_P6_High_T1_12_Right/4.png", "Image/P1_D3_P6_High_T1_12_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1367.9450899999997 seconds to 1378.9986199999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P2_Low_T3_1_Mid", "images": ["Image/P9_D3_P2_Low_T3_1_Mid/1.png", "Image/P9_D3_P2_Low_T3_1_Mid/2.png", "Image/P9_D3_P2_Low_T3_1_Mid/3.png", "Image/P9_D3_P2_Low_T3_1_Mid/4.png", "Image/P9_D3_P2_Low_T3_1_Mid/5.png", "Image/P9_D3_P2_Low_T3_1_Mid/6.png", "Image/P9_D3_P2_Low_T3_1_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the bottle on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 739.3879 seconds to 752.9126 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P6_Low_T2_16_Mid", "images": ["Image/P23_D1_P6_Low_T2_16_Mid/1.png", "Image/P23_D1_P6_Low_T2_16_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1897.1384 seconds to 1906.5484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P1_High_T2_10_Left", "images": ["Image/P5_D1_P1_High_T2_10_Left/1.png", "Image/P5_D1_P1_High_T2_10_Left/2.png", "Image/P5_D1_P1_High_T2_10_Left/3.png", "Image/P5_D1_P1_High_T2_10_Left/4.png", "Image/P5_D1_P1_High_T2_10_Left/5.png", "Image/P5_D1_P1_High_T2_10_Left/6.png", "Image/P5_D1_P1_High_T2_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle next to the white cup. Finished.\"\nThe frames roughly correspond to the speech interval from 1581.8121 seconds to 1594.3523 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P1_Medium_T3_15_Right", "images": ["Image/P20_D1_P1_Medium_T3_15_Right/1.png", "Image/P20_D1_P1_Medium_T3_15_Right/2.png", "Image/P20_D1_P1_Medium_T3_15_Right/3.png", "Image/P20_D1_P1_Medium_T3_15_Right/4.png", "Image/P20_D1_P1_Medium_T3_15_Right/5.png", "Image/P20_D1_P1_Medium_T3_15_Right/6.png", "Image/P20_D1_P1_Medium_T3_15_Right/7.png", "Image/P20_D1_P1_Medium_T3_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant? This one. Thanks. Finish.\"\nThe frames roughly correspond to the speech interval from 133.80003 seconds to 148.27490999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P9_High_T1_1_Mid", "images": ["Image/P13_D2_P9_High_T1_1_Mid/1.png", "Image/P13_D2_P9_High_T1_1_Mid/2.png", "Image/P13_D2_P9_High_T1_1_Mid/3.png", "Image/P13_D2_P9_High_T1_1_Mid/4.png", "Image/P13_D2_P9_High_T1_1_Mid/5.png", "Image/P13_D2_P9_High_T1_1_Mid/6.png", "Image/P13_D2_P9_High_T1_1_Mid/7.png", "Image/P13_D2_P9_High_T1_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the green bottle from the bottom shelf up to the top shelf over here\"\nThe frames roughly correspond to the speech interval from 795.85019 seconds to 809.2544300000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P8_High_T3_7_Mid", "images": ["Image/P26_D3_P8_High_T3_7_Mid/1.png", "Image/P26_D3_P8_High_T3_7_Mid/2.png", "Image/P26_D3_P8_High_T3_7_Mid/3.png", "Image/P26_D3_P8_High_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the middle vase down there? Finish.\"\nThe frames roughly correspond to the speech interval from 999.0791 seconds to 1008.6839 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P5_Medium_T3_9_Left", "images": ["Image/P19_D2_P5_Medium_T3_9_Left/1.png", "Image/P19_D2_P5_Medium_T3_9_Left/2.png", "Image/P19_D2_P5_Medium_T3_9_Left/3.png", "Image/P19_D2_P5_Medium_T3_9_Left/4.png", "Image/P19_D2_P5_Medium_T3_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this glass here. Finish.\"\nThe frames roughly correspond to the speech interval from 1473.5857 seconds to 1485.9461 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P1_Low_T3_24_Left", "images": ["Image/P7_D3_P1_Low_T3_24_Left/1.png", "Image/P7_D3_P1_Low_T3_24_Left/2.png", "Image/P7_D3_P1_Low_T3_24_Left/3.png", "Image/P7_D3_P1_Low_T3_24_Left/4.png", "Image/P7_D3_P1_Low_T3_24_Left/5.png", "Image/P7_D3_P1_Low_T3_24_Left/6.png", "Image/P7_D3_P1_Low_T3_24_Left/7.png", "Image/P7_D3_P1_Low_T3_24_Left/8.png", "Image/P7_D3_P1_Low_T3_24_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2054.5086 seconds to 2070.1409000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P2_Low_T1_7_Right", "images": ["Image/P25_D2_P2_Low_T1_7_Right/1.png", "Image/P25_D2_P2_Low_T1_7_Right/2.png", "Image/P25_D2_P2_Low_T1_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 725.588 seconds to 733.8011 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P4_Medium_T2_4_Right", "images": ["Image/P25_D1_P4_Medium_T2_4_Right/1.png", "Image/P25_D1_P4_Medium_T2_4_Right/2.png", "Image/P25_D1_P4_Medium_T2_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the first plant. Finish.\"\nThe frames roughly correspond to the speech interval from 298.36990000000003 seconds to 307.041 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P9_Medium_T1_3_Mid", "images": ["Image/P17_D1_P9_Medium_T1_3_Mid/1.png", "Image/P17_D1_P9_Medium_T1_3_Mid/2.png", "Image/P17_D1_P9_Medium_T1_3_Mid/3.png", "Image/P17_D1_P9_Medium_T1_3_Mid/4.png", "Image/P17_D1_P9_Medium_T1_3_Mid/5.png", "Image/P17_D1_P9_Medium_T1_3_Mid/6.png", "Image/P17_D1_P9_Medium_T1_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green object in the center and put it right here? Finish\"\nThe frames roughly correspond to the speech interval from 1474.11641 seconds to 1487.2971100000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P4_Low_T2_23_Right", "images": ["Image/P19_D1_P4_Low_T2_23_Right/1.png", "Image/P19_D1_P4_Low_T2_23_Right/2.png", "Image/P19_D1_P4_Low_T2_23_Right/3.png", "Image/P19_D1_P4_Low_T2_23_Right/4.png", "Image/P19_D1_P4_Low_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start\"\nThe frames roughly correspond to the speech interval from 475.5712 seconds to 487.0255 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P5_Medium_T1_15_Left", "images": ["Image/P1_D3_P5_Medium_T1_15_Left/1.png", "Image/P1_D3_P5_Medium_T1_15_Left/2.png", "Image/P1_D3_P5_Medium_T1_15_Left/3.png", "Image/P1_D3_P5_Medium_T1_15_Left/4.png", "Image/P1_D3_P5_Medium_T1_15_Left/5.png", "Image/P1_D3_P5_Medium_T1_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the left plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1439.0874 seconds to 1450.6550999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P6_High_T2_9_Left", "images": ["Image/P20_D3_P6_High_T2_9_Left/1.png", "Image/P20_D3_P6_High_T2_9_Left/2.png", "Image/P20_D3_P6_High_T2_9_Left/3.png", "Image/P20_D3_P6_High_T2_9_Left/4.png", "Image/P20_D3_P6_High_T2_9_Left/5.png", "Image/P20_D3_P6_High_T2_9_Left/6.png", "Image/P20_D3_P6_High_T2_9_Left/7.png", "Image/P20_D3_P6_High_T2_9_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1494.3348 seconds to 1509.7846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P4_Medium_T3_2_Mid", "images": ["Image/P2_D1_P4_Medium_T3_2_Mid/1.png", "Image/P2_D1_P4_Medium_T3_2_Mid/2.png", "Image/P2_D1_P4_Medium_T3_2_Mid/3.png", "Image/P2_D1_P4_Medium_T3_2_Mid/4.png", "Image/P2_D1_P4_Medium_T3_2_Mid/5.png", "Image/P2_D1_P4_Medium_T3_2_Mid/6.png", "Image/P2_D1_P4_Medium_T3_2_Mid/7.png", "Image/P2_D1_P4_Medium_T3_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Could you please clean the middle\"\nThe frames roughly correspond to the speech interval from 742.8997999999999 seconds to 758.0303999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P8_High_T1_9_Right", "images": ["Image/P16_D3_P8_High_T1_9_Right/1.png", "Image/P16_D3_P8_High_T1_9_Right/2.png", "Image/P16_D3_P8_High_T1_9_Right/3.png", "Image/P16_D3_P8_High_T1_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that vase up there? Finish\"\nThe frames roughly correspond to the speech interval from 179.2821 seconds to 189.3172 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P1_Low_T1_20_Left", "images": ["Image/P8_D2_P1_Low_T1_20_Left/1.png", "Image/P8_D2_P1_Low_T1_20_Left/2.png", "Image/P8_D2_P1_Low_T1_20_Left/3.png", "Image/P8_D2_P1_Low_T1_20_Left/4.png", "Image/P8_D2_P1_Low_T1_20_Left/5.png", "Image/P8_D2_P1_Low_T1_20_Left/6.png", "Image/P8_D2_P1_Low_T1_20_Left/7.png", "Image/P8_D2_P1_Low_T1_20_Left/8.png", "Image/P8_D2_P1_Low_T1_20_Left/9.png", "Image/P8_D2_P1_Low_T1_20_Left/10.png", "Image/P8_D2_P1_Low_T1_20_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 2070.86949 seconds to 2088.50631 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P6_Low_T1_14_Left", "images": ["Image/P20_D3_P6_Low_T1_14_Left/1.png", "Image/P20_D3_P6_Low_T1_14_Left/2.png", "Image/P20_D3_P6_Low_T1_14_Left/3.png", "Image/P20_D3_P6_Low_T1_14_Left/4.png", "Image/P20_D3_P6_Low_T1_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 1624.0536 seconds to 1635.6071 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P7_Low_T2_7_Mid", "images": ["Image/P14_D2_P7_Low_T2_7_Mid/1.png", "Image/P14_D2_P7_Low_T2_7_Mid/2.png", "Image/P14_D2_P7_Low_T2_7_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1587.8426 seconds to 1596.4089 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P9_Low_T2_22_Left", "images": ["Image/P23_D3_P9_Low_T2_22_Left/1.png", "Image/P23_D3_P9_Low_T2_22_Left/2.png", "Image/P23_D3_P9_Low_T2_22_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 832.2838 seconds to 840.7606000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P2_Low_T1_25_Mid", "images": ["Image/P5_D2_P2_Low_T1_25_Mid/1.png", "Image/P5_D2_P2_Low_T1_25_Mid/2.png", "Image/P5_D2_P2_Low_T1_25_Mid/3.png", "Image/P5_D2_P2_Low_T1_25_Mid/4.png", "Image/P5_D2_P2_Low_T1_25_Mid/5.png", "Image/P5_D2_P2_Low_T1_25_Mid/6.png", "Image/P5_D2_P2_Low_T1_25_Mid/7.png", "Image/P5_D2_P2_Low_T1_25_Mid/8.png", "Image/P5_D2_P2_Low_T1_25_Mid/9.png", "Image/P5_D2_P2_Low_T1_25_Mid/10.png", "Image/P5_D2_P2_Low_T1_25_Mid/11.png", "Image/P5_D2_P2_Low_T1_25_Mid/12.png", "Image/P5_D2_P2_Low_T1_25_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the cup in front of me to the shelf\"\nThe frames roughly correspond to the speech interval from 319.5582 seconds to 338.20869999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P6_Medium_T3_24_Right", "images": ["Image/P22_D3_P6_Medium_T3_24_Right/1.png", "Image/P22_D3_P6_Medium_T3_24_Right/2.png", "Image/P22_D3_P6_Medium_T3_24_Right/3.png", "Image/P22_D3_P6_Medium_T3_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 124.76335 seconds to 136.48298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P2_High_T3_1_Left", "images": ["Image/P26_D2_P2_High_T3_1_Left/1.png", "Image/P26_D2_P2_High_T3_1_Left/2.png", "Image/P26_D2_P2_High_T3_1_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the green base? Finish.\"\nThe frames roughly correspond to the speech interval from 1420.3 seconds to 1428.9559 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P2_Medium_T1_3_Right", "images": ["Image/P27_D3_P2_Medium_T1_3_Right/1.png", "Image/P27_D3_P2_Medium_T1_3_Right/2.png", "Image/P27_D3_P2_Medium_T1_3_Right/3.png", "Image/P27_D3_P2_Medium_T1_3_Right/4.png", "Image/P27_D3_P2_Medium_T1_3_Right/5.png", "Image/P27_D3_P2_Medium_T1_3_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the third vase onto the floor under that cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 325.5095 seconds to 337.34090000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P7_Medium_T1_23_Left", "images": ["Image/P30_D3_P7_Medium_T1_23_Left/1.png", "Image/P30_D3_P7_Medium_T1_23_Left/2.png", "Image/P30_D3_P7_Medium_T1_23_Left/3.png", "Image/P30_D3_P7_Medium_T1_23_Left/4.png", "Image/P30_D3_P7_Medium_T1_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1348.6559 seconds to 1361.964 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P3_High_T3_20_Right", "images": ["Image/P9_D3_P3_High_T3_20_Right/1.png", "Image/P9_D3_P3_High_T3_20_Right/2.png", "Image/P9_D3_P3_High_T3_20_Right/3.png", "Image/P9_D3_P3_High_T3_20_Right/4.png", "Image/P9_D3_P3_High_T3_20_Right/5.png", "Image/P9_D3_P3_High_T3_20_Right/6.png", "Image/P9_D3_P3_High_T3_20_Right/7.png", "Image/P9_D3_P3_High_T3_20_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to clean the bottle\"\nThe frames roughly correspond to the speech interval from 770.9381 seconds to 785.5042 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P9_Medium_T2_4_Left", "images": ["Image/P19_D1_P9_Medium_T2_4_Left/1.png", "Image/P19_D1_P9_Medium_T2_4_Left/2.png", "Image/P19_D1_P9_Medium_T2_4_Left/3.png", "Image/P19_D1_P9_Medium_T2_4_Left/4.png", "Image/P19_D1_P9_Medium_T2_4_Left/5.png", "Image/P19_D1_P9_Medium_T2_4_Left/6.png", "Image/P19_D1_P9_Medium_T2_4_Left/7.png", "Image/P19_D1_P9_Medium_T2_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 557.9915 seconds to 571.8958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P3_High_T1_10_Left", "images": ["Image/P27_D3_P3_High_T1_10_Left/1.png", "Image/P27_D3_P3_High_T1_10_Left/2.png", "Image/P27_D3_P3_High_T1_10_Left/3.png", "Image/P27_D3_P3_High_T1_10_Left/4.png", "Image/P27_D3_P3_High_T1_10_Left/5.png", "Image/P27_D3_P3_High_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that red glass to the bottom shelf? Finish\"\nThe frames roughly correspond to the speech interval from 587.3863 seconds to 598.9929999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P3_High_T2_13_Left", "images": ["Image/P16_D1_P3_High_T2_13_Left/1.png", "Image/P16_D1_P3_High_T2_13_Left/2.png", "Image/P16_D1_P3_High_T2_13_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Please pour water onto this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1251.95528 seconds to 1265.7501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P1_High_T1_17_Right", "images": ["Image/P18_D2_P1_High_T1_17_Right/1.png", "Image/P18_D2_P1_High_T1_17_Right/2.png", "Image/P18_D2_P1_High_T1_17_Right/3.png", "Image/P18_D2_P1_High_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that mug to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 144.02960000000002 seconds to 153.91440000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P9_Low_T1_12_Left", "images": ["Image/P1_D1_P9_Low_T1_12_Left/1.png", "Image/P1_D1_P9_Low_T1_12_Left/2.png", "Image/P1_D1_P9_Low_T1_12_Left/3.png", "Image/P1_D1_P9_Low_T1_12_Left/4.png", "Image/P1_D1_P9_Low_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 326.7477 seconds to 336.7998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P9_Low_T2_21_Left", "images": ["Image/P20_D1_P9_Low_T2_21_Left/1.png", "Image/P20_D1_P9_Low_T2_21_Left/2.png", "Image/P20_D1_P9_Low_T2_21_Left/3.png", "Image/P20_D1_P9_Low_T2_21_Left/4.png", "Image/P20_D1_P9_Low_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey robot\"\nThe frames roughly correspond to the speech interval from 653.7202 seconds to 666.3176000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P1_Low_T1_18_Right", "images": ["Image/P7_D1_P1_Low_T1_18_Right/1.png", "Image/P7_D1_P1_Low_T1_18_Right/2.png", "Image/P7_D1_P1_Low_T1_18_Right/3.png", "Image/P7_D1_P1_Low_T1_18_Right/4.png", "Image/P7_D1_P1_Low_T1_18_Right/5.png", "Image/P7_D1_P1_Low_T1_18_Right/6.png", "Image/P7_D1_P1_Low_T1_18_Right/7.png", "Image/P7_D1_P1_Low_T1_18_Right/8.png", "Image/P7_D1_P1_Low_T1_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 761.1082 seconds to 777.7559 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P8_Low_T1_26_Right", "images": ["Image/P22_D1_P8_Low_T1_26_Right/1.png", "Image/P22_D1_P8_Low_T1_26_Right/2.png", "Image/P22_D1_P8_Low_T1_26_Right/3.png", "Image/P22_D1_P8_Low_T1_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Put the cup up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1516.728 seconds to 1527.613 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P7_Low_T2_9_Mid", "images": ["Image/P27_D1_P7_Low_T2_9_Mid/1.png", "Image/P27_D1_P7_Low_T2_9_Mid/2.png", "Image/P27_D1_P7_Low_T2_9_Mid/3.png", "Image/P27_D1_P7_Low_T2_9_Mid/4.png", "Image/P27_D1_P7_Low_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that cup? Finish\"\nThe frames roughly correspond to the speech interval from 1090.7033 seconds to 1101.561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P4_High_T3_11_Left", "images": ["Image/P1_D2_P4_High_T3_11_Left/1.png", "Image/P1_D2_P4_High_T3_11_Left/2.png", "Image/P1_D2_P4_High_T3_11_Left/3.png", "Image/P1_D2_P4_High_T3_11_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 936.2671 seconds to 945.3579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P5_Medium_T1_3_Mid", "images": ["Image/P25_D3_P5_Medium_T1_3_Mid/1.png", "Image/P25_D3_P5_Medium_T1_3_Mid/2.png", "Image/P25_D3_P5_Medium_T1_3_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1933.739 seconds to 1941.9796 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P3_Low_T2_11_Left", "images": ["Image/P23_D2_P3_Low_T2_11_Left/1.png", "Image/P23_D2_P3_Low_T2_11_Left/2.png", "Image/P23_D2_P3_Low_T2_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 45.70773000000001 seconds to 55.57454000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P4_Low_T2_23_Left", "images": ["Image/P8_D3_P4_Low_T2_23_Left/1.png", "Image/P8_D3_P4_Low_T2_23_Left/2.png", "Image/P8_D3_P4_Low_T2_23_Left/3.png", "Image/P8_D3_P4_Low_T2_23_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you water that planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1925.8805 seconds to 1935.8595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P1_High_T2_10_Mid", "images": ["Image/P19_D1_P1_High_T2_10_Mid/1.png", "Image/P19_D1_P1_High_T2_10_Mid/2.png", "Image/P19_D1_P1_High_T2_10_Mid/3.png", "Image/P19_D1_P1_High_T2_10_Mid/4.png", "Image/P19_D1_P1_High_T2_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start\"\nThe frames roughly correspond to the speech interval from 504.1482 seconds to 516.1892 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P4_Medium_T2_19_Left", "images": ["Image/P19_D3_P4_Medium_T2_19_Left/1.png", "Image/P19_D3_P4_Medium_T2_19_Left/2.png", "Image/P19_D3_P4_Medium_T2_19_Left/3.png", "Image/P19_D3_P4_Medium_T2_19_Left/4.png", "Image/P19_D3_P4_Medium_T2_19_Left/5.png", "Image/P19_D3_P4_Medium_T2_19_Left/6.png", "Image/P19_D3_P4_Medium_T2_19_Left/7.png", "Image/P19_D3_P4_Medium_T2_19_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 2271.7506 seconds to 2288.4966 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P5_Low_T2_25_Mid", "images": ["Image/P29_D2_P5_Low_T2_25_Mid/1.png", "Image/P29_D2_P5_Low_T2_25_Mid/2.png", "Image/P29_D2_P5_Low_T2_25_Mid/3.png", "Image/P29_D2_P5_Low_T2_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 671.5724 seconds to 682.875 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P3_High_T1_26_Right", "images": ["Image/P14_D3_P3_High_T1_26_Right/1.png", "Image/P14_D3_P3_High_T1_26_Right/2.png", "Image/P14_D3_P3_High_T1_26_Right/3.png", "Image/P14_D3_P3_High_T1_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that mug to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1080.29636 seconds to 1090.02956 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P8_High_T3_4_Right", "images": ["Image/P10_D1_P8_High_T3_4_Right/1.png", "Image/P10_D1_P8_High_T3_4_Right/2.png", "Image/P10_D1_P8_High_T3_4_Right/3.png", "Image/P10_D1_P8_High_T3_4_Right/4.png", "Image/P10_D1_P8_High_T3_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2099.6727499999997 seconds to 2111.8521 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P6_Low_T1_16_Left", "images": ["Image/P19_D3_P6_Low_T1_16_Left/1.png", "Image/P19_D3_P6_Low_T1_16_Left/2.png", "Image/P19_D3_P6_Low_T1_16_Left/3.png", "Image/P19_D3_P6_Low_T1_16_Left/4.png", "Image/P19_D3_P6_Low_T1_16_Left/5.png", "Image/P19_D3_P6_Low_T1_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this glass over here. Finish.\"\nThe frames roughly correspond to the speech interval from 2087.4788799999997 seconds to 2101.27265 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P2_High_T1_17_Left", "images": ["Image/P2_D2_P2_High_T1_17_Left/1.png", "Image/P2_D2_P2_High_T1_17_Left/2.png", "Image/P2_D2_P2_High_T1_17_Left/3.png", "Image/P2_D2_P2_High_T1_17_Left/4.png", "Image/P2_D2_P2_High_T1_17_Left/5.png", "Image/P2_D2_P2_High_T1_17_Left/6.png", "Image/P2_D2_P2_High_T1_17_Left/7.png", "Image/P2_D2_P2_High_T1_17_Left/8.png", "Image/P2_D2_P2_High_T1_17_Left/9.png", "Image/P2_D2_P2_High_T1_17_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that brown cup all the way to the bottom of that cabinet down there? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 3010.8001 seconds to 3027.4762 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P9_High_T2_14_Right", "images": ["Image/P21_D2_P9_High_T2_14_Right/1.png", "Image/P21_D2_P9_High_T2_14_Right/2.png", "Image/P21_D2_P9_High_T2_14_Right/3.png", "Image/P21_D2_P9_High_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1779.7684000000002 seconds to 1790.8332 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P6_High_T3_18_Right", "images": ["Image/P17_D3_P6_High_T3_18_Right/1.png", "Image/P17_D3_P6_High_T3_18_Right/2.png", "Image/P17_D3_P6_High_T3_18_Right/3.png", "Image/P17_D3_P6_High_T3_18_Right/4.png", "Image/P17_D3_P6_High_T3_18_Right/5.png", "Image/P17_D3_P6_High_T3_18_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean up this brown mug on the shelf over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1133.8305 seconds to 1145.8154 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P4_Low_T2_12_Mid", "images": ["Image/P19_D2_P4_Low_T2_12_Mid/1.png", "Image/P19_D2_P4_Low_T2_12_Mid/2.png", "Image/P19_D2_P4_Low_T2_12_Mid/3.png", "Image/P19_D2_P4_Low_T2_12_Mid/4.png", "Image/P19_D2_P4_Low_T2_12_Mid/5.png", "Image/P19_D2_P4_Low_T2_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1613.0766999999998 seconds to 1625.7431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P3_Medium_T2_2_Left", "images": ["Image/P19_D3_P3_Medium_T2_2_Left/1.png", "Image/P19_D3_P3_Medium_T2_2_Left/2.png", "Image/P19_D3_P3_Medium_T2_2_Left/3.png", "Image/P19_D3_P3_Medium_T2_2_Left/4.png", "Image/P19_D3_P3_Medium_T2_2_Left/5.png", "Image/P19_D3_P3_Medium_T2_2_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that glass there. Finish.\"\nThe frames roughly correspond to the speech interval from 2005.04256 seconds to 2018.55832 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P4_Medium_T1_26_Left", "images": ["Image/P9_D3_P4_Medium_T1_26_Left/1.png", "Image/P9_D3_P4_Medium_T1_26_Left/2.png", "Image/P9_D3_P4_Medium_T1_26_Left/3.png", "Image/P9_D3_P4_Medium_T1_26_Left/4.png", "Image/P9_D3_P4_Medium_T1_26_Left/5.png", "Image/P9_D3_P4_Medium_T1_26_Left/6.png", "Image/P9_D3_P4_Medium_T1_26_Left/7.png", "Image/P9_D3_P4_Medium_T1_26_Left/8.png", "Image/P9_D3_P4_Medium_T1_26_Left/9.png", "Image/P9_D3_P4_Medium_T1_26_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the most left cup to the second floor? Finished.\"\nThe frames roughly correspond to the speech interval from 249.32054 seconds to 277.078 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P4_Medium_T2_20_Left", "images": ["Image/P5_D3_P4_Medium_T2_20_Left/1.png", "Image/P5_D3_P4_Medium_T2_20_Left/2.png", "Image/P5_D3_P4_Medium_T2_20_Left/3.png", "Image/P5_D3_P4_Medium_T2_20_Left/4.png", "Image/P5_D3_P4_Medium_T2_20_Left/5.png", "Image/P5_D3_P4_Medium_T2_20_Left/6.png", "Image/P5_D3_P4_Medium_T2_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the leftmost bottle placed on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 2883.4982999999997 seconds to 2896.5957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P6_Low_T2_12_Right", "images": ["Image/P26_D1_P6_Low_T2_12_Right/1.png", "Image/P26_D1_P6_Low_T2_12_Right/2.png", "Image/P26_D1_P6_Low_T2_12_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water in here? Finish.\"\nThe frames roughly correspond to the speech interval from 213.48639999999995 seconds to 224.7463 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P5_Medium_T1_11_Left", "images": ["Image/P13_D1_P5_Medium_T1_11_Left/1.png", "Image/P13_D1_P5_Medium_T1_11_Left/2.png", "Image/P13_D1_P5_Medium_T1_11_Left/3.png", "Image/P13_D1_P5_Medium_T1_11_Left/4.png", "Image/P13_D1_P5_Medium_T1_11_Left/5.png", "Image/P13_D1_P5_Medium_T1_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move this cup over to here please? Finish.\"\nThe frames roughly correspond to the speech interval from 197.98921 seconds to 210.58647 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P1_High_T3_9_Mid", "images": ["Image/P25_D3_P1_High_T3_9_Mid/1.png", "Image/P25_D3_P1_High_T3_9_Mid/2.png", "Image/P25_D3_P1_High_T3_9_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1885.4019 seconds to 1893.1136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P2_Medium_T3_11_Left", "images": ["Image/P7_D1_P2_Medium_T3_11_Left/1.png", "Image/P7_D1_P2_Medium_T3_11_Left/2.png", "Image/P7_D1_P2_Medium_T3_11_Left/3.png", "Image/P7_D1_P2_Medium_T3_11_Left/4.png", "Image/P7_D1_P2_Medium_T3_11_Left/5.png", "Image/P7_D1_P2_Medium_T3_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey Friday\"\nThe frames roughly correspond to the speech interval from 785.5938 seconds to 800.4585 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P1_Medium_T3_13_Right", "images": ["Image/P15_D3_P1_Medium_T3_13_Right/1.png", "Image/P15_D3_P1_Medium_T3_13_Right/2.png", "Image/P15_D3_P1_Medium_T3_13_Right/3.png", "Image/P15_D3_P1_Medium_T3_13_Right/4.png", "Image/P15_D3_P1_Medium_T3_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 66.66980000000002 seconds to 79.33570000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P4_Low_T3_11_Right", "images": ["Image/P7_D1_P4_Low_T3_11_Right/1.png", "Image/P7_D1_P4_Low_T3_11_Right/2.png", "Image/P7_D1_P4_Low_T3_11_Right/3.png", "Image/P7_D1_P4_Low_T3_11_Right/4.png", "Image/P7_D1_P4_Low_T3_11_Right/5.png", "Image/P7_D1_P4_Low_T3_11_Right/6.png", "Image/P7_D1_P4_Low_T3_11_Right/7.png", "Image/P7_D1_P4_Low_T3_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 542.7318 seconds to 558.2943 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P2_Low_T2_13_Mid", "images": ["Image/P8_D1_P2_Low_T2_13_Mid/1.png", "Image/P8_D1_P2_Low_T2_13_Mid/2.png", "Image/P8_D1_P2_Low_T2_13_Mid/3.png", "Image/P8_D1_P2_Low_T2_13_Mid/4.png", "Image/P8_D1_P2_Low_T2_13_Mid/5.png", "Image/P8_D1_P2_Low_T2_13_Mid/6.png", "Image/P8_D1_P2_Low_T2_13_Mid/7.png", "Image/P8_D1_P2_Low_T2_13_Mid/8.png", "Image/P8_D1_P2_Low_T2_13_Mid/9.png", "Image/P8_D1_P2_Low_T2_13_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Hi. Could you water the planet that is just in front of me? That's all. Finish.\"\nThe frames roughly correspond to the speech interval from 265.9735999999999 seconds to 284.3721999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P6_Medium_T3_21_Right", "images": ["Image/P11_D2_P6_Medium_T3_21_Right/1.png", "Image/P11_D2_P6_Medium_T3_21_Right/2.png", "Image/P11_D2_P6_Medium_T3_21_Right/3.png", "Image/P11_D2_P6_Medium_T3_21_Right/4.png", "Image/P11_D2_P6_Medium_T3_21_Right/5.png", "Image/P11_D2_P6_Medium_T3_21_Right/6.png", "Image/P11_D2_P6_Medium_T3_21_Right/7.png", "Image/P11_D2_P6_Medium_T3_21_Right/8.png", "Image/P11_D2_P6_Medium_T3_21_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the first white glass from the right side. Please. Finish.\"\nThe frames roughly correspond to the speech interval from 899.5284 seconds to 915.191 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P5_High_T1_2_Right", "images": ["Image/P21_D2_P5_High_T1_2_Right/1.png", "Image/P21_D2_P5_High_T1_2_Right/2.png", "Image/P21_D2_P5_High_T1_2_Right/3.png", "Image/P21_D2_P5_High_T1_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1690.4565000000002 seconds to 1700.9223000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P3_High_T1_8_Right", "images": ["Image/P17_D2_P3_High_T1_8_Right/1.png", "Image/P17_D2_P3_High_T1_8_Right/2.png", "Image/P17_D2_P3_High_T1_8_Right/3.png", "Image/P17_D2_P3_High_T1_8_Right/4.png", "Image/P17_D2_P3_High_T1_8_Right/5.png", "Image/P17_D2_P3_High_T1_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that green mug down to the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 538.5847 seconds to 552.4765 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P3_Low_T3_6_Right", "images": ["Image/P17_D3_P3_Low_T3_6_Right/1.png", "Image/P17_D3_P3_Low_T3_6_Right/2.png", "Image/P17_D3_P3_Low_T3_6_Right/3.png", "Image/P17_D3_P3_Low_T3_6_Right/4.png", "Image/P17_D3_P3_Low_T3_6_Right/5.png", "Image/P17_D3_P3_Low_T3_6_Right/6.png", "Image/P17_D3_P3_Low_T3_6_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this green plant on this top shelf over here? Finish.\"\nThe frames roughly correspond to the speech interval from 1352.5269 seconds to 1365.6808 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P9_Medium_T1_2_Left", "images": ["Image/P16_D3_P9_Medium_T1_2_Left/1.png", "Image/P16_D3_P9_Medium_T1_2_Left/2.png", "Image/P16_D3_P9_Medium_T1_2_Left/3.png", "Image/P16_D3_P9_Medium_T1_2_Left/4.png", "Image/P16_D3_P9_Medium_T1_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that vase to the top of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 132.3709 seconds to 150.2302 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P3_High_T3_24_Mid", "images": ["Image/P15_D3_P3_High_T3_24_Mid/1.png", "Image/P15_D3_P3_High_T3_24_Mid/2.png", "Image/P15_D3_P3_High_T3_24_Mid/3.png", "Image/P15_D3_P3_High_T3_24_Mid/4.png", "Image/P15_D3_P3_High_T3_24_Mid/5.png", "Image/P15_D3_P3_High_T3_24_Mid/6.png", "Image/P15_D3_P3_High_T3_24_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 282.1621 seconds to 295.15090000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P1_High_T3_10_Right", "images": ["Image/P8_D2_P1_High_T3_10_Right/1.png", "Image/P8_D2_P1_High_T3_10_Right/2.png", "Image/P8_D2_P1_High_T3_10_Right/3.png", "Image/P8_D2_P1_High_T3_10_Right/4.png", "Image/P8_D2_P1_High_T3_10_Right/5.png", "Image/P8_D2_P1_High_T3_10_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please help me with cleaning that brown bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 2369.8858 seconds to 2381.9383000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P8_Low_T3_17_Mid", "images": ["Image/P26_D1_P8_Low_T3_17_Mid/1.png", "Image/P26_D1_P8_Low_T3_17_Mid/2.png", "Image/P26_D1_P8_Low_T3_17_Mid/3.png", "Image/P26_D1_P8_Low_T3_17_Mid/4.png", "Image/P26_D1_P8_Low_T3_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you please clean the mug in the basket? Finish.\"\nThe frames roughly correspond to the speech interval from 534.0758999999999 seconds to 545.42 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P4_Low_T2_2_Left", "images": ["Image/P11_D2_P4_Low_T2_2_Left/1.png", "Image/P11_D2_P4_Low_T2_2_Left/2.png", "Image/P11_D2_P4_Low_T2_2_Left/3.png", "Image/P11_D2_P4_Low_T2_2_Left/4.png", "Image/P11_D2_P4_Low_T2_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Pour the water in the glass please. Finish.\"\nThe frames roughly correspond to the speech interval from 732.5982 seconds to 744.0117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P5_Low_T2_12_Right", "images": ["Image/P2_D1_P5_Low_T2_12_Right/1.png", "Image/P2_D1_P5_Low_T2_12_Right/2.png", "Image/P2_D1_P5_Low_T2_12_Right/3.png", "Image/P2_D1_P5_Low_T2_12_Right/4.png", "Image/P2_D1_P5_Low_T2_12_Right/5.png", "Image/P2_D1_P5_Low_T2_12_Right/6.png", "Image/P2_D1_P5_Low_T2_12_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please pour some water into this vessel? Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 948.4685 seconds to 962.5277 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P4_Medium_T2_8_Mid", "images": ["Image/P16_D3_P4_Medium_T2_8_Mid/1.png", "Image/P16_D3_P4_Medium_T2_8_Mid/2.png", "Image/P16_D3_P4_Medium_T2_8_Mid/3.png", "Image/P16_D3_P4_Medium_T2_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 248.4073 seconds to 257.6504 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P2_Low_T1_18_Left", "images": ["Image/P15_D3_P2_Low_T1_18_Left/1.png", "Image/P15_D3_P2_Low_T1_18_Left/2.png", "Image/P15_D3_P2_Low_T1_18_Left/3.png", "Image/P15_D3_P2_Low_T1_18_Left/4.png", "Image/P15_D3_P2_Low_T1_18_Left/5.png", "Image/P15_D3_P2_Low_T1_18_Left/6.png", "Image/P15_D3_P2_Low_T1_18_Left/7.png", "Image/P15_D3_P2_Low_T1_18_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 550.2900999999999 seconds to 563.9155 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P6_High_T2_3_Mid", "images": ["Image/P24_D2_P6_High_T2_3_Mid/1.png", "Image/P24_D2_P6_High_T2_3_Mid/2.png", "Image/P24_D2_P6_High_T2_3_Mid/3.png", "Image/P24_D2_P6_High_T2_3_Mid/4.png", "Image/P24_D2_P6_High_T2_3_Mid/5.png", "Image/P24_D2_P6_High_T2_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you pour water? Pour water in this bottle\"\nThe frames roughly correspond to the speech interval from 285.3914 seconds to 296.94449999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P1_High_T2_19_Left", "images": ["Image/P20_D3_P1_High_T2_19_Left/1.png", "Image/P20_D3_P1_High_T2_19_Left/2.png", "Image/P20_D3_P1_High_T2_19_Left/3.png", "Image/P20_D3_P1_High_T2_19_Left/4.png", "Image/P20_D3_P1_High_T2_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Hey add some water into that white bottle over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1438.3351 seconds to 1452.0744 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P5_High_T2_10_Left", "images": ["Image/P21_D1_P5_High_T2_10_Left/1.png", "Image/P21_D1_P5_High_T2_10_Left/2.png", "Image/P21_D1_P5_High_T2_10_Left/3.png", "Image/P21_D1_P5_High_T2_10_Left/4.png", "Image/P21_D1_P5_High_T2_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 932.4428 seconds to 947.0011 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P2_Low_T2_3_Right", "images": ["Image/P27_D2_P2_Low_T2_3_Right/1.png", "Image/P27_D2_P2_Low_T2_3_Right/2.png", "Image/P27_D2_P2_Low_T2_3_Right/3.png", "Image/P27_D2_P2_Low_T2_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put water in that vase? Finish\"\nThe frames roughly correspond to the speech interval from 1569.413 seconds to 1579.2683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P6_Medium_T2_22_Mid", "images": ["Image/P27_D1_P6_Medium_T2_22_Mid/1.png", "Image/P27_D1_P6_Medium_T2_22_Mid/2.png", "Image/P27_D1_P6_Medium_T2_22_Mid/3.png", "Image/P27_D1_P6_Medium_T2_22_Mid/4.png", "Image/P27_D1_P6_Medium_T2_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that middle plant? Finish\"\nThe frames roughly correspond to the speech interval from 1109.495 seconds to 1121.8421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P7_High_T1_12_Mid", "images": ["Image/P16_D3_P7_High_T1_12_Mid/1.png", "Image/P16_D3_P7_High_T1_12_Mid/2.png", "Image/P16_D3_P7_High_T1_12_Mid/3.png", "Image/P16_D3_P7_High_T1_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 332.6781 seconds to 346.6545 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P6_High_T3_27_Mid", "images": ["Image/P24_D1_P6_High_T3_27_Mid/1.png", "Image/P24_D1_P6_High_T3_27_Mid/2.png", "Image/P24_D1_P6_High_T3_27_Mid/3.png", "Image/P24_D1_P6_High_T3_27_Mid/4.png", "Image/P24_D1_P6_High_T3_27_Mid/5.png", "Image/P24_D1_P6_High_T3_27_Mid/6.png", "Image/P24_D1_P6_High_T3_27_Mid/7.png", "Image/P24_D1_P6_High_T3_27_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you clean this one\"\nThe frames roughly correspond to the speech interval from 928.9441 seconds to 942.1804 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P6_High_T3_16_Left", "images": ["Image/P29_D3_P6_High_T3_16_Left/1.png", "Image/P29_D3_P6_High_T3_16_Left/2.png", "Image/P29_D3_P6_High_T3_16_Left/3.png", "Image/P29_D3_P6_High_T3_16_Left/4.png", "Image/P29_D3_P6_High_T3_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1553.8701999999998 seconds to 1564.7689999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P7_High_T2_25_Right", "images": ["Image/P15_D1_P7_High_T2_25_Right/1.png", "Image/P15_D1_P7_High_T2_25_Right/2.png", "Image/P15_D1_P7_High_T2_25_Right/3.png", "Image/P15_D1_P7_High_T2_25_Right/4.png", "Image/P15_D1_P7_High_T2_25_Right/5.png", "Image/P15_D1_P7_High_T2_25_Right/6.png", "Image/P15_D1_P7_High_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1671.42897 seconds to 1684.48637 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P2_High_T3_4_Left", "images": ["Image/P27_D3_P2_High_T3_4_Left/1.png", "Image/P27_D3_P2_High_T3_4_Left/2.png", "Image/P27_D3_P2_High_T3_4_Left/3.png", "Image/P27_D3_P2_High_T3_4_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean that plant? Finish\"\nThe frames roughly correspond to the speech interval from 512.7036 seconds to 522.3783000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P8_Medium_T3_15_Mid", "images": ["Image/P15_D2_P8_Medium_T3_15_Mid/1.png", "Image/P15_D2_P8_Medium_T3_15_Mid/2.png", "Image/P15_D2_P8_Medium_T3_15_Mid/3.png", "Image/P15_D2_P8_Medium_T3_15_Mid/4.png", "Image/P15_D2_P8_Medium_T3_15_Mid/5.png", "Image/P15_D2_P8_Medium_T3_15_Mid/6.png", "Image/P15_D2_P8_Medium_T3_15_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please Clean the middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2675.5958 seconds to 2688.0674 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P7_Medium_T2_9_Left", "images": ["Image/P2_D3_P7_Medium_T2_9_Left/1.png", "Image/P2_D3_P7_Medium_T2_9_Left/2.png", "Image/P2_D3_P7_Medium_T2_9_Left/3.png", "Image/P2_D3_P7_Medium_T2_9_Left/4.png", "Image/P2_D3_P7_Medium_T2_9_Left/5.png", "Image/P2_D3_P7_Medium_T2_9_Left/6.png", "Image/P2_D3_P7_Medium_T2_9_Left/7.png", "Image/P2_D3_P7_Medium_T2_9_Left/8.png", "Image/P2_D3_P7_Medium_T2_9_Left/9.png", "Image/P2_D3_P7_Medium_T2_9_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the green cup on the very left side? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2100.1720000000005 seconds to 2117.3452 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P8_High_T1_11_Left", "images": ["Image/P20_D1_P8_High_T1_11_Left/1.png", "Image/P20_D1_P8_High_T1_11_Left/2.png", "Image/P20_D1_P8_High_T1_11_Left/3.png", "Image/P20_D1_P8_High_T1_11_Left/4.png", "Image/P20_D1_P8_High_T1_11_Left/5.png", "Image/P20_D1_P8_High_T1_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move the bottle into\"\nThe frames roughly correspond to the speech interval from 540.8447 seconds to 553.6914 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P1_Medium_T2_26_Left", "images": ["Image/P27_D3_P1_Medium_T2_26_Left/1.png", "Image/P27_D3_P1_Medium_T2_26_Left/2.png", "Image/P27_D3_P1_Medium_T2_26_Left/3.png", "Image/P27_D3_P1_Medium_T2_26_Left/4.png", "Image/P27_D3_P1_Medium_T2_26_Left/5.png", "Image/P27_D3_P1_Medium_T2_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put water in the first cup over there? Finish\"\nThe frames roughly correspond to the speech interval from 183.03790000000004 seconds to 194.9681 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P7_High_T3_3_Mid", "images": ["Image/P13_D1_P7_High_T3_3_Mid/1.png", "Image/P13_D1_P7_High_T3_3_Mid/2.png", "Image/P13_D1_P7_High_T3_3_Mid/3.png", "Image/P13_D1_P7_High_T3_3_Mid/4.png", "Image/P13_D1_P7_High_T3_3_Mid/5.png", "Image/P13_D1_P7_High_T3_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean off the green vase down here\"\nThe frames roughly correspond to the speech interval from 489.5004 seconds to 500.9142 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P8_Medium_T3_4_Left", "images": ["Image/P2_D3_P8_Medium_T3_4_Left/1.png", "Image/P2_D3_P8_Medium_T3_4_Left/2.png", "Image/P2_D3_P8_Medium_T3_4_Left/3.png", "Image/P2_D3_P8_Medium_T3_4_Left/4.png", "Image/P2_D3_P8_Medium_T3_4_Left/5.png", "Image/P2_D3_P8_Medium_T3_4_Left/6.png", "Image/P2_D3_P8_Medium_T3_4_Left/7.png", "Image/P2_D3_P8_Medium_T3_4_Left/8.png", "Image/P2_D3_P8_Medium_T3_4_Left/9.png", "Image/P2_D3_P8_Medium_T3_4_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the plant on the very left side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1904.8727000000003 seconds to 1924.5827000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P1_Low_T3_26_Right", "images": ["Image/P30_D2_P1_Low_T3_26_Right/1.png", "Image/P30_D2_P1_Low_T3_26_Right/2.png", "Image/P30_D2_P1_Low_T3_26_Right/3.png", "Image/P30_D2_P1_Low_T3_26_Right/4.png", "Image/P30_D2_P1_Low_T3_26_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup that's on top. Finish.\"\nThe frames roughly correspond to the speech interval from 483.6795 seconds to 497.0006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P9_Medium_T3_23_Right", "images": ["Image/P29_D2_P9_Medium_T3_23_Right/1.png", "Image/P29_D2_P9_Medium_T3_23_Right/2.png", "Image/P29_D2_P9_Medium_T3_23_Right/3.png", "Image/P29_D2_P9_Medium_T3_23_Right/4.png", "Image/P29_D2_P9_Medium_T3_23_Right/5.png", "Image/P29_D2_P9_Medium_T3_23_Right/6.png", "Image/P29_D2_P9_Medium_T3_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 575.4970000000001 seconds to 587.8562000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P3_Low_T3_22_Mid", "images": ["Image/P6_D2_P3_Low_T3_22_Mid/1.png", "Image/P6_D2_P3_Low_T3_22_Mid/2.png", "Image/P6_D2_P3_Low_T3_22_Mid/3.png", "Image/P6_D2_P3_Low_T3_22_Mid/4.png", "Image/P6_D2_P3_Low_T3_22_Mid/5.png", "Image/P6_D2_P3_Low_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Start. Hi Friday\"\nThe frames roughly correspond to the speech interval from 1143.8955999999998 seconds to 1168.4529 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P8_Medium_T3_17_Left", "images": ["Image/P20_D2_P8_Medium_T3_17_Left/1.png", "Image/P20_D2_P8_Medium_T3_17_Left/2.png", "Image/P20_D2_P8_Medium_T3_17_Left/3.png", "Image/P20_D2_P8_Medium_T3_17_Left/4.png", "Image/P20_D2_P8_Medium_T3_17_Left/5.png", "Image/P20_D2_P8_Medium_T3_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that left cup. Finish.\"\nThe frames roughly correspond to the speech interval from 3295.0256000000004 seconds to 3306.9548000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P2_Medium_T1_2_Right", "images": ["Image/P30_D3_P2_Medium_T1_2_Right/1.png", "Image/P30_D3_P2_Medium_T1_2_Right/2.png", "Image/P30_D3_P2_Medium_T1_2_Right/3.png", "Image/P30_D3_P2_Medium_T1_2_Right/4.png", "Image/P30_D3_P2_Medium_T1_2_Right/5.png", "Image/P30_D3_P2_Medium_T1_2_Right/6.png", "Image/P30_D3_P2_Medium_T1_2_Right/7.png", "Image/P30_D3_P2_Medium_T1_2_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase furthest on the right down to the bottom underneath the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1303.7376 seconds to 1318.7146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P8_Medium_T3_12_Left", "images": ["Image/P28_D1_P8_Medium_T3_12_Left/1.png", "Image/P28_D1_P8_Medium_T3_12_Left/2.png", "Image/P28_D1_P8_Medium_T3_12_Left/3.png", "Image/P28_D1_P8_Medium_T3_12_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1609.3181 seconds to 1618.5881 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P5_Medium_T3_18_Mid", "images": ["Image/P23_D1_P5_Medium_T3_18_Mid/1.png", "Image/P23_D1_P5_Medium_T3_18_Mid/2.png", "Image/P23_D1_P5_Medium_T3_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this up. Finish.\"\nThe frames roughly correspond to the speech interval from 1571.5401 seconds to 1579.8634 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P8_Low_T2_16_Left", "images": ["Image/P2_D3_P8_Low_T2_16_Left/1.png", "Image/P2_D3_P8_Low_T2_16_Left/2.png", "Image/P2_D3_P8_Low_T2_16_Left/3.png", "Image/P2_D3_P8_Low_T2_16_Left/4.png", "Image/P2_D3_P8_Low_T2_16_Left/5.png", "Image/P2_D3_P8_Low_T2_16_Left/6.png", "Image/P2_D3_P8_Low_T2_16_Left/7.png", "Image/P2_D3_P8_Low_T2_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the cup on the floor? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1735.2319000000002 seconds to 1748.0628000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P5_Medium_T1_6_Right", "images": ["Image/P27_D1_P5_Medium_T1_6_Right/1.png", "Image/P27_D1_P5_Medium_T1_6_Right/2.png", "Image/P27_D1_P5_Medium_T1_6_Right/3.png", "Image/P27_D1_P5_Medium_T1_6_Right/4.png", "Image/P27_D1_P5_Medium_T1_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant right there? Finish\"\nThe frames roughly correspond to the speech interval from 684.92577 seconds to 696.8402100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P5_Low_T3_2_Mid", "images": ["Image/P17_D2_P5_Low_T3_2_Mid/1.png", "Image/P17_D2_P5_Low_T3_2_Mid/2.png", "Image/P17_D2_P5_Low_T3_2_Mid/3.png", "Image/P17_D2_P5_Low_T3_2_Mid/4.png", "Image/P17_D2_P5_Low_T3_2_Mid/5.png", "Image/P17_D2_P5_Low_T3_2_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that green vase right in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 423.4684 seconds to 435.3691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P6_High_T2_1_Mid", "images": ["Image/P23_D2_P6_High_T2_1_Mid/1.png", "Image/P23_D2_P6_High_T2_1_Mid/2.png", "Image/P23_D2_P6_High_T2_1_Mid/3.png", "Image/P23_D2_P6_High_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 519.7505 seconds to 531.1513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P8_High_T3_16_Left", "images": ["Image/P22_D2_P8_High_T3_16_Left/1.png", "Image/P22_D2_P8_High_T3_16_Left/2.png", "Image/P22_D2_P8_High_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 620.3940499999999 seconds to 629.12268 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P8_Low_T2_3_Mid", "images": ["Image/P6_D1_P8_Low_T2_3_Mid/1.png", "Image/P6_D1_P8_Low_T2_3_Mid/2.png", "Image/P6_D1_P8_Low_T2_3_Mid/3.png", "Image/P6_D1_P8_Low_T2_3_Mid/4.png", "Image/P6_D1_P8_Low_T2_3_Mid/5.png", "Image/P6_D1_P8_Low_T2_3_Mid/6.png", "Image/P6_D1_P8_Low_T2_3_Mid/7.png", "Image/P6_D1_P8_Low_T2_3_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2087.0303000000004 seconds to 2102.0906000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P2_Low_T3_26_Mid", "images": ["Image/P25_D1_P2_Low_T3_26_Mid/1.png", "Image/P25_D1_P2_Low_T3_26_Mid/2.png", "Image/P25_D1_P2_Low_T3_26_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 262.6934 seconds to 270.6838 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P4_High_T2_21_Left", "images": ["Image/P3_D3_P4_High_T2_21_Left/1.png", "Image/P3_D3_P4_High_T2_21_Left/2.png", "Image/P3_D3_P4_High_T2_21_Left/3.png", "Image/P3_D3_P4_High_T2_21_Left/4.png", "Image/P3_D3_P4_High_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1088.2873 seconds to 1098.7837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P6_Low_T1_12_Left", "images": ["Image/P28_D2_P6_Low_T1_12_Left/1.png", "Image/P28_D2_P6_Low_T1_12_Left/2.png", "Image/P28_D2_P6_Low_T1_12_Left/3.png", "Image/P28_D2_P6_Low_T1_12_Left/4.png", "Image/P28_D2_P6_Low_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that object over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1062.2853 seconds to 1072.3771000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P9_Medium_T1_2_Right", "images": ["Image/P12_D1_P9_Medium_T1_2_Right/1.png", "Image/P12_D1_P9_Medium_T1_2_Right/2.png", "Image/P12_D1_P9_Medium_T1_2_Right/3.png", "Image/P12_D1_P9_Medium_T1_2_Right/4.png", "Image/P12_D1_P9_Medium_T1_2_Right/5.png", "Image/P12_D1_P9_Medium_T1_2_Right/6.png", "Image/P12_D1_P9_Medium_T1_2_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1598.2195 seconds to 1612.4043 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P5_High_T3_25_Left", "images": ["Image/P2_D2_P5_High_T3_25_Left/1.png", "Image/P2_D2_P5_High_T3_25_Left/2.png", "Image/P2_D2_P5_High_T3_25_Left/3.png", "Image/P2_D2_P5_High_T3_25_Left/4.png", "Image/P2_D2_P5_High_T3_25_Left/5.png", "Image/P2_D2_P5_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2539.4015999999997 seconds to 2553.8781999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P7_Medium_T2_5_Left", "images": ["Image/P7_D2_P7_Medium_T2_5_Left/1.png", "Image/P7_D2_P7_Medium_T2_5_Left/2.png", "Image/P7_D2_P7_Medium_T2_5_Left/3.png", "Image/P7_D2_P7_Medium_T2_5_Left/4.png", "Image/P7_D2_P7_Medium_T2_5_Left/5.png", "Image/P7_D2_P7_Medium_T2_5_Left/6.png", "Image/P7_D2_P7_Medium_T2_5_Left/7.png", "Image/P7_D2_P7_Medium_T2_5_Left/8.png", "Image/P7_D2_P7_Medium_T2_5_Left/9.png", "Image/P7_D2_P7_Medium_T2_5_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 991.2407000000002 seconds to 1008.0000000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P2_High_T2_26_Mid", "images": ["Image/P29_D1_P2_High_T2_26_Mid/1.png", "Image/P29_D1_P2_High_T2_26_Mid/2.png", "Image/P29_D1_P2_High_T2_26_Mid/3.png", "Image/P29_D1_P2_High_T2_26_Mid/4.png", "Image/P29_D1_P2_High_T2_26_Mid/5.png", "Image/P29_D1_P2_High_T2_26_Mid/6.png", "Image/P29_D1_P2_High_T2_26_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2518.5345 seconds to 2530.7972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P4_Medium_T2_10_Mid", "images": ["Image/P15_D1_P4_Medium_T2_10_Mid/1.png", "Image/P15_D1_P4_Medium_T2_10_Mid/2.png", "Image/P15_D1_P4_Medium_T2_10_Mid/3.png", "Image/P15_D1_P4_Medium_T2_10_Mid/4.png", "Image/P15_D1_P4_Medium_T2_10_Mid/5.png", "Image/P15_D1_P4_Medium_T2_10_Mid/6.png", "Image/P15_D1_P4_Medium_T2_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1788.1332699999998 seconds to 1801.48247 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P7_High_T3_8_Mid", "images": ["Image/P22_D3_P7_High_T3_8_Mid/1.png", "Image/P22_D3_P7_High_T3_8_Mid/2.png", "Image/P22_D3_P7_High_T3_8_Mid/3.png", "Image/P22_D3_P7_High_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 524.2922 seconds to 533.3122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P7_Medium_T1_3_Right", "images": ["Image/P9_D2_P7_Medium_T1_3_Right/1.png", "Image/P9_D2_P7_Medium_T1_3_Right/2.png", "Image/P9_D2_P7_Medium_T1_3_Right/3.png", "Image/P9_D2_P7_Medium_T1_3_Right/4.png", "Image/P9_D2_P7_Medium_T1_3_Right/5.png", "Image/P9_D2_P7_Medium_T1_3_Right/6.png", "Image/P9_D2_P7_Medium_T1_3_Right/7.png", "Image/P9_D2_P7_Medium_T1_3_Right/8.png", "Image/P9_D2_P7_Medium_T1_3_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the right bottle on this top of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3275.7637999999997 seconds to 3292.8566 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P4_High_T1_22_Left", "images": ["Image/P2_D2_P4_High_T1_22_Left/1.png", "Image/P2_D2_P4_High_T1_22_Left/2.png", "Image/P2_D2_P4_High_T1_22_Left/3.png", "Image/P2_D2_P4_High_T1_22_Left/4.png", "Image/P2_D2_P4_High_T1_22_Left/5.png", "Image/P2_D2_P4_High_T1_22_Left/6.png", "Image/P2_D2_P4_High_T1_22_Left/7.png", "Image/P2_D2_P4_High_T1_22_Left/8.png", "Image/P2_D2_P4_High_T1_22_Left/9.png", "Image/P2_D2_P4_High_T1_22_Left/10.png", "Image/P2_D2_P4_High_T1_22_Left/11.png", "Image/P2_D2_P4_High_T1_22_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that plant to the desktop right there for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2941.1733999999997 seconds to 2958.4480999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P3_Medium_T2_8_Right", "images": ["Image/P21_D3_P3_Medium_T2_8_Right/1.png", "Image/P21_D3_P3_Medium_T2_8_Right/2.png", "Image/P21_D3_P3_Medium_T2_8_Right/3.png", "Image/P21_D3_P3_Medium_T2_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 696.766 seconds to 708.3878 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P3_Low_T3_21_Mid", "images": ["Image/P25_D3_P3_Low_T3_21_Mid/1.png", "Image/P25_D3_P3_Low_T3_21_Mid/2.png", "Image/P25_D3_P3_Low_T3_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1949.7352 seconds to 1957.7399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P1_Low_T1_2_Right", "images": ["Image/P28_D2_P1_Low_T1_2_Right/1.png", "Image/P28_D2_P1_Low_T1_2_Right/2.png", "Image/P28_D2_P1_Low_T1_2_Right/3.png", "Image/P28_D2_P1_Low_T1_2_Right/4.png", "Image/P28_D2_P1_Low_T1_2_Right/5.png", "Image/P28_D2_P1_Low_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that green vase to the very bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1147.6289 seconds to 1158.7072 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P4_Medium_T2_12_Mid", "images": ["Image/P13_D2_P4_Medium_T2_12_Mid/1.png", "Image/P13_D2_P4_Medium_T2_12_Mid/2.png", "Image/P13_D2_P4_Medium_T2_12_Mid/3.png", "Image/P13_D2_P4_Medium_T2_12_Mid/4.png", "Image/P13_D2_P4_Medium_T2_12_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the middle brown vase\"\nThe frames roughly correspond to the speech interval from 1228.1890600000002 seconds to 1238.80956 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P9_Low_T1_23_Right", "images": ["Image/P14_D2_P9_Low_T1_23_Right/1.png", "Image/P14_D2_P9_Low_T1_23_Right/2.png", "Image/P14_D2_P9_Low_T1_23_Right/3.png", "Image/P14_D2_P9_Low_T1_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant up to the top. Finish.\"\nThe frames roughly correspond to the speech interval from 1541.08525 seconds to 1549.8768799999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P5_High_T3_23_Mid", "images": ["Image/P8_D2_P5_High_T3_23_Mid/1.png", "Image/P8_D2_P5_High_T3_23_Mid/2.png", "Image/P8_D2_P5_High_T3_23_Mid/3.png", "Image/P8_D2_P5_High_T3_23_Mid/4.png", "Image/P8_D2_P5_High_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the planet? Finish.\"\nThe frames roughly correspond to the speech interval from 2445.8045 seconds to 2457.4973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P1_Low_T2_10_Right", "images": ["Image/P27_D2_P1_Low_T2_10_Right/1.png", "Image/P27_D2_P1_Low_T2_10_Right/2.png", "Image/P27_D2_P1_Low_T2_10_Right/3.png", "Image/P27_D2_P1_Low_T2_10_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You put water in that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1602.1799999999998 seconds to 1612.049 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P7_High_T2_25_Mid", "images": ["Image/P11_D1_P7_High_T2_25_Mid/1.png", "Image/P11_D1_P7_High_T2_25_Mid/2.png", "Image/P11_D1_P7_High_T2_25_Mid/3.png", "Image/P11_D1_P7_High_T2_25_Mid/4.png", "Image/P11_D1_P7_High_T2_25_Mid/5.png", "Image/P11_D1_P7_High_T2_25_Mid/6.png", "Image/P11_D1_P7_High_T2_25_Mid/7.png", "Image/P11_D1_P7_High_T2_25_Mid/8.png", "Image/P11_D1_P7_High_T2_25_Mid/9.png", "Image/P11_D1_P7_High_T2_25_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass in a basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2726.3068999999996 seconds to 2742.2142999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P6_Low_T2_24_Right", "images": ["Image/P27_D2_P6_Low_T2_24_Right/1.png", "Image/P27_D2_P6_Low_T2_24_Right/2.png", "Image/P27_D2_P6_Low_T2_24_Right/3.png", "Image/P27_D2_P6_Low_T2_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1298.03723 seconds to 1308.21828 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P2_Medium_T2_12_Mid", "images": ["Image/P5_D2_P2_Medium_T2_12_Mid/1.png", "Image/P5_D2_P2_Medium_T2_12_Mid/2.png", "Image/P5_D2_P2_Medium_T2_12_Mid/3.png", "Image/P5_D2_P2_Medium_T2_12_Mid/4.png", "Image/P5_D2_P2_Medium_T2_12_Mid/5.png", "Image/P5_D2_P2_Medium_T2_12_Mid/6.png", "Image/P5_D2_P2_Medium_T2_12_Mid/7.png", "Image/P5_D2_P2_Medium_T2_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. There are three wells in front of me. Pour water into the middle one. Finish.\"\nThe frames roughly correspond to the speech interval from 870.9526000000001 seconds to 884.8154999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P9_Low_T2_16_Left", "images": ["Image/P27_D3_P9_Low_T2_16_Left/1.png", "Image/P27_D3_P9_Low_T2_16_Left/2.png", "Image/P27_D3_P9_Low_T2_16_Left/3.png", "Image/P27_D3_P9_Low_T2_16_Left/4.png", "Image/P27_D3_P9_Low_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in that cup? Finished.\"\nThe frames roughly correspond to the speech interval from 475.4144 seconds to 487.62 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P1_High_T3_27_Mid", "images": ["Image/P1_D1_P1_High_T3_27_Mid/1.png", "Image/P1_D1_P1_High_T3_27_Mid/2.png", "Image/P1_D1_P1_High_T3_27_Mid/3.png", "Image/P1_D1_P1_High_T3_27_Mid/4.png", "Image/P1_D1_P1_High_T3_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 307.70410000000004 seconds to 317.8655 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P5_High_T1_7_Left", "images": ["Image/P9_D1_P5_High_T1_7_Left/1.png", "Image/P9_D1_P5_High_T1_7_Left/2.png", "Image/P9_D1_P5_High_T1_7_Left/3.png", "Image/P9_D1_P5_High_T1_7_Left/4.png", "Image/P9_D1_P5_High_T1_7_Left/5.png", "Image/P9_D1_P5_High_T1_7_Left/6.png", "Image/P9_D1_P5_High_T1_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to move this green cup from here to here? Finish.\"\nThe frames roughly correspond to the speech interval from 1689.7116 seconds to 1703.2222 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P6_Medium_T2_18_Left", "images": ["Image/P24_D3_P6_Medium_T2_18_Left/1.png", "Image/P24_D3_P6_Medium_T2_18_Left/2.png", "Image/P24_D3_P6_Medium_T2_18_Left/3.png", "Image/P24_D3_P6_Medium_T2_18_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1795.2547 seconds to 1805.2902 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P7_Medium_T2_15_Mid", "images": ["Image/P12_D3_P7_Medium_T2_15_Mid/1.png", "Image/P12_D3_P7_Medium_T2_15_Mid/2.png", "Image/P12_D3_P7_Medium_T2_15_Mid/3.png", "Image/P12_D3_P7_Medium_T2_15_Mid/4.png", "Image/P12_D3_P7_Medium_T2_15_Mid/5.png", "Image/P12_D3_P7_Medium_T2_15_Mid/6.png", "Image/P12_D3_P7_Medium_T2_15_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2610.31309 seconds to 2622.42249 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P3_Medium_T2_7_Right", "images": ["Image/P15_D1_P3_Medium_T2_7_Right/1.png", "Image/P15_D1_P3_Medium_T2_7_Right/2.png", "Image/P15_D1_P3_Medium_T2_7_Right/3.png", "Image/P15_D1_P3_Medium_T2_7_Right/4.png", "Image/P15_D1_P3_Medium_T2_7_Right/5.png", "Image/P15_D1_P3_Medium_T2_7_Right/6.png", "Image/P15_D1_P3_Medium_T2_7_Right/7.png", "Image/P15_D1_P3_Medium_T2_7_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1441.0002499999998 seconds to 1456.35191 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P2_Medium_T1_10_Left", "images": ["Image/P4_D3_P2_Medium_T1_10_Left/1.png", "Image/P4_D3_P2_Medium_T1_10_Left/2.png", "Image/P4_D3_P2_Medium_T1_10_Left/3.png", "Image/P4_D3_P2_Medium_T1_10_Left/4.png", "Image/P4_D3_P2_Medium_T1_10_Left/5.png", "Image/P4_D3_P2_Medium_T1_10_Left/6.png", "Image/P4_D3_P2_Medium_T1_10_Left/7.png", "Image/P4_D3_P2_Medium_T1_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the left bottle to the ground? Finish\"\nThe frames roughly correspond to the speech interval from 792.538 seconds to 807.9314999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P5_Low_T2_6_Right", "images": ["Image/P20_D2_P5_Low_T2_6_Right/1.png", "Image/P20_D2_P5_Low_T2_6_Right/2.png", "Image/P20_D2_P5_Low_T2_6_Right/3.png", "Image/P20_D2_P5_Low_T2_6_Right/4.png", "Image/P20_D2_P5_Low_T2_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 2833.4551 seconds to 2843.9499 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P1_High_T1_18_Left", "images": ["Image/P18_D3_P1_High_T1_18_Left/1.png", "Image/P18_D3_P1_High_T1_18_Left/2.png", "Image/P18_D3_P1_High_T1_18_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1486.1884 seconds to 1495.2229000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P9_Low_T3_23_Left", "images": ["Image/P25_D3_P9_Low_T3_23_Left/1.png", "Image/P25_D3_P9_Low_T3_23_Left/2.png", "Image/P25_D3_P9_Low_T3_23_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1964.2278 seconds to 1972.496 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P8_Low_T1_18_Left", "images": ["Image/P26_D3_P8_Low_T1_18_Left/1.png", "Image/P26_D3_P8_Low_T1_18_Left/2.png", "Image/P26_D3_P8_Low_T1_18_Left/3.png", "Image/P26_D3_P8_Low_T1_18_Left/4.png", "Image/P26_D3_P8_Low_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that vase to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 770.8202 seconds to 781.6356000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P5_Low_T3_5_Right", "images": ["Image/P14_D1_P5_Low_T3_5_Right/1.png", "Image/P14_D1_P5_Low_T3_5_Right/2.png", "Image/P14_D1_P5_Low_T3_5_Right/3.png", "Image/P14_D1_P5_Low_T3_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 212.6133 seconds to 221.8004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P6_Medium_T1_15_Mid", "images": ["Image/P22_D1_P6_Medium_T1_15_Mid/1.png", "Image/P22_D1_P6_Medium_T1_15_Mid/2.png", "Image/P22_D1_P6_Medium_T1_15_Mid/3.png", "Image/P22_D1_P6_Medium_T1_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move this plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1498.0747 seconds to 1509.3921 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P8_Low_T2_27_Left", "images": ["Image/P24_D2_P8_Low_T2_27_Left/1.png", "Image/P24_D2_P8_Low_T2_27_Left/2.png", "Image/P24_D2_P8_Low_T2_27_Left/3.png", "Image/P24_D2_P8_Low_T2_27_Left/4.png", "Image/P24_D2_P8_Low_T2_27_Left/5.png", "Image/P24_D2_P8_Low_T2_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water in this bottle? Please finish.\"\nThe frames roughly correspond to the speech interval from 442.1432999999999 seconds to 454.0175 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P2_Medium_T1_15_Mid", "images": ["Image/P19_D2_P2_Medium_T1_15_Mid/1.png", "Image/P19_D2_P2_Medium_T1_15_Mid/2.png", "Image/P19_D2_P2_Medium_T1_15_Mid/3.png", "Image/P19_D2_P2_Medium_T1_15_Mid/4.png", "Image/P19_D2_P2_Medium_T1_15_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this flowerpot down here.\"\nThe frames roughly correspond to the speech interval from 1928.6827 seconds to 1940.6668 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P3_High_T3_27_Right", "images": ["Image/P2_D2_P3_High_T3_27_Right/1.png", "Image/P2_D2_P3_High_T3_27_Right/2.png", "Image/P2_D2_P3_High_T3_27_Right/3.png", "Image/P2_D2_P3_High_T3_27_Right/4.png", "Image/P2_D2_P3_High_T3_27_Right/5.png", "Image/P2_D2_P3_High_T3_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Hello\"\nThe frames roughly correspond to the speech interval from 2489.88991 seconds to 2509.5028 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P2_High_T1_24_Left", "images": ["Image/P11_D2_P2_High_T1_24_Left/1.png", "Image/P11_D2_P2_High_T1_24_Left/2.png", "Image/P11_D2_P2_High_T1_24_Left/3.png", "Image/P11_D2_P2_High_T1_24_Left/4.png", "Image/P11_D2_P2_High_T1_24_Left/5.png", "Image/P11_D2_P2_High_T1_24_Left/6.png", "Image/P11_D2_P2_High_T1_24_Left/7.png", "Image/P11_D2_P2_High_T1_24_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please move the plant to the bottom of the shelf in the front\"\nThe frames roughly correspond to the speech interval from 513.16492 seconds to 526.9941 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P5_Medium_T3_19_Left", "images": ["Image/P27_D2_P5_Medium_T3_19_Left/1.png", "Image/P27_D2_P5_Medium_T3_19_Left/2.png", "Image/P27_D2_P5_Medium_T3_19_Left/3.png", "Image/P27_D2_P5_Medium_T3_19_Left/4.png", "Image/P27_D2_P5_Medium_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the vase on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1228.40423 seconds to 1238.75911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P8_High_T3_4_Mid", "images": ["Image/P29_D1_P8_High_T3_4_Mid/1.png", "Image/P29_D1_P8_High_T3_4_Mid/2.png", "Image/P29_D1_P8_High_T3_4_Mid/3.png", "Image/P29_D1_P8_High_T3_4_Mid/4.png", "Image/P29_D1_P8_High_T3_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2587.3538 seconds to 2597.6409999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P7_Medium_T3_19_Left", "images": ["Image/P23_D1_P7_Medium_T3_19_Left/1.png", "Image/P23_D1_P7_Medium_T3_19_Left/2.png", "Image/P23_D1_P7_Medium_T3_19_Left/3.png", "Image/P23_D1_P7_Medium_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1524.87919 seconds to 1535.0270799999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P1_Low_T2_23_Right", "images": ["Image/P20_D1_P1_Low_T2_23_Right/1.png", "Image/P20_D1_P1_Low_T2_23_Right/2.png", "Image/P20_D1_P1_Low_T2_23_Right/3.png", "Image/P20_D1_P1_Low_T2_23_Right/4.png", "Image/P20_D1_P1_Low_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 567.3194 seconds to 579.4726 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P7_Medium_T3_21_Left", "images": ["Image/P15_D1_P7_Medium_T3_21_Left/1.png", "Image/P15_D1_P7_Medium_T3_21_Left/2.png", "Image/P15_D1_P7_Medium_T3_21_Left/3.png", "Image/P15_D1_P7_Medium_T3_21_Left/4.png", "Image/P15_D1_P7_Medium_T3_21_Left/5.png", "Image/P15_D1_P7_Medium_T3_21_Left/6.png", "Image/P15_D1_P7_Medium_T3_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white pot at the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 1506.2844699999998 seconds to 1519.07467 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P8_High_T3_15_Mid", "images": ["Image/P10_D3_P8_High_T3_15_Mid/1.png", "Image/P10_D3_P8_High_T3_15_Mid/2.png", "Image/P10_D3_P8_High_T3_15_Mid/3.png", "Image/P10_D3_P8_High_T3_15_Mid/4.png", "Image/P10_D3_P8_High_T3_15_Mid/5.png", "Image/P10_D3_P8_High_T3_15_Mid/6.png", "Image/P10_D3_P8_High_T3_15_Mid/7.png", "Image/P10_D3_P8_High_T3_15_Mid/8.png", "Image/P10_D3_P8_High_T3_15_Mid/9.png", "Image/P10_D3_P8_High_T3_15_Mid/10.png", "Image/P10_D3_P8_High_T3_15_Mid/11.png", "Image/P10_D3_P8_High_T3_15_Mid/12.png", "Image/P10_D3_P8_High_T3_15_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 715.46769 seconds to 734.74629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P5_Low_T2_19_Left", "images": ["Image/P7_D1_P5_Low_T2_19_Left/1.png", "Image/P7_D1_P5_Low_T2_19_Left/2.png", "Image/P7_D1_P5_Low_T2_19_Left/3.png", "Image/P7_D1_P5_Low_T2_19_Left/4.png", "Image/P7_D1_P5_Low_T2_19_Left/5.png", "Image/P7_D1_P5_Low_T2_19_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 283.529 seconds to 297.5464 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P5_Medium_T1_27_Right", "images": ["Image/P12_D3_P5_Medium_T1_27_Right/1.png", "Image/P12_D3_P5_Medium_T1_27_Right/2.png", "Image/P12_D3_P5_Medium_T1_27_Right/3.png", "Image/P12_D3_P5_Medium_T1_27_Right/4.png", "Image/P12_D3_P5_Medium_T1_27_Right/5.png", "Image/P12_D3_P5_Medium_T1_27_Right/6.png", "Image/P12_D3_P5_Medium_T1_27_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2590.44939 seconds to 2603.53359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P1_Low_T3_15_Left", "images": ["Image/P14_D1_P1_Low_T3_15_Left/1.png", "Image/P14_D1_P1_Low_T3_15_Left/2.png", "Image/P14_D1_P1_Low_T3_15_Left/3.png", "Image/P14_D1_P1_Low_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 390.9698 seconds to 400.4906 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P1_Low_T2_19_Left", "images": ["Image/P5_D2_P1_Low_T2_19_Left/1.png", "Image/P5_D2_P1_Low_T2_19_Left/2.png", "Image/P5_D2_P1_Low_T2_19_Left/3.png", "Image/P5_D2_P1_Low_T2_19_Left/4.png", "Image/P5_D2_P1_Low_T2_19_Left/5.png", "Image/P5_D2_P1_Low_T2_19_Left/6.png", "Image/P5_D2_P1_Low_T2_19_Left/7.png", "Image/P5_D2_P1_Low_T2_19_Left/8.png", "Image/P5_D2_P1_Low_T2_19_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour water into the bottle in front of you on the top of the shelf. Finish\"\nThe frames roughly correspond to the speech interval from 67.86126999999999 seconds to 83.60336999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P3_Medium_T1_4_Left", "images": ["Image/P14_D2_P3_Medium_T1_4_Left/1.png", "Image/P14_D2_P3_Medium_T1_4_Left/2.png", "Image/P14_D2_P3_Medium_T1_4_Left/3.png", "Image/P14_D2_P3_Medium_T1_4_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1729.11 seconds to 1738.2865 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P5_High_T1_2_Mid", "images": ["Image/P17_D2_P5_High_T1_2_Mid/1.png", "Image/P17_D2_P5_High_T1_2_Mid/2.png", "Image/P17_D2_P5_High_T1_2_Mid/3.png", "Image/P17_D2_P5_High_T1_2_Mid/4.png", "Image/P17_D2_P5_High_T1_2_Mid/5.png", "Image/P17_D2_P5_High_T1_2_Mid/6.png", "Image/P17_D2_P5_High_T1_2_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that bottle to the top shelf over there? Finish.\"\nThe frames roughly correspond to the speech interval from 69.80489999999999 seconds to 83.12519999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P4_Low_T1_3_Mid", "images": ["Image/P25_D1_P4_Low_T1_3_Mid/1.png", "Image/P25_D1_P4_Low_T1_3_Mid/2.png", "Image/P25_D1_P4_Low_T1_3_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase over here. Finish.\"\nThe frames roughly correspond to the speech interval from 460.1056 seconds to 468.4009 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P9_High_T3_10_Right", "images": ["Image/P15_D3_P9_High_T3_10_Right/1.png", "Image/P15_D3_P9_High_T3_10_Right/2.png", "Image/P15_D3_P9_High_T3_10_Right/3.png", "Image/P15_D3_P9_High_T3_10_Right/4.png", "Image/P15_D3_P9_High_T3_10_Right/5.png", "Image/P15_D3_P9_High_T3_10_Right/6.png", "Image/P15_D3_P9_High_T3_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the pot at the lowest level of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 312.5095 seconds to 325.27200000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P7_High_T3_1_Left", "images": ["Image/P27_D3_P7_High_T3_1_Left/1.png", "Image/P27_D3_P7_High_T3_1_Left/2.png", "Image/P27_D3_P7_High_T3_1_Left/3.png", "Image/P27_D3_P7_High_T3_1_Left/4.png", "Image/P27_D3_P7_High_T3_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean that green glass? Finish\"\nThe frames roughly correspond to the speech interval from 495.1789 seconds to 505.5906 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P9_Medium_T3_4_Mid", "images": ["Image/P28_D1_P9_Medium_T3_4_Mid/1.png", "Image/P28_D1_P9_Medium_T3_4_Mid/2.png", "Image/P28_D1_P9_Medium_T3_4_Mid/3.png", "Image/P28_D1_P9_Medium_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1734.4295 seconds to 1743.1703 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P8_Low_T3_27_Left", "images": ["Image/P11_D3_P8_Low_T3_27_Left/1.png", "Image/P11_D3_P8_Low_T3_27_Left/2.png", "Image/P11_D3_P8_Low_T3_27_Left/3.png", "Image/P11_D3_P8_Low_T3_27_Left/4.png", "Image/P11_D3_P8_Low_T3_27_Left/5.png", "Image/P11_D3_P8_Low_T3_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the glass which is under the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1762.8597 seconds to 1774.65 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P2_Medium_T2_23_Left", "images": ["Image/P17_D3_P2_Medium_T2_23_Left/1.png", "Image/P17_D3_P2_Medium_T2_23_Left/2.png", "Image/P17_D3_P2_Medium_T2_23_Left/3.png", "Image/P17_D3_P2_Medium_T2_23_Left/4.png", "Image/P17_D3_P2_Medium_T2_23_Left/5.png", "Image/P17_D3_P2_Medium_T2_23_Left/6.png", "Image/P17_D3_P2_Medium_T2_23_Left/7.png", "Image/P17_D3_P2_Medium_T2_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the plant to the left on the shelf in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 1321.1093 seconds to 1339.205 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P4_Medium_T3_14_Mid", "images": ["Image/P23_D1_P4_Medium_T3_14_Mid/1.png", "Image/P23_D1_P4_Medium_T3_14_Mid/2.png", "Image/P23_D1_P4_Medium_T3_14_Mid/3.png", "Image/P23_D1_P4_Medium_T3_14_Mid/4.png", "Image/P23_D1_P4_Medium_T3_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1799.4223 seconds to 1809.6237 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P1_High_T1_21_Right", "images": ["Image/P24_D2_P1_High_T1_21_Right/1.png", "Image/P24_D2_P1_High_T1_21_Right/2.png", "Image/P24_D2_P1_High_T1_21_Right/3.png", "Image/P24_D2_P1_High_T1_21_Right/4.png", "Image/P24_D2_P1_High_T1_21_Right/5.png", "Image/P24_D2_P1_High_T1_21_Right/6.png", "Image/P24_D2_P1_High_T1_21_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle down to this area? Please finish.\"\nThe frames roughly correspond to the speech interval from 594.3862 seconds to 608.7508 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P4_High_T3_25_Right", "images": ["Image/P25_D1_P4_High_T3_25_Right/1.png", "Image/P25_D1_P4_High_T3_25_Right/2.png", "Image/P25_D1_P4_High_T3_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 124.2046 seconds to 133.0842 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P5_Medium_T3_16_Left", "images": ["Image/P21_D3_P5_Medium_T3_16_Left/1.png", "Image/P21_D3_P5_Medium_T3_16_Left/2.png", "Image/P21_D3_P5_Medium_T3_16_Left/3.png", "Image/P21_D3_P5_Medium_T3_16_Left/4.png", "Image/P21_D3_P5_Medium_T3_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 600.9275 seconds to 613.1345 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P9_Low_T3_3_Mid", "images": ["Image/P16_D2_P9_Low_T3_3_Mid/1.png", "Image/P16_D2_P9_Low_T3_3_Mid/2.png", "Image/P16_D2_P9_Low_T3_3_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 625.72032 seconds to 636.8278200000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P8_High_T3_11_Right", "images": ["Image/P11_D2_P8_High_T3_11_Right/1.png", "Image/P11_D2_P8_High_T3_11_Right/2.png", "Image/P11_D2_P8_High_T3_11_Right/3.png", "Image/P11_D2_P8_High_T3_11_Right/4.png", "Image/P11_D2_P8_High_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the right glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1078.5377 seconds to 1090.0908 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P6_Low_T2_13_Right", "images": ["Image/P24_D3_P6_Low_T2_13_Right/1.png", "Image/P24_D3_P6_Low_T2_13_Right/2.png", "Image/P24_D3_P6_Low_T2_13_Right/3.png", "Image/P24_D3_P6_Low_T2_13_Right/4.png", "Image/P24_D3_P6_Low_T2_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1907.31 seconds to 1917.7212 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P4_High_T1_7_Right", "images": ["Image/P13_D2_P4_High_T1_7_Right/1.png", "Image/P13_D2_P4_High_T1_7_Right/2.png", "Image/P13_D2_P4_High_T1_7_Right/3.png", "Image/P13_D2_P4_High_T1_7_Right/4.png", "Image/P13_D2_P4_High_T1_7_Right/5.png", "Image/P13_D2_P4_High_T1_7_Right/6.png", "Image/P13_D2_P4_High_T1_7_Right/7.png", "Image/P13_D2_P4_High_T1_7_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the green glass from over there to this shelf right here\"\nThe frames roughly correspond to the speech interval from 977.63046 seconds to 990.93746 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P5_Medium_T1_10_Right", "images": ["Image/P26_D2_P5_Medium_T1_10_Right/1.png", "Image/P26_D2_P5_Medium_T1_10_Right/2.png", "Image/P26_D2_P5_Medium_T1_10_Right/3.png", "Image/P26_D2_P5_Medium_T1_10_Right/4.png", "Image/P26_D2_P5_Medium_T1_10_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move this space on the right over here? Finish.\"\nThe frames roughly correspond to the speech interval from 1472.7349 seconds to 1483.2149 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P6_High_T2_6_Mid", "images": ["Image/P5_D2_P6_High_T2_6_Mid/1.png", "Image/P5_D2_P6_High_T2_6_Mid/2.png", "Image/P5_D2_P6_High_T2_6_Mid/3.png", "Image/P5_D2_P6_High_T2_6_Mid/4.png", "Image/P5_D2_P6_High_T2_6_Mid/5.png", "Image/P5_D2_P6_High_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant to the right-hand side of you. Finish.\"\nThe frames roughly correspond to the speech interval from 914.4311 seconds to 926.5958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P1_High_T2_23_Right", "images": ["Image/P28_D3_P1_High_T2_23_Right/1.png", "Image/P28_D3_P1_High_T2_23_Right/2.png", "Image/P28_D3_P1_High_T2_23_Right/3.png", "Image/P28_D3_P1_High_T2_23_Right/4.png", "Image/P28_D3_P1_High_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into that plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 579.873 seconds to 591.0483 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P7_Low_T1_22_Mid", "images": ["Image/P20_D1_P7_Low_T1_22_Mid/1.png", "Image/P20_D1_P7_Low_T1_22_Mid/2.png", "Image/P20_D1_P7_Low_T1_22_Mid/3.png", "Image/P20_D1_P7_Low_T1_22_Mid/4.png", "Image/P20_D1_P7_Low_T1_22_Mid/5.png", "Image/P20_D1_P7_Low_T1_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant on the floor over here. Finish.\"\nThe frames roughly correspond to the speech interval from 461.3332 seconds to 474.0833 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P2_High_T2_11_Right", "images": ["Image/P21_D3_P2_High_T2_11_Right/1.png", "Image/P21_D3_P2_High_T2_11_Right/2.png", "Image/P21_D3_P2_High_T2_11_Right/3.png", "Image/P21_D3_P2_High_T2_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 422.235 seconds to 435.3756 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P5_Medium_T1_8_Right", "images": ["Image/P14_D3_P5_Medium_T1_8_Right/1.png", "Image/P14_D3_P5_Medium_T1_8_Right/2.png", "Image/P14_D3_P5_Medium_T1_8_Right/3.png", "Image/P14_D3_P5_Medium_T1_8_Right/4.png", "Image/P14_D3_P5_Medium_T1_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1307.11776 seconds to 1317.30956 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P7_Medium_T2_18_Mid", "images": ["Image/P25_D1_P7_Medium_T2_18_Mid/1.png", "Image/P25_D1_P7_Medium_T2_18_Mid/2.png", "Image/P25_D1_P7_Medium_T2_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 567.3451 seconds to 575.7379 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P9_Medium_T1_18_Right", "images": ["Image/P2_D3_P9_Medium_T1_18_Right/1.png", "Image/P2_D3_P9_Medium_T1_18_Right/2.png", "Image/P2_D3_P9_Medium_T1_18_Right/3.png", "Image/P2_D3_P9_Medium_T1_18_Right/4.png", "Image/P2_D3_P9_Medium_T1_18_Right/5.png", "Image/P2_D3_P9_Medium_T1_18_Right/6.png", "Image/P2_D3_P9_Medium_T1_18_Right/7.png", "Image/P2_D3_P9_Medium_T1_18_Right/8.png", "Image/P2_D3_P9_Medium_T1_18_Right/9.png", "Image/P2_D3_P9_Medium_T1_18_Right/10.png", "Image/P2_D3_P9_Medium_T1_18_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start? Hi\"\nThe frames roughly correspond to the speech interval from 1710.7744700000003 seconds to 1727.6851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P6_Medium_T1_13_Mid", "images": ["Image/P2_D3_P6_Medium_T1_13_Mid/1.png", "Image/P2_D3_P6_Medium_T1_13_Mid/2.png", "Image/P2_D3_P6_Medium_T1_13_Mid/3.png", "Image/P2_D3_P6_Medium_T1_13_Mid/4.png", "Image/P2_D3_P6_Medium_T1_13_Mid/5.png", "Image/P2_D3_P6_Medium_T1_13_Mid/6.png", "Image/P2_D3_P6_Medium_T1_13_Mid/7.png", "Image/P2_D3_P6_Medium_T1_13_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi\"\nThe frames roughly correspond to the speech interval from 1687.2218500000004 seconds to 1702.1430900000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P3_Low_T2_16_Mid", "images": ["Image/P9_D2_P3_Low_T2_16_Mid/1.png", "Image/P9_D2_P3_Low_T2_16_Mid/2.png", "Image/P9_D2_P3_Low_T2_16_Mid/3.png", "Image/P9_D2_P3_Low_T2_16_Mid/4.png", "Image/P9_D2_P3_Low_T2_16_Mid/5.png", "Image/P9_D2_P3_Low_T2_16_Mid/6.png", "Image/P9_D2_P3_Low_T2_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water on that cup on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 2969.0244 seconds to 2982.0773 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P7_High_T3_5_Left", "images": ["Image/P17_D2_P7_High_T3_5_Left/1.png", "Image/P17_D2_P7_High_T3_5_Left/2.png", "Image/P17_D2_P7_High_T3_5_Left/3.png", "Image/P17_D2_P7_High_T3_5_Left/4.png", "Image/P17_D2_P7_High_T3_5_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this green plant down there? Finish.\"\nThe frames roughly correspond to the speech interval from 655.6932 seconds to 671.6448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P8_Low_T1_9_Left", "images": ["Image/P24_D3_P8_Low_T1_9_Left/1.png", "Image/P24_D3_P8_Low_T1_9_Left/2.png", "Image/P24_D3_P8_Low_T1_9_Left/3.png", "Image/P24_D3_P8_Low_T1_9_Left/4.png", "Image/P24_D3_P8_Low_T1_9_Left/5.png", "Image/P24_D3_P8_Low_T1_9_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1736.2901 seconds to 1747.7597 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P2_Low_T3_2_Mid", "images": ["Image/P23_D3_P2_Low_T3_2_Mid/1.png", "Image/P23_D3_P2_Low_T3_2_Mid/2.png", "Image/P23_D3_P2_Low_T3_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the waste. Finish.\"\nThe frames roughly correspond to the speech interval from 948.2088 seconds to 956.2265 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P8_High_T3_12_Left", "images": ["Image/P22_D1_P8_High_T3_12_Left/1.png", "Image/P22_D1_P8_High_T3_12_Left/2.png", "Image/P22_D1_P8_High_T3_12_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Clean the vase. Finish\"\nThe frames roughly correspond to the speech interval from 1654.1591 seconds to 1662.9716 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P2_High_T2_16_Right", "images": ["Image/P28_D3_P2_High_T2_16_Right/1.png", "Image/P28_D3_P2_High_T2_16_Right/2.png", "Image/P28_D3_P2_High_T2_16_Right/3.png", "Image/P28_D3_P2_High_T2_16_Right/4.png", "Image/P28_D3_P2_High_T2_16_Right/5.png", "Image/P28_D3_P2_High_T2_16_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 255.0072 seconds to 266.8675 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P6_Low_T1_21_Right", "images": ["Image/P5_D1_P6_Low_T1_21_Right/1.png", "Image/P5_D1_P6_Low_T1_21_Right/2.png", "Image/P5_D1_P6_Low_T1_21_Right/3.png", "Image/P5_D1_P6_Low_T1_21_Right/4.png", "Image/P5_D1_P6_Low_T1_21_Right/5.png", "Image/P5_D1_P6_Low_T1_21_Right/6.png", "Image/P5_D1_P6_Low_T1_21_Right/7.png", "Image/P5_D1_P6_Low_T1_21_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move the bottle to my right hand side\"\nThe frames roughly correspond to the speech interval from 1602.1903 seconds to 1615.3831 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P8_Medium_T1_6_Left", "images": ["Image/P5_D2_P8_Medium_T1_6_Left/1.png", "Image/P5_D2_P8_Medium_T1_6_Left/2.png", "Image/P5_D2_P8_Medium_T1_6_Left/3.png", "Image/P5_D2_P8_Medium_T1_6_Left/4.png", "Image/P5_D2_P8_Medium_T1_6_Left/5.png", "Image/P5_D2_P8_Medium_T1_6_Left/6.png", "Image/P5_D2_P8_Medium_T1_6_Left/7.png", "Image/P5_D2_P8_Medium_T1_6_Left/8.png", "Image/P5_D2_P8_Medium_T1_6_Left/9.png", "Image/P5_D2_P8_Medium_T1_6_Left/10.png", "Image/P5_D2_P8_Medium_T1_6_Left/11.png", "Image/P5_D2_P8_Medium_T1_6_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the land-most plant to the shelf that's floating and attached to the wall in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 730.5908 seconds to 749.4366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P8_Medium_T2_13_Mid", "images": ["Image/P16_D2_P8_Medium_T2_13_Mid/1.png", "Image/P16_D2_P8_Medium_T2_13_Mid/2.png", "Image/P16_D2_P8_Medium_T2_13_Mid/3.png", "Image/P16_D2_P8_Medium_T2_13_Mid/4.png", "Image/P16_D2_P8_Medium_T2_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 742.2987 seconds to 753.8799 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P5_Medium_T2_21_Right", "images": ["Image/P16_D1_P5_Medium_T2_21_Right/1.png", "Image/P16_D1_P5_Medium_T2_21_Right/2.png", "Image/P16_D1_P5_Medium_T2_21_Right/3.png", "Image/P16_D1_P5_Medium_T2_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water into this vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1468.7023 seconds to 1478.9609 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P2_Medium_T1_12_Right", "images": ["Image/P1_D1_P2_Medium_T1_12_Right/1.png", "Image/P1_D1_P2_Medium_T1_12_Right/2.png", "Image/P1_D1_P2_Medium_T1_12_Right/3.png", "Image/P1_D1_P2_Medium_T1_12_Right/4.png", "Image/P1_D1_P2_Medium_T1_12_Right/5.png", "Image/P1_D1_P2_Medium_T1_12_Right/6.png", "Image/P1_D1_P2_Medium_T1_12_Right/7.png", "Image/P1_D1_P2_Medium_T1_12_Right/8.png", "Image/P1_D1_P2_Medium_T1_12_Right/9.png", "Image/P1_D1_P2_Medium_T1_12_Right/10.png", "Image/P1_D1_P2_Medium_T1_12_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this to there? Finish.\"\nThe frames roughly correspond to the speech interval from 117.98908 seconds to 133.97057999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P8_High_T3_12_Mid", "images": ["Image/P29_D3_P8_High_T3_12_Mid/1.png", "Image/P29_D3_P8_High_T3_12_Mid/2.png", "Image/P29_D3_P8_High_T3_12_Mid/3.png", "Image/P29_D3_P8_High_T3_12_Mid/4.png", "Image/P29_D3_P8_High_T3_12_Mid/5.png", "Image/P29_D3_P8_High_T3_12_Mid/6.png", "Image/P29_D3_P8_High_T3_12_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 1294.52796 seconds to 1307.22338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P8_Low_T1_12_Mid", "images": ["Image/P22_D3_P8_Low_T1_12_Mid/1.png", "Image/P22_D3_P8_Low_T1_12_Mid/2.png", "Image/P22_D3_P8_Low_T1_12_Mid/3.png", "Image/P22_D3_P8_Low_T1_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 193.8747 seconds to 203.6891 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P8_Low_T1_14_Mid", "images": ["Image/P19_D2_P8_Low_T1_14_Mid/1.png", "Image/P19_D2_P8_Low_T1_14_Mid/2.png", "Image/P19_D2_P8_Low_T1_14_Mid/3.png", "Image/P19_D2_P8_Low_T1_14_Mid/4.png", "Image/P19_D2_P8_Low_T1_14_Mid/5.png", "Image/P19_D2_P8_Low_T1_14_Mid/6.png", "Image/P19_D2_P8_Low_T1_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this flowerpot to this shelf here. Finish.\"\nThe frames roughly correspond to the speech interval from 1543.1984 seconds to 1556.5201 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P5_Low_T1_16_Mid", "images": ["Image/P16_D1_P5_Low_T1_16_Mid/1.png", "Image/P16_D1_P5_Low_T1_16_Mid/2.png", "Image/P16_D1_P5_Low_T1_16_Mid/3.png", "Image/P16_D1_P5_Low_T1_16_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this mug over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1356.3823 seconds to 1368.1164 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P9_Medium_T2_5_Right", "images": ["Image/P22_D2_P9_Medium_T2_5_Right/1.png", "Image/P22_D2_P9_Medium_T2_5_Right/2.png", "Image/P22_D2_P9_Medium_T2_5_Right/3.png", "Image/P22_D2_P9_Medium_T2_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 985.1373999999998 seconds to 994.5748 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P6_High_T2_26_Mid", "images": ["Image/P23_D1_P6_High_T2_26_Mid/1.png", "Image/P23_D1_P6_High_T2_26_Mid/2.png", "Image/P23_D1_P6_High_T2_26_Mid/3.png", "Image/P23_D1_P6_High_T2_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1669.6743 seconds to 1679.697 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P1_Low_T3_10_Left", "images": ["Image/P9_D2_P1_Low_T3_10_Left/1.png", "Image/P9_D2_P1_Low_T3_10_Left/2.png", "Image/P9_D2_P1_Low_T3_10_Left/3.png", "Image/P9_D2_P1_Low_T3_10_Left/4.png", "Image/P9_D2_P1_Low_T3_10_Left/5.png", "Image/P9_D2_P1_Low_T3_10_Left/6.png", "Image/P9_D2_P1_Low_T3_10_Left/7.png", "Image/P9_D2_P1_Low_T3_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the bottle on the top of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3107.1540999999997 seconds to 3121.847 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P8_Low_T2_7_Mid", "images": ["Image/P30_D2_P8_Low_T2_7_Mid/1.png", "Image/P30_D2_P8_Low_T2_7_Mid/2.png", "Image/P30_D2_P8_Low_T2_7_Mid/3.png", "Image/P30_D2_P8_Low_T2_7_Mid/4.png", "Image/P30_D2_P8_Low_T2_7_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 259.1097 seconds to 271.03920000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P2_Low_T3_14_Right", "images": ["Image/P27_D3_P2_Low_T3_14_Right/1.png", "Image/P27_D3_P2_Low_T3_14_Right/2.png", "Image/P27_D3_P2_Low_T3_14_Right/3.png", "Image/P27_D3_P2_Low_T3_14_Right/4.png", "Image/P27_D3_P2_Low_T3_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean that plant? Finish\"\nThe frames roughly correspond to the speech interval from 374.0489 seconds to 384.2365 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P9_Low_T3_12_Left", "images": ["Image/P25_D2_P9_Low_T3_12_Left/1.png", "Image/P25_D2_P9_Low_T3_12_Left/2.png", "Image/P25_D2_P9_Low_T3_12_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 675.96414 seconds to 684.10672 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P2_Low_T2_2_Mid", "images": ["Image/P18_D3_P2_Low_T2_2_Mid/1.png", "Image/P18_D3_P2_Low_T2_2_Mid/2.png", "Image/P18_D3_P2_Low_T2_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1287.7179 seconds to 1296.403 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P9_Medium_T3_22_Mid", "images": ["Image/P25_D1_P9_Medium_T3_22_Mid/1.png", "Image/P25_D1_P9_Medium_T3_22_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 161.0775 seconds to 168.8311 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P6_Medium_T3_17_Right", "images": ["Image/P19_D3_P6_Medium_T3_17_Right/1.png", "Image/P19_D3_P6_Medium_T3_17_Right/2.png", "Image/P19_D3_P6_Medium_T3_17_Right/3.png", "Image/P19_D3_P6_Medium_T3_17_Right/4.png", "Image/P19_D3_P6_Medium_T3_17_Right/5.png", "Image/P19_D3_P6_Medium_T3_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 2296.097 seconds to 2309.9732 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P2_High_T1_4_Mid", "images": ["Image/P14_D1_P2_High_T1_4_Mid/1.png", "Image/P14_D1_P2_High_T1_4_Mid/2.png", "Image/P14_D1_P2_High_T1_4_Mid/3.png", "Image/P14_D1_P2_High_T1_4_Mid/4.png", "Image/P14_D1_P2_High_T1_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move that plant down to the basket\"\nThe frames roughly correspond to the speech interval from 88.79669000000001 seconds to 99.68151 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P7_Medium_T2_26_Right", "images": ["Image/P8_D3_P7_Medium_T2_26_Right/1.png", "Image/P8_D3_P7_Medium_T2_26_Right/2.png", "Image/P8_D3_P7_Medium_T2_26_Right/3.png", "Image/P8_D3_P7_Medium_T2_26_Right/4.png", "Image/P8_D3_P7_Medium_T2_26_Right/5.png", "Image/P8_D3_P7_Medium_T2_26_Right/6.png", "Image/P8_D3_P7_Medium_T2_26_Right/7.png", "Image/P8_D3_P7_Medium_T2_26_Right/8.png", "Image/P8_D3_P7_Medium_T2_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you pour some water to the white glass? That is on the right. That is on the right. Yeah. Finish.\"\nThe frames roughly correspond to the speech interval from 1942.6384 seconds to 1957.9784 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P1_Medium_T1_6_Mid", "images": ["Image/P22_D3_P1_Medium_T1_6_Mid/1.png", "Image/P22_D3_P1_Medium_T1_6_Mid/2.png", "Image/P22_D3_P1_Medium_T1_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move the middle plant under the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 421.2989 seconds to 431.8501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P4_Low_T3_19_Mid", "images": ["Image/P16_D2_P4_Low_T3_19_Mid/1.png", "Image/P16_D2_P4_Low_T3_19_Mid/2.png", "Image/P16_D2_P4_Low_T3_19_Mid/3.png", "Image/P16_D2_P4_Low_T3_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1060.4937 seconds to 1071.0179 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P4_Medium_T2_11_Right", "images": ["Image/P25_D3_P4_Medium_T2_11_Right/1.png", "Image/P25_D3_P4_Medium_T2_11_Right/2.png", "Image/P25_D3_P4_Medium_T2_11_Right/3.png", "Image/P25_D3_P4_Medium_T2_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1979.3606 seconds to 1988.0493 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P1_High_T2_22_Left", "images": ["Image/P19_D2_P1_High_T2_22_Left/1.png", "Image/P19_D2_P1_High_T2_22_Left/2.png", "Image/P19_D2_P1_High_T2_22_Left/3.png", "Image/P19_D2_P1_High_T2_22_Left/4.png", "Image/P19_D2_P1_High_T2_22_Left/5.png", "Image/P19_D2_P1_High_T2_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please pour into this flowerpot here. Okay. Okay\"\nThe frames roughly correspond to the speech interval from 1380.29481 seconds to 1395.2426799999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P9_High_T2_22_Right", "images": ["Image/P17_D3_P9_High_T2_22_Right/1.png", "Image/P17_D3_P9_High_T2_22_Right/2.png", "Image/P17_D3_P9_High_T2_22_Right/3.png", "Image/P17_D3_P9_High_T2_22_Right/4.png", "Image/P17_D3_P9_High_T2_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in the plant on the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 926.7443 seconds to 938.2127 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P2_Low_T3_1_Left", "images": ["Image/P6_D1_P2_Low_T3_1_Left/1.png", "Image/P6_D1_P2_Low_T3_1_Left/2.png", "Image/P6_D1_P2_Low_T3_1_Left/3.png", "Image/P6_D1_P2_Low_T3_1_Left/4.png", "Image/P6_D1_P2_Low_T3_1_Left/5.png", "Image/P6_D1_P2_Low_T3_1_Left/6.png", "Image/P6_D1_P2_Low_T3_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2170.954 seconds to 2183.9794 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P3_Medium_T2_25_Right", "images": ["Image/P5_D2_P3_Medium_T2_25_Right/1.png", "Image/P5_D2_P3_Medium_T2_25_Right/2.png", "Image/P5_D2_P3_Medium_T2_25_Right/3.png", "Image/P5_D2_P3_Medium_T2_25_Right/4.png", "Image/P5_D2_P3_Medium_T2_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Pour water into the cup\"\nThe frames roughly correspond to the speech interval from 647.2535 seconds to 660.6975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P2_High_T2_1_Right", "images": ["Image/P16_D1_P2_High_T2_1_Right/1.png", "Image/P16_D1_P2_High_T2_1_Right/2.png", "Image/P16_D1_P2_High_T2_1_Right/3.png", "Image/P16_D1_P2_High_T2_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into this mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1673.5343 seconds to 1685.1423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P8_High_T2_17_Mid", "images": ["Image/P21_D3_P8_High_T2_17_Mid/1.png", "Image/P21_D3_P8_High_T2_17_Mid/2.png", "Image/P21_D3_P8_High_T2_17_Mid/3.png", "Image/P21_D3_P8_High_T2_17_Mid/4.png", "Image/P21_D3_P8_High_T2_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start fill that middle mug with water finish.\"\nThe frames roughly correspond to the speech interval from 232.42369 seconds to 243.14095 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P5_High_T3_6_Mid", "images": ["Image/P16_D1_P5_High_T3_6_Mid/1.png", "Image/P16_D1_P5_High_T3_6_Mid/2.png", "Image/P16_D1_P5_High_T3_6_Mid/3.png", "Image/P16_D1_P5_High_T3_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1333.3451 seconds to 1343.2561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P6_High_T1_13_Mid", "images": ["Image/P30_D2_P6_High_T1_13_Mid/1.png", "Image/P30_D2_P6_High_T1_13_Mid/2.png", "Image/P30_D2_P6_High_T1_13_Mid/3.png", "Image/P30_D2_P6_High_T1_13_Mid/4.png", "Image/P30_D2_P6_High_T1_13_Mid/5.png", "Image/P30_D2_P6_High_T1_13_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that object over here. Finish.\"\nThe frames roughly correspond to the speech interval from 163.1884 seconds to 175.1029 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P5_Medium_T3_21_Right", "images": ["Image/P26_D3_P5_Medium_T3_21_Right/1.png", "Image/P26_D3_P5_Medium_T3_21_Right/2.png", "Image/P26_D3_P5_Medium_T3_21_Right/3.png", "Image/P26_D3_P5_Medium_T3_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 828.2682 seconds to 837.9969000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D2_P1_Low_T2_12_Left", "images": ["Image/P12_D2_P1_Low_T2_12_Left/1.png", "Image/P12_D2_P1_Low_T2_12_Left/2.png", "Image/P12_D2_P1_Low_T2_12_Left/3.png", "Image/P12_D2_P1_Low_T2_12_Left/4.png", "Image/P12_D2_P1_Low_T2_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 713.4984999999999 seconds to 724.7876 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P2_Medium_T1_16_Mid", "images": ["Image/P29_D1_P2_Medium_T1_16_Mid/1.png", "Image/P29_D1_P2_Medium_T1_16_Mid/2.png", "Image/P29_D1_P2_Medium_T1_16_Mid/3.png", "Image/P29_D1_P2_Medium_T1_16_Mid/4.png", "Image/P29_D1_P2_Medium_T1_16_Mid/5.png", "Image/P29_D1_P2_Medium_T1_16_Mid/6.png", "Image/P29_D1_P2_Medium_T1_16_Mid/7.png", "Image/P29_D1_P2_Medium_T1_16_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2499.6578999999997 seconds to 2512.8817 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P1_High_T2_19_Right", "images": ["Image/P25_D1_P1_High_T2_19_Right/1.png", "Image/P25_D1_P1_High_T2_19_Right/2.png", "Image/P25_D1_P1_High_T2_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 101.9327 seconds to 110.35346 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P4_Medium_T3_25_Mid", "images": ["Image/P4_D3_P4_Medium_T3_25_Mid/1.png", "Image/P4_D3_P4_Medium_T3_25_Mid/2.png", "Image/P4_D3_P4_Medium_T3_25_Mid/3.png", "Image/P4_D3_P4_Medium_T3_25_Mid/4.png", "Image/P4_D3_P4_Medium_T3_25_Mid/5.png", "Image/P4_D3_P4_Medium_T3_25_Mid/6.png", "Image/P4_D3_P4_Medium_T3_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the photo in the middle on the desk? Finished.\"\nThe frames roughly correspond to the speech interval from 271.4942299999999 seconds to 286.6802599999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P1_Medium_T3_3_Mid", "images": ["Image/P7_D3_P1_Medium_T3_3_Mid/1.png", "Image/P7_D3_P1_Medium_T3_3_Mid/2.png", "Image/P7_D3_P1_Medium_T3_3_Mid/3.png", "Image/P7_D3_P1_Medium_T3_3_Mid/4.png", "Image/P7_D3_P1_Medium_T3_3_Mid/5.png", "Image/P7_D3_P1_Medium_T3_3_Mid/6.png", "Image/P7_D3_P1_Medium_T3_3_Mid/7.png", "Image/P7_D3_P1_Medium_T3_3_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2076.4193 seconds to 2090.5618 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P9_High_T3_19_Right", "images": ["Image/P7_D3_P9_High_T3_19_Right/1.png", "Image/P7_D3_P9_High_T3_19_Right/2.png", "Image/P7_D3_P9_High_T3_19_Right/3.png", "Image/P7_D3_P9_High_T3_19_Right/4.png", "Image/P7_D3_P9_High_T3_19_Right/5.png", "Image/P7_D3_P9_High_T3_19_Right/6.png", "Image/P7_D3_P9_High_T3_19_Right/7.png", "Image/P7_D3_P9_High_T3_19_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2207.9766 seconds to 2223.7887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
