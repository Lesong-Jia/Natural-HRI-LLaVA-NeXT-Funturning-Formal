{"id": "P30_D1_P9_Low_T1_16_Left", "images": ["Image/P30_D1_P9_Low_T1_16_Left/1.png", "Image/P30_D1_P9_Low_T1_16_Left/2.png", "Image/P30_D1_P9_Low_T1_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move that cup up here. Finish.\"\nThe frames roughly correspond to the speech interval from 961.0777 seconds to 969.6383 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P2_High_T2_23_Right", "images": ["Image/P25_D1_P2_High_T2_23_Right/1.png", "Image/P25_D1_P2_High_T2_23_Right/2.png", "Image/P25_D1_P2_High_T2_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 139.93380000000002 seconds to 148.4249 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P3_Medium_T1_14_Right", "images": ["Image/P10_D1_P3_Medium_T1_14_Right/1.png", "Image/P10_D1_P3_Medium_T1_14_Right/2.png", "Image/P10_D1_P3_Medium_T1_14_Right/3.png", "Image/P10_D1_P3_Medium_T1_14_Right/4.png", "Image/P10_D1_P3_Medium_T1_14_Right/5.png", "Image/P10_D1_P3_Medium_T1_14_Right/6.png", "Image/P10_D1_P3_Medium_T1_14_Right/7.png", "Image/P10_D1_P3_Medium_T1_14_Right/8.png", "Image/P10_D1_P3_Medium_T1_14_Right/9.png", "Image/P10_D1_P3_Medium_T1_14_Right/10.png", "Image/P10_D1_P3_Medium_T1_14_Right/11.png", "Image/P10_D1_P3_Medium_T1_14_Right/12.png", "Image/P10_D1_P3_Medium_T1_14_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2488.08 seconds to 2508.3196 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P7_Low_T1_27_Right", "images": ["Image/P2_D1_P7_Low_T1_27_Right/1.png", "Image/P2_D1_P7_Low_T1_27_Right/2.png", "Image/P2_D1_P7_Low_T1_27_Right/3.png", "Image/P2_D1_P7_Low_T1_27_Right/4.png", "Image/P2_D1_P7_Low_T1_27_Right/5.png", "Image/P2_D1_P7_Low_T1_27_Right/6.png", "Image/P2_D1_P7_Low_T1_27_Right/7.png", "Image/P2_D1_P7_Low_T1_27_Right/8.png", "Image/P2_D1_P7_Low_T1_27_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 517.3294 seconds to 532.6386 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P8_Medium_T3_13_Mid", "images": ["Image/P18_D1_P8_Medium_T3_13_Mid/1.png", "Image/P18_D1_P8_Medium_T3_13_Mid/2.png", "Image/P18_D1_P8_Medium_T3_13_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 678.10294 seconds to 688.7110700000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P8_High_T3_14_Left", "images": ["Image/P7_D2_P8_High_T3_14_Left/1.png", "Image/P7_D2_P8_High_T3_14_Left/2.png", "Image/P7_D2_P8_High_T3_14_Left/3.png", "Image/P7_D2_P8_High_T3_14_Left/4.png", "Image/P7_D2_P8_High_T3_14_Left/5.png", "Image/P7_D2_P8_High_T3_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1014.0293 seconds to 1026.7924000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P7_Low_T1_13_Right", "images": ["Image/P25_D1_P7_Low_T1_13_Right/1.png", "Image/P25_D1_P7_Low_T1_13_Right/2.png", "Image/P25_D1_P7_Low_T1_13_Right/3.png", "Image/P25_D1_P7_Low_T1_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 383.3367 seconds to 392.106 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P7_Medium_T2_9_Mid", "images": ["Image/P26_D2_P7_Medium_T2_9_Mid/1.png", "Image/P26_D2_P7_Medium_T2_9_Mid/2.png", "Image/P26_D2_P7_Medium_T2_9_Mid/3.png", "Image/P26_D2_P7_Medium_T2_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour water into the middle vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1508.7167 seconds to 1521.3847999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P6_Low_T2_12_Right", "images": ["Image/P27_D3_P6_Low_T2_12_Right/1.png", "Image/P27_D3_P6_Low_T2_12_Right/2.png", "Image/P27_D3_P6_Low_T2_12_Right/3.png", "Image/P27_D3_P6_Low_T2_12_Right/4.png", "Image/P27_D3_P6_Low_T2_12_Right/5.png", "Image/P27_D3_P6_Low_T2_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water into the red vase in there? Finished.\"\nThe frames roughly correspond to the speech interval from 99.56209000000004 seconds to 112.24308000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P4_Low_T2_19_Mid", "images": ["Image/P26_D3_P4_Low_T2_19_Mid/1.png", "Image/P26_D3_P4_Low_T2_19_Mid/2.png", "Image/P26_D3_P4_Low_T2_19_Mid/3.png", "Image/P26_D3_P4_Low_T2_19_Mid/4.png", "Image/P26_D3_P4_Low_T2_19_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put water into that vase on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 807.9587 seconds to 821.9762000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P1_High_T3_10_Left", "images": ["Image/P25_D2_P1_High_T3_10_Left/1.png", "Image/P25_D2_P1_High_T3_10_Left/2.png", "Image/P25_D2_P1_High_T3_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 986.0454 seconds to 993.9496 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P2_High_T3_26_Right", "images": ["Image/P21_D2_P2_High_T3_26_Right/1.png", "Image/P21_D2_P2_High_T3_26_Right/2.png", "Image/P21_D2_P2_High_T3_26_Right/3.png", "Image/P21_D2_P2_High_T3_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1620.6208 seconds to 1632.5357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P3_Low_T3_10_Mid", "images": ["Image/P28_D2_P3_Low_T3_10_Mid/1.png", "Image/P28_D2_P3_Low_T3_10_Mid/2.png", "Image/P28_D2_P3_Low_T3_10_Mid/3.png", "Image/P28_D2_P3_Low_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1230.3134 seconds to 1239.1937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P3_Low_T3_12_Left", "images": ["Image/P5_D2_P3_Low_T3_12_Left/1.png", "Image/P5_D2_P3_Low_T3_12_Left/2.png", "Image/P5_D2_P3_Low_T3_12_Left/3.png", "Image/P5_D2_P3_Low_T3_12_Left/4.png", "Image/P5_D2_P3_Low_T3_12_Left/5.png", "Image/P5_D2_P3_Low_T3_12_Left/6.png", "Image/P5_D2_P3_Low_T3_12_Left/7.png", "Image/P5_D2_P3_Low_T3_12_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase to the left side of you on top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 98.95049999999998 seconds to 111.90740999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P4_Medium_T1_23_Mid", "images": ["Image/P21_D1_P4_Medium_T1_23_Mid/1.png", "Image/P21_D1_P4_Medium_T1_23_Mid/2.png", "Image/P21_D1_P4_Medium_T1_23_Mid/3.png", "Image/P21_D1_P4_Medium_T1_23_Mid/4.png", "Image/P21_D1_P4_Medium_T1_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 870.5263199999999 seconds to 883.0399299999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P5_Low_T1_14_Right", "images": ["Image/P1_D1_P5_Low_T1_14_Right/1.png", "Image/P1_D1_P5_Low_T1_14_Right/2.png", "Image/P1_D1_P5_Low_T1_14_Right/3.png", "Image/P1_D1_P5_Low_T1_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 173.24437999999998 seconds to 183.12899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P3_Medium_T3_4_Left", "images": ["Image/P4_D3_P3_Medium_T3_4_Left/1.png", "Image/P4_D3_P3_Medium_T3_4_Left/2.png", "Image/P4_D3_P3_Medium_T3_4_Left/3.png", "Image/P4_D3_P3_Medium_T3_4_Left/4.png", "Image/P4_D3_P3_Medium_T3_4_Left/5.png", "Image/P4_D3_P3_Medium_T3_4_Left/6.png", "Image/P4_D3_P3_Medium_T3_4_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 242.67311999999993 seconds to 257.7912299999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P5_Low_T1_21_Left", "images": ["Image/P7_D2_P5_Low_T1_21_Left/1.png", "Image/P7_D2_P5_Low_T1_21_Left/2.png", "Image/P7_D2_P5_Low_T1_21_Left/3.png", "Image/P7_D2_P5_Low_T1_21_Left/4.png", "Image/P7_D2_P5_Low_T1_21_Left/5.png", "Image/P7_D2_P5_Low_T1_21_Left/6.png", "Image/P7_D2_P5_Low_T1_21_Left/7.png", "Image/P7_D2_P5_Low_T1_21_Left/8.png", "Image/P7_D2_P5_Low_T1_21_Left/9.png", "Image/P7_D2_P5_Low_T1_21_Left/10.png", "Image/P7_D2_P5_Low_T1_21_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1162.2902 seconds to 1179.6329 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P4_Low_T1_25_Right", "images": ["Image/P10_D3_P4_Low_T1_25_Right/1.png", "Image/P10_D3_P4_Low_T1_25_Right/2.png", "Image/P10_D3_P4_Low_T1_25_Right/3.png", "Image/P10_D3_P4_Low_T1_25_Right/4.png", "Image/P10_D3_P4_Low_T1_25_Right/5.png", "Image/P10_D3_P4_Low_T1_25_Right/6.png", "Image/P10_D3_P4_Low_T1_25_Right/7.png", "Image/P10_D3_P4_Low_T1_25_Right/8.png", "Image/P10_D3_P4_Low_T1_25_Right/9.png", "Image/P10_D3_P4_Low_T1_25_Right/10.png", "Image/P10_D3_P4_Low_T1_25_Right/11.png", "Image/P10_D3_P4_Low_T1_25_Right/12.png", "Image/P10_D3_P4_Low_T1_25_Right/13.png", "Image/P10_D3_P4_Low_T1_25_Right/14.png", "Image/P10_D3_P4_Low_T1_25_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 552.74359 seconds to 576.97749 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P9_High_T1_22_Right", "images": ["Image/P15_D1_P9_High_T1_22_Right/1.png", "Image/P15_D1_P9_High_T1_22_Right/2.png", "Image/P15_D1_P9_High_T1_22_Right/3.png", "Image/P15_D1_P9_High_T1_22_Right/4.png", "Image/P15_D1_P9_High_T1_22_Right/5.png", "Image/P15_D1_P9_High_T1_22_Right/6.png", "Image/P15_D1_P9_High_T1_22_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start\"\nThe frames roughly correspond to the speech interval from 2055.92497 seconds to 2077.70937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P3_Medium_T3_15_Left", "images": ["Image/P13_D3_P3_Medium_T3_15_Left/1.png", "Image/P13_D3_P3_Medium_T3_15_Left/2.png", "Image/P13_D3_P3_Medium_T3_15_Left/3.png", "Image/P13_D3_P3_Medium_T3_15_Left/4.png", "Image/P13_D3_P3_Medium_T3_15_Left/5.png", "Image/P13_D3_P3_Medium_T3_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday Can you clean the left plant over here\"\nThe frames roughly correspond to the speech interval from 1848.3589 seconds to 1859.703 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P6_High_T3_18_Left", "images": ["Image/P22_D1_P6_High_T3_18_Left/1.png", "Image/P22_D1_P6_High_T3_18_Left/2.png", "Image/P22_D1_P6_High_T3_18_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 1534.7543 seconds to 1543.1328 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P2_High_T3_18_Left", "images": ["Image/P30_D2_P2_High_T3_18_Left/1.png", "Image/P30_D2_P2_High_T3_18_Left/2.png", "Image/P30_D2_P2_High_T3_18_Left/3.png", "Image/P30_D2_P2_High_T3_18_Left/4.png", "Image/P30_D2_P2_High_T3_18_Left/5.png", "Image/P30_D2_P2_High_T3_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the red object on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 142.2251 seconds to 153.8886 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P9_Low_T1_12_Left", "images": ["Image/P18_D3_P9_Low_T1_12_Left/1.png", "Image/P18_D3_P9_Low_T1_12_Left/2.png", "Image/P18_D3_P9_Low_T1_12_Left/3.png", "Image/P18_D3_P9_Low_T1_12_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1520.3752 seconds to 1529.8122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P1_Medium_T1_13_Left", "images": ["Image/P4_D3_P1_Medium_T1_13_Left/1.png", "Image/P4_D3_P1_Medium_T1_13_Left/2.png", "Image/P4_D3_P1_Medium_T1_13_Left/3.png", "Image/P4_D3_P1_Medium_T1_13_Left/4.png", "Image/P4_D3_P1_Medium_T1_13_Left/5.png", "Image/P4_D3_P1_Medium_T1_13_Left/6.png", "Image/P4_D3_P1_Medium_T1_13_Left/7.png", "Image/P4_D3_P1_Medium_T1_13_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the left plant to the ground under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 626.394 seconds to 642.2896999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P2_Medium_T1_5_Left", "images": ["Image/P21_D3_P2_Medium_T1_5_Left/1.png", "Image/P21_D3_P2_Medium_T1_5_Left/2.png", "Image/P21_D3_P2_Medium_T1_5_Left/3.png", "Image/P21_D3_P2_Medium_T1_5_Left/4.png", "Image/P21_D3_P2_Medium_T1_5_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 442.2927 seconds to 453.4989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P3_Low_T3_5_Mid", "images": ["Image/P23_D3_P3_Low_T3_5_Mid/1.png", "Image/P23_D3_P3_Low_T3_5_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 931.1156 seconds to 940.2043 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P2_Low_T1_22_Mid", "images": ["Image/P2_D1_P2_Low_T1_22_Mid/1.png", "Image/P2_D1_P2_Low_T1_22_Mid/2.png", "Image/P2_D1_P2_Low_T1_22_Mid/3.png", "Image/P2_D1_P2_Low_T1_22_Mid/4.png", "Image/P2_D1_P2_Low_T1_22_Mid/5.png", "Image/P2_D1_P2_Low_T1_22_Mid/6.png", "Image/P2_D1_P2_Low_T1_22_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi\"\nThe frames roughly correspond to the speech interval from 767.2048 seconds to 781.8747999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P4_High_T3_7_Right", "images": ["Image/P17_D2_P4_High_T3_7_Right/1.png", "Image/P17_D2_P4_High_T3_7_Right/2.png", "Image/P17_D2_P4_High_T3_7_Right/3.png", "Image/P17_D2_P4_High_T3_7_Right/4.png", "Image/P17_D2_P4_High_T3_7_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this green mug? Finish.\"\nThe frames roughly correspond to the speech interval from 261.96939999999995 seconds to 273.35569999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P6_High_T1_21_Right", "images": ["Image/P28_D3_P6_High_T1_21_Right/1.png", "Image/P28_D3_P6_High_T1_21_Right/2.png", "Image/P28_D3_P6_High_T1_21_Right/3.png", "Image/P28_D3_P6_High_T1_21_Right/4.png", "Image/P28_D3_P6_High_T1_21_Right/5.png", "Image/P28_D3_P6_High_T1_21_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please move that white vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 275.4821 seconds to 287.2324 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P2_Medium_T2_24_Mid", "images": ["Image/P26_D2_P2_Medium_T2_24_Mid/1.png", "Image/P26_D2_P2_Medium_T2_24_Mid/2.png", "Image/P26_D2_P2_Medium_T2_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the middle plant please? Finish.\"\nThe frames roughly correspond to the speech interval from 1201.20054 seconds to 1214.28391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P1_High_T3_25_Left", "images": ["Image/P17_D1_P1_High_T3_25_Left/1.png", "Image/P17_D1_P1_High_T3_25_Left/2.png", "Image/P17_D1_P1_High_T3_25_Left/3.png", "Image/P17_D1_P1_High_T3_25_Left/4.png", "Image/P17_D1_P1_High_T3_25_Left/5.png", "Image/P17_D1_P1_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white mug on the shelf to my left? Finish\"\nThe frames roughly correspond to the speech interval from 1877.1669 seconds to 1891.782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P9_High_T2_6_Mid", "images": ["Image/P2_D1_P9_High_T2_6_Mid/1.png", "Image/P2_D1_P9_High_T2_6_Mid/2.png", "Image/P2_D1_P9_High_T2_6_Mid/3.png", "Image/P2_D1_P9_High_T2_6_Mid/4.png", "Image/P2_D1_P9_High_T2_6_Mid/5.png", "Image/P2_D1_P9_High_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Could you please water that plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 673.3581999999999 seconds to 685.6047 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P5_Medium_T2_2_Left", "images": ["Image/P23_D2_P5_Medium_T2_2_Left/1.png", "Image/P23_D2_P5_Medium_T2_2_Left/2.png", "Image/P23_D2_P5_Medium_T2_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the left one. Finish.\"\nThe frames roughly correspond to the speech interval from 557.1251 seconds to 566.5491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P2_High_T3_5_Left", "images": ["Image/P11_D1_P2_High_T3_5_Left/1.png", "Image/P11_D1_P2_High_T3_5_Left/2.png", "Image/P11_D1_P2_High_T3_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2819.5245999999997 seconds to 2828.1296 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P4_High_T1_21_Left", "images": ["Image/P1_D3_P4_High_T1_21_Left/1.png", "Image/P1_D3_P4_High_T1_21_Left/2.png", "Image/P1_D3_P4_High_T1_21_Left/3.png", "Image/P1_D3_P4_High_T1_21_Left/4.png", "Image/P1_D3_P4_High_T1_21_Left/5.png", "Image/P1_D3_P4_High_T1_21_Left/6.png", "Image/P1_D3_P4_High_T1_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the left bottle to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1787.3313999999998 seconds to 1800.3060999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P1_Low_T3_17_Left", "images": ["Image/P15_D2_P1_Low_T3_17_Left/1.png", "Image/P15_D2_P1_Low_T3_17_Left/2.png", "Image/P15_D2_P1_Low_T3_17_Left/3.png", "Image/P15_D2_P1_Low_T3_17_Left/4.png", "Image/P15_D2_P1_Low_T3_17_Left/5.png", "Image/P15_D2_P1_Low_T3_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the cup at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3001.0569 seconds to 3012.0116 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P3_Medium_T1_19_Left", "images": ["Image/P22_D1_P3_Medium_T1_19_Left/1.png", "Image/P22_D1_P3_Medium_T1_19_Left/2.png", "Image/P22_D1_P3_Medium_T1_19_Left/3.png", "Image/P22_D1_P3_Medium_T1_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this vase down to the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 1687.3169 seconds to 1698.3412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P6_High_T1_27_Left", "images": ["Image/P13_D3_P6_High_T1_27_Left/1.png", "Image/P13_D3_P6_High_T1_27_Left/2.png", "Image/P13_D3_P6_High_T1_27_Left/3.png", "Image/P13_D3_P6_High_T1_27_Left/4.png", "Image/P13_D3_P6_High_T1_27_Left/5.png", "Image/P13_D3_P6_High_T1_27_Left/6.png", "Image/P13_D3_P6_High_T1_27_Left/7.png", "Image/P13_D3_P6_High_T1_27_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the white cup from over there to this table right here\"\nThe frames roughly correspond to the speech interval from 1362.86069 seconds to 1376.00076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P9_High_T1_2_Mid", "images": ["Image/P15_D2_P9_High_T1_2_Mid/1.png", "Image/P15_D2_P9_High_T1_2_Mid/2.png", "Image/P15_D2_P9_High_T1_2_Mid/3.png", "Image/P15_D2_P9_High_T1_2_Mid/4.png", "Image/P15_D2_P9_High_T1_2_Mid/5.png", "Image/P15_D2_P9_High_T1_2_Mid/6.png", "Image/P15_D2_P9_High_T1_2_Mid/7.png", "Image/P15_D2_P9_High_T1_2_Mid/8.png", "Image/P15_D2_P9_High_T1_2_Mid/9.png", "Image/P15_D2_P9_High_T1_2_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green container from the floor to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2770.6132 seconds to 2786.0787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P6_High_T1_8_Mid", "images": ["Image/P3_D1_P6_High_T1_8_Mid/1.png", "Image/P3_D1_P6_High_T1_8_Mid/2.png", "Image/P3_D1_P6_High_T1_8_Mid/3.png", "Image/P3_D1_P6_High_T1_8_Mid/4.png", "Image/P3_D1_P6_High_T1_8_Mid/5.png", "Image/P3_D1_P6_High_T1_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the second item to this space? Finish.\"\nThe frames roughly correspond to the speech interval from 1364.1374 seconds to 1376.6532 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P7_Low_T3_6_Mid", "images": ["Image/P12_D2_P7_Low_T3_6_Mid/1.png", "Image/P12_D2_P7_Low_T3_6_Mid/2.png", "Image/P12_D2_P7_Low_T3_6_Mid/3.png", "Image/P12_D2_P7_Low_T3_6_Mid/4.png", "Image/P12_D2_P7_Low_T3_6_Mid/5.png", "Image/P12_D2_P7_Low_T3_6_Mid/6.png", "Image/P12_D2_P7_Low_T3_6_Mid/7.png", "Image/P12_D2_P7_Low_T3_6_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 266.972 seconds to 281.2673 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P9_Medium_T3_7_Left", "images": ["Image/P1_D3_P9_Medium_T3_7_Left/1.png", "Image/P1_D3_P9_Medium_T3_7_Left/2.png", "Image/P1_D3_P9_Medium_T3_7_Left/3.png", "Image/P1_D3_P9_Medium_T3_7_Left/4.png", "Image/P1_D3_P9_Medium_T3_7_Left/5.png", "Image/P1_D3_P9_Medium_T3_7_Left/6.png", "Image/P1_D3_P9_Medium_T3_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the left cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1386.30777 seconds to 1398.3502199999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P6_Medium_T1_20_Right", "images": ["Image/P4_D3_P6_Medium_T1_20_Right/1.png", "Image/P4_D3_P6_Medium_T1_20_Right/2.png", "Image/P4_D3_P6_Medium_T1_20_Right/3.png", "Image/P4_D3_P6_Medium_T1_20_Right/4.png", "Image/P4_D3_P6_Medium_T1_20_Right/5.png", "Image/P4_D3_P6_Medium_T1_20_Right/6.png", "Image/P4_D3_P6_Medium_T1_20_Right/7.png", "Image/P4_D3_P6_Medium_T1_20_Right/8.png", "Image/P4_D3_P6_Medium_T1_20_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right white bottle to the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 356.5557999999999 seconds to 372.53599999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P6_Medium_T1_14_Mid", "images": ["Image/P17_D3_P6_Medium_T1_14_Mid/1.png", "Image/P17_D3_P6_Medium_T1_14_Mid/2.png", "Image/P17_D3_P6_Medium_T1_14_Mid/3.png", "Image/P17_D3_P6_Medium_T1_14_Mid/4.png", "Image/P17_D3_P6_Medium_T1_14_Mid/5.png", "Image/P17_D3_P6_Medium_T1_14_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the middle plant to the desk over here? Finish\"\nThe frames roughly correspond to the speech interval from 1154.7664 seconds to 1167.0159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P4_Low_T1_1_Right", "images": ["Image/P21_D2_P4_Low_T1_1_Right/1.png", "Image/P21_D2_P4_Low_T1_1_Right/2.png", "Image/P21_D2_P4_Low_T1_1_Right/3.png", "Image/P21_D2_P4_Low_T1_1_Right/4.png", "Image/P21_D2_P4_Low_T1_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1549.7957 seconds to 1561.1827 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P2_Low_T3_4_Right", "images": ["Image/P29_D3_P2_Low_T3_4_Right/1.png", "Image/P29_D3_P2_Low_T3_4_Right/2.png", "Image/P29_D3_P2_Low_T3_4_Right/3.png", "Image/P29_D3_P2_Low_T3_4_Right/4.png", "Image/P29_D3_P2_Low_T3_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1685.9975999999997 seconds to 1696.2154999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P5_Medium_T2_9_Right", "images": ["Image/P25_D2_P5_Medium_T2_9_Right/1.png", "Image/P25_D2_P5_Medium_T2_9_Right/2.png", "Image/P25_D2_P5_Medium_T2_9_Right/3.png", "Image/P25_D2_P5_Medium_T2_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the first cup. Finish.\"\nThe frames roughly correspond to the speech interval from 739.9402 seconds to 748.8622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P6_High_T3_9_Mid", "images": ["Image/P19_D1_P6_High_T3_9_Mid/1.png", "Image/P19_D1_P6_High_T3_9_Mid/2.png", "Image/P19_D1_P6_High_T3_9_Mid/3.png", "Image/P19_D1_P6_High_T3_9_Mid/4.png", "Image/P19_D1_P6_High_T3_9_Mid/5.png", "Image/P19_D1_P6_High_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1176.8111 seconds to 1192.7755 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P7_Medium_T1_22_Left", "images": ["Image/P21_D3_P7_Medium_T1_22_Left/1.png", "Image/P21_D3_P7_Medium_T1_22_Left/2.png", "Image/P21_D3_P7_Medium_T1_22_Left/3.png", "Image/P21_D3_P7_Medium_T1_22_Left/4.png", "Image/P21_D3_P7_Medium_T1_22_Left/5.png", "Image/P21_D3_P7_Medium_T1_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 364.3561 seconds to 376.4663 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P1_Medium_T2_15_Right", "images": ["Image/P24_D1_P1_Medium_T2_15_Right/1.png", "Image/P24_D1_P1_Medium_T2_15_Right/2.png", "Image/P24_D1_P1_Medium_T2_15_Right/3.png", "Image/P24_D1_P1_Medium_T2_15_Right/4.png", "Image/P24_D1_P1_Medium_T2_15_Right/5.png", "Image/P24_D1_P1_Medium_T2_15_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1127.8458 seconds to 1141.4036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P3_Medium_T3_22_Left", "images": ["Image/P18_D2_P3_Medium_T3_22_Left/1.png", "Image/P18_D2_P3_Medium_T3_22_Left/2.png", "Image/P18_D2_P3_Medium_T3_22_Left/3.png", "Image/P18_D2_P3_Medium_T3_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that first plant. Finish.\"\nThe frames roughly correspond to the speech interval from 123.94540000000002 seconds to 134.99820000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P5_Low_T3_25_Right", "images": ["Image/P8_D2_P5_Low_T3_25_Right/1.png", "Image/P8_D2_P5_Low_T3_25_Right/2.png", "Image/P8_D2_P5_Low_T3_25_Right/3.png", "Image/P8_D2_P5_Low_T3_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 2335.3078 seconds to 2345.4691000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P4_Medium_T1_4_Right", "images": ["Image/P27_D2_P4_Medium_T1_4_Right/1.png", "Image/P27_D2_P4_Medium_T1_4_Right/2.png", "Image/P27_D2_P4_Medium_T1_4_Right/3.png", "Image/P27_D2_P4_Medium_T1_4_Right/4.png", "Image/P27_D2_P4_Medium_T1_4_Right/5.png", "Image/P27_D2_P4_Medium_T1_4_Right/6.png", "Image/P27_D2_P4_Medium_T1_4_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that third plant onto the center of that shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1407.6493 seconds to 1420.3864999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P1_High_T2_14_Left", "images": ["Image/P10_D1_P1_High_T2_14_Left/1.png", "Image/P10_D1_P1_High_T2_14_Left/2.png", "Image/P10_D1_P1_High_T2_14_Left/3.png", "Image/P10_D1_P1_High_T2_14_Left/4.png", "Image/P10_D1_P1_High_T2_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2467.1452 seconds to 2479.0191 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P5_Medium_T1_6_Right", "images": ["Image/P4_D1_P5_Medium_T1_6_Right/1.png", "Image/P4_D1_P5_Medium_T1_6_Right/2.png", "Image/P4_D1_P5_Medium_T1_6_Right/3.png", "Image/P4_D1_P5_Medium_T1_6_Right/4.png", "Image/P4_D1_P5_Medium_T1_6_Right/5.png", "Image/P4_D1_P5_Medium_T1_6_Right/6.png", "Image/P4_D1_P5_Medium_T1_6_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant right here? Finish\"\nThe frames roughly correspond to the speech interval from 2604.5874999999996 seconds to 2619.9257 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P2_Medium_T1_3_Left", "images": ["Image/P16_D3_P2_Medium_T1_3_Left/1.png", "Image/P16_D3_P2_Medium_T1_3_Left/2.png", "Image/P16_D3_P2_Medium_T1_3_Left/3.png", "Image/P16_D3_P2_Medium_T1_3_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase down to the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 386.38159999999993 seconds to 395.76339999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P5_High_T3_19_Mid", "images": ["Image/P17_D3_P5_High_T3_19_Mid/1.png", "Image/P17_D3_P5_High_T3_19_Mid/2.png", "Image/P17_D3_P5_High_T3_19_Mid/3.png", "Image/P17_D3_P5_High_T3_19_Mid/4.png", "Image/P17_D3_P5_High_T3_19_Mid/5.png", "Image/P17_D3_P5_High_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white vase on the shelf in front of me? Finish\"\nThe frames roughly correspond to the speech interval from 1018.7549 seconds to 1031.3662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P9_High_T3_5_Mid", "images": ["Image/P19_D1_P9_High_T3_5_Mid/1.png", "Image/P19_D1_P9_High_T3_5_Mid/2.png", "Image/P19_D1_P9_High_T3_5_Mid/3.png", "Image/P19_D1_P9_High_T3_5_Mid/4.png", "Image/P19_D1_P9_High_T3_5_Mid/5.png", "Image/P19_D1_P9_High_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1129.8455 seconds to 1142.1925 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P3_High_T3_15_Left", "images": ["Image/P14_D1_P3_High_T3_15_Left/1.png", "Image/P14_D1_P3_High_T3_15_Left/2.png", "Image/P14_D1_P3_High_T3_15_Left/3.png", "Image/P14_D1_P3_High_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this mess up. Finish.\"\nThe frames roughly correspond to the speech interval from 153.28730000000002 seconds to 162.6961 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P4_Medium_T3_11_Mid", "images": ["Image/P24_D3_P4_Medium_T3_11_Mid/1.png", "Image/P24_D3_P4_Medium_T3_11_Mid/2.png", "Image/P24_D3_P4_Medium_T3_11_Mid/3.png", "Image/P24_D3_P4_Medium_T3_11_Mid/4.png", "Image/P24_D3_P4_Medium_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1941.3862 seconds to 1951.8669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P2_High_T3_3_Mid", "images": ["Image/P1_D2_P2_High_T3_3_Mid/1.png", "Image/P1_D2_P2_High_T3_3_Mid/2.png", "Image/P1_D2_P2_High_T3_3_Mid/3.png", "Image/P1_D2_P2_High_T3_3_Mid/4.png", "Image/P1_D2_P2_High_T3_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1071.9938000000002 seconds to 1083.1157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P6_High_T1_1_Left", "images": ["Image/P8_D3_P6_High_T1_1_Left/1.png", "Image/P8_D3_P6_High_T1_1_Left/2.png", "Image/P8_D3_P6_High_T1_1_Left/3.png", "Image/P8_D3_P6_High_T1_1_Left/4.png", "Image/P8_D3_P6_High_T1_1_Left/5.png", "Image/P8_D3_P6_High_T1_1_Left/6.png", "Image/P8_D3_P6_High_T1_1_Left/7.png", "Image/P8_D3_P6_High_T1_1_Left/8.png", "Image/P8_D3_P6_High_T1_1_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Hi\"\nThe frames roughly correspond to the speech interval from 1707.7402 seconds to 1722.2177 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P8_Low_T2_6_Mid", "images": ["Image/P14_D2_P8_Low_T2_6_Mid/1.png", "Image/P14_D2_P8_Low_T2_6_Mid/2.png", "Image/P14_D2_P8_Low_T2_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1825.1067 seconds to 1832.9928999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P8_Medium_T1_17_Right", "images": ["Image/P3_D3_P8_Medium_T1_17_Right/1.png", "Image/P3_D3_P8_Medium_T1_17_Right/2.png", "Image/P3_D3_P8_Medium_T1_17_Right/3.png", "Image/P3_D3_P8_Medium_T1_17_Right/4.png", "Image/P3_D3_P8_Medium_T1_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star\"\nThe frames roughly correspond to the speech interval from 1069.924 seconds to 1080.519 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P4_Low_T1_3_Left", "images": ["Image/P28_D2_P4_Low_T1_3_Left/1.png", "Image/P28_D2_P4_Low_T1_3_Left/2.png", "Image/P28_D2_P4_Low_T1_3_Left/3.png", "Image/P28_D2_P4_Low_T1_3_Left/4.png", "Image/P28_D2_P4_Low_T1_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that over here. Finish.\"\nThe frames roughly correspond to the speech interval from 835.0443 seconds to 846.30415 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P6_Medium_T3_21_Left", "images": ["Image/P29_D2_P6_Medium_T3_21_Left/1.png", "Image/P29_D2_P6_Medium_T3_21_Left/2.png", "Image/P29_D2_P6_Medium_T3_21_Left/3.png", "Image/P29_D2_P6_Medium_T3_21_Left/4.png", "Image/P29_D2_P6_Medium_T3_21_Left/5.png", "Image/P29_D2_P6_Medium_T3_21_Left/6.png", "Image/P29_D2_P6_Medium_T3_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 56.131339999999994 seconds to 69.89783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P8_High_T1_26_Left", "images": ["Image/P16_D1_P8_High_T1_26_Left/1.png", "Image/P16_D1_P8_High_T1_26_Left/2.png", "Image/P16_D1_P8_High_T1_26_Left/3.png", "Image/P16_D1_P8_High_T1_26_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that mug up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1519.6087 seconds to 1529.3647 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P6_Low_T1_10_Mid", "images": ["Image/P18_D2_P6_Low_T1_10_Mid/1.png", "Image/P18_D2_P6_Low_T1_10_Mid/2.png", "Image/P18_D2_P6_Low_T1_10_Mid/3.png", "Image/P18_D2_P6_Low_T1_10_Mid/4.png", "Image/P18_D2_P6_Low_T1_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that bottle to the middle shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 60.79015000000002 seconds to 71.94251000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P5_Low_T3_8_Left", "images": ["Image/P7_D3_P5_Low_T3_8_Left/1.png", "Image/P7_D3_P5_Low_T3_8_Left/2.png", "Image/P7_D3_P5_Low_T3_8_Left/3.png", "Image/P7_D3_P5_Low_T3_8_Left/4.png", "Image/P7_D3_P5_Low_T3_8_Left/5.png", "Image/P7_D3_P5_Low_T3_8_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. If ready\"\nThe frames roughly correspond to the speech interval from 1796.8798000000002 seconds to 1809.0735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P6_Medium_T3_26_Right", "images": ["Image/P30_D1_P6_Medium_T3_26_Right/1.png", "Image/P30_D1_P6_Medium_T3_26_Right/2.png", "Image/P30_D1_P6_Medium_T3_26_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 877.0431000000001 seconds to 885.4074 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P1_Medium_T1_16_Right", "images": ["Image/P28_D1_P1_Medium_T1_16_Right/1.png", "Image/P28_D1_P1_Medium_T1_16_Right/2.png", "Image/P28_D1_P1_Medium_T1_16_Right/3.png", "Image/P28_D1_P1_Medium_T1_16_Right/4.png", "Image/P28_D1_P1_Medium_T1_16_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1661.9761 seconds to 1676.0486 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P1_Medium_T1_24_Right", "images": ["Image/P14_D1_P1_Medium_T1_24_Right/1.png", "Image/P14_D1_P1_Medium_T1_24_Right/2.png", "Image/P14_D1_P1_Medium_T1_24_Right/3.png", "Image/P14_D1_P1_Medium_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 409.7334 seconds to 419.366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P5_Medium_T1_17_Left", "images": ["Image/P14_D2_P5_Medium_T1_17_Left/1.png", "Image/P14_D2_P5_Medium_T1_17_Left/2.png", "Image/P14_D2_P5_Medium_T1_17_Left/3.png", "Image/P14_D2_P5_Medium_T1_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that first mug over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1652.4695 seconds to 1661.5351999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P7_Low_T2_26_Right", "images": ["Image/P6_D1_P7_Low_T2_26_Right/1.png", "Image/P6_D1_P7_Low_T2_26_Right/2.png", "Image/P6_D1_P7_Low_T2_26_Right/3.png", "Image/P6_D1_P7_Low_T2_26_Right/4.png", "Image/P6_D1_P7_Low_T2_26_Right/5.png", "Image/P6_D1_P7_Low_T2_26_Right/6.png", "Image/P6_D1_P7_Low_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2531.2293 seconds to 2546.6774 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P5_Medium_T3_26_Left", "images": ["Image/P29_D1_P5_Medium_T3_26_Left/1.png", "Image/P29_D1_P5_Medium_T3_26_Left/2.png", "Image/P29_D1_P5_Medium_T3_26_Left/3.png", "Image/P29_D1_P5_Medium_T3_26_Left/4.png", "Image/P29_D1_P5_Medium_T3_26_Left/5.png", "Image/P29_D1_P5_Medium_T3_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2952.1807 seconds to 2963.5517999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P6_High_T1_11_Left", "images": ["Image/P8_D1_P6_High_T1_11_Left/1.png", "Image/P8_D1_P6_High_T1_11_Left/2.png", "Image/P8_D1_P6_High_T1_11_Left/3.png", "Image/P8_D1_P6_High_T1_11_Left/4.png", "Image/P8_D1_P6_High_T1_11_Left/5.png", "Image/P8_D1_P6_High_T1_11_Left/6.png", "Image/P8_D1_P6_High_T1_11_Left/7.png", "Image/P8_D1_P6_High_T1_11_Left/8.png", "Image/P8_D1_P6_High_T1_11_Left/9.png", "Image/P8_D1_P6_High_T1_11_Left/10.png", "Image/P8_D1_P6_High_T1_11_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. So\"\nThe frames roughly correspond to the speech interval from 639.0062999999999 seconds to 657.1000999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P7_Low_T2_18_Left", "images": ["Image/P3_D3_P7_Low_T2_18_Left/1.png", "Image/P3_D3_P7_Low_T2_18_Left/2.png", "Image/P3_D3_P7_Low_T2_18_Left/3.png", "Image/P3_D3_P7_Low_T2_18_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1106.887 seconds to 1116.3811999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P6_High_T2_22_Right", "images": ["Image/P18_D3_P6_High_T2_22_Right/1.png", "Image/P18_D3_P6_High_T2_22_Right/2.png", "Image/P18_D3_P6_High_T2_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1442.4105 seconds to 1450.6504 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P9_Medium_T1_23_Mid", "images": ["Image/P7_D2_P9_Medium_T1_23_Mid/1.png", "Image/P7_D2_P9_Medium_T1_23_Mid/2.png", "Image/P7_D2_P9_Medium_T1_23_Mid/3.png", "Image/P7_D2_P9_Medium_T1_23_Mid/4.png", "Image/P7_D2_P9_Medium_T1_23_Mid/5.png", "Image/P7_D2_P9_Medium_T1_23_Mid/6.png", "Image/P7_D2_P9_Medium_T1_23_Mid/7.png", "Image/P7_D2_P9_Medium_T1_23_Mid/8.png", "Image/P7_D2_P9_Medium_T1_23_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1034.5879000000002 seconds to 1052.2524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P1_Medium_T1_5_Left", "images": ["Image/P25_D3_P1_Medium_T1_5_Left/1.png", "Image/P25_D3_P1_Medium_T1_5_Left/2.png", "Image/P25_D3_P1_Medium_T1_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant down there. Finish.\"\nThe frames roughly correspond to the speech interval from 2119.2765 seconds to 2127.6997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P4_Medium_T2_19_Mid", "images": ["Image/P28_D3_P4_Medium_T2_19_Mid/1.png", "Image/P28_D3_P4_Medium_T2_19_Mid/2.png", "Image/P28_D3_P4_Medium_T2_19_Mid/3.png", "Image/P28_D3_P4_Medium_T2_19_Mid/4.png", "Image/P28_D3_P4_Medium_T2_19_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water in that middle vase. Finish.\"\nThe frames roughly correspond to the speech interval from 314.3758 seconds to 325.77660000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P1_Medium_T1_1_Left", "images": ["Image/P9_D1_P1_Medium_T1_1_Left/1.png", "Image/P9_D1_P1_Medium_T1_1_Left/2.png", "Image/P9_D1_P1_Medium_T1_1_Left/3.png", "Image/P9_D1_P1_Medium_T1_1_Left/4.png", "Image/P9_D1_P1_Medium_T1_1_Left/5.png", "Image/P9_D1_P1_Medium_T1_1_Left/6.png", "Image/P9_D1_P1_Medium_T1_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this bottle to this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1213.21194 seconds to 1228.09711 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P4_High_T1_23_Right", "images": ["Image/P15_D2_P4_High_T1_23_Right/1.png", "Image/P15_D2_P4_High_T1_23_Right/2.png", "Image/P15_D2_P4_High_T1_23_Right/3.png", "Image/P15_D2_P4_High_T1_23_Right/4.png", "Image/P15_D2_P4_High_T1_23_Right/5.png", "Image/P15_D2_P4_High_T1_23_Right/6.png", "Image/P15_D2_P4_High_T1_23_Right/7.png", "Image/P15_D2_P4_High_T1_23_Right/8.png", "Image/P15_D2_P4_High_T1_23_Right/9.png", "Image/P15_D2_P4_High_T1_23_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the left shelf into the middle shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3073.998 seconds to 3089.3515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P8_Low_T1_5_Right", "images": ["Image/P15_D1_P8_Low_T1_5_Right/1.png", "Image/P15_D1_P8_Low_T1_5_Right/2.png", "Image/P15_D1_P8_Low_T1_5_Right/3.png", "Image/P15_D1_P8_Low_T1_5_Right/4.png", "Image/P15_D1_P8_Low_T1_5_Right/5.png", "Image/P15_D1_P8_Low_T1_5_Right/6.png", "Image/P15_D1_P8_Low_T1_5_Right/7.png", "Image/P15_D1_P8_Low_T1_5_Right/8.png", "Image/P15_D1_P8_Low_T1_5_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the floor onto the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1722.4457699999998 seconds to 1737.47877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P9_Low_T2_19_Left", "images": ["Image/P6_D3_P9_Low_T2_19_Left/1.png", "Image/P6_D3_P9_Low_T2_19_Left/2.png", "Image/P6_D3_P9_Low_T2_19_Left/3.png", "Image/P6_D3_P9_Low_T2_19_Left/4.png", "Image/P6_D3_P9_Low_T2_19_Left/5.png", "Image/P6_D3_P9_Low_T2_19_Left/6.png", "Image/P6_D3_P9_Low_T2_19_Left/7.png", "Image/P6_D3_P9_Low_T2_19_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4589.584699999999 seconds to 4603.2432 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P8_Low_T1_3_Mid", "images": ["Image/P26_D2_P8_Low_T1_3_Mid/1.png", "Image/P26_D2_P8_Low_T1_3_Mid/2.png", "Image/P26_D2_P8_Low_T1_3_Mid/3.png", "Image/P26_D2_P8_Low_T1_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this green base up here? Finish.\"\nThe frames roughly correspond to the speech interval from 1435.736 seconds to 1445.5636 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P6_Medium_T3_18_Left", "images": ["Image/P3_D3_P6_Medium_T3_18_Left/1.png", "Image/P3_D3_P6_Medium_T3_18_Left/2.png", "Image/P3_D3_P6_Medium_T3_18_Left/3.png", "Image/P3_D3_P6_Medium_T3_18_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean the first item? Finish.\"\nThe frames roughly correspond to the speech interval from 789.2558999999999 seconds to 798.8894 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D1_P8_Medium_T2_7_Right", "images": ["Image/P25_D1_P8_Medium_T2_7_Right/1.png", "Image/P25_D1_P8_Medium_T2_7_Right/2.png", "Image/P25_D1_P8_Medium_T2_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 339.2688 seconds to 350.38870000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P6_Low_T3_10_Mid", "images": ["Image/P7_D1_P6_Low_T3_10_Mid/1.png", "Image/P7_D1_P6_Low_T3_10_Mid/2.png", "Image/P7_D1_P6_Low_T3_10_Mid/3.png", "Image/P7_D1_P6_Low_T3_10_Mid/4.png", "Image/P7_D1_P6_Low_T3_10_Mid/5.png", "Image/P7_D1_P6_Low_T3_10_Mid/6.png", "Image/P7_D1_P6_Low_T3_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 208.9745 seconds to 225.9973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P8_High_T3_3_Left", "images": ["Image/P19_D2_P8_High_T3_3_Left/1.png", "Image/P19_D2_P8_High_T3_3_Left/2.png", "Image/P19_D2_P8_High_T3_3_Left/3.png", "Image/P19_D2_P8_High_T3_3_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Please clean this vase here. Finish.\"\nThe frames roughly correspond to the speech interval from 1823.6986 seconds to 1836.6591 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P9_Medium_T1_6_Mid", "images": ["Image/P30_D3_P9_Medium_T1_6_Mid/1.png", "Image/P30_D3_P9_Medium_T1_6_Mid/2.png", "Image/P30_D3_P9_Medium_T1_6_Mid/3.png", "Image/P30_D3_P9_Medium_T1_6_Mid/4.png", "Image/P30_D3_P9_Medium_T1_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1402.5968 seconds to 1412.8685 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P6_Low_T2_23_Left", "images": ["Image/P28_D3_P6_Low_T2_23_Left/1.png", "Image/P28_D3_P6_Low_T2_23_Left/2.png", "Image/P28_D3_P6_Low_T2_23_Left/3.png", "Image/P28_D3_P6_Low_T2_23_Left/4.png", "Image/P28_D3_P6_Low_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 106.51037 seconds to 117.33899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P4_Medium_T3_24_Mid", "images": ["Image/P17_D3_P4_Medium_T3_24_Mid/1.png", "Image/P17_D3_P4_Medium_T3_24_Mid/2.png", "Image/P17_D3_P4_Medium_T3_24_Mid/3.png", "Image/P17_D3_P4_Medium_T3_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the middle plant on the desk over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1197.7228 seconds to 1210.1256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P5_Low_T1_19_Right", "images": ["Image/P9_D3_P5_Low_T1_19_Right/1.png", "Image/P9_D3_P5_Low_T1_19_Right/2.png", "Image/P9_D3_P5_Low_T1_19_Right/3.png", "Image/P9_D3_P5_Low_T1_19_Right/4.png", "Image/P9_D3_P5_Low_T1_19_Right/5.png", "Image/P9_D3_P5_Low_T1_19_Right/6.png", "Image/P9_D3_P5_Low_T1_19_Right/7.png", "Image/P9_D3_P5_Low_T1_19_Right/8.png", "Image/P9_D3_P5_Low_T1_19_Right/9.png", "Image/P9_D3_P5_Low_T1_19_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the bottle from the left to the second layer on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 547.2533000000001 seconds to 563.4172 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P9_High_T1_22_Left", "images": ["Image/P26_D2_P9_High_T1_22_Left/1.png", "Image/P26_D2_P9_High_T1_22_Left/2.png", "Image/P26_D2_P9_High_T1_22_Left/3.png", "Image/P26_D2_P9_High_T1_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this plant to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1490.6508 seconds to 1501.3530999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P1_High_T1_22_Left", "images": ["Image/P13_D3_P1_High_T1_22_Left/1.png", "Image/P13_D3_P1_High_T1_22_Left/2.png", "Image/P13_D3_P1_High_T1_22_Left/3.png", "Image/P13_D3_P1_High_T1_22_Left/4.png", "Image/P13_D3_P1_High_T1_22_Left/5.png", "Image/P13_D3_P1_High_T1_22_Left/6.png", "Image/P13_D3_P1_High_T1_22_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the plant from the top shelf down to the floor\"\nThe frames roughly correspond to the speech interval from 1405.25071 seconds to 1417.91317 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P6_High_T2_4_Left", "images": ["Image/P1_D2_P6_High_T2_4_Left/1.png", "Image/P1_D2_P6_High_T2_4_Left/2.png", "Image/P1_D2_P6_High_T2_4_Left/3.png", "Image/P1_D2_P6_High_T2_4_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help watering the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1108.6773 seconds to 1117.9069 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P3_High_T3_17_Left", "images": ["Image/P19_D2_P3_High_T3_17_Left/1.png", "Image/P19_D2_P3_High_T3_17_Left/2.png", "Image/P19_D2_P3_High_T3_17_Left/3.png", "Image/P19_D2_P3_High_T3_17_Left/4.png", "Image/P19_D2_P3_High_T3_17_Left/5.png", "Image/P19_D2_P3_High_T3_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this glass here. Finish.\"\nThe frames roughly correspond to the speech interval from 1844.8294 seconds to 1858.471 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P2_High_T2_10_Mid", "images": ["Image/P3_D1_P2_High_T2_10_Mid/1.png", "Image/P3_D1_P2_High_T2_10_Mid/2.png", "Image/P3_D1_P2_High_T2_10_Mid/3.png", "Image/P3_D1_P2_High_T2_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1540.9333 seconds to 1551.6946 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P9_Low_T3_22_Left", "images": ["Image/P13_D2_P9_Low_T3_22_Left/1.png", "Image/P13_D2_P9_Low_T3_22_Left/2.png", "Image/P13_D2_P9_Low_T3_22_Left/3.png", "Image/P13_D2_P9_Low_T3_22_Left/4.png", "Image/P13_D2_P9_Low_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the plant down here\"\nThe frames roughly correspond to the speech interval from 1266.59426 seconds to 1276.83796 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P7_Low_T3_5_Left", "images": ["Image/P28_D1_P7_Low_T3_5_Left/1.png", "Image/P28_D1_P7_Low_T3_5_Left/2.png", "Image/P28_D1_P7_Low_T3_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1438.8387 seconds to 1447.5816 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P5_High_T1_26_Left", "images": ["Image/P28_D2_P5_High_T1_26_Left/1.png", "Image/P28_D2_P5_High_T1_26_Left/2.png", "Image/P28_D2_P5_High_T1_26_Left/3.png", "Image/P28_D2_P5_High_T1_26_Left/4.png", "Image/P28_D2_P5_High_T1_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that white vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1011.7151 seconds to 1021.8194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P4_Medium_T3_2_Left", "images": ["Image/P13_D1_P4_Medium_T3_2_Left/1.png", "Image/P13_D1_P4_Medium_T3_2_Left/2.png", "Image/P13_D1_P4_Medium_T3_2_Left/3.png", "Image/P13_D1_P4_Medium_T3_2_Left/4.png", "Image/P13_D1_P4_Medium_T3_2_Left/5.png", "Image/P13_D1_P4_Medium_T3_2_Left/6.png", "Image/P13_D1_P4_Medium_T3_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the far left green glass\"\nThe frames roughly correspond to the speech interval from 635.5754 seconds to 648.089 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P7_Medium_T3_21_Right", "images": ["Image/P3_D3_P7_Medium_T3_21_Right/1.png", "Image/P3_D3_P7_Medium_T3_21_Right/2.png", "Image/P3_D3_P7_Medium_T3_21_Right/3.png", "Image/P3_D3_P7_Medium_T3_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you clean the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 808.2738999999999 seconds to 818.4499999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P4_High_T3_17_Left", "images": ["Image/P15_D3_P4_High_T3_17_Left/1.png", "Image/P15_D3_P4_High_T3_17_Left/2.png", "Image/P15_D3_P4_High_T3_17_Left/3.png", "Image/P15_D3_P4_High_T3_17_Left/4.png", "Image/P15_D3_P4_High_T3_17_Left/5.png", "Image/P15_D3_P4_High_T3_17_Left/6.png", "Image/P15_D3_P4_High_T3_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the cup on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 403.2236 seconds to 419.3294 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P8_Medium_T2_10_Mid", "images": ["Image/P30_D3_P8_Medium_T2_10_Mid/1.png", "Image/P30_D3_P8_Medium_T2_10_Mid/2.png", "Image/P30_D3_P8_Medium_T2_10_Mid/3.png", "Image/P30_D3_P8_Medium_T2_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1368.6322 seconds to 1377.9575 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P2_Low_T2_14_Right", "images": ["Image/P30_D3_P2_Low_T2_14_Right/1.png", "Image/P30_D3_P2_Low_T2_14_Right/2.png", "Image/P30_D3_P2_Low_T2_14_Right/3.png", "Image/P30_D3_P2_Low_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Order the plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1187.28346 seconds to 1199.23909 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P9_Medium_T2_18_Left", "images": ["Image/P13_D1_P9_Medium_T2_18_Left/1.png", "Image/P13_D1_P9_Medium_T2_18_Left/2.png", "Image/P13_D1_P9_Medium_T2_18_Left/3.png", "Image/P13_D1_P9_Medium_T2_18_Left/4.png", "Image/P13_D1_P9_Medium_T2_18_Left/5.png", "Image/P13_D1_P9_Medium_T2_18_Left/6.png", "Image/P13_D1_P9_Medium_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you pour water into the far left brown cup\"\nThe frames roughly correspond to the speech interval from 288.2175 seconds to 300.7595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P5_Low_T2_8_Mid", "images": ["Image/P27_D3_P5_Low_T2_8_Mid/1.png", "Image/P27_D3_P5_Low_T2_8_Mid/2.png", "Image/P27_D3_P5_Low_T2_8_Mid/3.png", "Image/P27_D3_P5_Low_T2_8_Mid/4.png", "Image/P27_D3_P5_Low_T2_8_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put water in that cup right there? Finish\"\nThe frames roughly correspond to the speech interval from 412.8569 seconds to 423.6854 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P2_Low_T2_27_Right", "images": ["Image/P23_D1_P2_Low_T2_27_Right/1.png", "Image/P23_D1_P2_Low_T2_27_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1847.4185 seconds to 1856.3819 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P2_Low_T1_18_Right", "images": ["Image/P4_D1_P2_Low_T1_18_Right/1.png", "Image/P4_D1_P2_Low_T1_18_Right/2.png", "Image/P4_D1_P2_Low_T1_18_Right/3.png", "Image/P4_D1_P2_Low_T1_18_Right/4.png", "Image/P4_D1_P2_Low_T1_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put this cap here? Finish.\"\nThe frames roughly correspond to the speech interval from 2567.3518999999997 seconds to 2579.3765999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P9_Medium_T1_23_Mid", "images": ["Image/P19_D2_P9_Medium_T1_23_Mid/1.png", "Image/P19_D2_P9_Medium_T1_23_Mid/2.png", "Image/P19_D2_P9_Medium_T1_23_Mid/3.png", "Image/P19_D2_P9_Medium_T1_23_Mid/4.png", "Image/P19_D2_P9_Medium_T1_23_Mid/5.png", "Image/P19_D2_P9_Medium_T1_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1566.3475999999998 seconds to 1580.9503 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P3_High_T1_25_Mid", "images": ["Image/P3_D1_P3_High_T1_25_Mid/1.png", "Image/P3_D1_P3_High_T1_25_Mid/2.png", "Image/P3_D1_P3_High_T1_25_Mid/3.png", "Image/P3_D1_P3_High_T1_25_Mid/4.png", "Image/P3_D1_P3_High_T1_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you move this item to this way? Finish\"\nThe frames roughly correspond to the speech interval from 1271.0409 seconds to 1281.6904 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P4_High_T3_1_Left", "images": ["Image/P3_D1_P4_High_T3_1_Left/1.png", "Image/P3_D1_P4_High_T3_1_Left/2.png", "Image/P3_D1_P4_High_T3_1_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1634.1686 seconds to 1643.3849 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P4_Low_T2_1_Mid", "images": ["Image/P22_D2_P4_Low_T2_1_Mid/1.png", "Image/P22_D2_P4_Low_T2_1_Mid/2.png", "Image/P22_D2_P4_Low_T2_1_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 652.9948699999999 seconds to 661.1928999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P3_High_T1_26_Left", "images": ["Image/P27_D2_P3_High_T1_26_Left/1.png", "Image/P27_D2_P3_High_T1_26_Left/2.png", "Image/P27_D2_P3_High_T1_26_Left/3.png", "Image/P27_D2_P3_High_T1_26_Left/4.png", "Image/P27_D2_P3_High_T1_26_Left/5.png", "Image/P27_D2_P3_High_T1_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that cup to the bottom of that cabinet? Finish\"\nThe frames roughly correspond to the speech interval from 1653.0717 seconds to 1664.5421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P4_Low_T2_8_Left", "images": ["Image/P18_D2_P4_Low_T2_8_Left/1.png", "Image/P18_D2_P4_Low_T2_8_Left/2.png", "Image/P18_D2_P4_Low_T2_8_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Pour the water into the green cup. Finish.\"\nThe frames roughly correspond to the speech interval from 237.936 seconds to 248.29190000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P2_Medium_T1_25_Mid", "images": ["Image/P11_D1_P2_Medium_T1_25_Mid/1.png", "Image/P11_D1_P2_Medium_T1_25_Mid/2.png", "Image/P11_D1_P2_Medium_T1_25_Mid/3.png", "Image/P11_D1_P2_Medium_T1_25_Mid/4.png", "Image/P11_D1_P2_Medium_T1_25_Mid/5.png", "Image/P11_D1_P2_Medium_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the middle white glass to the middle basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2543.0476999999996 seconds to 2556.6474 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P8_High_T1_22_Right", "images": ["Image/P25_D1_P8_High_T1_22_Right/1.png", "Image/P25_D1_P8_High_T1_22_Right/2.png", "Image/P25_D1_P8_High_T1_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 233.476 seconds to 241.7442 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P8_Low_T1_3_Mid", "images": ["Image/P9_D1_P8_Low_T1_3_Mid/1.png", "Image/P9_D1_P8_Low_T1_3_Mid/2.png", "Image/P9_D1_P8_Low_T1_3_Mid/3.png", "Image/P9_D1_P8_Low_T1_3_Mid/4.png", "Image/P9_D1_P8_Low_T1_3_Mid/5.png", "Image/P9_D1_P8_Low_T1_3_Mid/6.png", "Image/P9_D1_P8_Low_T1_3_Mid/7.png", "Image/P9_D1_P8_Low_T1_3_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this green bottle to here? Finish.\"\nThe frames roughly correspond to the speech interval from 1557.6515 seconds to 1571.9119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P1_Medium_T2_8_Mid", "images": ["Image/P25_D2_P1_Medium_T2_8_Mid/1.png", "Image/P25_D2_P1_Medium_T2_8_Mid/2.png", "Image/P25_D2_P1_Medium_T2_8_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the green cup. Finish.\"\nThe frames roughly correspond to the speech interval from 824.0444 seconds to 832.27 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P3_Medium_T2_14_Left", "images": ["Image/P14_D3_P3_Medium_T2_14_Left/1.png", "Image/P14_D3_P3_Medium_T2_14_Left/2.png", "Image/P14_D3_P3_Medium_T2_14_Left/3.png", "Image/P14_D3_P3_Medium_T2_14_Left/4.png", "Image/P14_D3_P3_Medium_T2_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1194.52446 seconds to 1203.96836 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P1_Low_T3_17_Right", "images": ["Image/P18_D2_P1_Low_T3_17_Right/1.png", "Image/P18_D2_P1_Low_T3_17_Right/2.png", "Image/P18_D2_P1_Low_T3_17_Right/3.png", "Image/P18_D2_P1_Low_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 393.1971 seconds to 402.6771 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P5_Low_T3_23_Right", "images": ["Image/P30_D2_P5_Low_T3_23_Right/1.png", "Image/P30_D2_P5_Low_T3_23_Right/2.png", "Image/P30_D2_P5_Low_T3_23_Right/3.png", "Image/P30_D2_P5_Low_T3_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 107.35590000000002 seconds to 117.20933 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P2_Low_T3_2_Right", "images": ["Image/P10_D3_P2_Low_T3_2_Right/1.png", "Image/P10_D3_P2_Low_T3_2_Right/2.png", "Image/P10_D3_P2_Low_T3_2_Right/3.png", "Image/P10_D3_P2_Low_T3_2_Right/4.png", "Image/P10_D3_P2_Low_T3_2_Right/5.png", "Image/P10_D3_P2_Low_T3_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 170.91442999999998 seconds to 185.61344 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P7_High_T1_20_Left", "images": ["Image/P3_D3_P7_High_T1_20_Left/1.png", "Image/P3_D3_P7_High_T1_20_Left/2.png", "Image/P3_D3_P7_High_T1_20_Left/3.png", "Image/P3_D3_P7_High_T1_20_Left/4.png", "Image/P3_D3_P7_High_T1_20_Left/5.png", "Image/P3_D3_P7_High_T1_20_Left/6.png", "Image/P3_D3_P7_High_T1_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star\"\nThe frames roughly correspond to the speech interval from 645.4974 seconds to 658.4302 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P9_Medium_T2_21_Mid", "images": ["Image/P4_D1_P9_Medium_T2_21_Mid/1.png", "Image/P4_D1_P9_Medium_T2_21_Mid/2.png", "Image/P4_D1_P9_Medium_T2_21_Mid/3.png", "Image/P4_D1_P9_Medium_T2_21_Mid/4.png", "Image/P4_D1_P9_Medium_T2_21_Mid/5.png", "Image/P4_D1_P9_Medium_T2_21_Mid/6.png", "Image/P4_D1_P9_Medium_T2_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the white bottle in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2496.7935 seconds to 2512.4222999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P9_High_T3_6_Right", "images": ["Image/P28_D2_P9_High_T3_6_Right/1.png", "Image/P28_D2_P9_High_T3_6_Right/2.png", "Image/P28_D2_P9_High_T3_6_Right/3.png", "Image/P28_D2_P9_High_T3_6_Right/4.png", "Image/P28_D2_P9_High_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 871.4992 seconds to 881.9542 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P2_High_T3_7_Mid", "images": ["Image/P30_D3_P2_High_T3_7_Mid/1.png", "Image/P30_D3_P2_High_T3_7_Mid/2.png", "Image/P30_D3_P2_High_T3_7_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1474.5766 seconds to 1485.7811 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P4_Low_T1_2_Left", "images": ["Image/P30_D2_P4_Low_T1_2_Left/1.png", "Image/P30_D2_P4_Low_T1_2_Left/2.png", "Image/P30_D2_P4_Low_T1_2_Left/3.png", "Image/P30_D2_P4_Low_T1_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that object over here. Finish.\"\nThe frames roughly correspond to the speech interval from 125.24228 seconds to 135.2221 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P9_Low_T2_26_Mid", "images": ["Image/P18_D1_P9_Low_T2_26_Mid/1.png", "Image/P18_D1_P9_Low_T2_26_Mid/2.png", "Image/P18_D1_P9_Low_T2_26_Mid/3.png", "Image/P18_D1_P9_Low_T2_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 699.568 seconds to 710.5772000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P3_Low_T2_22_Left", "images": ["Image/P16_D1_P3_Low_T2_22_Left/1.png", "Image/P16_D1_P3_Low_T2_22_Left/2.png", "Image/P16_D1_P3_Low_T2_22_Left/3.png", "Image/P16_D1_P3_Low_T2_22_Left/4.png", "Image/P16_D1_P3_Low_T2_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish\"\nThe frames roughly correspond to the speech interval from 1629.8256 seconds to 1641.0308 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P7_High_T3_16_Left", "images": ["Image/P10_D3_P7_High_T3_16_Left/1.png", "Image/P10_D3_P7_High_T3_16_Left/2.png", "Image/P10_D3_P7_High_T3_16_Left/3.png", "Image/P10_D3_P7_High_T3_16_Left/4.png", "Image/P10_D3_P7_High_T3_16_Left/5.png", "Image/P10_D3_P7_High_T3_16_Left/6.png", "Image/P10_D3_P7_High_T3_16_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 353.46723 seconds to 367.05228 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P1_Medium_T1_11_Mid", "images": ["Image/P12_D1_P1_Medium_T1_11_Mid/1.png", "Image/P12_D1_P1_Medium_T1_11_Mid/2.png", "Image/P12_D1_P1_Medium_T1_11_Mid/3.png", "Image/P12_D1_P1_Medium_T1_11_Mid/4.png", "Image/P12_D1_P1_Medium_T1_11_Mid/5.png", "Image/P12_D1_P1_Medium_T1_11_Mid/6.png", "Image/P12_D1_P1_Medium_T1_11_Mid/7.png", "Image/P12_D1_P1_Medium_T1_11_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1352.1718 seconds to 1366.5087 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P8_High_T3_15_Right", "images": ["Image/P2_D1_P8_High_T3_15_Right/1.png", "Image/P2_D1_P8_High_T3_15_Right/2.png", "Image/P2_D1_P8_High_T3_15_Right/3.png", "Image/P2_D1_P8_High_T3_15_Right/4.png", "Image/P2_D1_P8_High_T3_15_Right/5.png", "Image/P2_D1_P8_High_T3_15_Right/6.png", "Image/P2_D1_P8_High_T3_15_Right/7.png", "Image/P2_D1_P8_High_T3_15_Right/8.png", "Image/P2_D1_P8_High_T3_15_Right/9.png", "Image/P2_D1_P8_High_T3_15_Right/10.png", "Image/P2_D1_P8_High_T3_15_Right/11.png", "Image/P2_D1_P8_High_T3_15_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello. Uh\"\nThe frames roughly correspond to the speech interval from 434.6166999999999 seconds to 453.96359999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P6_Medium_T2_10_Mid", "images": ["Image/P12_D3_P6_Medium_T2_10_Mid/1.png", "Image/P12_D3_P6_Medium_T2_10_Mid/2.png", "Image/P12_D3_P6_Medium_T2_10_Mid/3.png", "Image/P12_D3_P6_Medium_T2_10_Mid/4.png", "Image/P12_D3_P6_Medium_T2_10_Mid/5.png", "Image/P12_D3_P6_Medium_T2_10_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2628.51989 seconds to 2640.08619 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P9_Low_T3_18_Right", "images": ["Image/P22_D3_P9_Low_T3_18_Right/1.png", "Image/P22_D3_P9_Low_T3_18_Right/2.png", "Image/P22_D3_P9_Low_T3_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 354.3449 seconds to 363.7668000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P4_Medium_T1_10_Right", "images": ["Image/P22_D1_P4_Medium_T1_10_Right/1.png", "Image/P22_D1_P4_Medium_T1_10_Right/2.png", "Image/P22_D1_P4_Medium_T1_10_Right/3.png", "Image/P22_D1_P4_Medium_T1_10_Right/4.png", "Image/P22_D1_P4_Medium_T1_10_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move this vase to the middle. Finish\"\nThe frames roughly correspond to the speech interval from 1747.2688 seconds to 1759.9768 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P4_High_T1_7_Right", "images": ["Image/P18_D1_P4_High_T1_7_Right/1.png", "Image/P18_D1_P4_High_T1_7_Right/2.png", "Image/P18_D1_P4_High_T1_7_Right/3.png", "Image/P18_D1_P4_High_T1_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 902.9647 seconds to 913.0008 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P7_High_T3_20_Right", "images": ["Image/P29_D3_P7_High_T3_20_Right/1.png", "Image/P29_D3_P7_High_T3_20_Right/2.png", "Image/P29_D3_P7_High_T3_20_Right/3.png", "Image/P29_D3_P7_High_T3_20_Right/4.png", "Image/P29_D3_P7_High_T3_20_Right/5.png", "Image/P29_D3_P7_High_T3_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 1338.1949999999997 seconds to 1349.5393 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P4_Low_T3_11_Mid", "images": ["Image/P1_D3_P4_Low_T3_11_Mid/1.png", "Image/P1_D3_P4_Low_T3_11_Mid/2.png", "Image/P1_D3_P4_Low_T3_11_Mid/3.png", "Image/P1_D3_P4_Low_T3_11_Mid/4.png", "Image/P1_D3_P4_Low_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean that bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1695.7938999999997 seconds to 1706.2210999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P3_High_T1_5_Left", "images": ["Image/P22_D3_P3_High_T1_5_Left/1.png", "Image/P22_D3_P3_High_T1_5_Left/2.png", "Image/P22_D3_P3_High_T1_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move that plant down there. Finish.\"\nThe frames roughly correspond to the speech interval from 105.73479 seconds to 118.91611 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P8_Low_T1_5_Mid", "images": ["Image/P12_D3_P8_Low_T1_5_Mid/1.png", "Image/P12_D3_P8_Low_T1_5_Mid/2.png", "Image/P12_D3_P8_Low_T1_5_Mid/3.png", "Image/P12_D3_P8_Low_T1_5_Mid/4.png", "Image/P12_D3_P8_Low_T1_5_Mid/5.png", "Image/P12_D3_P8_Low_T1_5_Mid/6.png", "Image/P12_D3_P8_Low_T1_5_Mid/7.png", "Image/P12_D3_P8_Low_T1_5_Mid/8.png", "Image/P12_D3_P8_Low_T1_5_Mid/9.png", "Image/P12_D3_P8_Low_T1_5_Mid/10.png", "Image/P12_D3_P8_Low_T1_5_Mid/11.png", "Image/P12_D3_P8_Low_T1_5_Mid/12.png", "Image/P12_D3_P8_Low_T1_5_Mid/13.png", "Image/P12_D3_P8_Low_T1_5_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. This says headset Wi-Fi connection loss. Okay. Start. Please move the plant on the floor to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2667.28719 seconds to 2687.28989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P2_Low_T2_10_Mid", "images": ["Image/P30_D2_P2_Low_T2_10_Mid/1.png", "Image/P30_D2_P2_Low_T2_10_Mid/2.png", "Image/P30_D2_P2_Low_T2_10_Mid/3.png", "Image/P30_D2_P2_Low_T2_10_Mid/4.png", "Image/P30_D2_P2_Low_T2_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in the red vase. Finish.\"\nThe frames roughly correspond to the speech interval from 322.30690000000004 seconds to 333.40010000000007 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P2_Medium_T2_23_Mid", "images": ["Image/P25_D3_P2_Medium_T2_23_Mid/1.png", "Image/P25_D3_P2_Medium_T2_23_Mid/2.png", "Image/P25_D3_P2_Medium_T2_23_Mid/3.png", "Image/P25_D3_P2_Medium_T2_23_Mid/4.png", "Image/P25_D3_P2_Medium_T2_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the second plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1784.5644 seconds to 1794.6018 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P6_Low_T3_1_Left", "images": ["Image/P3_D1_P6_Low_T3_1_Left/1.png", "Image/P3_D1_P6_Low_T3_1_Left/2.png", "Image/P3_D1_P6_Low_T3_1_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1593.641 seconds to 1602.8438999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P9_High_T3_25_Mid", "images": ["Image/P4_D3_P9_High_T3_25_Mid/1.png", "Image/P4_D3_P9_High_T3_25_Mid/2.png", "Image/P4_D3_P9_High_T3_25_Mid/3.png", "Image/P4_D3_P9_High_T3_25_Mid/4.png", "Image/P4_D3_P9_High_T3_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 674.3114999999999 seconds to 688.1335999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P8_Low_T2_26_Left", "images": ["Image/P11_D2_P8_Low_T2_26_Left/1.png", "Image/P11_D2_P8_Low_T2_26_Left/2.png", "Image/P11_D2_P8_Low_T2_26_Left/3.png", "Image/P11_D2_P8_Low_T2_26_Left/4.png", "Image/P11_D2_P8_Low_T2_26_Left/5.png", "Image/P11_D2_P8_Low_T2_26_Left/6.png", "Image/P11_D2_P8_Low_T2_26_Left/7.png", "Image/P11_D2_P8_Low_T2_26_Left/8.png", "Image/P11_D2_P8_Low_T2_26_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour the water in a white glass on the bottom of the shelf. Please finish.\"\nThe frames roughly correspond to the speech interval from 643.7255 seconds to 662.2759000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P3_Medium_T2_8_Right", "images": ["Image/P9_D3_P3_Medium_T2_8_Right/1.png", "Image/P9_D3_P3_Medium_T2_8_Right/2.png", "Image/P9_D3_P3_Medium_T2_8_Right/3.png", "Image/P9_D3_P3_Medium_T2_8_Right/4.png", "Image/P9_D3_P3_Medium_T2_8_Right/5.png", "Image/P9_D3_P3_Medium_T2_8_Right/6.png", "Image/P9_D3_P3_Medium_T2_8_Right/7.png", "Image/P9_D3_P3_Medium_T2_8_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the right cup? Finish.\"\nThe frames roughly correspond to the speech interval from 335.8046 seconds to 349.5384 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P9_High_T2_26_Left", "images": ["Image/P1_D1_P9_High_T2_26_Left/1.png", "Image/P1_D1_P9_High_T2_26_Left/2.png", "Image/P1_D1_P9_High_T2_26_Left/3.png", "Image/P1_D1_P9_High_T2_26_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 443.7625 seconds to 452.8814 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P3_Low_T3_17_Left", "images": ["Image/P22_D2_P3_Low_T3_17_Left/1.png", "Image/P22_D2_P3_Low_T3_17_Left/2.png", "Image/P22_D2_P3_Low_T3_17_Left/3.png", "Image/P22_D2_P3_Low_T3_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 684.9957999999999 seconds to 694.2948999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P7_Low_T2_15_Right", "images": ["Image/P7_D1_P7_Low_T2_15_Right/1.png", "Image/P7_D1_P7_Low_T2_15_Right/2.png", "Image/P7_D1_P7_Low_T2_15_Right/3.png", "Image/P7_D1_P7_Low_T2_15_Right/4.png", "Image/P7_D1_P7_Low_T2_15_Right/5.png", "Image/P7_D1_P7_Low_T2_15_Right/6.png", "Image/P7_D1_P7_Low_T2_15_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 234.5879 seconds to 250.1357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P2_Medium_T3_18_Mid", "images": ["Image/P26_D1_P2_Medium_T3_18_Mid/1.png", "Image/P26_D1_P2_Medium_T3_18_Mid/2.png", "Image/P26_D1_P2_Medium_T3_18_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the middle vase? Finish.\"\nThe frames roughly correspond to the speech interval from 188.5424 seconds to 202.1557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P1_High_T2_24_Mid", "images": ["Image/P22_D3_P1_High_T2_24_Mid/1.png", "Image/P22_D3_P1_High_T2_24_Mid/2.png", "Image/P22_D3_P1_High_T2_24_Mid/3.png", "Image/P22_D3_P1_High_T2_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 319.5999 seconds to 329.8595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P1_Low_T1_21_Right", "images": ["Image/P3_D1_P1_Low_T1_21_Right/1.png", "Image/P3_D1_P1_Low_T1_21_Right/2.png", "Image/P3_D1_P1_Low_T1_21_Right/3.png", "Image/P3_D1_P1_Low_T1_21_Right/4.png", "Image/P3_D1_P1_Low_T1_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this space? Finish.\"\nThe frames roughly correspond to the speech interval from 1615.4993 seconds to 1626.4141 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P4_Medium_T3_5_Left", "images": ["Image/P9_D2_P4_Medium_T3_5_Left/1.png", "Image/P9_D2_P4_Medium_T3_5_Left/2.png", "Image/P9_D2_P4_Medium_T3_5_Left/3.png", "Image/P9_D2_P4_Medium_T3_5_Left/4.png", "Image/P9_D2_P4_Medium_T3_5_Left/5.png", "Image/P9_D2_P4_Medium_T3_5_Left/6.png", "Image/P9_D2_P4_Medium_T3_5_Left/7.png", "Image/P9_D2_P4_Medium_T3_5_Left/8.png", "Image/P9_D2_P4_Medium_T3_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the left plant on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2849.8638 seconds to 2864.8476 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P9_High_T2_3_Left", "images": ["Image/P21_D1_P9_High_T2_3_Left/1.png", "Image/P21_D1_P9_High_T2_3_Left/2.png", "Image/P21_D1_P9_High_T2_3_Left/3.png", "Image/P21_D1_P9_High_T2_3_Left/4.png", "Image/P21_D1_P9_High_T2_3_Left/5.png", "Image/P21_D1_P9_High_T2_3_Left/6.png", "Image/P21_D1_P9_High_T2_3_Left/7.png", "Image/P21_D1_P9_High_T2_3_Left/8.png", "Image/P21_D1_P9_High_T2_3_Left/9.png", "Image/P21_D1_P9_High_T2_3_Left/10.png", "Image/P21_D1_P9_High_T2_3_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1050.1209 seconds to 1061.3117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P6_Medium_T1_27_Mid", "images": ["Image/P15_D2_P6_Medium_T1_27_Mid/1.png", "Image/P15_D2_P6_Medium_T1_27_Mid/2.png", "Image/P15_D2_P6_Medium_T1_27_Mid/3.png", "Image/P15_D2_P6_Medium_T1_27_Mid/4.png", "Image/P15_D2_P6_Medium_T1_27_Mid/5.png", "Image/P15_D2_P6_Medium_T1_27_Mid/6.png", "Image/P15_D2_P6_Medium_T1_27_Mid/7.png", "Image/P15_D2_P6_Medium_T1_27_Mid/8.png", "Image/P15_D2_P6_Medium_T1_27_Mid/9.png", "Image/P15_D2_P6_Medium_T1_27_Mid/10.png", "Image/P15_D2_P6_Medium_T1_27_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2802.8098999999997 seconds to 2819.1937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P3_Medium_T2_14_Right", "images": ["Image/P25_D1_P3_Medium_T2_14_Right/1.png", "Image/P25_D1_P3_Medium_T2_14_Right/2.png", "Image/P25_D1_P3_Medium_T2_14_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 248.6629 seconds to 256.8603 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P3_High_T1_17_Right", "images": ["Image/P1_D1_P3_High_T1_17_Right/1.png", "Image/P1_D1_P3_High_T1_17_Right/2.png", "Image/P1_D1_P3_High_T1_17_Right/3.png", "Image/P1_D1_P3_High_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 363.1689 seconds to 372.7607 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P6_High_T2_16_Right", "images": ["Image/P11_D3_P6_High_T2_16_Right/1.png", "Image/P11_D3_P6_High_T2_16_Right/2.png", "Image/P11_D3_P6_High_T2_16_Right/3.png", "Image/P11_D3_P6_High_T2_16_Right/4.png", "Image/P11_D3_P6_High_T2_16_Right/5.png", "Image/P11_D3_P6_High_T2_16_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour the water in the red bottle in the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1513.0946 seconds to 1525.0518 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P7_Low_T1_22_Right", "images": ["Image/P22_D3_P7_Low_T1_22_Right/1.png", "Image/P22_D3_P7_Low_T1_22_Right/2.png", "Image/P22_D3_P7_Low_T1_22_Right/3.png", "Image/P22_D3_P7_Low_T1_22_Right/4.png", "Image/P22_D3_P7_Low_T1_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 386.8618 seconds to 397.9131 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P6_High_T3_10_Right", "images": ["Image/P7_D2_P6_High_T3_10_Right/1.png", "Image/P7_D2_P6_High_T3_10_Right/2.png", "Image/P7_D2_P6_High_T3_10_Right/3.png", "Image/P7_D2_P6_High_T3_10_Right/4.png", "Image/P7_D2_P6_High_T3_10_Right/5.png", "Image/P7_D2_P6_High_T3_10_Right/6.png", "Image/P7_D2_P6_High_T3_10_Right/7.png", "Image/P7_D2_P6_High_T3_10_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1102.4762 seconds to 1118.3300000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P7_Medium_T2_25_Right", "images": ["Image/P13_D3_P7_Medium_T2_25_Right/1.png", "Image/P13_D3_P7_Medium_T2_25_Right/2.png", "Image/P13_D3_P7_Medium_T2_25_Right/3.png", "Image/P13_D3_P7_Medium_T2_25_Right/4.png", "Image/P13_D3_P7_Medium_T2_25_Right/5.png", "Image/P13_D3_P7_Medium_T2_25_Right/6.png", "Image/P13_D3_P7_Medium_T2_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday Can you fill up the right white cup with water\"\nThe frames roughly correspond to the speech interval from 1634.0626 seconds to 1646.506 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P1_Low_T1_8_Right", "images": ["Image/P11_D1_P1_Low_T1_8_Right/1.png", "Image/P11_D1_P1_Low_T1_8_Right/2.png", "Image/P11_D1_P1_Low_T1_8_Right/3.png", "Image/P11_D1_P1_Low_T1_8_Right/4.png", "Image/P11_D1_P1_Low_T1_8_Right/5.png", "Image/P11_D1_P1_Low_T1_8_Right/6.png", "Image/P11_D1_P1_Low_T1_8_Right/7.png", "Image/P11_D1_P1_Low_T1_8_Right/8.png", "Image/P11_D1_P1_Low_T1_8_Right/9.png", "Image/P11_D1_P1_Low_T1_8_Right/10.png", "Image/P11_D1_P1_Low_T1_8_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green glass to the left basket. Please finish.\"\nThe frames roughly correspond to the speech interval from 2495.2452999999996 seconds to 2511.8668999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P6_Medium_T2_19_Left", "images": ["Image/P20_D2_P6_Medium_T2_19_Left/1.png", "Image/P20_D2_P6_Medium_T2_19_Left/2.png", "Image/P20_D2_P6_Medium_T2_19_Left/3.png", "Image/P20_D2_P6_Medium_T2_19_Left/4.png", "Image/P20_D2_P6_Medium_T2_19_Left/5.png", "Image/P20_D2_P6_Medium_T2_19_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey robot\"\nThe frames roughly correspond to the speech interval from 3192.9905000000003 seconds to 3205.9079 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P7_Low_T1_19_Right", "images": ["Image/P8_D1_P7_Low_T1_19_Right/1.png", "Image/P8_D1_P7_Low_T1_19_Right/2.png", "Image/P8_D1_P7_Low_T1_19_Right/3.png", "Image/P8_D1_P7_Low_T1_19_Right/4.png", "Image/P8_D1_P7_Low_T1_19_Right/5.png", "Image/P8_D1_P7_Low_T1_19_Right/6.png", "Image/P8_D1_P7_Low_T1_19_Right/7.png", "Image/P8_D1_P7_Low_T1_19_Right/8.png", "Image/P8_D1_P7_Low_T1_19_Right/9.png", "Image/P8_D1_P7_Low_T1_19_Right/10.png", "Image/P8_D1_P7_Low_T1_19_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. So there is a plastic bottle on the ground. Could you just grab it and put it here? Finish.\"\nThe frames roughly correspond to the speech interval from 666.5428999999999 seconds to 686.7339999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P8_Medium_T3_20_Mid", "images": ["Image/P21_D1_P8_Medium_T3_20_Mid/1.png", "Image/P21_D1_P8_Medium_T3_20_Mid/2.png", "Image/P21_D1_P8_Medium_T3_20_Mid/3.png", "Image/P21_D1_P8_Medium_T3_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1213.3181 seconds to 1223.6596 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P6_Medium_T2_11_Right", "images": ["Image/P19_D2_P6_Medium_T2_11_Right/1.png", "Image/P19_D2_P6_Medium_T2_11_Right/2.png", "Image/P19_D2_P6_Medium_T2_11_Right/3.png", "Image/P19_D2_P6_Medium_T2_11_Right/4.png", "Image/P19_D2_P6_Medium_T2_11_Right/5.png", "Image/P19_D2_P6_Medium_T2_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1591.2225999999998 seconds to 1604.5297999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P2_High_T2_6_Mid", "images": ["Image/P25_D2_P2_High_T2_6_Mid/1.png", "Image/P25_D2_P2_High_T2_6_Mid/2.png", "Image/P25_D2_P2_High_T2_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Water this plant. Finish.\"\nThe frames roughly correspond to the speech interval from 953.945 seconds to 962.3384 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P7_Medium_T3_24_Mid", "images": ["Image/P19_D3_P7_Medium_T3_24_Mid/1.png", "Image/P19_D3_P7_Medium_T3_24_Mid/2.png", "Image/P19_D3_P7_Medium_T3_24_Mid/3.png", "Image/P19_D3_P7_Medium_T3_24_Mid/4.png", "Image/P19_D3_P7_Medium_T3_24_Mid/5.png", "Image/P19_D3_P7_Medium_T3_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2320.0249999999996 seconds to 2334.9053 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P5_Medium_T2_22_Right", "images": ["Image/P22_D2_P5_Medium_T2_22_Right/1.png", "Image/P22_D2_P5_Medium_T2_22_Right/2.png", "Image/P22_D2_P5_Medium_T2_22_Right/3.png", "Image/P22_D2_P5_Medium_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 793.7112999999999 seconds to 802.7719999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P6_High_T1_13_Right", "images": ["Image/P22_D3_P6_High_T1_13_Right/1.png", "Image/P22_D3_P6_High_T1_13_Right/2.png", "Image/P22_D3_P6_High_T1_13_Right/3.png", "Image/P22_D3_P6_High_T1_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that little plant to the desk.\"\nThe frames roughly correspond to the speech interval from 69.39035000000001 seconds to 81.10996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P2_High_T1_17_Left", "images": ["Image/P9_D1_P2_High_T1_17_Left/1.png", "Image/P9_D1_P2_High_T1_17_Left/2.png", "Image/P9_D1_P2_High_T1_17_Left/3.png", "Image/P9_D1_P2_High_T1_17_Left/4.png", "Image/P9_D1_P2_High_T1_17_Left/5.png", "Image/P9_D1_P2_High_T1_17_Left/6.png", "Image/P9_D1_P2_High_T1_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this cup to the box? This box. Finish.\"\nThe frames roughly correspond to the speech interval from 1938.405 seconds to 1953.9715 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P2_Medium_T2_19_Right", "images": ["Image/P14_D3_P2_Medium_T2_19_Right/1.png", "Image/P14_D3_P2_Medium_T2_19_Right/2.png", "Image/P14_D3_P2_Medium_T2_19_Right/3.png", "Image/P14_D3_P2_Medium_T2_19_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Fill up that last bottle\"\nThe frames roughly correspond to the speech interval from 1324.04516 seconds to 1332.98816 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P8_Medium_T2_20_Mid", "images": ["Image/P15_D3_P8_Medium_T2_20_Mid/1.png", "Image/P15_D3_P8_Medium_T2_20_Mid/2.png", "Image/P15_D3_P8_Medium_T2_20_Mid/3.png", "Image/P15_D3_P8_Medium_T2_20_Mid/4.png", "Image/P15_D3_P8_Medium_T2_20_Mid/5.png", "Image/P15_D3_P8_Medium_T2_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 25.369000000000018 seconds to 39.885700000000014 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P8_High_T2_9_Mid", "images": ["Image/P9_D1_P8_High_T2_9_Mid/1.png", "Image/P9_D1_P8_High_T2_9_Mid/2.png", "Image/P9_D1_P8_High_T2_9_Mid/3.png", "Image/P9_D1_P8_High_T2_9_Mid/4.png", "Image/P9_D1_P8_High_T2_9_Mid/5.png", "Image/P9_D1_P8_High_T2_9_Mid/6.png", "Image/P9_D1_P8_High_T2_9_Mid/7.png", "Image/P9_D1_P8_High_T2_9_Mid/8.png", "Image/P9_D1_P8_High_T2_9_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the green bottle on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1660.3003 seconds to 1676.2694 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P7_Medium_T1_17_Mid", "images": ["Image/P15_D2_P7_Medium_T1_17_Mid/1.png", "Image/P15_D2_P7_Medium_T1_17_Mid/2.png", "Image/P15_D2_P7_Medium_T1_17_Mid/3.png", "Image/P15_D2_P7_Medium_T1_17_Mid/4.png", "Image/P15_D2_P7_Medium_T1_17_Mid/5.png", "Image/P15_D2_P7_Medium_T1_17_Mid/6.png", "Image/P15_D2_P7_Medium_T1_17_Mid/7.png", "Image/P15_D2_P7_Medium_T1_17_Mid/8.png", "Image/P15_D2_P7_Medium_T1_17_Mid/9.png", "Image/P15_D2_P7_Medium_T1_17_Mid/10.png", "Image/P15_D2_P7_Medium_T1_17_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 3172.6061 seconds to 3190.0339 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P9_Low_T3_26_Right", "images": ["Image/P14_D3_P9_Low_T3_26_Right/1.png", "Image/P14_D3_P9_Low_T3_26_Right/2.png", "Image/P14_D3_P9_Low_T3_26_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1099.05896 seconds to 1107.63846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P6_Medium_T3_11_Right", "images": ["Image/P23_D3_P6_Medium_T3_11_Right/1.png", "Image/P23_D3_P6_Medium_T3_11_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 683.06233 seconds to 691.176 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P1_Medium_T3_6_Mid", "images": ["Image/P1_D2_P1_Medium_T3_6_Mid/1.png", "Image/P1_D2_P1_Medium_T3_6_Mid/2.png", "Image/P1_D2_P1_Medium_T3_6_Mid/3.png", "Image/P1_D2_P1_Medium_T3_6_Mid/4.png", "Image/P1_D2_P1_Medium_T3_6_Mid/5.png", "Image/P1_D2_P1_Medium_T3_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the middle plant? Finish.\"\nThe frames roughly correspond to the speech interval from 788.3736 seconds to 799.7894000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P5_High_T2_17_Right", "images": ["Image/P23_D3_P5_High_T2_17_Right/1.png", "Image/P23_D3_P5_High_T2_17_Right/2.png", "Image/P23_D3_P5_High_T2_17_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 900.9652 seconds to 909.2323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P1_High_T1_15_Left", "images": ["Image/P7_D3_P1_High_T1_15_Left/1.png", "Image/P7_D3_P1_High_T1_15_Left/2.png", "Image/P7_D3_P1_High_T1_15_Left/3.png", "Image/P7_D3_P1_High_T1_15_Left/4.png", "Image/P7_D3_P1_High_T1_15_Left/5.png", "Image/P7_D3_P1_High_T1_15_Left/6.png", "Image/P7_D3_P1_High_T1_15_Left/7.png", "Image/P7_D3_P1_High_T1_15_Left/8.png", "Image/P7_D3_P1_High_T1_15_Left/9.png", "Image/P7_D3_P1_High_T1_15_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2003.9796 seconds to 2023.8157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P9_High_T1_9_Right", "images": ["Image/P17_D2_P9_High_T1_9_Right/1.png", "Image/P17_D2_P9_High_T1_9_Right/2.png", "Image/P17_D2_P9_High_T1_9_Right/3.png", "Image/P17_D2_P9_High_T1_9_Right/4.png", "Image/P17_D2_P9_High_T1_9_Right/5.png", "Image/P17_D2_P9_High_T1_9_Right/6.png", "Image/P17_D2_P9_High_T1_9_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that green vase on the bottom to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 356.22079999999994 seconds to 369.0394 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P8_Medium_T2_7_Right", "images": ["Image/P22_D2_P8_Medium_T2_7_Right/1.png", "Image/P22_D2_P8_Medium_T2_7_Right/2.png", "Image/P22_D2_P8_Medium_T2_7_Right/3.png", "Image/P22_D2_P8_Medium_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water in that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1016.8608 seconds to 1026.7986999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P3_Medium_T3_26_Left", "images": ["Image/P26_D2_P3_Medium_T3_26_Left/1.png", "Image/P26_D2_P3_Medium_T3_26_Left/2.png", "Image/P26_D2_P3_Medium_T3_26_Left/3.png", "Image/P26_D2_P3_Medium_T3_26_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the cup on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1578.4996999999998 seconds to 1587.7699 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P3_Medium_T1_13_Mid", "images": ["Image/P20_D3_P3_Medium_T1_13_Mid/1.png", "Image/P20_D3_P3_Medium_T1_13_Mid/2.png", "Image/P20_D3_P3_Medium_T1_13_Mid/3.png", "Image/P20_D3_P3_Medium_T1_13_Mid/4.png", "Image/P20_D3_P3_Medium_T1_13_Mid/5.png", "Image/P20_D3_P3_Medium_T1_13_Mid/6.png", "Image/P20_D3_P3_Medium_T1_13_Mid/7.png", "Image/P20_D3_P3_Medium_T1_13_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that middle plant onto the lowest level of that shelf? Finished.\"\nThe frames roughly correspond to the speech interval from 1644.4194 seconds to 1660.0225 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P4_Low_T2_8_Left", "images": ["Image/P12_D1_P4_Low_T2_8_Left/1.png", "Image/P12_D1_P4_Low_T2_8_Left/2.png", "Image/P12_D1_P4_Low_T2_8_Left/3.png", "Image/P12_D1_P4_Low_T2_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1764.9243 seconds to 1777.0758 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D3_P6_Low_T2_15_Left", "images": ["Image/P25_D3_P6_Low_T2_15_Left/1.png", "Image/P25_D3_P6_Low_T2_15_Left/2.png", "Image/P25_D3_P6_Low_T2_15_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Water the plants. Finish.\"\nThe frames roughly correspond to the speech interval from 2009.2663 seconds to 2017.7014 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P1_High_T2_11_Mid", "images": ["Image/P15_D2_P1_High_T2_11_Mid/1.png", "Image/P15_D2_P1_High_T2_11_Mid/2.png", "Image/P15_D2_P1_High_T2_11_Mid/3.png", "Image/P15_D2_P1_High_T2_11_Mid/4.png", "Image/P15_D2_P1_High_T2_11_Mid/5.png", "Image/P15_D2_P1_High_T2_11_Mid/6.png", "Image/P15_D2_P1_High_T2_11_Mid/7.png", "Image/P15_D2_P1_High_T2_11_Mid/8.png", "Image/P15_D2_P1_High_T2_11_Mid/9.png", "Image/P15_D2_P1_High_T2_11_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the brown Container at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2974.4286 seconds to 2989.5443 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P2_High_T2_5_Mid", "images": ["Image/P6_D3_P2_High_T2_5_Mid/1.png", "Image/P6_D3_P2_High_T2_5_Mid/2.png", "Image/P6_D3_P2_High_T2_5_Mid/3.png", "Image/P6_D3_P2_High_T2_5_Mid/4.png", "Image/P6_D3_P2_High_T2_5_Mid/5.png", "Image/P6_D3_P2_High_T2_5_Mid/6.png", "Image/P6_D3_P2_High_T2_5_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4748.0725 seconds to 4762.0639 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P6_Medium_T1_1_Left", "images": ["Image/P6_D2_P6_Medium_T1_1_Left/1.png", "Image/P6_D2_P6_Medium_T1_1_Left/2.png", "Image/P6_D2_P6_Medium_T1_1_Left/3.png", "Image/P6_D2_P6_Medium_T1_1_Left/4.png", "Image/P6_D2_P6_Medium_T1_1_Left/5.png", "Image/P6_D2_P6_Medium_T1_1_Left/6.png", "Image/P6_D2_P6_Medium_T1_1_Left/7.png", "Image/P6_D2_P6_Medium_T1_1_Left/8.png", "Image/P6_D2_P6_Medium_T1_1_Left/9.png", "Image/P6_D2_P6_Medium_T1_1_Left/10.png", "Image/P6_D2_P6_Medium_T1_1_Left/11.png", "Image/P6_D2_P6_Medium_T1_1_Left/12.png", "Image/P6_D2_P6_Medium_T1_1_Left/13.png", "Image/P6_D2_P6_Medium_T1_1_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 681.5187 seconds to 706.6331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P8_High_T2_4_Mid", "images": ["Image/P19_D3_P8_High_T2_4_Mid/1.png", "Image/P19_D3_P8_High_T2_4_Mid/2.png", "Image/P19_D3_P8_High_T2_4_Mid/3.png", "Image/P19_D3_P8_High_T2_4_Mid/4.png", "Image/P19_D3_P8_High_T2_4_Mid/5.png", "Image/P19_D3_P8_High_T2_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that flowerpot right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2569.9572 seconds to 2585.0456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P2_Low_T3_24_Mid", "images": ["Image/P12_D3_P2_Low_T3_24_Mid/1.png", "Image/P12_D3_P2_Low_T3_24_Mid/2.png", "Image/P12_D3_P2_Low_T3_24_Mid/3.png", "Image/P12_D3_P2_Low_T3_24_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2569.08219 seconds to 2582.37459 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P4_Medium_T3_18_Right", "images": ["Image/P14_D1_P4_Medium_T3_18_Right/1.png", "Image/P14_D1_P4_Medium_T3_18_Right/2.png", "Image/P14_D1_P4_Medium_T3_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 427.703 seconds to 436.9737 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P6_High_T1_17_Right", "images": ["Image/P30_D1_P6_High_T1_17_Right/1.png", "Image/P30_D1_P6_High_T1_17_Right/2.png", "Image/P30_D1_P6_High_T1_17_Right/3.png", "Image/P30_D1_P6_High_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 827.8365000000001 seconds to 836.4931 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P8_Low_T1_3_Right", "images": ["Image/P1_D2_P8_Low_T1_3_Right/1.png", "Image/P1_D2_P8_Low_T1_3_Right/2.png", "Image/P1_D2_P8_Low_T1_3_Right/3.png", "Image/P1_D2_P8_Low_T1_3_Right/4.png", "Image/P1_D2_P8_Low_T1_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this bottle to there? Finish.\"\nThe frames roughly correspond to the speech interval from 896.8115 seconds to 907.364 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P6_High_T2_26_Mid", "images": ["Image/P12_D3_P6_High_T2_26_Mid/1.png", "Image/P12_D3_P6_High_T2_26_Mid/2.png", "Image/P12_D3_P6_High_T2_26_Mid/3.png", "Image/P12_D3_P6_High_T2_26_Mid/4.png", "Image/P12_D3_P6_High_T2_26_Mid/5.png", "Image/P12_D3_P6_High_T2_26_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2513.91759 seconds to 2525.53989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P2_Medium_T1_27_Mid", "images": ["Image/P6_D1_P2_Medium_T1_27_Mid/1.png", "Image/P6_D1_P2_Medium_T1_27_Mid/2.png", "Image/P6_D1_P2_Medium_T1_27_Mid/3.png", "Image/P6_D1_P2_Medium_T1_27_Mid/4.png", "Image/P6_D1_P2_Medium_T1_27_Mid/5.png", "Image/P6_D1_P2_Medium_T1_27_Mid/6.png", "Image/P6_D1_P2_Medium_T1_27_Mid/7.png", "Image/P6_D1_P2_Medium_T1_27_Mid/8.png", "Image/P6_D1_P2_Medium_T1_27_Mid/9.png", "Image/P6_D1_P2_Medium_T1_27_Mid/10.png", "Image/P6_D1_P2_Medium_T1_27_Mid/11.png", "Image/P6_D1_P2_Medium_T1_27_Mid/12.png", "Image/P6_D1_P2_Medium_T1_27_Mid/13.png", "Image/P6_D1_P2_Medium_T1_27_Mid/14.png", "Image/P6_D1_P2_Medium_T1_27_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2214.3826000000004 seconds to 2234.7041000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P3_High_T3_9_Mid", "images": ["Image/P30_D1_P3_High_T3_9_Mid/1.png", "Image/P30_D1_P3_High_T3_9_Mid/2.png", "Image/P30_D1_P3_High_T3_9_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 749.3283000000001 seconds to 757.0680000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P8_Medium_T1_10_Mid", "images": ["Image/P23_D3_P8_Medium_T1_10_Mid/1.png", "Image/P23_D3_P8_Medium_T1_10_Mid/2.png", "Image/P23_D3_P8_Medium_T1_10_Mid/3.png", "Image/P23_D3_P8_Medium_T1_10_Mid/4.png", "Image/P23_D3_P8_Medium_T1_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 716.2885 seconds to 727.3403000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P7_Low_T2_3_Right", "images": ["Image/P15_D2_P7_Low_T2_3_Right/1.png", "Image/P15_D2_P7_Low_T2_3_Right/2.png", "Image/P15_D2_P7_Low_T2_3_Right/3.png", "Image/P15_D2_P7_Low_T2_3_Right/4.png", "Image/P15_D2_P7_Low_T2_3_Right/5.png", "Image/P15_D2_P7_Low_T2_3_Right/6.png", "Image/P15_D2_P7_Low_T2_3_Right/7.png", "Image/P15_D2_P7_Low_T2_3_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2833.6423999999997 seconds to 2847.326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P7_High_T1_4_Mid", "images": ["Image/P28_D3_P7_High_T1_4_Mid/1.png", "Image/P28_D3_P7_High_T1_4_Mid/2.png", "Image/P28_D3_P7_High_T1_4_Mid/3.png", "Image/P28_D3_P7_High_T1_4_Mid/4.png", "Image/P28_D3_P7_High_T1_4_Mid/5.png", "Image/P28_D3_P7_High_T1_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to the top shelf of the desk. Finish\"\nThe frames roughly correspond to the speech interval from 490.3653 seconds to 502.74 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P5_Low_T2_18_Mid", "images": ["Image/P21_D1_P5_Low_T2_18_Mid/1.png", "Image/P21_D1_P5_Low_T2_18_Mid/2.png", "Image/P21_D1_P5_Low_T2_18_Mid/3.png", "Image/P21_D1_P5_Low_T2_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1321.1694 seconds to 1331.4407 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P6_Low_T2_8_Right", "images": ["Image/P20_D1_P6_Low_T2_8_Right/1.png", "Image/P20_D1_P6_Low_T2_8_Right/2.png", "Image/P20_D1_P6_Low_T2_8_Right/3.png", "Image/P20_D1_P6_Low_T2_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour some water in here. Finished.\"\nThe frames roughly correspond to the speech interval from 686.4466 seconds to 697.1493 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P8_Medium_T3_12_Left", "images": ["Image/P24_D2_P8_Medium_T3_12_Left/1.png", "Image/P24_D2_P8_Medium_T3_12_Left/2.png", "Image/P24_D2_P8_Medium_T3_12_Left/3.png", "Image/P24_D2_P8_Medium_T3_12_Left/4.png", "Image/P24_D2_P8_Medium_T3_12_Left/5.png", "Image/P24_D2_P8_Medium_T3_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this one please? Finish.\"\nThe frames roughly correspond to the speech interval from 369.8293 seconds to 385.4331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P2_Low_T3_21_Mid", "images": ["Image/P19_D2_P2_Low_T3_21_Mid/1.png", "Image/P19_D2_P2_Low_T3_21_Mid/2.png", "Image/P19_D2_P2_Low_T3_21_Mid/3.png", "Image/P19_D2_P2_Low_T3_21_Mid/4.png", "Image/P19_D2_P2_Low_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase here.\"\nThe frames roughly correspond to the speech interval from 1659.1808999999998 seconds to 1674.9933999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P5_Medium_T1_2_Mid", "images": ["Image/P21_D1_P5_Medium_T1_2_Mid/1.png", "Image/P21_D1_P5_Medium_T1_2_Mid/2.png", "Image/P21_D1_P5_Medium_T1_2_Mid/3.png", "Image/P21_D1_P5_Medium_T1_2_Mid/4.png", "Image/P21_D1_P5_Medium_T1_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 953.9346 seconds to 965.8899 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P1_Medium_T2_23_Left", "images": ["Image/P24_D2_P1_Medium_T2_23_Left/1.png", "Image/P24_D2_P1_Medium_T2_23_Left/2.png", "Image/P24_D2_P1_Medium_T2_23_Left/3.png", "Image/P24_D2_P1_Medium_T2_23_Left/4.png", "Image/P24_D2_P1_Medium_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 575.1611 seconds to 586.0062 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P2_Medium_T3_25_Right", "images": ["Image/P29_D3_P2_Medium_T3_25_Right/1.png", "Image/P29_D3_P2_Medium_T3_25_Right/2.png", "Image/P29_D3_P2_Medium_T3_25_Right/3.png", "Image/P29_D3_P2_Medium_T3_25_Right/4.png", "Image/P29_D3_P2_Medium_T3_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddie\"\nThe frames roughly correspond to the speech interval from 1490.8134999999995 seconds to 1501.8369999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P8_Low_T3_7_Right", "images": ["Image/P13_D1_P8_Low_T3_7_Right/1.png", "Image/P13_D1_P8_Low_T3_7_Right/2.png", "Image/P13_D1_P8_Low_T3_7_Right/3.png", "Image/P13_D1_P8_Low_T3_7_Right/4.png", "Image/P13_D1_P8_Low_T3_7_Right/5.png", "Image/P13_D1_P8_Low_T3_7_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean this green glass down here please? Finish.\"\nThe frames roughly correspond to the speech interval from 698.0615 seconds to 709.6426 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P7_Medium_T2_6_Left", "images": ["Image/P11_D2_P7_Medium_T2_6_Left/1.png", "Image/P11_D2_P7_Medium_T2_6_Left/2.png", "Image/P11_D2_P7_Medium_T2_6_Left/3.png", "Image/P11_D2_P7_Medium_T2_6_Left/4.png", "Image/P11_D2_P7_Medium_T2_6_Left/5.png", "Image/P11_D2_P7_Medium_T2_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water the first plant from the left side\"\nThe frames roughly correspond to the speech interval from 831.5249 seconds to 843.5783 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P7_Medium_T2_22_Right", "images": ["Image/P20_D3_P7_Medium_T2_22_Right/1.png", "Image/P20_D3_P7_Medium_T2_22_Right/2.png", "Image/P20_D3_P7_Medium_T2_22_Right/3.png", "Image/P20_D3_P7_Medium_T2_22_Right/4.png", "Image/P20_D3_P7_Medium_T2_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water the right plant. Finish\"\nThe frames roughly correspond to the speech interval from 1834.7743 seconds to 1845.352 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P2_Medium_T2_12_Left", "images": ["Image/P15_D3_P2_Medium_T2_12_Left/1.png", "Image/P15_D3_P2_Medium_T2_12_Left/2.png", "Image/P15_D3_P2_Medium_T2_12_Left/3.png", "Image/P15_D3_P2_Medium_T2_12_Left/4.png", "Image/P15_D3_P2_Medium_T2_12_Left/5.png", "Image/P15_D3_P2_Medium_T2_12_Left/6.png", "Image/P15_D3_P2_Medium_T2_12_Left/7.png", "Image/P15_D3_P2_Medium_T2_12_Left/8.png", "Image/P15_D3_P2_Medium_T2_12_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the pot at the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 611.3000999999999 seconds to 625.9008 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P6_Low_T3_24_Left", "images": ["Image/P28_D1_P6_Low_T3_24_Left/1.png", "Image/P28_D1_P6_Low_T3_24_Left/2.png", "Image/P28_D1_P6_Low_T3_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1306.29581 seconds to 1317.49987 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P3_Medium_T1_16_Right", "images": ["Image/P20_D1_P3_Medium_T1_16_Right/1.png", "Image/P20_D1_P3_Medium_T1_16_Right/2.png", "Image/P20_D1_P3_Medium_T1_16_Right/3.png", "Image/P20_D1_P3_Medium_T1_16_Right/4.png", "Image/P20_D1_P3_Medium_T1_16_Right/5.png", "Image/P20_D1_P3_Medium_T1_16_Right/6.png", "Image/P20_D1_P3_Medium_T1_16_Right/7.png", "Image/P20_D1_P3_Medium_T1_16_Right/8.png", "Image/P20_D1_P3_Medium_T1_16_Right/9.png", "Image/P20_D1_P3_Medium_T1_16_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move this cup onto the basket on the floor\"\nThe frames roughly correspond to the speech interval from 289.73069999999996 seconds to 304.4143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P5_Low_T3_27_Mid", "images": ["Image/P12_D1_P5_Low_T3_27_Mid/1.png", "Image/P12_D1_P5_Low_T3_27_Mid/2.png", "Image/P12_D1_P5_Low_T3_27_Mid/3.png", "Image/P12_D1_P5_Low_T3_27_Mid/4.png", "Image/P12_D1_P5_Low_T3_27_Mid/5.png", "Image/P12_D1_P5_Low_T3_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1702.1457 seconds to 1715.0911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P9_High_T2_14_Right", "images": ["Image/P12_D1_P9_High_T2_14_Right/1.png", "Image/P12_D1_P9_High_T2_14_Right/2.png", "Image/P12_D1_P9_High_T2_14_Right/3.png", "Image/P12_D1_P9_High_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1746.857 seconds to 1757.9915 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P4_Medium_T2_17_Right", "images": ["Image/P18_D1_P4_Medium_T2_17_Right/1.png", "Image/P18_D1_P4_Medium_T2_17_Right/2.png", "Image/P18_D1_P4_Medium_T2_17_Right/3.png", "Image/P18_D1_P4_Medium_T2_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 993.0417 seconds to 1002.7711 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P3_Low_T3_17_Right", "images": ["Image/P13_D1_P3_Low_T3_17_Right/1.png", "Image/P13_D1_P3_Low_T3_17_Right/2.png", "Image/P13_D1_P3_Low_T3_17_Right/3.png", "Image/P13_D1_P3_Low_T3_17_Right/4.png", "Image/P13_D1_P3_Low_T3_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the brown cup here\"\nThe frames roughly correspond to the speech interval from 551.8195000000001 seconds to 562.8444 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P9_Low_T1_17_Left", "images": ["Image/P6_D2_P9_Low_T1_17_Left/1.png", "Image/P6_D2_P9_Low_T1_17_Left/2.png", "Image/P6_D2_P9_Low_T1_17_Left/3.png", "Image/P6_D2_P9_Low_T1_17_Left/4.png", "Image/P6_D2_P9_Low_T1_17_Left/5.png", "Image/P6_D2_P9_Low_T1_17_Left/6.png", "Image/P6_D2_P9_Low_T1_17_Left/7.png", "Image/P6_D2_P9_Low_T1_17_Left/8.png", "Image/P6_D2_P9_Low_T1_17_Left/9.png", "Image/P6_D2_P9_Low_T1_17_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1004.4523 seconds to 1020.8232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P2_Medium_T3_26_Mid", "images": ["Image/P12_D2_P2_Medium_T3_26_Mid/1.png", "Image/P12_D2_P2_Medium_T3_26_Mid/2.png", "Image/P12_D2_P2_Medium_T3_26_Mid/3.png", "Image/P12_D2_P2_Medium_T3_26_Mid/4.png", "Image/P12_D2_P2_Medium_T3_26_Mid/5.png", "Image/P12_D2_P2_Medium_T3_26_Mid/6.png", "Image/P12_D2_P2_Medium_T3_26_Mid/7.png", "Image/P12_D2_P2_Medium_T3_26_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the middle white vase in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 169.18560000000002 seconds to 183.1318 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P2_High_T1_13_Left", "images": ["Image/P17_D2_P2_High_T1_13_Left/1.png", "Image/P17_D2_P2_High_T1_13_Left/2.png", "Image/P17_D2_P2_High_T1_13_Left/3.png", "Image/P17_D2_P2_High_T1_13_Left/4.png", "Image/P17_D2_P2_High_T1_13_Left/5.png", "Image/P17_D2_P2_High_T1_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to the way bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 309.18539999999996 seconds to 324.02340000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P2_High_T2_2_Right", "images": ["Image/P5_D3_P2_High_T2_2_Right/1.png", "Image/P5_D3_P2_High_T2_2_Right/2.png", "Image/P5_D3_P2_High_T2_2_Right/3.png", "Image/P5_D3_P2_High_T2_2_Right/4.png", "Image/P5_D3_P2_High_T2_2_Right/5.png", "Image/P5_D3_P2_High_T2_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Water the plant on the table. Pour water into the bottle on the shelf that is next to the plant.\"\nThe frames roughly correspond to the speech interval from 2636.3489 seconds to 2653.7469 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P4_High_T2_3_Right", "images": ["Image/P4_D2_P4_High_T2_3_Right/1.png", "Image/P4_D2_P4_High_T2_3_Right/2.png", "Image/P4_D2_P4_High_T2_3_Right/3.png", "Image/P4_D2_P4_High_T2_3_Right/4.png", "Image/P4_D2_P4_High_T2_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1624.2958999999998 seconds to 1641.8755999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P9_High_T1_23_Right", "images": ["Image/P11_D1_P9_High_T1_23_Right/1.png", "Image/P11_D1_P9_High_T1_23_Right/2.png", "Image/P11_D1_P9_High_T1_23_Right/3.png", "Image/P11_D1_P9_High_T1_23_Right/4.png", "Image/P11_D1_P9_High_T1_23_Right/5.png", "Image/P11_D1_P9_High_T1_23_Right/6.png", "Image/P11_D1_P9_High_T1_23_Right/7.png", "Image/P11_D1_P9_High_T1_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant to the right side of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2417.21033 seconds to 2432.1611 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P2_Low_T1_23_Right", "images": ["Image/P12_D2_P2_Low_T1_23_Right/1.png", "Image/P12_D2_P2_Low_T1_23_Right/2.png", "Image/P12_D2_P2_Low_T1_23_Right/3.png", "Image/P12_D2_P2_Low_T1_23_Right/4.png", "Image/P12_D2_P2_Low_T1_23_Right/5.png", "Image/P12_D2_P2_Low_T1_23_Right/6.png", "Image/P12_D2_P2_Low_T1_23_Right/7.png", "Image/P12_D2_P2_Low_T1_23_Right/8.png", "Image/P12_D2_P2_Low_T1_23_Right/9.png", "Image/P12_D2_P2_Low_T1_23_Right/10.png", "Image/P12_D2_P2_Low_T1_23_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please take the plant in front of me on the shelf and move it to the bottom of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 217.0686 seconds to 234.2309 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P3_High_T3_14_Right", "images": ["Image/P24_D2_P3_High_T3_14_Right/1.png", "Image/P24_D2_P3_High_T3_14_Right/2.png", "Image/P24_D2_P3_High_T3_14_Right/3.png", "Image/P24_D2_P3_High_T3_14_Right/4.png", "Image/P24_D2_P3_High_T3_14_Right/5.png", "Image/P24_D2_P3_High_T3_14_Right/6.png", "Image/P24_D2_P3_High_T3_14_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 312.3271 seconds to 325.1883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D2_P8_Low_T2_7_Left", "images": ["Image/P12_D2_P8_Low_T2_7_Left/1.png", "Image/P12_D2_P8_Low_T2_7_Left/2.png", "Image/P12_D2_P8_Low_T2_7_Left/3.png", "Image/P12_D2_P8_Low_T2_7_Left/4.png", "Image/P12_D2_P8_Low_T2_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 600.23282 seconds to 611.42432 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P6_High_T1_3_Left", "images": ["Image/P12_D2_P6_High_T1_3_Left/1.png", "Image/P12_D2_P6_High_T1_3_Left/2.png", "Image/P12_D2_P6_High_T1_3_Left/3.png", "Image/P12_D2_P6_High_T1_3_Left/4.png", "Image/P12_D2_P6_High_T1_3_Left/5.png", "Image/P12_D2_P6_High_T1_3_Left/6.png", "Image/P12_D2_P6_High_T1_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 338.89689999999996 seconds to 351.96590000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D1_P6_High_T3_14_Right", "images": ["Image/P2_D1_P6_High_T3_14_Right/1.png", "Image/P2_D1_P6_High_T3_14_Right/2.png", "Image/P2_D1_P6_High_T3_14_Right/3.png", "Image/P2_D1_P6_High_T3_14_Right/4.png", "Image/P2_D1_P6_High_T3_14_Right/5.png", "Image/P2_D1_P6_High_T3_14_Right/6.png", "Image/P2_D1_P6_High_T3_14_Right/7.png", "Image/P2_D1_P6_High_T3_14_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Could you please clean this up for me\"\nThe frames roughly correspond to the speech interval from 721.5344 seconds to 737.0513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P4_High_T3_4_Right", "images": ["Image/P23_D2_P4_High_T3_4_Right/1.png", "Image/P23_D2_P4_High_T3_4_Right/2.png", "Image/P23_D2_P4_High_T3_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start cleaning up the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 447.0335 seconds to 460.3672 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P1_Medium_T2_16_Left", "images": ["Image/P18_D3_P1_Medium_T2_16_Left/1.png", "Image/P18_D3_P1_Medium_T2_16_Left/2.png", "Image/P18_D3_P1_Medium_T2_16_Left/3.png", "Image/P18_D3_P1_Medium_T2_16_Left/4.png", "Image/P18_D3_P1_Medium_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1303.4618 seconds to 1313.3578 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P7_Medium_T1_20_Left", "images": ["Image/P17_D2_P7_Medium_T1_20_Left/1.png", "Image/P17_D2_P7_Medium_T1_20_Left/2.png", "Image/P17_D2_P7_Medium_T1_20_Left/3.png", "Image/P17_D2_P7_Medium_T1_20_Left/4.png", "Image/P17_D2_P7_Medium_T1_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off those bottles down there? Finish.\"\nThe frames roughly correspond to the speech interval from 11.32739999999999 seconds to 37.38448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P6_High_T1_20_Right", "images": ["Image/P4_D1_P6_High_T1_20_Right/1.png", "Image/P4_D1_P6_High_T1_20_Right/2.png", "Image/P4_D1_P6_High_T1_20_Right/3.png", "Image/P4_D1_P6_High_T1_20_Right/4.png", "Image/P4_D1_P6_High_T1_20_Right/5.png", "Image/P4_D1_P6_High_T1_20_Right/6.png", "Image/P4_D1_P6_High_T1_20_Right/7.png", "Image/P4_D1_P6_High_T1_20_Right/8.png", "Image/P4_D1_P6_High_T1_20_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white cap to the left? Finish\"\nThe frames roughly correspond to the speech interval from 2540.8066 seconds to 2559.6665999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P1_High_T3_11_Left", "images": ["Image/P29_D3_P1_High_T3_11_Left/1.png", "Image/P29_D3_P1_High_T3_11_Left/2.png", "Image/P29_D3_P1_High_T3_11_Left/3.png", "Image/P29_D3_P1_High_T3_11_Left/4.png", "Image/P29_D3_P1_High_T3_11_Left/5.png", "Image/P29_D3_P1_High_T3_11_Left/6.png", "Image/P29_D3_P1_High_T3_11_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1789.6736 seconds to 1801.6859 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P3_Medium_T2_17_Right", "images": ["Image/P22_D2_P3_Medium_T2_17_Right/1.png", "Image/P22_D2_P3_Medium_T2_17_Right/2.png", "Image/P22_D2_P3_Medium_T2_17_Right/3.png", "Image/P22_D2_P3_Medium_T2_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water in that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1034.2894 seconds to 1046.2882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P1_High_T2_26_Mid", "images": ["Image/P30_D2_P1_High_T2_26_Mid/1.png", "Image/P30_D2_P1_High_T2_26_Mid/2.png", "Image/P30_D2_P1_High_T2_26_Mid/3.png", "Image/P30_D2_P1_High_T2_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 541.4618 seconds to 554.6993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P3_Medium_T1_21_Mid", "images": ["Image/P21_D1_P3_Medium_T1_21_Mid/1.png", "Image/P21_D1_P3_Medium_T1_21_Mid/2.png", "Image/P21_D1_P3_Medium_T1_21_Mid/3.png", "Image/P21_D1_P3_Medium_T1_21_Mid/4.png", "Image/P21_D1_P3_Medium_T1_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1302.8786 seconds to 1314.8643 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P6_High_T2_16_Right", "images": ["Image/P27_D2_P6_High_T2_16_Right/1.png", "Image/P27_D2_P6_High_T2_16_Right/2.png", "Image/P27_D2_P6_High_T2_16_Right/3.png", "Image/P27_D2_P6_High_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put water in that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1686.8683999999998 seconds to 1702.5271 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P1_Medium_T1_13_Mid", "images": ["Image/P1_D1_P1_Medium_T1_13_Mid/1.png", "Image/P1_D1_P1_Medium_T1_13_Mid/2.png", "Image/P1_D1_P1_Medium_T1_13_Mid/3.png", "Image/P1_D1_P1_Medium_T1_13_Mid/4.png", "Image/P1_D1_P1_Medium_T1_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 257.87710000000004 seconds to 267.7054 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P3_Medium_T3_20_Left", "images": ["Image/P27_D1_P3_Medium_T3_20_Left/1.png", "Image/P27_D1_P3_Medium_T3_20_Left/2.png", "Image/P27_D1_P3_Medium_T3_20_Left/3.png", "Image/P27_D1_P3_Medium_T3_20_Left/4.png", "Image/P27_D1_P3_Medium_T3_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this glass? Finish.\"\nThe frames roughly correspond to the speech interval from 803.6342000000001 seconds to 814.1151 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P1_Medium_T2_9_Right", "images": ["Image/P17_D3_P1_Medium_T2_9_Right/1.png", "Image/P17_D3_P1_Medium_T2_9_Right/2.png", "Image/P17_D3_P1_Medium_T2_9_Right/3.png", "Image/P17_D3_P1_Medium_T2_9_Right/4.png", "Image/P17_D3_P1_Medium_T2_9_Right/5.png", "Image/P17_D3_P1_Medium_T2_9_Right/6.png", "Image/P17_D3_P1_Medium_T2_9_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the green cup over there on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1065.7076 seconds to 1080.0289 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P4_Medium_T3_3_Right", "images": ["Image/P12_D2_P4_Medium_T3_3_Right/1.png", "Image/P12_D2_P4_Medium_T3_3_Right/2.png", "Image/P12_D2_P4_Medium_T3_3_Right/3.png", "Image/P12_D2_P4_Medium_T3_3_Right/4.png", "Image/P12_D2_P4_Medium_T3_3_Right/5.png", "Image/P12_D2_P4_Medium_T3_3_Right/6.png", "Image/P12_D2_P4_Medium_T3_3_Right/7.png", "Image/P12_D2_P4_Medium_T3_3_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 359.20450000000005 seconds to 373.6263 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P3_Medium_T3_25_Mid", "images": ["Image/P28_D2_P3_Medium_T3_25_Mid/1.png", "Image/P28_D2_P3_Medium_T3_25_Mid/2.png", "Image/P28_D2_P3_Medium_T3_25_Mid/3.png", "Image/P28_D2_P3_Medium_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that middle white vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1079.4371 seconds to 1089.3172 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P1_Low_T3_3_Right", "images": ["Image/P19_D3_P1_Low_T3_3_Right/1.png", "Image/P19_D3_P1_Low_T3_3_Right/2.png", "Image/P19_D3_P1_Low_T3_3_Right/3.png", "Image/P19_D3_P1_Low_T3_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Please clean that base right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2615.2382 seconds to 2627.7942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P1_Medium_T1_18_Mid", "images": ["Image/P4_D1_P1_Medium_T1_18_Mid/1.png", "Image/P4_D1_P1_Medium_T1_18_Mid/2.png", "Image/P4_D1_P1_Medium_T1_18_Mid/3.png", "Image/P4_D1_P1_Medium_T1_18_Mid/4.png", "Image/P4_D1_P1_Medium_T1_18_Mid/5.png", "Image/P4_D1_P1_Medium_T1_18_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the middle cap to the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 2065.97352 seconds to 2077.66954 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P6_Low_T2_27_Left", "images": ["Image/P4_D1_P6_Low_T2_27_Left/1.png", "Image/P4_D1_P6_Low_T2_27_Left/2.png", "Image/P4_D1_P6_Low_T2_27_Left/3.png", "Image/P4_D1_P6_Low_T2_27_Left/4.png", "Image/P4_D1_P6_Low_T2_27_Left/5.png", "Image/P4_D1_P6_Low_T2_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in this white cap? Finish.\"\nThe frames roughly correspond to the speech interval from 2454.7401 seconds to 2467.4735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P5_Low_T2_16_Mid", "images": ["Image/P28_D1_P5_Low_T2_16_Mid/1.png", "Image/P28_D1_P5_Low_T2_16_Mid/2.png", "Image/P28_D1_P5_Low_T2_16_Mid/3.png", "Image/P28_D1_P5_Low_T2_16_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1507.2571 seconds to 1519.6722 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P5_Low_T1_22_Right", "images": ["Image/P17_D3_P5_Low_T1_22_Right/1.png", "Image/P17_D3_P5_Low_T1_22_Right/2.png", "Image/P17_D3_P5_Low_T1_22_Right/3.png", "Image/P17_D3_P5_Low_T1_22_Right/4.png", "Image/P17_D3_P5_Low_T1_22_Right/5.png", "Image/P17_D3_P5_Low_T1_22_Right/6.png", "Image/P17_D3_P5_Low_T1_22_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant on the shelf in front of me to this shelf right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1217.4207 seconds to 1232.4384 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P8_Low_T2_7_Left", "images": ["Image/P20_D1_P8_Low_T2_7_Left/1.png", "Image/P20_D1_P8_Low_T2_7_Left/2.png", "Image/P20_D1_P8_Low_T2_7_Left/3.png", "Image/P20_D1_P8_Low_T2_7_Left/4.png", "Image/P20_D1_P8_Low_T2_7_Left/5.png", "Image/P20_D1_P8_Low_T2_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Awesome water into the cup on the ground. Finish.\"\nThe frames roughly correspond to the speech interval from 435.79052 seconds to 447.19065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P7_High_T3_9_Right", "images": ["Image/P13_D3_P7_High_T3_9_Right/1.png", "Image/P13_D3_P7_High_T3_9_Right/2.png", "Image/P13_D3_P7_High_T3_9_Right/3.png", "Image/P13_D3_P7_High_T3_9_Right/4.png", "Image/P13_D3_P7_High_T3_9_Right/5.png", "Image/P13_D3_P7_High_T3_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean the green cup please? Finish.\"\nThe frames roughly correspond to the speech interval from 1712.9317999999998 seconds to 1724.0396999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P2_Low_T3_8_Right", "images": ["Image/P1_D3_P2_Low_T3_8_Right/1.png", "Image/P1_D3_P2_Low_T3_8_Right/2.png", "Image/P1_D3_P2_Low_T3_8_Right/3.png", "Image/P1_D3_P2_Low_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean that one? Finish.\"\nThe frames roughly correspond to the speech interval from 1456.5445999999995 seconds to 1465.6079999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P9_Medium_T3_9_Mid", "images": ["Image/P11_D1_P9_Medium_T3_9_Mid/1.png", "Image/P11_D1_P9_Medium_T3_9_Mid/2.png", "Image/P11_D1_P9_Medium_T3_9_Mid/3.png", "Image/P11_D1_P9_Medium_T3_9_Mid/4.png", "Image/P11_D1_P9_Medium_T3_9_Mid/5.png", "Image/P11_D1_P9_Medium_T3_9_Mid/6.png", "Image/P11_D1_P9_Medium_T3_9_Mid/7.png", "Image/P11_D1_P9_Medium_T3_9_Mid/8.png", "Image/P11_D1_P9_Medium_T3_9_Mid/9.png", "Image/P11_D1_P9_Medium_T3_9_Mid/10.png", "Image/P11_D1_P9_Medium_T3_9_Mid/11.png", "Image/P11_D1_P9_Medium_T3_9_Mid/12.png", "Image/P11_D1_P9_Medium_T3_9_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please clean the second green bottle in the basket\"\nThe frames roughly correspond to the speech interval from 2465.0813 seconds to 2485.0982999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P2_Low_T2_16_Right", "images": ["Image/P14_D1_P2_Low_T2_16_Right/1.png", "Image/P14_D1_P2_Low_T2_16_Right/2.png", "Image/P14_D1_P2_Low_T2_16_Right/3.png", "Image/P14_D1_P2_Low_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 271.273 seconds to 280.2365 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P8_Medium_T3_14_Mid", "images": ["Image/P3_D2_P8_Medium_T3_14_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you give some water to this bottle?\"\nThe frames roughly correspond to the speech interval from 2217.80612 seconds to 2227.1171 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P9_High_T2_27_Left", "images": ["Image/P12_D3_P9_High_T2_27_Left/1.png", "Image/P12_D3_P9_High_T2_27_Left/2.png", "Image/P12_D3_P9_High_T2_27_Left/3.png", "Image/P12_D3_P9_High_T2_27_Left/4.png", "Image/P12_D3_P9_High_T2_27_Left/5.png", "Image/P12_D3_P9_High_T2_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to that mug. The white mug. Finish.\"\nThe frames roughly correspond to the speech interval from 2733.5446899999997 seconds to 2745.54249 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P1_Low_T2_1_Right", "images": ["Image/P20_D2_P1_Low_T2_1_Right/1.png", "Image/P20_D2_P1_Low_T2_1_Right/2.png", "Image/P20_D2_P1_Low_T2_1_Right/3.png", "Image/P20_D2_P1_Low_T2_1_Right/4.png", "Image/P20_D2_P1_Low_T2_1_Right/5.png", "Image/P20_D2_P1_Low_T2_1_Right/6.png", "Image/P20_D2_P1_Low_T2_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. The bottle on the top of that shelf\"\nThe frames roughly correspond to the speech interval from 3081.3379000000004 seconds to 3095.6192 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P1_High_T3_5_Mid", "images": ["Image/P30_D3_P1_High_T3_5_Mid/1.png", "Image/P30_D3_P1_High_T3_5_Mid/2.png", "Image/P30_D3_P1_High_T3_5_Mid/3.png", "Image/P30_D3_P1_High_T3_5_Mid/4.png", "Image/P30_D3_P1_High_T3_5_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1167.89335 seconds to 1179.12561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P3_Medium_T1_4_Right", "images": ["Image/P8_D1_P3_Medium_T1_4_Right/1.png", "Image/P8_D1_P3_Medium_T1_4_Right/2.png", "Image/P8_D1_P3_Medium_T1_4_Right/3.png", "Image/P8_D1_P3_Medium_T1_4_Right/4.png", "Image/P8_D1_P3_Medium_T1_4_Right/5.png", "Image/P8_D1_P3_Medium_T1_4_Right/6.png", "Image/P8_D1_P3_Medium_T1_4_Right/7.png", "Image/P8_D1_P3_Medium_T1_4_Right/8.png", "Image/P8_D1_P3_Medium_T1_4_Right/9.png", "Image/P8_D1_P3_Medium_T1_4_Right/10.png", "Image/P8_D1_P3_Medium_T1_4_Right/11.png", "Image/P8_D1_P3_Medium_T1_4_Right/12.png", "Image/P8_D1_P3_Medium_T1_4_Right/13.png", "Image/P8_D1_P3_Medium_T1_4_Right/14.png", "Image/P8_D1_P3_Medium_T1_4_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"um star hi could you move the the rightmost planet to the uh wooden basket below that is on the floor just place place it there thank you finished\"\nThe frames roughly correspond to the speech interval from 231.7862999999999 seconds to 256.1280999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P5_Medium_T1_3_Left", "images": ["Image/P18_D1_P5_Medium_T1_3_Left/1.png", "Image/P18_D1_P5_Medium_T1_3_Left/2.png", "Image/P18_D1_P5_Medium_T1_3_Left/3.png", "Image/P18_D1_P5_Medium_T1_3_Left/4.png", "Image/P18_D1_P5_Medium_T1_3_Left/5.png", "Image/P18_D1_P5_Medium_T1_3_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this vase to the corner right here. Finish.\"\nThe frames roughly correspond to the speech interval from 810.5086 seconds to 824.0942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P6_High_T1_8_Mid", "images": ["Image/P9_D2_P6_High_T1_8_Mid/1.png", "Image/P9_D2_P6_High_T1_8_Mid/2.png", "Image/P9_D2_P6_High_T1_8_Mid/3.png", "Image/P9_D2_P6_High_T1_8_Mid/4.png", "Image/P9_D2_P6_High_T1_8_Mid/5.png", "Image/P9_D2_P6_High_T1_8_Mid/6.png", "Image/P9_D2_P6_High_T1_8_Mid/7.png", "Image/P9_D2_P6_High_T1_8_Mid/8.png", "Image/P9_D2_P6_High_T1_8_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the green cup from the right to the left desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3083.1016 seconds to 3097.835 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P1_Medium_T2_7_Mid", "images": ["Image/P16_D3_P1_Medium_T2_7_Mid/1.png", "Image/P16_D3_P1_Medium_T2_7_Mid/2.png", "Image/P16_D3_P1_Medium_T2_7_Mid/3.png", "Image/P16_D3_P1_Medium_T2_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 449.0072 seconds to 458.1944 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P1_Low_T2_4_Mid", "images": ["Image/P3_D2_P1_Low_T2_4_Mid/1.png", "Image/P3_D2_P1_Low_T2_4_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2585.1236 seconds to 2595.1738 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P4_Low_T3_9_Right", "images": ["Image/P7_D2_P4_Low_T3_9_Right/1.png", "Image/P7_D2_P4_Low_T3_9_Right/2.png", "Image/P7_D2_P4_Low_T3_9_Right/3.png", "Image/P7_D2_P4_Low_T3_9_Right/4.png", "Image/P7_D2_P4_Low_T3_9_Right/5.png", "Image/P7_D2_P4_Low_T3_9_Right/6.png", "Image/P7_D2_P4_Low_T3_9_Right/7.png", "Image/P7_D2_P4_Low_T3_9_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1251.8487000000002 seconds to 1267.8161000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P2_High_T3_15_Left", "images": ["Image/P18_D3_P2_High_T3_15_Left/1.png", "Image/P18_D3_P2_High_T3_15_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1456.5944 seconds to 1464.4169000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P2_High_T1_15_Mid", "images": ["Image/P20_D2_P2_High_T1_15_Mid/1.png", "Image/P20_D2_P2_High_T1_15_Mid/2.png", "Image/P20_D2_P2_High_T1_15_Mid/3.png", "Image/P20_D2_P2_High_T1_15_Mid/4.png", "Image/P20_D2_P2_High_T1_15_Mid/5.png", "Image/P20_D2_P2_High_T1_15_Mid/6.png", "Image/P20_D2_P2_High_T1_15_Mid/7.png", "Image/P20_D2_P2_High_T1_15_Mid/8.png", "Image/P20_D2_P2_High_T1_15_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant over to the floor. I mean the bottom of that table over there. Finish.\"\nThe frames roughly correspond to the speech interval from 2962.5212 seconds to 2978.0531 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P6_Low_T1_19_Left", "images": ["Image/P14_D2_P6_Low_T1_19_Left/1.png", "Image/P14_D2_P6_Low_T1_19_Left/2.png", "Image/P14_D2_P6_Low_T1_19_Left/3.png", "Image/P14_D2_P6_Low_T1_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move that bottle over there\"\nThe frames roughly correspond to the speech interval from 1524.6022799999998 seconds to 1533.72553 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P6_Low_T3_1_Left", "images": ["Image/P25_D2_P6_Low_T3_1_Left/1.png", "Image/P25_D2_P6_Low_T3_1_Left/2.png", "Image/P25_D2_P6_Low_T3_1_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this. Finish.\"\nThe frames roughly correspond to the speech interval from 709.4012 seconds to 717.2222 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P6_Medium_T3_14_Right", "images": ["Image/P28_D2_P6_Medium_T3_14_Right/1.png", "Image/P28_D2_P6_Medium_T3_14_Right/2.png", "Image/P28_D2_P6_Medium_T3_14_Right/3.png", "Image/P28_D2_P6_Medium_T3_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1197.6717 seconds to 1207.3456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D2_P1_High_T1_15_Mid", "images": ["Image/P12_D2_P1_High_T1_15_Mid/1.png", "Image/P12_D2_P1_High_T1_15_Mid/2.png", "Image/P12_D2_P1_High_T1_15_Mid/3.png", "Image/P12_D2_P1_High_T1_15_Mid/4.png", "Image/P12_D2_P1_High_T1_15_Mid/5.png", "Image/P12_D2_P1_High_T1_15_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 582.1627699999999 seconds to 593.78711 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P8_High_T1_27_Right", "images": ["Image/P14_D1_P8_High_T1_27_Right/1.png", "Image/P14_D1_P8_High_T1_27_Right/2.png", "Image/P14_D1_P8_High_T1_27_Right/3.png", "Image/P14_D1_P8_High_T1_27_Right/4.png", "Image/P14_D1_P8_High_T1_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Remove that to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 311.09590000000003 seconds to 321.2026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P6_High_T2_9_Right", "images": ["Image/P11_D1_P6_High_T2_9_Right/1.png", "Image/P11_D1_P6_High_T2_9_Right/2.png", "Image/P11_D1_P6_High_T2_9_Right/3.png", "Image/P11_D1_P6_High_T2_9_Right/4.png", "Image/P11_D1_P6_High_T2_9_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the green glass. Finish.\"\nThe frames roughly correspond to the speech interval from 2797.7396999999996 seconds to 2808.9844999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P6_Low_T3_12_Mid", "images": ["Image/P30_D3_P6_Low_T3_12_Mid/1.png", "Image/P30_D3_P6_Low_T3_12_Mid/2.png", "Image/P30_D3_P6_Low_T3_12_Mid/3.png", "Image/P30_D3_P6_Low_T3_12_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1512.0349 seconds to 1521.5695 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P4_Medium_T2_19_Right", "images": ["Image/P29_D2_P4_Medium_T2_19_Right/1.png", "Image/P29_D2_P4_Medium_T2_19_Right/2.png", "Image/P29_D2_P4_Medium_T2_19_Right/3.png", "Image/P29_D2_P4_Medium_T2_19_Right/4.png", "Image/P29_D2_P4_Medium_T2_19_Right/5.png", "Image/P29_D2_P4_Medium_T2_19_Right/6.png", "Image/P29_D2_P4_Medium_T2_19_Right/7.png", "Image/P29_D2_P4_Medium_T2_19_Right/8.png", "Image/P29_D2_P4_Medium_T2_19_Right/9.png", "Image/P29_D2_P4_Medium_T2_19_Right/10.png", "Image/P29_D2_P4_Medium_T2_19_Right/11.png", "Image/P29_D2_P4_Medium_T2_19_Right/12.png", "Image/P29_D2_P4_Medium_T2_19_Right/13.png", "Image/P29_D2_P4_Medium_T2_19_Right/14.png", "Image/P29_D2_P4_Medium_T2_19_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 134.13785 seconds to 151.3717 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P5_Low_T1_2_Right", "images": ["Image/P29_D1_P5_Low_T1_2_Right/1.png", "Image/P29_D1_P5_Low_T1_2_Right/2.png", "Image/P29_D1_P5_Low_T1_2_Right/3.png", "Image/P29_D1_P5_Low_T1_2_Right/4.png", "Image/P29_D1_P5_Low_T1_2_Right/5.png", "Image/P29_D1_P5_Low_T1_2_Right/6.png", "Image/P29_D1_P5_Low_T1_2_Right/7.png", "Image/P29_D1_P5_Low_T1_2_Right/8.png", "Image/P29_D1_P5_Low_T1_2_Right/9.png", "Image/P29_D1_P5_Low_T1_2_Right/10.png", "Image/P29_D1_P5_Low_T1_2_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2455.3081599999996 seconds to 2472.6834599999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P6_High_T1_2_Left", "images": ["Image/P26_D1_P6_High_T1_2_Left/1.png", "Image/P26_D1_P6_High_T1_2_Left/2.png", "Image/P26_D1_P6_High_T1_2_Left/3.png", "Image/P26_D1_P6_High_T1_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this space over here? Finish.\"\nThe frames roughly correspond to the speech interval from 326.86349999999993 seconds to 338.94589999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P4_Low_T1_11_Left", "images": ["Image/P3_D3_P4_Low_T1_11_Left/1.png", "Image/P3_D3_P4_Low_T1_11_Left/2.png", "Image/P3_D3_P4_Low_T1_11_Left/3.png", "Image/P3_D3_P4_Low_T1_11_Left/4.png", "Image/P3_D3_P4_Low_T1_11_Left/5.png", "Image/P3_D3_P4_Low_T1_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the bottle to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 710.1927999999999 seconds to 721.7621999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P6_High_T3_10_Mid", "images": ["Image/P14_D1_P6_High_T3_10_Mid/1.png", "Image/P14_D1_P6_High_T3_10_Mid/2.png", "Image/P14_D1_P6_High_T3_10_Mid/3.png", "Image/P14_D1_P6_High_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean up the bottle\"\nThe frames roughly correspond to the speech interval from 194.5587 seconds to 204.3588 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P4_High_T1_18_Mid", "images": ["Image/P13_D3_P4_High_T1_18_Mid/1.png", "Image/P13_D3_P4_High_T1_18_Mid/2.png", "Image/P13_D3_P4_High_T1_18_Mid/3.png", "Image/P13_D3_P4_High_T1_18_Mid/4.png", "Image/P13_D3_P4_High_T1_18_Mid/5.png", "Image/P13_D3_P4_High_T1_18_Mid/6.png", "Image/P13_D3_P4_High_T1_18_Mid/7.png", "Image/P13_D3_P4_High_T1_18_Mid/8.png", "Image/P13_D3_P4_High_T1_18_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the brown cup from over there to the third shelf over here\"\nThe frames roughly correspond to the speech interval from 1787.8217 seconds to 1802.0752 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P7_High_T3_20_Mid", "images": ["Image/P18_D3_P7_High_T3_20_Mid/1.png", "Image/P18_D3_P7_High_T3_20_Mid/2.png", "Image/P18_D3_P7_High_T3_20_Mid/3.png", "Image/P18_D3_P7_High_T3_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1336.2577 seconds to 1348.3247 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P7_High_T2_3_Right", "images": ["Image/P28_D1_P7_High_T2_3_Right/1.png", "Image/P28_D1_P7_High_T2_3_Right/2.png", "Image/P28_D1_P7_High_T2_3_Right/3.png", "Image/P28_D1_P7_High_T2_3_Right/4.png", "Image/P28_D1_P7_High_T2_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into this green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1569.3528 seconds to 1579.3882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P6_High_T3_17_Mid", "images": ["Image/P4_D3_P6_High_T3_17_Mid/1.png", "Image/P4_D3_P6_High_T3_17_Mid/2.png", "Image/P4_D3_P6_High_T3_17_Mid/3.png", "Image/P4_D3_P6_High_T3_17_Mid/4.png", "Image/P4_D3_P6_High_T3_17_Mid/5.png", "Image/P4_D3_P6_High_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the bottle in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 724.6524 seconds to 737.6401999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P9_High_T3_23_Mid", "images": ["Image/P20_D3_P9_High_T3_23_Mid/1.png", "Image/P20_D3_P9_High_T3_23_Mid/2.png", "Image/P20_D3_P9_High_T3_23_Mid/3.png", "Image/P20_D3_P9_High_T3_23_Mid/4.png", "Image/P20_D3_P9_High_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1520.4762 seconds to 1531.8481 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P7_Medium_T2_24_Right", "images": ["Image/P4_D3_P7_Medium_T2_24_Right/1.png", "Image/P4_D3_P7_Medium_T2_24_Right/2.png", "Image/P4_D3_P7_Medium_T2_24_Right/3.png", "Image/P4_D3_P7_Medium_T2_24_Right/4.png", "Image/P4_D3_P7_Medium_T2_24_Right/5.png", "Image/P4_D3_P7_Medium_T2_24_Right/6.png", "Image/P4_D3_P7_Medium_T2_24_Right/7.png", "Image/P4_D3_P7_Medium_T2_24_Right/8.png", "Image/P4_D3_P7_Medium_T2_24_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the rice plant on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 698.3555999999999 seconds to 717.4692 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P4_Medium_T2_19_Right", "images": ["Image/P15_D2_P4_Medium_T2_19_Right/1.png", "Image/P15_D2_P4_Medium_T2_19_Right/2.png", "Image/P15_D2_P4_Medium_T2_19_Right/3.png", "Image/P15_D2_P4_Medium_T2_19_Right/4.png", "Image/P15_D2_P4_Medium_T2_19_Right/5.png", "Image/P15_D2_P4_Medium_T2_19_Right/6.png", "Image/P15_D2_P4_Medium_T2_19_Right/7.png", "Image/P15_D2_P4_Medium_T2_19_Right/8.png", "Image/P15_D2_P4_Medium_T2_19_Right/9.png", "Image/P15_D2_P4_Medium_T2_19_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the White container at the right side. Finish.\"\nThe frames roughly correspond to the speech interval from 2945.0293 seconds to 2961.1622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P2_Medium_T3_3_Left", "images": ["Image/P2_D1_P2_Medium_T3_3_Left/1.png", "Image/P2_D1_P2_Medium_T3_3_Left/2.png", "Image/P2_D1_P2_Medium_T3_3_Left/3.png", "Image/P2_D1_P2_Medium_T3_3_Left/4.png", "Image/P2_D1_P2_Medium_T3_3_Left/5.png", "Image/P2_D1_P2_Medium_T3_3_Left/6.png", "Image/P2_D1_P2_Medium_T3_3_Left/7.png", "Image/P2_D1_P2_Medium_T3_3_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 360.64922 seconds to 375.88749999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P3_Medium_T2_5_Mid", "images": ["Image/P4_D1_P3_Medium_T2_5_Mid/1.png", "Image/P4_D1_P3_Medium_T2_5_Mid/2.png", "Image/P4_D1_P3_Medium_T2_5_Mid/3.png", "Image/P4_D1_P3_Medium_T2_5_Mid/4.png", "Image/P4_D1_P3_Medium_T2_5_Mid/5.png", "Image/P4_D1_P3_Medium_T2_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2431.5369 seconds to 2445.5634999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P1_Low_T2_16_Left", "images": ["Image/P22_D3_P1_Low_T2_16_Left/1.png", "Image/P22_D3_P1_Low_T2_16_Left/2.png", "Image/P22_D3_P1_Low_T2_16_Left/3.png", "Image/P22_D3_P1_Low_T2_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 506.9195 seconds to 517.1658 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P6_High_T3_17_Mid", "images": ["Image/P20_D1_P6_High_T3_17_Mid/1.png", "Image/P20_D1_P6_High_T3_17_Mid/2.png", "Image/P20_D1_P6_High_T3_17_Mid/3.png", "Image/P20_D1_P6_High_T3_17_Mid/4.png", "Image/P20_D1_P6_High_T3_17_Mid/5.png", "Image/P20_D1_P6_High_T3_17_Mid/6.png", "Image/P20_D1_P6_High_T3_17_Mid/7.png", "Image/P20_D1_P6_High_T3_17_Mid/8.png", "Image/P20_D1_P6_High_T3_17_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey robot\"\nThe frames roughly correspond to the speech interval from 589.5497 seconds to 605.1539 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P5_Medium_T2_4_Mid", "images": ["Image/P6_D1_P5_Medium_T2_4_Mid/1.png", "Image/P6_D1_P5_Medium_T2_4_Mid/2.png", "Image/P6_D1_P5_Medium_T2_4_Mid/3.png", "Image/P6_D1_P5_Medium_T2_4_Mid/4.png", "Image/P6_D1_P5_Medium_T2_4_Mid/5.png", "Image/P6_D1_P5_Medium_T2_4_Mid/6.png", "Image/P6_D1_P5_Medium_T2_4_Mid/7.png", "Image/P6_D1_P5_Medium_T2_4_Mid/8.png", "Image/P6_D1_P5_Medium_T2_4_Mid/9.png", "Image/P6_D1_P5_Medium_T2_4_Mid/10.png", "Image/P6_D1_P5_Medium_T2_4_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"That. Hey\"\nThe frames roughly correspond to the speech interval from 2334.1774 seconds to 2354.4017000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P3_Low_T1_7_Mid", "images": ["Image/P7_D3_P3_Low_T1_7_Mid/1.png", "Image/P7_D3_P3_Low_T1_7_Mid/2.png", "Image/P7_D3_P3_Low_T1_7_Mid/3.png", "Image/P7_D3_P3_Low_T1_7_Mid/4.png", "Image/P7_D3_P3_Low_T1_7_Mid/5.png", "Image/P7_D3_P3_Low_T1_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1728.8109000000002 seconds to 1740.8231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P6_High_T1_24_Right", "images": ["Image/P24_D3_P6_High_T1_24_Right/1.png", "Image/P24_D3_P6_High_T1_24_Right/2.png", "Image/P24_D3_P6_High_T1_24_Right/3.png", "Image/P24_D3_P6_High_T1_24_Right/4.png", "Image/P24_D3_P6_High_T1_24_Right/5.png", "Image/P24_D3_P6_High_T1_24_Right/6.png", "Image/P24_D3_P6_High_T1_24_Right/7.png", "Image/P24_D3_P6_High_T1_24_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet over here on the table? Finish.\"\nThe frames roughly correspond to the speech interval from 1984.4535 seconds to 1997.3015 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P7_Low_T3_14_Right", "images": ["Image/P10_D1_P7_Low_T3_14_Right/1.png", "Image/P10_D1_P7_Low_T3_14_Right/2.png", "Image/P10_D1_P7_Low_T3_14_Right/3.png", "Image/P10_D1_P7_Low_T3_14_Right/4.png", "Image/P10_D1_P7_Low_T3_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2121.22096 seconds to 2131.14569 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P1_Low_T3_14_Right", "images": ["Image/P12_D1_P1_Low_T3_14_Right/1.png", "Image/P12_D1_P1_Low_T3_14_Right/2.png", "Image/P12_D1_P1_Low_T3_14_Right/3.png", "Image/P12_D1_P1_Low_T3_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1665.0897 seconds to 1677.1731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P8_High_T1_12_Left", "images": ["Image/P2_D2_P8_High_T1_12_Left/1.png", "Image/P2_D2_P8_High_T1_12_Left/2.png", "Image/P2_D2_P8_High_T1_12_Left/3.png", "Image/P2_D2_P8_High_T1_12_Left/4.png", "Image/P2_D2_P8_High_T1_12_Left/5.png", "Image/P2_D2_P8_High_T1_12_Left/6.png", "Image/P2_D2_P8_High_T1_12_Left/7.png", "Image/P2_D2_P8_High_T1_12_Left/8.png", "Image/P2_D2_P8_High_T1_12_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2616.8806 seconds to 2632.694 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P7_Medium_T1_16_Left", "images": ["Image/P12_D2_P7_Medium_T1_16_Left/1.png", "Image/P12_D2_P7_Medium_T1_16_Left/2.png", "Image/P12_D2_P7_Medium_T1_16_Left/3.png", "Image/P12_D2_P7_Medium_T1_16_Left/4.png", "Image/P12_D2_P7_Medium_T1_16_Left/5.png", "Image/P12_D2_P7_Medium_T1_16_Left/6.png", "Image/P12_D2_P7_Medium_T1_16_Left/7.png", "Image/P12_D2_P7_Medium_T1_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 382.9249 seconds to 396.5798999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P3_Medium_T3_23_Right", "images": ["Image/P12_D3_P3_Medium_T3_23_Right/1.png", "Image/P12_D3_P3_Medium_T3_23_Right/2.png", "Image/P12_D3_P3_Medium_T3_23_Right/3.png", "Image/P12_D3_P3_Medium_T3_23_Right/4.png", "Image/P12_D3_P3_Medium_T3_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2377.48749 seconds to 2390.4612899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P3_High_T1_13_Left", "images": ["Image/P1_D2_P3_High_T1_13_Left/1.png", "Image/P1_D2_P3_High_T1_13_Left/2.png", "Image/P1_D2_P3_High_T1_13_Left/3.png", "Image/P1_D2_P3_High_T1_13_Left/4.png", "Image/P1_D2_P3_High_T1_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 736.30451 seconds to 746.7330900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P2_Low_T2_22_Right", "images": ["Image/P9_D2_P2_Low_T2_22_Right/1.png", "Image/P9_D2_P2_Low_T2_22_Right/2.png", "Image/P9_D2_P2_Low_T2_22_Right/3.png", "Image/P9_D2_P2_Low_T2_22_Right/4.png", "Image/P9_D2_P2_Low_T2_22_Right/5.png", "Image/P9_D2_P2_Low_T2_22_Right/6.png", "Image/P9_D2_P2_Low_T2_22_Right/7.png", "Image/P9_D2_P2_Low_T2_22_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water on the plant on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 2798.4017 seconds to 2812.9682 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P3_Medium_T1_20_Left", "images": ["Image/P2_D2_P3_Medium_T1_20_Left/1.png", "Image/P2_D2_P3_Medium_T1_20_Left/2.png", "Image/P2_D2_P3_Medium_T1_20_Left/3.png", "Image/P2_D2_P3_Medium_T1_20_Left/4.png", "Image/P2_D2_P3_Medium_T1_20_Left/5.png", "Image/P2_D2_P3_Medium_T1_20_Left/6.png", "Image/P2_D2_P3_Medium_T1_20_Left/7.png", "Image/P2_D2_P3_Medium_T1_20_Left/8.png", "Image/P2_D2_P3_Medium_T1_20_Left/9.png", "Image/P2_D2_P3_Medium_T1_20_Left/10.png", "Image/P2_D2_P3_Medium_T1_20_Left/11.png", "Image/P2_D2_P3_Medium_T1_20_Left/12.png", "Image/P2_D2_P3_Medium_T1_20_Left/13.png", "Image/P2_D2_P3_Medium_T1_20_Left/14.png", "Image/P2_D2_P3_Medium_T1_20_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2730.0236999999997 seconds to 2750.4424 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P5_Low_T2_15_Right", "images": ["Image/P10_D3_P5_Low_T2_15_Right/1.png", "Image/P10_D3_P5_Low_T2_15_Right/2.png", "Image/P10_D3_P5_Low_T2_15_Right/3.png", "Image/P10_D3_P5_Low_T2_15_Right/4.png", "Image/P10_D3_P5_Low_T2_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey\"\nThe frames roughly correspond to the speech interval from 410.96998999999994 seconds to 422.75949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P1_Low_T1_6_Left", "images": ["Image/P25_D2_P1_Low_T1_6_Left/1.png", "Image/P25_D2_P1_Low_T1_6_Left/2.png", "Image/P25_D2_P1_Low_T1_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the bottom. Finish.\"\nThe frames roughly correspond to the speech interval from 624.04282 seconds to 632.56143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P6_Medium_T1_21_Right", "images": ["Image/P12_D1_P6_Medium_T1_21_Right/1.png", "Image/P12_D1_P6_Medium_T1_21_Right/2.png", "Image/P12_D1_P6_Medium_T1_21_Right/3.png", "Image/P12_D1_P6_Medium_T1_21_Right/4.png", "Image/P12_D1_P6_Medium_T1_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1374.4991 seconds to 1385.816 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P5_Low_T1_19_Mid", "images": ["Image/P2_D3_P5_Low_T1_19_Mid/1.png", "Image/P2_D3_P5_Low_T1_19_Mid/2.png", "Image/P2_D3_P5_Low_T1_19_Mid/3.png", "Image/P2_D3_P5_Low_T1_19_Mid/4.png", "Image/P2_D3_P5_Low_T1_19_Mid/5.png", "Image/P2_D3_P5_Low_T1_19_Mid/6.png", "Image/P2_D3_P5_Low_T1_19_Mid/7.png", "Image/P2_D3_P5_Low_T1_19_Mid/8.png", "Image/P2_D3_P5_Low_T1_19_Mid/9.png", "Image/P2_D3_P5_Low_T1_19_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2074.6128000000003 seconds to 2092.1096 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P1_High_T2_20_Right", "images": ["Image/P20_D2_P1_High_T2_20_Right/1.png", "Image/P20_D2_P1_High_T2_20_Right/2.png", "Image/P20_D2_P1_High_T2_20_Right/3.png", "Image/P20_D2_P1_High_T2_20_Right/4.png", "Image/P20_D2_P1_High_T2_20_Right/5.png", "Image/P20_D2_P1_High_T2_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that white bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2896.4702 seconds to 2907.8704 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P7_High_T3_16_Mid", "images": ["Image/P15_D3_P7_High_T3_16_Mid/1.png", "Image/P15_D3_P7_High_T3_16_Mid/2.png", "Image/P15_D3_P7_High_T3_16_Mid/3.png", "Image/P15_D3_P7_High_T3_16_Mid/4.png", "Image/P15_D3_P7_High_T3_16_Mid/5.png", "Image/P15_D3_P7_High_T3_16_Mid/6.png", "Image/P15_D3_P7_High_T3_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the Brown cup under the table. Finish.\"\nThe frames roughly correspond to the speech interval from 727.9069999999999 seconds to 741.0186 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P8_Medium_T1_19_Right", "images": ["Image/P30_D2_P8_Medium_T1_19_Right/1.png", "Image/P30_D2_P8_Medium_T1_19_Right/2.png", "Image/P30_D2_P8_Medium_T1_19_Right/3.png", "Image/P30_D2_P8_Medium_T1_19_Right/4.png", "Image/P30_D2_P8_Medium_T1_19_Right/5.png", "Image/P30_D2_P8_Medium_T1_19_Right/6.png", "Image/P30_D2_P8_Medium_T1_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the object for this on the right up to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 279.4059 seconds to 293.8125 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P3_Low_T3_8_Right", "images": ["Image/P6_D3_P3_Low_T3_8_Right/1.png", "Image/P6_D3_P3_Low_T3_8_Right/2.png", "Image/P6_D3_P3_Low_T3_8_Right/3.png", "Image/P6_D3_P3_Low_T3_8_Right/4.png", "Image/P6_D3_P3_Low_T3_8_Right/5.png", "Image/P6_D3_P3_Low_T3_8_Right/6.png", "Image/P6_D3_P3_Low_T3_8_Right/7.png", "Image/P6_D3_P3_Low_T3_8_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3578.0867 seconds to 3593.7173000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P5_High_T1_12_Left", "images": ["Image/P3_D1_P5_High_T1_12_Left/1.png", "Image/P3_D1_P5_High_T1_12_Left/2.png", "Image/P3_D1_P5_High_T1_12_Left/3.png", "Image/P3_D1_P5_High_T1_12_Left/4.png", "Image/P3_D1_P5_High_T1_12_Left/5.png", "Image/P3_D1_P5_High_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this space? Finish.\"\nThe frames roughly correspond to the speech interval from 1324.014 seconds to 1336.5156 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P1_High_T3_6_Right", "images": ["Image/P27_D3_P1_High_T3_6_Right/1.png", "Image/P27_D3_P1_High_T3_6_Right/2.png", "Image/P27_D3_P1_High_T3_6_Right/3.png", "Image/P27_D3_P1_High_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 307.915 seconds to 317.992 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D2_P5_Low_T3_25_Left", "images": ["Image/P28_D2_P5_Low_T3_25_Left/1.png", "Image/P28_D2_P5_Low_T3_25_Left/2.png", "Image/P28_D2_P5_Low_T3_25_Left/3.png", "Image/P28_D2_P5_Low_T3_25_Left/4.png", "Image/P28_D2_P5_Low_T3_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 943.1317 seconds to 953.4599 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P9_High_T1_15_Left", "images": ["Image/P24_D2_P9_High_T1_15_Left/1.png", "Image/P24_D2_P9_High_T1_15_Left/2.png", "Image/P24_D2_P9_High_T1_15_Left/3.png", "Image/P24_D2_P9_High_T1_15_Left/4.png", "Image/P24_D2_P9_High_T1_15_Left/5.png", "Image/P24_D2_P9_High_T1_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this area please? Finish.\"\nThe frames roughly correspond to the speech interval from 61.17073999999999 seconds to 76.25605999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P5_High_T1_19_Mid", "images": ["Image/P19_D2_P5_High_T1_19_Mid/1.png", "Image/P19_D2_P5_High_T1_19_Mid/2.png", "Image/P19_D2_P5_High_T1_19_Mid/3.png", "Image/P19_D2_P5_High_T1_19_Mid/4.png", "Image/P19_D2_P5_High_T1_19_Mid/5.png", "Image/P19_D2_P5_High_T1_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this container over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1355.18185 seconds to 1368.15553 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P5_Medium_T3_7_Left", "images": ["Image/P19_D3_P5_Medium_T3_7_Left/1.png", "Image/P19_D3_P5_Medium_T3_7_Left/2.png", "Image/P19_D3_P5_Medium_T3_7_Left/3.png", "Image/P19_D3_P5_Medium_T3_7_Left/4.png", "Image/P19_D3_P5_Medium_T3_7_Left/5.png", "Image/P19_D3_P5_Medium_T3_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 2426.6517999999996 seconds to 2439.9033999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P8_Low_T2_3_Right", "images": ["Image/P29_D1_P8_Low_T2_3_Right/1.png", "Image/P29_D1_P8_Low_T2_3_Right/2.png", "Image/P29_D1_P8_Low_T2_3_Right/3.png", "Image/P29_D1_P8_Low_T2_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2437.2552499999997 seconds to 2448.8367999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P4_High_T1_12_Right", "images": ["Image/P7_D2_P4_High_T1_12_Right/1.png", "Image/P7_D2_P4_High_T1_12_Right/2.png", "Image/P7_D2_P4_High_T1_12_Right/3.png", "Image/P7_D2_P4_High_T1_12_Right/4.png", "Image/P7_D2_P4_High_T1_12_Right/5.png", "Image/P7_D2_P4_High_T1_12_Right/6.png", "Image/P7_D2_P4_High_T1_12_Right/7.png", "Image/P7_D2_P4_High_T1_12_Right/8.png", "Image/P7_D2_P4_High_T1_12_Right/9.png", "Image/P7_D2_P4_High_T1_12_Right/10.png", "Image/P7_D2_P4_High_T1_12_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 941.5606 seconds to 959.3224800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P2_Medium_T1_26_Mid", "images": ["Image/P21_D1_P2_Medium_T1_26_Mid/1.png", "Image/P21_D1_P2_Medium_T1_26_Mid/2.png", "Image/P21_D1_P2_Medium_T1_26_Mid/3.png", "Image/P21_D1_P2_Medium_T1_26_Mid/4.png", "Image/P21_D1_P2_Medium_T1_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1284.1558 seconds to 1295.7658 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P9_Medium_T2_7_Left", "images": ["Image/P17_D3_P9_Medium_T2_7_Left/1.png", "Image/P17_D3_P9_Medium_T2_7_Left/2.png", "Image/P17_D3_P9_Medium_T2_7_Left/3.png", "Image/P17_D3_P9_Medium_T2_7_Left/4.png", "Image/P17_D3_P9_Medium_T2_7_Left/5.png", "Image/P17_D3_P9_Medium_T2_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the leftmost green mug on the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1110.7252 seconds to 1123.9614 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P4_Medium_T1_8_Mid", "images": ["Image/P4_D2_P4_Medium_T1_8_Mid/1.png", "Image/P4_D2_P4_Medium_T1_8_Mid/2.png", "Image/P4_D2_P4_Medium_T1_8_Mid/3.png", "Image/P4_D2_P4_Medium_T1_8_Mid/4.png", "Image/P4_D2_P4_Medium_T1_8_Mid/5.png", "Image/P4_D2_P4_Medium_T1_8_Mid/6.png", "Image/P4_D2_P4_Medium_T1_8_Mid/7.png", "Image/P4_D2_P4_Medium_T1_8_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you bring the bottle in the middle to this desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1439.9121699999998 seconds to 1454.44438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P2_Medium_T3_6_Right", "images": ["Image/P15_D1_P2_Medium_T3_6_Right/1.png", "Image/P15_D1_P2_Medium_T3_6_Right/2.png", "Image/P15_D1_P2_Medium_T3_6_Right/3.png", "Image/P15_D1_P2_Medium_T3_6_Right/4.png", "Image/P15_D1_P2_Medium_T3_6_Right/5.png", "Image/P15_D1_P2_Medium_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2154.4506699999997 seconds to 2165.84997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P8_High_T3_3_Mid", "images": ["Image/P15_D1_P8_High_T3_3_Mid/1.png", "Image/P15_D1_P8_High_T3_3_Mid/2.png", "Image/P15_D1_P8_High_T3_3_Mid/3.png", "Image/P15_D1_P8_High_T3_3_Mid/4.png", "Image/P15_D1_P8_High_T3_3_Mid/5.png", "Image/P15_D1_P8_High_T3_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1898.01787 seconds to 1909.05487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P3_Medium_T2_17_Right", "images": ["Image/P7_D2_P3_Medium_T2_17_Right/1.png", "Image/P7_D2_P3_Medium_T2_17_Right/2.png", "Image/P7_D2_P3_Medium_T2_17_Right/3.png", "Image/P7_D2_P3_Medium_T2_17_Right/4.png", "Image/P7_D2_P3_Medium_T2_17_Right/5.png", "Image/P7_D2_P3_Medium_T2_17_Right/6.png", "Image/P7_D2_P3_Medium_T2_17_Right/7.png", "Image/P7_D2_P3_Medium_T2_17_Right/8.png", "Image/P7_D2_P3_Medium_T2_17_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1443.1782000000003 seconds to 1459.1869000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P4_Low_T2_16_Left", "images": ["Image/P12_D3_P4_Low_T2_16_Left/1.png", "Image/P12_D3_P4_Low_T2_16_Left/2.png", "Image/P12_D3_P4_Low_T2_16_Left/3.png", "Image/P12_D3_P4_Low_T2_16_Left/4.png", "Image/P12_D3_P4_Low_T2_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2232.8467 seconds to 2243.95335 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P1_High_T2_21_Left", "images": ["Image/P4_D1_P1_High_T2_21_Left/1.png", "Image/P4_D1_P1_High_T2_21_Left/2.png", "Image/P4_D1_P1_High_T2_21_Left/3.png", "Image/P4_D1_P1_High_T2_21_Left/4.png", "Image/P4_D1_P1_High_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the white cap? Finish.\"\nThe frames roughly correspond to the speech interval from 2474.7708 seconds to 2488.8012999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P5_Low_T2_18_Mid", "images": ["Image/P13_D2_P5_Low_T2_18_Mid/1.png", "Image/P13_D2_P5_Low_T2_18_Mid/2.png", "Image/P13_D2_P5_Low_T2_18_Mid/3.png", "Image/P13_D2_P5_Low_T2_18_Mid/4.png", "Image/P13_D2_P5_Low_T2_18_Mid/5.png", "Image/P13_D2_P5_Low_T2_18_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you fill the brown cup with water\"\nThe frames roughly correspond to the speech interval from 1040.36736 seconds to 1051.27996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P7_Medium_T1_7_Left", "images": ["Image/P9_D1_P7_Medium_T1_7_Left/1.png", "Image/P9_D1_P7_Medium_T1_7_Left/2.png", "Image/P9_D1_P7_Medium_T1_7_Left/3.png", "Image/P9_D1_P7_Medium_T1_7_Left/4.png", "Image/P9_D1_P7_Medium_T1_7_Left/5.png", "Image/P9_D1_P7_Medium_T1_7_Left/6.png", "Image/P9_D1_P7_Medium_T1_7_Left/7.png", "Image/P9_D1_P7_Medium_T1_7_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the left cup to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 1492.1349 seconds to 1506.3132999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P8_Low_T3_18_Left", "images": ["Image/P20_D3_P8_Low_T3_18_Left/1.png", "Image/P20_D3_P8_Low_T3_18_Left/2.png", "Image/P20_D3_P8_Low_T3_18_Left/3.png", "Image/P20_D3_P8_Low_T3_18_Left/4.png", "Image/P20_D3_P8_Low_T3_18_Left/5.png", "Image/P20_D3_P8_Low_T3_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the orange cup on the floor? And right there. Finished.\"\nThe frames roughly correspond to the speech interval from 1257.0435 seconds to 1269.1657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D2_P4_High_T3_24_Left", "images": ["Image/P12_D2_P4_High_T3_24_Left/1.png", "Image/P12_D2_P4_High_T3_24_Left/2.png", "Image/P12_D2_P4_High_T3_24_Left/3.png", "Image/P12_D2_P4_High_T3_24_Left/4.png", "Image/P12_D2_P4_High_T3_24_Left/5.png", "Image/P12_D2_P4_High_T3_24_Left/6.png", "Image/P12_D2_P4_High_T3_24_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 289.731 seconds to 304.0127 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P2_Low_T3_4_Mid", "images": ["Image/P20_D1_P2_Low_T3_4_Mid/1.png", "Image/P20_D1_P2_Low_T3_4_Mid/2.png", "Image/P20_D1_P2_Low_T3_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this plant. Finished.\"\nThe frames roughly correspond to the speech interval from 758.3009999999999 seconds to 769.8541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P7_High_T1_15_Left", "images": ["Image/P23_D1_P7_High_T1_15_Left/1.png", "Image/P23_D1_P7_High_T1_15_Left/2.png", "Image/P23_D1_P7_High_T1_15_Left/3.png", "Image/P23_D1_P7_High_T1_15_Left/4.png", "Image/P23_D1_P7_High_T1_15_Left/5.png", "Image/P23_D1_P7_High_T1_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant. The plant. Here. Finish.\"\nThe frames roughly correspond to the speech interval from 1648.9479 seconds to 1661.2388999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P6_Low_T3_10_Right", "images": ["Image/P26_D3_P6_Low_T3_10_Right/1.png", "Image/P26_D3_P6_Low_T3_10_Right/2.png", "Image/P26_D3_P6_Low_T3_10_Right/3.png", "Image/P26_D3_P6_Low_T3_10_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase? Finish.\"\nThe frames roughly correspond to the speech interval from 1051.9887 seconds to 1062.6649 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P6_Medium_T2_3_Left", "images": ["Image/P14_D2_P6_Medium_T2_3_Left/1.png", "Image/P14_D2_P6_Medium_T2_3_Left/2.png", "Image/P14_D2_P6_Medium_T2_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill that first one up. Finish.\"\nThe frames roughly correspond to the speech interval from 1699.4247999999998 seconds to 1708.1120999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P9_High_T3_24_Left", "images": ["Image/P1_D3_P9_High_T3_24_Left/1.png", "Image/P1_D3_P9_High_T3_24_Left/2.png", "Image/P1_D3_P9_High_T3_24_Left/3.png", "Image/P1_D3_P9_High_T3_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You have cleaned the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1807.4203999999995 seconds to 1817.3471999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P7_Low_T3_3_Left", "images": ["Image/P5_D2_P7_Low_T3_3_Left/1.png", "Image/P5_D2_P7_Low_T3_3_Left/2.png", "Image/P5_D2_P7_Low_T3_3_Left/3.png", "Image/P5_D2_P7_Low_T3_3_Left/4.png", "Image/P5_D2_P7_Low_T3_3_Left/5.png", "Image/P5_D2_P7_Low_T3_3_Left/6.png", "Image/P5_D2_P7_Low_T3_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Laying the vase over there\"\nThe frames roughly correspond to the speech interval from 264.45399999999995 seconds to 278.3606 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P4_Medium_T3_8_Right", "images": ["Image/P24_D1_P4_Medium_T3_8_Right/1.png", "Image/P24_D1_P4_Medium_T3_8_Right/2.png", "Image/P24_D1_P4_Medium_T3_8_Right/3.png", "Image/P24_D1_P4_Medium_T3_8_Right/4.png", "Image/P24_D1_P4_Medium_T3_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1420.1635 seconds to 1430.6451 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P4_Medium_T1_10_Right", "images": ["Image/P1_D1_P4_Medium_T1_10_Right/1.png", "Image/P1_D1_P4_Medium_T1_10_Right/2.png", "Image/P1_D1_P4_Medium_T1_10_Right/3.png", "Image/P1_D1_P4_Medium_T1_10_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 157.63822 seconds to 166.7429 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P5_Medium_T2_19_Mid", "images": ["Image/P7_D2_P5_Medium_T2_19_Mid/1.png", "Image/P7_D2_P5_Medium_T2_19_Mid/2.png", "Image/P7_D2_P5_Medium_T2_19_Mid/3.png", "Image/P7_D2_P5_Medium_T2_19_Mid/4.png", "Image/P7_D2_P5_Medium_T2_19_Mid/5.png", "Image/P7_D2_P5_Medium_T2_19_Mid/6.png", "Image/P7_D2_P5_Medium_T2_19_Mid/7.png", "Image/P7_D2_P5_Medium_T2_19_Mid/8.png", "Image/P7_D2_P5_Medium_T2_19_Mid/9.png", "Image/P7_D2_P5_Medium_T2_19_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1227.9500000000005 seconds to 1245.5171 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P6_Medium_T3_4_Mid", "images": ["Image/P7_D3_P6_Medium_T3_4_Mid/1.png", "Image/P7_D3_P6_Medium_T3_4_Mid/2.png", "Image/P7_D3_P6_Medium_T3_4_Mid/3.png", "Image/P7_D3_P6_Medium_T3_4_Mid/4.png", "Image/P7_D3_P6_Medium_T3_4_Mid/5.png", "Image/P7_D3_P6_Medium_T3_4_Mid/6.png", "Image/P7_D3_P6_Medium_T3_4_Mid/7.png", "Image/P7_D3_P6_Medium_T3_4_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2160.2022 seconds to 2176.6147 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P5_Medium_T3_25_Right", "images": ["Image/P17_D3_P5_Medium_T3_25_Right/1.png", "Image/P17_D3_P5_Medium_T3_25_Right/2.png", "Image/P17_D3_P5_Medium_T3_25_Right/3.png", "Image/P17_D3_P5_Medium_T3_25_Right/4.png", "Image/P17_D3_P5_Medium_T3_25_Right/5.png", "Image/P17_D3_P5_Medium_T3_25_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that rightmost mug on the shelf in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 765.6093000000001 seconds to 778.4839900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P7_Medium_T3_9_Left", "images": ["Image/P27_D3_P7_Medium_T3_9_Left/1.png", "Image/P27_D3_P7_Medium_T3_9_Left/2.png", "Image/P27_D3_P7_Medium_T3_9_Left/3.png", "Image/P27_D3_P7_Medium_T3_9_Left/4.png", "Image/P27_D3_P7_Medium_T3_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the first cup on the ground over there? Finish.\"\nThe frames roughly correspond to the speech interval from 345.34630000000004 seconds to 366.9619 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P4_Low_T1_16_Left", "images": ["Image/P8_D2_P4_Low_T1_16_Left/1.png", "Image/P8_D2_P4_Low_T1_16_Left/2.png", "Image/P8_D2_P4_Low_T1_16_Left/3.png", "Image/P8_D2_P4_Low_T1_16_Left/4.png", "Image/P8_D2_P4_Low_T1_16_Left/5.png", "Image/P8_D2_P4_Low_T1_16_Left/6.png", "Image/P8_D2_P4_Low_T1_16_Left/7.png", "Image/P8_D2_P4_Low_T1_16_Left/8.png", "Image/P8_D2_P4_Low_T1_16_Left/9.png", "Image/P8_D2_P4_Low_T1_16_Left/10.png", "Image/P8_D2_P4_Low_T1_16_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Please help me with moving the brown cap to the\"\nThe frames roughly correspond to the speech interval from 2046.37191 seconds to 2063.70233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P9_Medium_T2_2_Mid", "images": ["Image/P9_D2_P9_Medium_T2_2_Mid/1.png", "Image/P9_D2_P9_Medium_T2_2_Mid/2.png", "Image/P9_D2_P9_Medium_T2_2_Mid/3.png", "Image/P9_D2_P9_Medium_T2_2_Mid/4.png", "Image/P9_D2_P9_Medium_T2_2_Mid/5.png", "Image/P9_D2_P9_Medium_T2_2_Mid/6.png", "Image/P9_D2_P9_Medium_T2_2_Mid/7.png", "Image/P9_D2_P9_Medium_T2_2_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the middle bottle on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 2874.7496 seconds to 2888.8574 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P7_Low_T1_12_Right", "images": ["Image/P8_D3_P7_Low_T1_12_Right/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Finish.\"\nThe frames roughly correspond to the speech interval from 1127.31165 seconds to 1138.69888 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P9_High_T2_8_Right", "images": ["Image/P23_D2_P9_High_T2_8_Right/1.png", "Image/P23_D2_P9_High_T2_8_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\" Start water the green one\"\nThe frames roughly correspond to the speech interval from 572.9394 seconds to 582.1813 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P6_Medium_T1_2_Mid", "images": ["Image/P14_D3_P6_Medium_T1_2_Mid/1.png", "Image/P14_D3_P6_Medium_T1_2_Mid/2.png", "Image/P14_D3_P6_Medium_T1_2_Mid/3.png", "Image/P14_D3_P6_Medium_T1_2_Mid/4.png", "Image/P14_D3_P6_Medium_T1_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that middle bottle over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1272.91526 seconds to 1283.32986 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D2_P3_Medium_T1_19_Mid", "images": ["Image/P6_D2_P3_Medium_T1_19_Mid/1.png", "Image/P6_D2_P3_Medium_T1_19_Mid/2.png", "Image/P6_D2_P3_Medium_T1_19_Mid/3.png", "Image/P6_D2_P3_Medium_T1_19_Mid/4.png", "Image/P6_D2_P3_Medium_T1_19_Mid/5.png", "Image/P6_D2_P3_Medium_T1_19_Mid/6.png", "Image/P6_D2_P3_Medium_T1_19_Mid/7.png", "Image/P6_D2_P3_Medium_T1_19_Mid/8.png", "Image/P6_D2_P3_Medium_T1_19_Mid/9.png", "Image/P6_D2_P3_Medium_T1_19_Mid/10.png", "Image/P6_D2_P3_Medium_T1_19_Mid/11.png", "Image/P6_D2_P3_Medium_T1_19_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 975.8292 seconds to 993.872 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P8_Medium_T1_25_Left", "images": ["Image/P18_D2_P8_Medium_T1_25_Left/1.png", "Image/P18_D2_P8_Medium_T1_25_Left/2.png", "Image/P18_D2_P8_Medium_T1_25_Left/3.png", "Image/P18_D2_P8_Medium_T1_25_Left/4.png", "Image/P18_D2_P8_Medium_T1_25_Left/5.png", "Image/P18_D2_P8_Medium_T1_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this cup on the left up to the top shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 373.0006 seconds to 385.2369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P5_Low_T3_9_Mid", "images": ["Image/P5_D3_P5_Low_T3_9_Mid/1.png", "Image/P5_D3_P5_Low_T3_9_Mid/2.png", "Image/P5_D3_P5_Low_T3_9_Mid/3.png", "Image/P5_D3_P5_Low_T3_9_Mid/4.png", "Image/P5_D3_P5_Low_T3_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup on the wooden shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2283.5198 seconds to 2294.4478 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P6_Medium_T2_19_Left", "images": ["Image/P21_D1_P6_Medium_T2_19_Left/1.png", "Image/P21_D1_P6_Medium_T2_19_Left/2.png", "Image/P21_D1_P6_Medium_T2_19_Left/3.png", "Image/P21_D1_P6_Medium_T2_19_Left/4.png", "Image/P21_D1_P6_Medium_T2_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1157.6515 seconds to 1168.829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P5_High_T2_5_Right", "images": ["Image/P1_D1_P5_High_T2_5_Right/1.png", "Image/P1_D1_P5_High_T2_5_Right/2.png", "Image/P1_D1_P5_High_T2_5_Right/3.png", "Image/P1_D1_P5_High_T2_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 345.682 seconds to 354.9261 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P7_Low_T1_27_Mid", "images": ["Image/P30_D2_P7_Low_T1_27_Mid/1.png", "Image/P30_D2_P7_Low_T1_27_Mid/2.png", "Image/P30_D2_P7_Low_T1_27_Mid/3.png", "Image/P30_D2_P7_Low_T1_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 447.5162 seconds to 456.8566 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P9_Low_T3_3_Left", "images": ["Image/P17_D2_P9_Low_T3_3_Left/1.png", "Image/P17_D2_P9_Low_T3_3_Left/2.png", "Image/P17_D2_P9_Low_T3_3_Left/3.png", "Image/P17_D2_P9_Low_T3_3_Left/4.png", "Image/P17_D2_P9_Low_T3_3_Left/5.png", "Image/P17_D2_P9_Low_T3_3_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that green object on the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 379.9114 seconds to 391.7433 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P9_Medium_T3_20_Mid", "images": ["Image/P3_D3_P9_Medium_T3_20_Mid/1.png", "Image/P3_D3_P9_Medium_T3_20_Mid/2.png", "Image/P3_D3_P9_Medium_T3_20_Mid/3.png", "Image/P3_D3_P9_Medium_T3_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 995.9008 seconds to 1005.3386 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P8_Low_T3_24_Right", "images": ["Image/P23_D2_P8_Low_T3_24_Right/1.png", "Image/P23_D2_P8_Low_T3_24_Right/2.png", "Image/P23_D2_P8_Low_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 176.5127 seconds to 186.96500000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P8_High_T2_16_Mid", "images": ["Image/P9_D2_P8_High_T2_16_Mid/1.png", "Image/P9_D2_P8_High_T2_16_Mid/2.png", "Image/P9_D2_P8_High_T2_16_Mid/3.png", "Image/P9_D2_P8_High_T2_16_Mid/4.png", "Image/P9_D2_P8_High_T2_16_Mid/5.png", "Image/P9_D2_P8_High_T2_16_Mid/6.png", "Image/P9_D2_P8_High_T2_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to pour some water in this red bottle on the floor\"\nThe frames roughly correspond to the speech interval from 3302.134 seconds to 3320.5333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P3_Low_T1_4_Left", "images": ["Image/P9_D1_P3_Low_T1_4_Left/1.png", "Image/P9_D1_P3_Low_T1_4_Left/2.png", "Image/P9_D1_P3_Low_T1_4_Left/3.png", "Image/P9_D1_P3_Low_T1_4_Left/4.png", "Image/P9_D1_P3_Low_T1_4_Left/5.png", "Image/P9_D1_P3_Low_T1_4_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Could you help me to move this plant in this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1994.6318 seconds to 2011.7958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P4_High_T2_6_Mid", "images": ["Image/P21_D2_P4_High_T2_6_Mid/1.png", "Image/P21_D2_P4_High_T2_6_Mid/2.png", "Image/P21_D2_P4_High_T2_6_Mid/3.png", "Image/P21_D2_P4_High_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1443.28107 seconds to 1453.06628 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P7_Medium_T1_12_Mid", "images": ["Image/P7_D3_P7_Medium_T1_12_Mid/1.png", "Image/P7_D3_P7_Medium_T1_12_Mid/2.png", "Image/P7_D3_P7_Medium_T1_12_Mid/3.png", "Image/P7_D3_P7_Medium_T1_12_Mid/4.png", "Image/P7_D3_P7_Medium_T1_12_Mid/5.png", "Image/P7_D3_P7_Medium_T1_12_Mid/6.png", "Image/P7_D3_P7_Medium_T1_12_Mid/7.png", "Image/P7_D3_P7_Medium_T1_12_Mid/8.png", "Image/P7_D3_P7_Medium_T1_12_Mid/9.png", "Image/P7_D3_P7_Medium_T1_12_Mid/10.png", "Image/P7_D3_P7_Medium_T1_12_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1934.0739 seconds to 1952.0447 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P5_Low_T3_14_Left", "images": ["Image/P14_D2_P5_Low_T3_14_Left/1.png", "Image/P14_D2_P5_Low_T3_14_Left/2.png", "Image/P14_D2_P5_Low_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1868.2519 seconds to 1876.3877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D2_P7_Medium_T1_8_Left", "images": ["Image/P1_D2_P7_Medium_T1_8_Left/1.png", "Image/P1_D2_P7_Medium_T1_8_Left/2.png", "Image/P1_D2_P7_Medium_T1_8_Left/3.png", "Image/P1_D2_P7_Medium_T1_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move that one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1056.7761 seconds to 1066.5918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P7_Low_T3_26_Left", "images": ["Image/P20_D2_P7_Low_T3_26_Left/1.png", "Image/P20_D2_P7_Low_T3_26_Left/2.png", "Image/P20_D2_P7_Low_T3_26_Left/3.png", "Image/P20_D2_P7_Low_T3_26_Left/4.png", "Image/P20_D2_P7_Low_T3_26_Left/5.png", "Image/P20_D2_P7_Low_T3_26_Left/6.png", "Image/P20_D2_P7_Low_T3_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3212.0890000000004 seconds to 3225.4657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P6_Medium_T1_9_Mid", "images": ["Image/P7_D2_P6_Medium_T1_9_Mid/1.png", "Image/P7_D2_P6_Medium_T1_9_Mid/2.png", "Image/P7_D2_P6_Medium_T1_9_Mid/3.png", "Image/P7_D2_P6_Medium_T1_9_Mid/4.png", "Image/P7_D2_P6_Medium_T1_9_Mid/5.png", "Image/P7_D2_P6_Medium_T1_9_Mid/6.png", "Image/P7_D2_P6_Medium_T1_9_Mid/7.png", "Image/P7_D2_P6_Medium_T1_9_Mid/8.png", "Image/P7_D2_P6_Medium_T1_9_Mid/9.png", "Image/P7_D2_P6_Medium_T1_9_Mid/10.png", "Image/P7_D2_P6_Medium_T1_9_Mid/11.png", "Image/P7_D2_P6_Medium_T1_9_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Today is Friday. Move the green glass\"\nThe frames roughly correspond to the speech interval from 1077.1549000000002 seconds to 1096.6984000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P1_Medium_T1_16_Left", "images": ["Image/P30_D3_P1_Medium_T1_16_Left/1.png", "Image/P30_D3_P1_Medium_T1_16_Left/2.png", "Image/P30_D3_P1_Medium_T1_16_Left/3.png", "Image/P30_D3_P1_Medium_T1_16_Left/4.png", "Image/P30_D3_P1_Medium_T1_16_Left/5.png", "Image/P30_D3_P1_Medium_T1_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup closest to me down to the floor over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1601.3592 seconds to 1613.233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P3_High_T2_11_Right", "images": ["Image/P27_D1_P3_High_T2_11_Right/1.png", "Image/P27_D1_P3_High_T2_11_Right/2.png", "Image/P27_D1_P3_High_T2_11_Right/3.png", "Image/P27_D1_P3_High_T2_11_Right/4.png", "Image/P27_D1_P3_High_T2_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in the red cup here? Finish.\"\nThe frames roughly correspond to the speech interval from 1017.735 seconds to 1028.9683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P2_Medium_T2_5_Left", "images": ["Image/P27_D2_P2_Medium_T2_5_Left/1.png", "Image/P27_D2_P2_Medium_T2_5_Left/2.png", "Image/P27_D2_P2_Medium_T2_5_Left/3.png", "Image/P27_D2_P2_Medium_T2_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1278.48662 seconds to 1291.96105 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P2_High_T2_22_Left", "images": ["Image/P12_D1_P2_High_T2_22_Left/1.png", "Image/P12_D1_P2_High_T2_22_Left/2.png", "Image/P12_D1_P2_High_T2_22_Left/3.png", "Image/P12_D1_P2_High_T2_22_Left/4.png", "Image/P12_D1_P2_High_T2_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1328.9813 seconds to 1339.5174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P3_High_T2_18_Mid", "images": ["Image/P29_D3_P3_High_T2_18_Mid/1.png", "Image/P29_D3_P3_High_T2_18_Mid/2.png", "Image/P29_D3_P3_High_T2_18_Mid/3.png", "Image/P29_D3_P3_High_T2_18_Mid/4.png", "Image/P29_D3_P3_High_T2_18_Mid/5.png", "Image/P29_D3_P3_High_T2_18_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1723.1785999999995 seconds to 1734.2167999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P3_Medium_T2_27_Mid", "images": ["Image/P17_D2_P3_Medium_T2_27_Mid/1.png", "Image/P17_D2_P3_Medium_T2_27_Mid/2.png", "Image/P17_D2_P3_Medium_T2_27_Mid/3.png", "Image/P17_D2_P3_Medium_T2_27_Mid/4.png", "Image/P17_D2_P3_Medium_T2_27_Mid/5.png", "Image/P17_D2_P3_Medium_T2_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that white cup in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 562.5842 seconds to 574.2195 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P1_Medium_T3_12_Right", "images": ["Image/P16_D1_P1_Medium_T3_12_Right/1.png", "Image/P16_D1_P1_Medium_T3_12_Right/2.png", "Image/P16_D1_P1_Medium_T3_12_Right/3.png", "Image/P16_D1_P1_Medium_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1537.2306 seconds to 1547.3916 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P7_High_T2_9_Mid", "images": ["Image/P14_D2_P7_High_T2_9_Mid/1.png", "Image/P14_D2_P7_High_T2_9_Mid/2.png", "Image/P14_D2_P7_High_T2_9_Mid/3.png", "Image/P14_D2_P7_High_T2_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1557.04202 seconds to 1565.81828 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P5_Low_T1_20_Mid", "images": ["Image/P11_D3_P5_Low_T1_20_Mid/1.png", "Image/P11_D3_P5_Low_T1_20_Mid/2.png", "Image/P11_D3_P5_Low_T1_20_Mid/3.png", "Image/P11_D3_P5_Low_T1_20_Mid/4.png", "Image/P11_D3_P5_Low_T1_20_Mid/5.png", "Image/P11_D3_P5_Low_T1_20_Mid/6.png", "Image/P11_D3_P5_Low_T1_20_Mid/7.png", "Image/P11_D3_P5_Low_T1_20_Mid/8.png", "Image/P11_D3_P5_Low_T1_20_Mid/9.png", "Image/P11_D3_P5_Low_T1_20_Mid/10.png", "Image/P11_D3_P5_Low_T1_20_Mid/11.png", "Image/P11_D3_P5_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white glass to the larger shelf on my right side. Finished.\"\nThe frames roughly correspond to the speech interval from 1783.5449 seconds to 1800.9626 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P3_Low_T1_2_Right", "images": ["Image/P13_D3_P3_Low_T1_2_Right/1.png", "Image/P13_D3_P3_Low_T1_2_Right/2.png", "Image/P13_D3_P3_Low_T1_2_Right/3.png", "Image/P13_D3_P3_Low_T1_2_Right/4.png", "Image/P13_D3_P3_Low_T1_2_Right/5.png", "Image/P13_D3_P3_Low_T1_2_Right/6.png", "Image/P13_D3_P3_Low_T1_2_Right/7.png", "Image/P13_D3_P3_Low_T1_2_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the green bottle from the top shelf all the way to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 1510.0636 seconds to 1523.7184 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P8_Medium_T2_15_Mid", "images": ["Image/P5_D1_P8_Medium_T2_15_Mid/1.png", "Image/P5_D1_P8_Medium_T2_15_Mid/2.png", "Image/P5_D1_P8_Medium_T2_15_Mid/3.png", "Image/P5_D1_P8_Medium_T2_15_Mid/4.png", "Image/P5_D1_P8_Medium_T2_15_Mid/5.png", "Image/P5_D1_P8_Medium_T2_15_Mid/6.png", "Image/P5_D1_P8_Medium_T2_15_Mid/7.png", "Image/P5_D1_P8_Medium_T2_15_Mid/8.png", "Image/P5_D1_P8_Medium_T2_15_Mid/9.png", "Image/P5_D1_P8_Medium_T2_15_Mid/10.png", "Image/P5_D1_P8_Medium_T2_15_Mid/11.png", "Image/P5_D1_P8_Medium_T2_15_Mid/12.png", "Image/P5_D1_P8_Medium_T2_15_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. There are three plants in front of me in the bucket placed on the ground. Water the one in the middle. Finished.\"\nThe frames roughly correspond to the speech interval from 1456.1986399999998 seconds to 1476.53413 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P7_Low_T3_24_Left", "images": ["Image/P6_D2_P7_Low_T3_24_Left/1.png", "Image/P6_D2_P7_Low_T3_24_Left/2.png", "Image/P6_D2_P7_Low_T3_24_Left/3.png", "Image/P6_D2_P7_Low_T3_24_Left/4.png", "Image/P6_D2_P7_Low_T3_24_Left/5.png", "Image/P6_D2_P7_Low_T3_24_Left/6.png", "Image/P6_D2_P7_Low_T3_24_Left/7.png", "Image/P6_D2_P7_Low_T3_24_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1204.6788 seconds to 1219.4068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P4_High_T2_19_Right", "images": ["Image/P16_D3_P4_High_T2_19_Right/1.png", "Image/P16_D3_P4_High_T2_19_Right/2.png", "Image/P16_D3_P4_High_T2_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 315.572 seconds to 324.11749999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P3_High_T3_1_Right", "images": ["Image/P2_D1_P3_High_T3_1_Right/1.png", "Image/P2_D1_P3_High_T3_1_Right/2.png", "Image/P2_D1_P3_High_T3_1_Right/3.png", "Image/P2_D1_P3_High_T3_1_Right/4.png", "Image/P2_D1_P3_High_T3_1_Right/5.png", "Image/P2_D1_P3_High_T3_1_Right/6.png", "Image/P2_D1_P3_High_T3_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hey\"\nThe frames roughly correspond to the speech interval from 902.6334 seconds to 916.7871 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P4_Medium_T2_6_Right", "images": ["Image/P26_D3_P4_Medium_T2_6_Right/1.png", "Image/P26_D3_P4_Medium_T2_6_Right/2.png", "Image/P26_D3_P4_Medium_T2_6_Right/3.png", "Image/P26_D3_P4_Medium_T2_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please water the plant on the right? Finish\"\nThe frames roughly correspond to the speech interval from 980.859 seconds to 991.3817 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P4_Medium_T3_9_Right", "images": ["Image/P28_D2_P4_Medium_T3_9_Right/1.png", "Image/P28_D2_P4_Medium_T3_9_Right/2.png", "Image/P28_D2_P4_Medium_T3_9_Right/3.png", "Image/P28_D2_P4_Medium_T3_9_Right/4.png", "Image/P28_D2_P4_Medium_T3_9_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 959.3897 seconds to 969.3841 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P7_High_T2_21_Right", "images": ["Image/P29_D1_P7_High_T2_21_Right/1.png", "Image/P29_D1_P7_High_T2_21_Right/2.png", "Image/P29_D1_P7_High_T2_21_Right/3.png", "Image/P29_D1_P7_High_T2_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2879.8819999999996 seconds to 2891.1137 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P6_High_T1_18_Left", "images": ["Image/P21_D1_P6_High_T1_18_Left/1.png", "Image/P21_D1_P6_High_T1_18_Left/2.png", "Image/P21_D1_P6_High_T1_18_Left/3.png", "Image/P21_D1_P6_High_T1_18_Left/4.png", "Image/P21_D1_P6_High_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1118.1609 seconds to 1129.8954 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P2_Medium_T3_25_Left", "images": ["Image/P7_D3_P2_Medium_T3_25_Left/1.png", "Image/P7_D3_P2_Medium_T3_25_Left/2.png", "Image/P7_D3_P2_Medium_T3_25_Left/3.png", "Image/P7_D3_P2_Medium_T3_25_Left/4.png", "Image/P7_D3_P2_Medium_T3_25_Left/5.png", "Image/P7_D3_P2_Medium_T3_25_Left/6.png", "Image/P7_D3_P2_Medium_T3_25_Left/7.png", "Image/P7_D3_P2_Medium_T3_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2117.4552000000003 seconds to 2132.2106000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P6_High_T3_12_Mid", "images": ["Image/P20_D2_P6_High_T3_12_Mid/1.png", "Image/P20_D2_P6_High_T3_12_Mid/2.png", "Image/P20_D2_P6_High_T3_12_Mid/3.png", "Image/P20_D2_P6_High_T3_12_Mid/4.png", "Image/P20_D2_P6_High_T3_12_Mid/5.png", "Image/P20_D2_P6_High_T3_12_Mid/6.png", "Image/P20_D2_P6_High_T3_12_Mid/7.png", "Image/P20_D2_P6_High_T3_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the brown bottle over there? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2793.7139500000003 seconds to 2807.1731000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P3_High_T1_1_Mid", "images": ["Image/P9_D2_P3_High_T1_1_Mid/1.png", "Image/P9_D2_P3_High_T1_1_Mid/2.png", "Image/P9_D2_P3_High_T1_1_Mid/3.png", "Image/P9_D2_P3_High_T1_1_Mid/4.png", "Image/P9_D2_P3_High_T1_1_Mid/5.png", "Image/P9_D2_P3_High_T1_1_Mid/6.png", "Image/P9_D2_P3_High_T1_1_Mid/7.png", "Image/P9_D2_P3_High_T1_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the green bottle to the floor under this desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3176.6697 seconds to 3191.3183 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P8_High_T1_23_Left", "images": ["Image/P21_D1_P8_High_T1_23_Left/1.png", "Image/P21_D1_P8_High_T1_23_Left/2.png", "Image/P21_D1_P8_High_T1_23_Left/3.png", "Image/P21_D1_P8_High_T1_23_Left/4.png", "Image/P21_D1_P8_High_T1_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1265.7399 seconds to 1277.4612 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P1_Medium_T1_10_Left", "images": ["Image/P16_D2_P1_Medium_T1_10_Left/1.png", "Image/P16_D2_P1_Medium_T1_10_Left/2.png", "Image/P16_D2_P1_Medium_T1_10_Left/3.png", "Image/P16_D2_P1_Medium_T1_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase down there. Finish.\"\nThe frames roughly correspond to the speech interval from 705.4816000000001 seconds to 715.1689000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P7_Medium_T3_20_Right", "images": ["Image/P30_D1_P7_Medium_T3_20_Right/1.png", "Image/P30_D1_P7_Medium_T3_20_Right/2.png", "Image/P30_D1_P7_Medium_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 861.4942000000001 seconds to 870.0691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P8_High_T3_5_Mid", "images": ["Image/P28_D3_P8_High_T3_5_Mid/1.png", "Image/P28_D3_P8_High_T3_5_Mid/2.png", "Image/P28_D3_P8_High_T3_5_Mid/3.png", "Image/P28_D3_P8_High_T3_5_Mid/4.png", "Image/P28_D3_P8_High_T3_5_Mid/5.png", "Image/P28_D3_P8_High_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that plant on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 470.1823 seconds to 481.4711 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P8_Low_T2_1_Mid", "images": ["Image/P5_D1_P8_Low_T2_1_Mid/1.png", "Image/P5_D1_P8_Low_T2_1_Mid/2.png", "Image/P5_D1_P8_Low_T2_1_Mid/3.png", "Image/P5_D1_P8_Low_T2_1_Mid/4.png", "Image/P5_D1_P8_Low_T2_1_Mid/5.png", "Image/P5_D1_P8_Low_T2_1_Mid/6.png", "Image/P5_D1_P8_Low_T2_1_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle which is in the bucket. Finished.\"\nThe frames roughly correspond to the speech interval from 2024.2993 seconds to 2036.4757 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P5_Low_T1_15_Right", "images": ["Image/P19_D3_P5_Low_T1_15_Right/1.png", "Image/P19_D3_P5_Low_T1_15_Right/2.png", "Image/P19_D3_P5_Low_T1_15_Right/3.png", "Image/P19_D3_P5_Low_T1_15_Right/4.png", "Image/P19_D3_P5_Low_T1_15_Right/5.png", "Image/P19_D3_P5_Low_T1_15_Right/6.png", "Image/P19_D3_P5_Low_T1_15_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that flowerpot to the shelf right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2492.1861999999996 seconds to 2506.5243 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P4_High_T3_19_Mid", "images": ["Image/P24_D2_P4_High_T3_19_Mid/1.png", "Image/P24_D2_P4_High_T3_19_Mid/2.png", "Image/P24_D2_P4_High_T3_19_Mid/3.png", "Image/P24_D2_P4_High_T3_19_Mid/4.png", "Image/P24_D2_P4_High_T3_19_Mid/5.png", "Image/P24_D2_P4_High_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Please finish.\"\nThe frames roughly correspond to the speech interval from 418.6469 seconds to 430.2402 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P2_High_T2_16_Right", "images": ["Image/P19_D3_P2_High_T2_16_Right/1.png", "Image/P19_D3_P2_High_T2_16_Right/2.png", "Image/P19_D3_P2_High_T2_16_Right/3.png", "Image/P19_D3_P2_High_T2_16_Right/4.png", "Image/P19_D3_P2_High_T2_16_Right/5.png", "Image/P19_D3_P2_High_T2_16_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 2365.3763999999996 seconds to 2378.0429999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P7_Medium_T2_13_Right", "images": ["Image/P12_D1_P7_Medium_T2_13_Right/1.png", "Image/P12_D1_P7_Medium_T2_13_Right/2.png", "Image/P12_D1_P7_Medium_T2_13_Right/3.png", "Image/P12_D1_P7_Medium_T2_13_Right/4.png", "Image/P12_D1_P7_Medium_T2_13_Right/5.png", "Image/P12_D1_P7_Medium_T2_13_Right/6.png", "Image/P12_D1_P7_Medium_T2_13_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1551.5319 seconds to 1563.6143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P6_High_T2_24_Left", "images": ["Image/P21_D2_P6_High_T2_24_Left/1.png", "Image/P21_D2_P6_High_T2_24_Left/2.png", "Image/P21_D2_P6_High_T2_24_Left/3.png", "Image/P21_D2_P6_High_T2_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1512.6315000000002 seconds to 1523.1526 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P9_Low_T3_6_Right", "images": ["Image/P28_D2_P9_Low_T3_6_Right/1.png", "Image/P28_D2_P9_Low_T3_6_Right/2.png", "Image/P28_D2_P9_Low_T3_6_Right/3.png", "Image/P28_D2_P9_Low_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1098.2826 seconds to 1106.9408 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P7_High_T3_12_Left", "images": ["Image/P24_D2_P7_High_T3_12_Left/1.png", "Image/P24_D2_P7_High_T3_12_Left/2.png", "Image/P24_D2_P7_High_T3_12_Left/3.png", "Image/P24_D2_P7_High_T3_12_Left/4.png", "Image/P24_D2_P7_High_T3_12_Left/5.png", "Image/P24_D2_P7_High_T3_12_Left/6.png", "Image/P24_D2_P7_High_T3_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 510.51699999999994 seconds to 523.2266999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D1_P4_Medium_T1_9_Mid", "images": ["Image/P10_D1_P4_Medium_T1_9_Mid/1.png", "Image/P10_D1_P4_Medium_T1_9_Mid/2.png", "Image/P10_D1_P4_Medium_T1_9_Mid/3.png", "Image/P10_D1_P4_Medium_T1_9_Mid/4.png", "Image/P10_D1_P4_Medium_T1_9_Mid/5.png", "Image/P10_D1_P4_Medium_T1_9_Mid/6.png", "Image/P10_D1_P4_Medium_T1_9_Mid/7.png", "Image/P10_D1_P4_Medium_T1_9_Mid/8.png", "Image/P10_D1_P4_Medium_T1_9_Mid/9.png", "Image/P10_D1_P4_Medium_T1_9_Mid/10.png", "Image/P10_D1_P4_Medium_T1_9_Mid/11.png", "Image/P10_D1_P4_Medium_T1_9_Mid/12.png", "Image/P10_D1_P4_Medium_T1_9_Mid/13.png", "Image/P10_D1_P4_Medium_T1_9_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 2185.2251 seconds to 2207.3985 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P6_Medium_T2_5_Right", "images": ["Image/P2_D2_P6_Medium_T2_5_Right/1.png", "Image/P2_D2_P6_Medium_T2_5_Right/2.png", "Image/P2_D2_P6_Medium_T2_5_Right/3.png", "Image/P2_D2_P6_Medium_T2_5_Right/4.png", "Image/P2_D2_P6_Medium_T2_5_Right/5.png", "Image/P2_D2_P6_Medium_T2_5_Right/6.png", "Image/P2_D2_P6_Medium_T2_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2639.0971 seconds to 2654.0326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P6_Low_T3_19_Mid", "images": ["Image/P12_D2_P6_Low_T3_19_Mid/1.png", "Image/P12_D2_P6_Low_T3_19_Mid/2.png", "Image/P12_D2_P6_Low_T3_19_Mid/3.png", "Image/P12_D2_P6_Low_T3_19_Mid/4.png", "Image/P12_D2_P6_Low_T3_19_Mid/5.png", "Image/P12_D2_P6_Low_T3_19_Mid/6.png", "Image/P12_D2_P6_Low_T3_19_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean the white base that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 93.73927 seconds to 108.01975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P3_High_T2_26_Mid", "images": ["Image/P24_D1_P3_High_T2_26_Mid/1.png", "Image/P24_D1_P3_High_T2_26_Mid/2.png", "Image/P24_D1_P3_High_T2_26_Mid/3.png", "Image/P24_D1_P3_High_T2_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1442.4665 seconds to 1452.3185 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P4_High_T1_2_Right", "images": ["Image/P2_D1_P4_High_T1_2_Right/1.png", "Image/P2_D1_P4_High_T1_2_Right/2.png", "Image/P2_D1_P4_High_T1_2_Right/3.png", "Image/P2_D1_P4_High_T1_2_Right/4.png", "Image/P2_D1_P4_High_T1_2_Right/5.png", "Image/P2_D1_P4_High_T1_2_Right/6.png", "Image/P2_D1_P4_High_T1_2_Right/7.png", "Image/P2_D1_P4_High_T1_2_Right/8.png", "Image/P2_D1_P4_High_T1_2_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Could you please move this green bottle from here to here\"\nThe frames roughly correspond to the speech interval from 923.4978 seconds to 941.3884 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P3_High_T3_4_Mid", "images": ["Image/P8_D2_P3_High_T3_4_Mid/1.png", "Image/P8_D2_P3_High_T3_4_Mid/2.png", "Image/P8_D2_P3_High_T3_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the planet. Finish.\"\nThe frames roughly correspond to the speech interval from 2430.2553000000003 seconds to 2439.4556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P3_Low_T1_24_Left", "images": ["Image/P21_D2_P3_Low_T1_24_Left/1.png", "Image/P21_D2_P3_Low_T1_24_Left/2.png", "Image/P21_D2_P3_Low_T1_24_Left/3.png", "Image/P21_D2_P3_Low_T1_24_Left/4.png", "Image/P21_D2_P3_Low_T1_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1672.9464 seconds to 1684.8061000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P4_Medium_T2_14_Left", "images": ["Image/P27_D1_P4_Medium_T2_14_Left/1.png", "Image/P27_D1_P4_Medium_T2_14_Left/2.png", "Image/P27_D1_P4_Medium_T2_14_Left/3.png", "Image/P27_D1_P4_Medium_T2_14_Left/4.png", "Image/P27_D1_P4_Medium_T2_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water this plant? Finish\"\nThe frames roughly correspond to the speech interval from 844.5300000000001 seconds to 856.0546 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P1_Low_T2_8_Right", "images": ["Image/P30_D3_P1_Low_T2_8_Right/1.png", "Image/P30_D3_P1_Low_T2_8_Right/2.png", "Image/P30_D3_P1_Low_T2_8_Right/3.png", "Image/P30_D3_P1_Low_T2_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1146.47001 seconds to 1159.77686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P8_Medium_T2_8_Right", "images": ["Image/P17_D3_P8_Medium_T2_8_Right/1.png", "Image/P17_D3_P8_Medium_T2_8_Right/2.png", "Image/P17_D3_P8_Medium_T2_8_Right/3.png", "Image/P17_D3_P8_Medium_T2_8_Right/4.png", "Image/P17_D3_P8_Medium_T2_8_Right/5.png", "Image/P17_D3_P8_Medium_T2_8_Right/6.png", "Image/P17_D3_P8_Medium_T2_8_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in the mug on the floor that's to the right? Finish.\"\nThe frames roughly correspond to the speech interval from 948.0142 seconds to 962.17 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P8_Medium_T3_1_Right", "images": ["Image/P20_D3_P8_Medium_T3_1_Right/1.png", "Image/P20_D3_P8_Medium_T3_1_Right/2.png", "Image/P20_D3_P8_Medium_T3_1_Right/3.png", "Image/P20_D3_P8_Medium_T3_1_Right/4.png", "Image/P20_D3_P8_Medium_T3_1_Right/5.png", "Image/P20_D3_P8_Medium_T3_1_Right/6.png", "Image/P20_D3_P8_Medium_T3_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that bottle on the right? The bottles on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1696.215 seconds to 1709.8562 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P6_Low_T2_4_Right", "images": ["Image/P7_D3_P6_Low_T2_4_Right/1.png", "Image/P7_D3_P6_Low_T2_4_Right/2.png", "Image/P7_D3_P6_Low_T2_4_Right/3.png", "Image/P7_D3_P6_Low_T2_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1655.62057 seconds to 1667.0199300000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P6_Low_T1_12_Left", "images": ["Image/P17_D1_P6_Low_T1_12_Left/1.png", "Image/P17_D1_P6_Low_T1_12_Left/2.png", "Image/P17_D1_P6_Low_T1_12_Left/3.png", "Image/P17_D1_P6_Low_T1_12_Left/4.png", "Image/P17_D1_P6_Low_T1_12_Left/5.png", "Image/P17_D1_P6_Low_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this brown object to the left side of the table? Finish.\"\nThe frames roughly correspond to the speech interval from 1728.2243 seconds to 1740.64 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P6_Low_T1_1_Left", "images": ["Image/P12_D3_P6_Low_T1_1_Left/1.png", "Image/P12_D3_P6_Low_T1_1_Left/2.png", "Image/P12_D3_P6_Low_T1_1_Left/3.png", "Image/P12_D3_P6_Low_T1_1_Left/4.png", "Image/P12_D3_P6_Low_T1_1_Left/5.png", "Image/P12_D3_P6_Low_T1_1_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2418.57959 seconds to 2429.85499 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P6_Medium_T1_25_Left", "images": ["Image/P21_D2_P6_Medium_T1_25_Left/1.png", "Image/P21_D2_P6_Medium_T1_25_Left/2.png", "Image/P21_D2_P6_Medium_T1_25_Left/3.png", "Image/P21_D2_P6_Medium_T1_25_Left/4.png", "Image/P21_D2_P6_Medium_T1_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1460.0403 seconds to 1472.3595 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P1_High_T3_18_Right", "images": ["Image/P19_D3_P1_High_T3_18_Right/1.png", "Image/P19_D3_P1_High_T3_18_Right/2.png", "Image/P19_D3_P1_High_T3_18_Right/3.png", "Image/P19_D3_P1_High_T3_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start.\"\nThe frames roughly correspond to the speech interval from 2227.7367999999997 seconds to 2241.4615 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P8_Medium_T1_15_Right", "images": ["Image/P26_D1_P8_Medium_T1_15_Right/1.png", "Image/P26_D1_P8_Medium_T1_15_Right/2.png", "Image/P26_D1_P8_Medium_T1_15_Right/3.png", "Image/P26_D1_P8_Medium_T1_15_Right/4.png", "Image/P26_D1_P8_Medium_T1_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant on the right up on the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 409.5343 seconds to 420.5866 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P9_Medium_T2_16_Right", "images": ["Image/P26_D2_P9_Medium_T2_16_Right/1.png", "Image/P26_D2_P9_Medium_T2_16_Right/2.png", "Image/P26_D2_P9_Medium_T2_16_Right/3.png", "Image/P26_D2_P9_Medium_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water into the cup on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1628.0106 seconds to 1640.5806 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D3_P7_Medium_T1_12_Mid", "images": ["Image/P17_D3_P7_Medium_T1_12_Mid/1.png", "Image/P17_D3_P7_Medium_T1_12_Mid/2.png", "Image/P17_D3_P7_Medium_T1_12_Mid/3.png", "Image/P17_D3_P7_Medium_T1_12_Mid/4.png", "Image/P17_D3_P7_Medium_T1_12_Mid/5.png", "Image/P17_D3_P7_Medium_T1_12_Mid/6.png", "Image/P17_D3_P7_Medium_T1_12_Mid/7.png", "Image/P17_D3_P7_Medium_T1_12_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that brown object in the center on the floor to the top shelf on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 877.4675 seconds to 892.3339000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P4_Low_T2_2_Left", "images": ["Image/P26_D2_P4_Low_T2_2_Left/1.png", "Image/P26_D2_P4_Low_T2_2_Left/2.png", "Image/P26_D2_P4_Low_T2_2_Left/3.png", "Image/P26_D2_P4_Low_T2_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you pour water into that\"\nThe frames roughly correspond to the speech interval from 1241.21968 seconds to 1250.12824 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P6_Medium_T3_25_Right", "images": ["Image/P30_D2_P6_Medium_T3_25_Right/1.png", "Image/P30_D2_P6_Medium_T3_25_Right/2.png", "Image/P30_D2_P6_Medium_T3_25_Right/3.png", "Image/P30_D2_P6_Medium_T3_25_Right/4.png", "Image/P30_D2_P6_Medium_T3_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup closest to me. Finish.\"\nThe frames roughly correspond to the speech interval from 428.307 seconds to 438.3968 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P8_Medium_T3_16_Right", "images": ["Image/P9_D1_P8_Medium_T3_16_Right/1.png", "Image/P9_D1_P8_Medium_T3_16_Right/2.png", "Image/P9_D1_P8_Medium_T3_16_Right/3.png", "Image/P9_D1_P8_Medium_T3_16_Right/4.png", "Image/P9_D1_P8_Medium_T3_16_Right/5.png", "Image/P9_D1_P8_Medium_T3_16_Right/6.png", "Image/P9_D1_P8_Medium_T3_16_Right/7.png", "Image/P9_D1_P8_Medium_T3_16_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the third cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1963.6236 seconds to 1979.6606 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P2_Medium_T2_13_Right", "images": ["Image/P9_D1_P2_Medium_T2_13_Right/1.png", "Image/P9_D1_P2_Medium_T2_13_Right/2.png", "Image/P9_D1_P2_Medium_T2_13_Right/3.png", "Image/P9_D1_P2_Medium_T2_13_Right/4.png", "Image/P9_D1_P2_Medium_T2_13_Right/5.png", "Image/P9_D1_P2_Medium_T2_13_Right/6.png", "Image/P9_D1_P2_Medium_T2_13_Right/7.png", "Image/P9_D1_P2_Medium_T2_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water in the most right plant? This one. Finish.\"\nThe frames roughly correspond to the speech interval from 1769.087 seconds to 1816.3975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P6_Medium_T3_7_Mid", "images": ["Image/P14_D1_P6_Medium_T3_7_Mid/1.png", "Image/P14_D1_P6_Medium_T3_7_Mid/2.png", "Image/P14_D1_P6_Medium_T3_7_Mid/3.png", "Image/P14_D1_P6_Medium_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that mug up. Finish.\"\nThe frames roughly correspond to the speech interval from 498.8899 seconds to 508.3553 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P8_Low_T2_26_Right", "images": ["Image/P2_D2_P8_Low_T2_26_Right/1.png", "Image/P2_D2_P8_Low_T2_26_Right/2.png", "Image/P2_D2_P8_Low_T2_26_Right/3.png", "Image/P2_D2_P8_Low_T2_26_Right/4.png", "Image/P2_D2_P8_Low_T2_26_Right/5.png", "Image/P2_D2_P8_Low_T2_26_Right/6.png", "Image/P2_D2_P8_Low_T2_26_Right/7.png", "Image/P2_D2_P8_Low_T2_26_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2984.854 seconds to 3000.8053999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P6_Low_T1_9_Mid", "images": ["Image/P13_D1_P6_Low_T1_9_Mid/1.png", "Image/P13_D1_P6_Low_T1_9_Mid/2.png", "Image/P13_D1_P6_Low_T1_9_Mid/3.png", "Image/P13_D1_P6_Low_T1_9_Mid/4.png", "Image/P13_D1_P6_Low_T1_9_Mid/5.png", "Image/P13_D1_P6_Low_T1_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday\"\nThe frames roughly correspond to the speech interval from 509.4195 seconds to 521.0422000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P7_High_T3_22_Right", "images": ["Image/P22_D1_P7_High_T3_22_Right/1.png", "Image/P22_D1_P7_High_T3_22_Right/2.png", "Image/P22_D1_P7_High_T3_22_Right/3.png", "Image/P22_D1_P7_High_T3_22_Right/4.png", "Image/P22_D1_P7_High_T3_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1618.6638 seconds to 1629.8417 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P9_High_T1_7_Right", "images": ["Image/P2_D2_P9_High_T1_7_Right/1.png", "Image/P2_D2_P9_High_T1_7_Right/2.png", "Image/P2_D2_P9_High_T1_7_Right/3.png", "Image/P2_D2_P9_High_T1_7_Right/4.png", "Image/P2_D2_P9_High_T1_7_Right/5.png", "Image/P2_D2_P9_High_T1_7_Right/6.png", "Image/P2_D2_P9_High_T1_7_Right/7.png", "Image/P2_D2_P9_High_T1_7_Right/8.png", "Image/P2_D2_P9_High_T1_7_Right/9.png", "Image/P2_D2_P9_High_T1_7_Right/10.png", "Image/P2_D2_P9_High_T1_7_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hey\"\nThe frames roughly correspond to the speech interval from 2516.4762 seconds to 2533.1787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P2_High_T1_1_Left", "images": ["Image/P11_D3_P2_High_T1_1_Left/1.png", "Image/P11_D3_P2_High_T1_1_Left/2.png", "Image/P11_D3_P2_High_T1_1_Left/3.png", "Image/P11_D3_P2_High_T1_1_Left/4.png", "Image/P11_D3_P2_High_T1_1_Left/5.png", "Image/P11_D3_P2_High_T1_1_Left/6.png", "Image/P11_D3_P2_High_T1_1_Left/7.png", "Image/P11_D3_P2_High_T1_1_Left/8.png", "Image/P11_D3_P2_High_T1_1_Left/9.png", "Image/P11_D3_P2_High_T1_1_Left/10.png", "Image/P11_D3_P2_High_T1_1_Left/11.png", "Image/P11_D3_P2_High_T1_1_Left/12.png", "Image/P11_D3_P2_High_T1_1_Left/13.png", "Image/P11_D3_P2_High_T1_1_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green bottle from top of the shelf to the bottom of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1666.8255 seconds to 1686.8711 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P4_Medium_T1_6_Right", "images": ["Image/P12_D3_P4_Medium_T1_6_Right/1.png", "Image/P12_D3_P4_Medium_T1_6_Right/2.png", "Image/P12_D3_P4_Medium_T1_6_Right/3.png", "Image/P12_D3_P4_Medium_T1_6_Right/4.png", "Image/P12_D3_P4_Medium_T1_6_Right/5.png", "Image/P12_D3_P4_Medium_T1_6_Right/6.png", "Image/P12_D3_P4_Medium_T1_6_Right/7.png", "Image/P12_D3_P4_Medium_T1_6_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2252.97498 seconds to 2266.25436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P2_Medium_T1_1_Right", "images": ["Image/P18_D3_P2_Medium_T1_1_Right/1.png", "Image/P18_D3_P2_Medium_T1_1_Right/2.png", "Image/P18_D3_P2_Medium_T1_1_Right/3.png", "Image/P18_D3_P2_Medium_T1_1_Right/4.png", "Image/P18_D3_P2_Medium_T1_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green bottle on the forward right-hand side to the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 1502.6694 seconds to 1512.8301 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P5_High_T2_12_Right", "images": ["Image/P26_D3_P5_High_T2_12_Right/1.png", "Image/P26_D3_P5_High_T2_12_Right/2.png", "Image/P26_D3_P5_High_T2_12_Right/3.png", "Image/P26_D3_P5_High_T2_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into the brown vase? Finish\"\nThe frames roughly correspond to the speech interval from 960.5768 seconds to 972.0741 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P2_High_T1_22_Right", "images": ["Image/P23_D2_P2_High_T1_22_Right/1.png", "Image/P23_D2_P2_High_T1_22_Right/2.png", "Image/P23_D2_P2_High_T1_22_Right/3.png", "Image/P23_D2_P2_High_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 141.615 seconds to 151.69170000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P6_Low_T1_8_Mid", "images": ["Image/P8_D2_P6_Low_T1_8_Mid/1.png", "Image/P8_D2_P6_Low_T1_8_Mid/2.png", "Image/P8_D2_P6_Low_T1_8_Mid/3.png", "Image/P8_D2_P6_Low_T1_8_Mid/4.png", "Image/P8_D2_P6_Low_T1_8_Mid/5.png", "Image/P8_D2_P6_Low_T1_8_Mid/6.png", "Image/P8_D2_P6_Low_T1_8_Mid/7.png", "Image/P8_D2_P6_Low_T1_8_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 2024.67158 seconds to 2039.52339 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P6_High_T2_19_Right", "images": ["Image/P13_D1_P6_High_T2_19_Right/1.png", "Image/P13_D1_P6_High_T2_19_Right/2.png", "Image/P13_D1_P6_High_T2_19_Right/3.png", "Image/P13_D1_P6_High_T2_19_Right/4.png", "Image/P13_D1_P6_High_T2_19_Right/5.png", "Image/P13_D1_P6_High_T2_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you pour water into the white jug please? Finish.\"\nThe frames roughly correspond to the speech interval from 422.6703 seconds to 434.2655 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P2_Medium_T1_24_Mid", "images": ["Image/P27_D1_P2_Medium_T1_24_Mid/1.png", "Image/P27_D1_P2_Medium_T1_24_Mid/2.png", "Image/P27_D1_P2_Medium_T1_24_Mid/3.png", "Image/P27_D1_P2_Medium_T1_24_Mid/4.png", "Image/P27_D1_P2_Medium_T1_24_Mid/5.png", "Image/P27_D1_P2_Medium_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take this middle plant and put it in that basket? Finish.\"\nThe frames roughly correspond to the speech interval from 760.5243600000001 seconds to 771.8683000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P9_Medium_T3_22_Right", "images": ["Image/P15_D1_P9_Medium_T3_22_Right/1.png", "Image/P15_D1_P9_Medium_T3_22_Right/2.png", "Image/P15_D1_P9_Medium_T3_22_Right/3.png", "Image/P15_D1_P9_Medium_T3_22_Right/4.png", "Image/P15_D1_P9_Medium_T3_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1813.42457 seconds to 1825.50807 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P4_Medium_T2_20_Right", "images": ["Image/P7_D3_P4_Medium_T2_20_Right/1.png", "Image/P7_D3_P4_Medium_T2_20_Right/2.png", "Image/P7_D3_P4_Medium_T2_20_Right/3.png", "Image/P7_D3_P4_Medium_T2_20_Right/4.png", "Image/P7_D3_P4_Medium_T2_20_Right/5.png", "Image/P7_D3_P4_Medium_T2_20_Right/6.png", "Image/P7_D3_P4_Medium_T2_20_Right/7.png", "Image/P7_D3_P4_Medium_T2_20_Right/8.png", "Image/P7_D3_P4_Medium_T2_20_Right/9.png", "Image/P7_D3_P4_Medium_T2_20_Right/10.png", "Image/P7_D3_P4_Medium_T2_20_Right/11.png", "Image/P7_D3_P4_Medium_T2_20_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1859.699 seconds to 1878.8256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P1_Medium_T2_11_Mid", "images": ["Image/P30_D1_P1_Medium_T2_11_Mid/1.png", "Image/P30_D1_P1_Medium_T2_11_Mid/2.png", "Image/P30_D1_P1_Medium_T2_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 892.3547000000001 seconds to 899.8861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P4_Medium_T3_13_Mid", "images": ["Image/P11_D3_P4_Medium_T3_13_Mid/1.png", "Image/P11_D3_P4_Medium_T3_13_Mid/2.png", "Image/P11_D3_P4_Medium_T3_13_Mid/3.png", "Image/P11_D3_P4_Medium_T3_13_Mid/4.png", "Image/P11_D3_P4_Medium_T3_13_Mid/5.png", "Image/P11_D3_P4_Medium_T3_13_Mid/6.png", "Image/P11_D3_P4_Medium_T3_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the second plant on the table. Finish\"\nThe frames roughly correspond to the speech interval from 1454.21467 seconds to 1466.82575 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P4_High_T3_4_Right", "images": ["Image/P21_D1_P4_High_T3_4_Right/1.png", "Image/P21_D1_P4_High_T3_4_Right/2.png", "Image/P21_D1_P4_High_T3_4_Right/3.png", "Image/P21_D1_P4_High_T3_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1248.271 seconds to 1258.9882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P5_Low_T3_4_Right", "images": ["Image/P25_D2_P5_Low_T3_4_Right/1.png", "Image/P25_D2_P5_Low_T3_4_Right/2.png", "Image/P25_D2_P5_Low_T3_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1025.1028 seconds to 1033.4415 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P3_Medium_T2_13_Mid", "images": ["Image/P26_D3_P3_Medium_T2_13_Mid/1.png", "Image/P26_D3_P3_Medium_T2_13_Mid/2.png", "Image/P26_D3_P3_Medium_T2_13_Mid/3.png", "Image/P26_D3_P3_Medium_T2_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you water the middle plant? Finish.\"\nThe frames roughly correspond to the speech interval from 752.1120000000001 seconds to 762.789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P7_High_T1_13_Right", "images": ["Image/P14_D1_P7_High_T1_13_Right/1.png", "Image/P14_D1_P7_High_T1_13_Right/2.png", "Image/P14_D1_P7_High_T1_13_Right/3.png", "Image/P14_D1_P7_High_T1_13_Right/4.png", "Image/P14_D1_P7_High_T1_13_Right/5.png", "Image/P14_D1_P7_High_T1_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that plant there up to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 171.6893 seconds to 184.1753 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P4_Low_T1_11_Left", "images": ["Image/P27_D1_P4_Low_T1_11_Left/1.png", "Image/P27_D1_P4_Low_T1_11_Left/2.png", "Image/P27_D1_P4_Low_T1_11_Left/3.png", "Image/P27_D1_P4_Low_T1_11_Left/4.png", "Image/P27_D1_P4_Low_T1_11_Left/5.png", "Image/P27_D1_P4_Low_T1_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that red glass to the center of the table? Finish.\"\nThe frames roughly correspond to the speech interval from 881.4177000000001 seconds to 893.4856000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P4_Low_T3_27_Mid", "images": ["Image/P13_D3_P4_Low_T3_27_Mid/1.png", "Image/P13_D3_P4_Low_T3_27_Mid/2.png", "Image/P13_D3_P4_Low_T3_27_Mid/3.png", "Image/P13_D3_P4_Low_T3_27_Mid/4.png", "Image/P13_D3_P4_Low_T3_27_Mid/5.png", "Image/P13_D3_P4_Low_T3_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the cup over there\"\nThe frames roughly correspond to the speech interval from 1654.9000999999998 seconds to 1665.8401 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P1_High_T2_19_Left", "images": ["Image/P14_D3_P1_High_T2_19_Left/1.png", "Image/P14_D3_P1_High_T2_19_Left/2.png", "Image/P14_D3_P1_High_T2_19_Left/3.png", "Image/P14_D3_P1_High_T2_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water that bottle\"\nThe frames roughly correspond to the speech interval from 989.28425 seconds to 999.01761 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P1_Low_T2_27_Left", "images": ["Image/P25_D1_P1_Low_T2_27_Left/1.png", "Image/P25_D1_P1_Low_T2_27_Left/2.png", "Image/P25_D1_P1_Low_T2_27_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 478.0634 seconds to 486.0654 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P6_Low_T1_1_Left", "images": ["Image/P2_D1_P6_Low_T1_1_Left/1.png", "Image/P2_D1_P6_Low_T1_1_Left/2.png", "Image/P2_D1_P6_Low_T1_1_Left/3.png", "Image/P2_D1_P6_Low_T1_1_Left/4.png", "Image/P2_D1_P6_Low_T1_1_Left/5.png", "Image/P2_D1_P6_Low_T1_1_Left/6.png", "Image/P2_D1_P6_Low_T1_1_Left/7.png", "Image/P2_D1_P6_Low_T1_1_Left/8.png", "Image/P2_D1_P6_Low_T1_1_Left/9.png", "Image/P2_D1_P6_Low_T1_1_Left/10.png", "Image/P2_D1_P6_Low_T1_1_Left/11.png", "Image/P2_D1_P6_Low_T1_1_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 461.85959999999994 seconds to 479.6860999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P1_Medium_T2_15_Left", "images": ["Image/P17_D2_P1_Medium_T2_15_Left/1.png", "Image/P17_D2_P1_Medium_T2_15_Left/2.png", "Image/P17_D2_P1_Medium_T2_15_Left/3.png", "Image/P17_D2_P1_Medium_T2_15_Left/4.png", "Image/P17_D2_P1_Medium_T2_15_Left/5.png", "Image/P17_D2_P1_Medium_T2_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant right here on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 99.078 seconds to 112.67669999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P5_High_T2_27_Right", "images": ["Image/P22_D3_P5_High_T2_27_Right/1.png", "Image/P22_D3_P5_High_T2_27_Right/2.png", "Image/P22_D3_P5_High_T2_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 472.2738 seconds to 481.9335 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P4_High_T1_27_Right", "images": ["Image/P25_D2_P4_High_T1_27_Right/1.png", "Image/P25_D2_P4_High_T1_27_Right/2.png", "Image/P25_D2_P4_High_T1_27_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup over here. Finish.\"\nThe frames roughly correspond to the speech interval from 773.8087 seconds to 782.3957 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P6_Low_T1_21_Right", "images": ["Image/P29_D3_P6_Low_T1_21_Right/1.png", "Image/P29_D3_P6_Low_T1_21_Right/2.png", "Image/P29_D3_P6_Low_T1_21_Right/3.png", "Image/P29_D3_P6_Low_T1_21_Right/4.png", "Image/P29_D3_P6_Low_T1_21_Right/5.png", "Image/P29_D3_P6_Low_T1_21_Right/6.png", "Image/P29_D3_P6_Low_T1_21_Right/7.png", "Image/P29_D3_P6_Low_T1_21_Right/8.png", "Image/P29_D3_P6_Low_T1_21_Right/9.png", "Image/P29_D3_P6_Low_T1_21_Right/10.png", "Image/P29_D3_P6_Low_T1_21_Right/11.png", "Image/P29_D3_P6_Low_T1_21_Right/12.png", "Image/P29_D3_P6_Low_T1_21_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1763.7544999999998 seconds to 1782.323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P8_High_T2_3_Right", "images": ["Image/P17_D2_P8_High_T2_3_Right/1.png", "Image/P17_D2_P8_High_T2_3_Right/2.png", "Image/P17_D2_P8_High_T2_3_Right/3.png", "Image/P17_D2_P8_High_T2_3_Right/4.png", "Image/P17_D2_P8_High_T2_3_Right/5.png", "Image/P17_D2_P8_High_T2_3_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that green object on the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 513.7242 seconds to 526.3918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P8_Low_T1_4_Mid", "images": ["Image/P1_D1_P8_Low_T1_4_Mid/1.png", "Image/P1_D1_P8_Low_T1_4_Mid/2.png", "Image/P1_D1_P8_Low_T1_4_Mid/3.png", "Image/P1_D1_P8_Low_T1_4_Mid/4.png", "Image/P1_D1_P8_Low_T1_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 560.3876 seconds to 570.6622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P2_Low_T2_2_Mid", "images": ["Image/P22_D3_P2_Low_T2_2_Mid/1.png", "Image/P22_D3_P2_Low_T2_2_Mid/2.png", "Image/P22_D3_P2_Low_T2_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water in that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 488.0861 seconds to 498.9853 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P7_High_T3_3_Mid", "images": ["Image/P30_D2_P7_High_T3_3_Mid/1.png", "Image/P30_D2_P7_High_T3_3_Mid/2.png", "Image/P30_D2_P7_High_T3_3_Mid/3.png", "Image/P30_D2_P7_High_T3_3_Mid/4.png", "Image/P30_D2_P7_High_T3_3_Mid/5.png", "Image/P30_D2_P7_High_T3_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle object in the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 88.36864000000001 seconds to 100.03214 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P1_High_T1_11_Left", "images": ["Image/P2_D1_P1_High_T1_11_Left/1.png", "Image/P2_D1_P1_High_T1_11_Left/2.png", "Image/P2_D1_P1_High_T1_11_Left/3.png", "Image/P2_D1_P1_High_T1_11_Left/4.png", "Image/P2_D1_P1_High_T1_11_Left/5.png", "Image/P2_D1_P1_High_T1_11_Left/6.png", "Image/P2_D1_P1_High_T1_11_Left/7.png", "Image/P2_D1_P1_High_T1_11_Left/8.png", "Image/P2_D1_P1_High_T1_11_Left/9.png", "Image/P2_D1_P1_High_T1_11_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 999.8222 seconds to 1019.2647 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P4_High_T3_14_Left", "images": ["Image/P29_D3_P4_High_T3_14_Left/1.png", "Image/P29_D3_P4_High_T3_14_Left/2.png", "Image/P29_D3_P4_High_T3_14_Left/3.png", "Image/P29_D3_P4_High_T3_14_Left/4.png", "Image/P29_D3_P4_High_T3_14_Left/5.png", "Image/P29_D3_P4_High_T3_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1508.5739999999998 seconds to 1520.7262999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P7_Medium_T2_13_Mid", "images": ["Image/P8_D1_P7_Medium_T2_13_Mid/1.png", "Image/P8_D1_P7_Medium_T2_13_Mid/2.png", "Image/P8_D1_P7_Medium_T2_13_Mid/3.png", "Image/P8_D1_P7_Medium_T2_13_Mid/4.png", "Image/P8_D1_P7_Medium_T2_13_Mid/5.png", "Image/P8_D1_P7_Medium_T2_13_Mid/6.png", "Image/P8_D1_P7_Medium_T2_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" So\"\nThe frames roughly correspond to the speech interval from 790.9665999999999 seconds to 805.3942 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P4_High_T3_14_Mid", "images": ["Image/P19_D1_P4_High_T3_14_Mid/1.png", "Image/P19_D1_P4_High_T3_14_Mid/2.png", "Image/P19_D1_P4_High_T3_14_Mid/3.png", "Image/P19_D1_P4_High_T3_14_Mid/4.png", "Image/P19_D1_P4_High_T3_14_Mid/5.png", "Image/P19_D1_P4_High_T3_14_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1074.2494499999998 seconds to 1088.90728 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P4_Low_T2_13_Left", "images": ["Image/P9_D3_P4_Low_T2_13_Left/1.png", "Image/P9_D3_P4_Low_T2_13_Left/2.png", "Image/P9_D3_P4_Low_T2_13_Left/3.png", "Image/P9_D3_P4_Low_T2_13_Left/4.png", "Image/P9_D3_P4_Low_T2_13_Left/5.png", "Image/P9_D3_P4_Low_T2_13_Left/6.png", "Image/P9_D3_P4_Low_T2_13_Left/7.png", "Image/P9_D3_P4_Low_T2_13_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water on the plant on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 578.457 seconds to 593.1207999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P6_Low_T2_21_Left", "images": ["Image/P10_D2_P6_Low_T2_21_Left/1.png", "Image/P10_D2_P6_Low_T2_21_Left/2.png", "Image/P10_D2_P6_Low_T2_21_Left/3.png", "Image/P10_D2_P6_Low_T2_21_Left/4.png", "Image/P10_D2_P6_Low_T2_21_Left/5.png", "Image/P10_D2_P6_Low_T2_21_Left/6.png", "Image/P10_D2_P6_Low_T2_21_Left/7.png", "Image/P10_D2_P6_Low_T2_21_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1376.15368 seconds to 1394.5547800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P8_Medium_T3_20_Right", "images": ["Image/P6_D2_P8_Medium_T3_20_Right/1.png", "Image/P6_D2_P8_Medium_T3_20_Right/2.png", "Image/P6_D2_P8_Medium_T3_20_Right/3.png", "Image/P6_D2_P8_Medium_T3_20_Right/4.png", "Image/P6_D2_P8_Medium_T3_20_Right/5.png", "Image/P6_D2_P8_Medium_T3_20_Right/6.png", "Image/P6_D2_P8_Medium_T3_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 450.49659999999994 seconds to 464.556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P1_Low_T2_10_Right", "images": ["Image/P2_D2_P1_Low_T2_10_Right/1.png", "Image/P2_D2_P1_Low_T2_10_Right/2.png", "Image/P2_D2_P1_Low_T2_10_Right/3.png", "Image/P2_D2_P1_Low_T2_10_Right/4.png", "Image/P2_D2_P1_Low_T2_10_Right/5.png", "Image/P2_D2_P1_Low_T2_10_Right/6.png", "Image/P2_D2_P1_Low_T2_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2965.3651999999997 seconds to 2976.9619 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P4_Low_T3_4_Mid", "images": ["Image/P2_D1_P4_Low_T3_4_Mid/1.png", "Image/P2_D1_P4_Low_T3_4_Mid/2.png", "Image/P2_D1_P4_Low_T3_4_Mid/3.png", "Image/P2_D1_P4_Low_T3_4_Mid/4.png", "Image/P2_D1_P4_Low_T3_4_Mid/5.png", "Image/P2_D1_P4_Low_T3_4_Mid/6.png", "Image/P2_D1_P4_Low_T3_4_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean this plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 819.1979999999999 seconds to 835.9962 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P8_Low_T3_15_Right", "images": ["Image/P13_D3_P8_Low_T3_15_Right/1.png", "Image/P13_D3_P8_Low_T3_15_Right/2.png", "Image/P13_D3_P8_Low_T3_15_Right/3.png", "Image/P13_D3_P8_Low_T3_15_Right/4.png", "Image/P13_D3_P8_Low_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday\"\nThe frames roughly correspond to the speech interval from 1731.2927 seconds to 1741.6909 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P2_High_T2_7_Mid", "images": ["Image/P10_D2_P2_High_T2_7_Mid/1.png", "Image/P10_D2_P2_High_T2_7_Mid/2.png", "Image/P10_D2_P2_High_T2_7_Mid/3.png", "Image/P10_D2_P2_High_T2_7_Mid/4.png", "Image/P10_D2_P2_High_T2_7_Mid/5.png", "Image/P10_D2_P2_High_T2_7_Mid/6.png", "Image/P10_D2_P2_High_T2_7_Mid/7.png", "Image/P10_D2_P2_High_T2_7_Mid/8.png", "Image/P10_D2_P2_High_T2_7_Mid/9.png", "Image/P10_D2_P2_High_T2_7_Mid/10.png", "Image/P10_D2_P2_High_T2_7_Mid/11.png", "Image/P10_D2_P2_High_T2_7_Mid/12.png", "Image/P10_D2_P2_High_T2_7_Mid/13.png", "Image/P10_D2_P2_High_T2_7_Mid/14.png", "Image/P10_D2_P2_High_T2_7_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1765.38148 seconds to 1786.5256800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P6_High_T3_20_Mid", "images": ["Image/P10_D1_P6_High_T3_20_Mid/1.png", "Image/P10_D1_P6_High_T3_20_Mid/2.png", "Image/P10_D1_P6_High_T3_20_Mid/3.png", "Image/P10_D1_P6_High_T3_20_Mid/4.png", "Image/P10_D1_P6_High_T3_20_Mid/5.png", "Image/P10_D1_P6_High_T3_20_Mid/6.png", "Image/P10_D1_P6_High_T3_20_Mid/7.png", "Image/P10_D1_P6_High_T3_20_Mid/8.png", "Image/P10_D1_P6_High_T3_20_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2161.81106 seconds to 2175.9678 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P1_Medium_T1_6_Mid", "images": ["Image/P29_D3_P1_Medium_T1_6_Mid/1.png", "Image/P29_D3_P1_Medium_T1_6_Mid/2.png", "Image/P29_D3_P1_Medium_T1_6_Mid/3.png", "Image/P29_D3_P1_Medium_T1_6_Mid/4.png", "Image/P29_D3_P1_Medium_T1_6_Mid/5.png", "Image/P29_D3_P1_Medium_T1_6_Mid/6.png", "Image/P29_D3_P1_Medium_T1_6_Mid/7.png", "Image/P29_D3_P1_Medium_T1_6_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1703.9408999999998 seconds to 1717.2053999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P5_Medium_T3_23_Mid", "images": ["Image/P13_D2_P5_Medium_T3_23_Mid/1.png", "Image/P13_D2_P5_Medium_T3_23_Mid/2.png", "Image/P13_D2_P5_Medium_T3_23_Mid/3.png", "Image/P13_D2_P5_Medium_T3_23_Mid/4.png", "Image/P13_D2_P5_Medium_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the middle plant\"\nThe frames roughly correspond to the speech interval from 1158.56136 seconds to 1168.54146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P5_Medium_T1_12_Right", "images": ["Image/P8_D1_P5_Medium_T1_12_Right/1.png", "Image/P8_D1_P5_Medium_T1_12_Right/2.png", "Image/P8_D1_P5_Medium_T1_12_Right/3.png", "Image/P8_D1_P5_Medium_T1_12_Right/4.png", "Image/P8_D1_P5_Medium_T1_12_Right/5.png", "Image/P8_D1_P5_Medium_T1_12_Right/6.png", "Image/P8_D1_P5_Medium_T1_12_Right/7.png", "Image/P8_D1_P5_Medium_T1_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 814.9613999999999 seconds to 830.2366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P6_High_T2_3_Right", "images": ["Image/P14_D3_P6_High_T2_3_Right/1.png", "Image/P14_D3_P6_High_T2_3_Right/2.png", "Image/P14_D3_P6_High_T2_3_Right/3.png", "Image/P14_D3_P6_High_T2_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill up that green bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 1007.07349 seconds to 1016.85061 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P5_Medium_T3_11_Left", "images": ["Image/P16_D3_P5_Medium_T3_11_Left/1.png", "Image/P16_D3_P5_Medium_T3_11_Left/2.png", "Image/P16_D3_P5_Medium_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 402.3199 seconds to 410.88019999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P3_Medium_T2_11_Left", "images": ["Image/P2_D3_P3_Medium_T2_11_Left/1.png", "Image/P2_D3_P3_Medium_T2_11_Left/2.png", "Image/P2_D3_P3_Medium_T2_11_Left/3.png", "Image/P2_D3_P3_Medium_T2_11_Left/4.png", "Image/P2_D3_P3_Medium_T2_11_Left/5.png", "Image/P2_D3_P3_Medium_T2_11_Left/6.png", "Image/P2_D3_P3_Medium_T2_11_Left/7.png", "Image/P2_D3_P3_Medium_T2_11_Left/8.png", "Image/P2_D3_P3_Medium_T2_11_Left/9.png", "Image/P2_D3_P3_Medium_T2_11_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please pour some water into the bottle\"\nThe frames roughly correspond to the speech interval from 1932.3507 seconds to 1949.7363000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P5_Medium_T3_23_Mid", "images": ["Image/P20_D3_P5_Medium_T3_23_Mid/1.png", "Image/P20_D3_P5_Medium_T3_23_Mid/2.png", "Image/P20_D3_P5_Medium_T3_23_Mid/3.png", "Image/P20_D3_P5_Medium_T3_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Can you clean the middle plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1717.0406 seconds to 1730.8067 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P3_Medium_T3_13_Mid", "images": ["Image/P22_D3_P3_Medium_T3_13_Mid/1.png", "Image/P22_D3_P3_Medium_T3_13_Mid/2.png", "Image/P22_D3_P3_Medium_T3_13_Mid/3.png", "Image/P22_D3_P3_Medium_T3_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 267.2475 seconds to 275.95980000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P7_Low_T2_10_Mid", "images": ["Image/P10_D2_P7_Low_T2_10_Mid/1.png", "Image/P10_D2_P7_Low_T2_10_Mid/2.png", "Image/P10_D2_P7_Low_T2_10_Mid/3.png", "Image/P10_D2_P7_Low_T2_10_Mid/4.png", "Image/P10_D2_P7_Low_T2_10_Mid/5.png", "Image/P10_D2_P7_Low_T2_10_Mid/6.png", "Image/P10_D2_P7_Low_T2_10_Mid/7.png", "Image/P10_D2_P7_Low_T2_10_Mid/8.png", "Image/P10_D2_P7_Low_T2_10_Mid/9.png", "Image/P10_D2_P7_Low_T2_10_Mid/10.png", "Image/P10_D2_P7_Low_T2_10_Mid/11.png", "Image/P10_D2_P7_Low_T2_10_Mid/12.png", "Image/P10_D2_P7_Low_T2_10_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1138.4435400000002 seconds to 1161.5501900000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P9_High_T2_1_Left", "images": ["Image/P14_D2_P9_High_T2_1_Left/1.png", "Image/P14_D2_P9_High_T2_1_Left/2.png", "Image/P14_D2_P9_High_T2_1_Left/3.png", "Image/P14_D2_P9_High_T2_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill that green bottle up. Finish.\"\nThe frames roughly correspond to the speech interval from 1668.3548999999998 seconds to 1677.0632999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
