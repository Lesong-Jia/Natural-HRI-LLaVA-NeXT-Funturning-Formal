{"id": "P14_D1_P5_Medium_T1_22_Right", "images": ["Image/P14_D1_P5_Medium_T1_22_Right/1.png", "Image/P14_D1_P5_Medium_T1_22_Right/2.png", "Image/P14_D1_P5_Medium_T1_22_Right/3.png", "Image/P14_D1_P5_Medium_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move this plant right here\"\nThe frames roughly correspond to the speech interval from 569.0747 seconds to 578.484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P2_Medium_T2_18_Right", "images": ["Image/P21_D2_P2_Medium_T2_18_Right/1.png", "Image/P21_D2_P2_Medium_T2_18_Right/2.png", "Image/P21_D2_P2_Medium_T2_18_Right/3.png", "Image/P21_D2_P2_Medium_T2_18_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill the right cup with water. Finish.\"\nThe frames roughly correspond to the speech interval from 1585.2782000000002 seconds to 1595.8294 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P7_High_T1_24_Right", "images": ["Image/P2_D2_P7_High_T1_24_Right/1.png", "Image/P2_D2_P7_High_T1_24_Right/2.png", "Image/P2_D2_P7_High_T1_24_Right/3.png", "Image/P2_D2_P7_High_T1_24_Right/4.png", "Image/P2_D2_P7_High_T1_24_Right/5.png", "Image/P2_D2_P7_High_T1_24_Right/6.png", "Image/P2_D2_P7_High_T1_24_Right/7.png", "Image/P2_D2_P7_High_T1_24_Right/8.png", "Image/P2_D2_P7_High_T1_24_Right/9.png", "Image/P2_D2_P7_High_T1_24_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2588.9163 seconds to 2606.3431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P1_Low_T3_19_Left", "images": ["Image/P28_D1_P1_Low_T3_19_Left/1.png", "Image/P28_D1_P1_Low_T3_19_Left/2.png", "Image/P28_D1_P1_Low_T3_19_Left/3.png", "Image/P28_D1_P1_Low_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1457.395 seconds to 1466.679 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P6_Low_T3_20_Left", "images": ["Image/P6_D2_P6_Low_T3_20_Left/1.png", "Image/P6_D2_P6_Low_T3_20_Left/2.png", "Image/P6_D2_P6_Low_T3_20_Left/3.png", "Image/P6_D2_P6_Low_T3_20_Left/4.png", "Image/P6_D2_P6_Low_T3_20_Left/5.png", "Image/P6_D2_P6_Low_T3_20_Left/6.png", "Image/P6_D2_P6_Low_T3_20_Left/7.png", "Image/P6_D2_P6_Low_T3_20_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hi Friday\"\nThe frames roughly correspond to the speech interval from 771.8291999999999 seconds to 785.4176 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P4_High_T3_22_Right", "images": ["Image/P30_D2_P4_High_T3_22_Right/1.png", "Image/P30_D2_P4_High_T3_22_Right/2.png", "Image/P30_D2_P4_High_T3_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 395.6774 seconds to 403.82130000000006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P3_Low_T1_13_Right", "images": ["Image/P7_D1_P3_Low_T1_13_Right/1.png", "Image/P7_D1_P3_Low_T1_13_Right/2.png", "Image/P7_D1_P3_Low_T1_13_Right/3.png", "Image/P7_D1_P3_Low_T1_13_Right/4.png", "Image/P7_D1_P3_Low_T1_13_Right/5.png", "Image/P7_D1_P3_Low_T1_13_Right/6.png", "Image/P7_D1_P3_Low_T1_13_Right/7.png", "Image/P7_D1_P3_Low_T1_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 664.6854 seconds to 680.721 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P8_High_T3_10_Mid", "images": ["Image/P11_D3_P8_High_T3_10_Mid/1.png", "Image/P11_D3_P8_High_T3_10_Mid/2.png", "Image/P11_D3_P8_High_T3_10_Mid/3.png", "Image/P11_D3_P8_High_T3_10_Mid/4.png", "Image/P11_D3_P8_High_T3_10_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the red bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 1975.6964 seconds to 1993.3044 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P5_High_T1_23_Right", "images": ["Image/P3_D3_P5_High_T1_23_Right/1.png", "Image/P3_D3_P5_High_T1_23_Right/2.png", "Image/P3_D3_P5_High_T1_23_Right/3.png", "Image/P3_D3_P5_High_T1_23_Right/4.png", "Image/P3_D3_P5_High_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 829.0887 seconds to 840.1686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P3_Medium_T2_4_Right", "images": ["Image/P30_D3_P3_Medium_T2_4_Right/1.png", "Image/P30_D3_P3_Medium_T2_4_Right/2.png", "Image/P30_D3_P3_Medium_T2_4_Right/3.png", "Image/P30_D3_P3_Medium_T2_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant closest to me. Finish.\"\nThe frames roughly correspond to the speech interval from 1285.3352 seconds to 1296.8177 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P5_Medium_T2_15_Left", "images": ["Image/P11_D2_P5_Medium_T2_15_Left/1.png", "Image/P11_D2_P5_Medium_T2_15_Left/2.png", "Image/P11_D2_P5_Medium_T2_15_Left/3.png", "Image/P11_D2_P5_Medium_T2_15_Left/4.png", "Image/P11_D2_P5_Medium_T2_15_Left/5.png", "Image/P11_D2_P5_Medium_T2_15_Left/6.png", "Image/P11_D2_P5_Medium_T2_15_Left/7.png", "Image/P11_D2_P5_Medium_T2_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the first plant from the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 969.1403 seconds to 983.873 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P8_High_T1_12_Right", "images": ["Image/P12_D2_P8_High_T1_12_Right/1.png", "Image/P12_D2_P8_High_T1_12_Right/2.png", "Image/P12_D2_P8_High_T1_12_Right/3.png", "Image/P12_D2_P8_High_T1_12_Right/4.png", "Image/P12_D2_P8_High_T1_12_Right/5.png", "Image/P12_D2_P8_High_T1_12_Right/6.png", "Image/P12_D2_P8_High_T1_12_Right/7.png", "Image/P12_D2_P8_High_T1_12_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the orange vase on the bottom up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 539.66716 seconds to 553.60003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P4_High_T1_18_Mid", "images": ["Image/P10_D2_P4_High_T1_18_Mid/1.png", "Image/P10_D2_P4_High_T1_18_Mid/2.png", "Image/P10_D2_P4_High_T1_18_Mid/3.png", "Image/P10_D2_P4_High_T1_18_Mid/4.png", "Image/P10_D2_P4_High_T1_18_Mid/5.png", "Image/P10_D2_P4_High_T1_18_Mid/6.png", "Image/P10_D2_P4_High_T1_18_Mid/7.png", "Image/P10_D2_P4_High_T1_18_Mid/8.png", "Image/P10_D2_P4_High_T1_18_Mid/9.png", "Image/P10_D2_P4_High_T1_18_Mid/10.png", "Image/P10_D2_P4_High_T1_18_Mid/11.png", "Image/P10_D2_P4_High_T1_18_Mid/12.png", "Image/P10_D2_P4_High_T1_18_Mid/13.png", "Image/P10_D2_P4_High_T1_18_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1346.12838 seconds to 1367.70298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P3_High_T1_2_Right", "images": ["Image/P19_D3_P3_High_T1_2_Right/1.png", "Image/P19_D3_P3_High_T1_2_Right/2.png", "Image/P19_D3_P3_High_T1_2_Right/3.png", "Image/P19_D3_P3_High_T1_2_Right/4.png", "Image/P19_D3_P3_High_T1_2_Right/5.png", "Image/P19_D3_P3_High_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that container down here. Finish.\"\nThe frames roughly correspond to the speech interval from 2593.3428 seconds to 2607.7915999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P1_Medium_T3_15_Mid", "images": ["Image/P23_D3_P1_Medium_T3_15_Mid/1.png", "Image/P23_D3_P1_Medium_T3_15_Mid/2.png", "Image/P23_D3_P1_Medium_T3_15_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 867.8211000000001 seconds to 876.9104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P3_Low_T1_7_Mid", "images": ["Image/P3_D3_P3_Low_T1_7_Mid/1.png", "Image/P3_D3_P3_Low_T1_7_Mid/2.png", "Image/P3_D3_P3_Low_T1_7_Mid/3.png", "Image/P3_D3_P3_Low_T1_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1013.8322 seconds to 1025.0818 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P5_Low_T1_2_Left", "images": ["Image/P15_D2_P5_Low_T1_2_Left/1.png", "Image/P15_D2_P5_Low_T1_2_Left/2.png", "Image/P15_D2_P5_Low_T1_2_Left/3.png", "Image/P15_D2_P5_Low_T1_2_Left/4.png", "Image/P15_D2_P5_Low_T1_2_Left/5.png", "Image/P15_D2_P5_Low_T1_2_Left/6.png", "Image/P15_D2_P5_Low_T1_2_Left/7.png", "Image/P15_D2_P5_Low_T1_2_Left/8.png", "Image/P15_D2_P5_Low_T1_2_Left/9.png", "Image/P15_D2_P5_Low_T1_2_Left/10.png", "Image/P15_D2_P5_Low_T1_2_Left/11.png", "Image/P15_D2_P5_Low_T1_2_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green pot from the middle shelf to the right shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3221.9102 seconds to 3238.9879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P9_High_T2_4_Right", "images": ["Image/P17_D1_P9_High_T2_4_Right/1.png", "Image/P17_D1_P9_High_T2_4_Right/2.png", "Image/P17_D1_P9_High_T2_4_Right/3.png", "Image/P17_D1_P9_High_T2_4_Right/4.png", "Image/P17_D1_P9_High_T2_4_Right/5.png", "Image/P17_D1_P9_High_T2_4_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that plant on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1563.9827 seconds to 1576.4263 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P5_High_T2_15_Right", "images": ["Image/P6_D3_P5_High_T2_15_Right/1.png", "Image/P6_D3_P5_High_T2_15_Right/2.png", "Image/P6_D3_P5_High_T2_15_Right/3.png", "Image/P6_D3_P5_High_T2_15_Right/4.png", "Image/P6_D3_P5_High_T2_15_Right/5.png", "Image/P6_D3_P5_High_T2_15_Right/6.png", "Image/P6_D3_P5_High_T2_15_Right/7.png", "Image/P6_D3_P5_High_T2_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4685.2288 seconds to 4703.8004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P9_Medium_T2_15_Right", "images": ["Image/P10_D1_P9_Medium_T2_15_Right/1.png", "Image/P10_D1_P9_Medium_T2_15_Right/2.png", "Image/P10_D1_P9_Medium_T2_15_Right/3.png", "Image/P10_D1_P9_Medium_T2_15_Right/4.png", "Image/P10_D1_P9_Medium_T2_15_Right/5.png", "Image/P10_D1_P9_Medium_T2_15_Right/6.png", "Image/P10_D1_P9_Medium_T2_15_Right/7.png", "Image/P10_D1_P9_Medium_T2_15_Right/8.png", "Image/P10_D1_P9_Medium_T2_15_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2317.395 seconds to 2331.7592999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P2_Low_T2_18_Left", "images": ["Image/P17_D2_P2_Low_T2_18_Left/1.png", "Image/P17_D2_P2_Low_T2_18_Left/2.png", "Image/P17_D2_P2_Low_T2_18_Left/3.png", "Image/P17_D2_P2_Low_T2_18_Left/4.png", "Image/P17_D2_P2_Low_T2_18_Left/5.png", "Image/P17_D2_P2_Low_T2_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour some water in that brown cup right in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 585.0517 seconds to 597.2007 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P9_Medium_T3_14_Left", "images": ["Image/P30_D2_P9_Medium_T3_14_Left/1.png", "Image/P30_D2_P9_Medium_T3_14_Left/2.png", "Image/P30_D2_P9_Medium_T3_14_Left/3.png", "Image/P30_D2_P9_Medium_T3_14_Left/4.png", "Image/P30_D2_P9_Medium_T3_14_Left/5.png", "Image/P30_D2_P9_Medium_T3_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move or clean the plant furthest away from you. Finish.\"\nThe frames roughly correspond to the speech interval from 375.9117 seconds to 388.1333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P3_Medium_T2_12_Mid", "images": ["Image/P30_D1_P3_Medium_T2_12_Mid/1.png", "Image/P30_D1_P3_Medium_T2_12_Mid/2.png", "Image/P30_D1_P3_Medium_T2_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 764.2927000000001 seconds to 773.0332000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P9_Medium_T1_14_Mid", "images": ["Image/P26_D1_P9_Medium_T1_14_Mid/1.png", "Image/P26_D1_P9_Medium_T1_14_Mid/2.png", "Image/P26_D1_P9_Medium_T1_14_Mid/3.png", "Image/P26_D1_P9_Medium_T1_14_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the middle plant up to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 170.20929999999998 seconds to 181.2898 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P1_Medium_T1_3_Left", "images": ["Image/P19_D3_P1_Medium_T1_3_Left/1.png", "Image/P19_D3_P1_Medium_T1_3_Left/2.png", "Image/P19_D3_P1_Medium_T1_3_Left/3.png", "Image/P19_D3_P1_Medium_T1_3_Left/4.png", "Image/P19_D3_P1_Medium_T1_3_Left/5.png", "Image/P19_D3_P1_Medium_T1_3_Left/6.png", "Image/P19_D3_P1_Medium_T1_3_Left/7.png", "Image/P19_D3_P1_Medium_T1_3_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase to the floor right there. Finish.\"\nThe frames roughly correspond to the speech interval from 2469.8727 seconds to 2484.3354999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P3_High_T3_19_Left", "images": ["Image/P15_D1_P3_High_T3_19_Left/1.png", "Image/P15_D1_P3_High_T3_19_Left/2.png", "Image/P15_D1_P3_High_T3_19_Left/3.png", "Image/P15_D1_P3_High_T3_19_Left/4.png", "Image/P15_D1_P3_High_T3_19_Left/5.png", "Image/P15_D1_P3_High_T3_19_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the white container on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1574.1297699999998 seconds to 1586.85207 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P2_Low_T1_19_Mid", "images": ["Image/P3_D2_P2_Low_T1_19_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you give some water to this bottle? Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2232.99407 seconds to 2243.48965 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P5_Medium_T2_25_Right", "images": ["Image/P25_D1_P5_Medium_T2_25_Right/1.png", "Image/P25_D1_P5_Medium_T2_25_Right/2.png", "Image/P25_D1_P5_Medium_T2_25_Right/3.png", "Image/P25_D1_P5_Medium_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the first one. Finish.\"\nThe frames roughly correspond to the speech interval from 530.5957 seconds to 539.3926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P5_High_T3_6_Left", "images": ["Image/P15_D2_P5_High_T3_6_Left/1.png", "Image/P15_D2_P5_High_T3_6_Left/2.png", "Image/P15_D2_P5_High_T3_6_Left/3.png", "Image/P15_D2_P5_High_T3_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 2696.2412 seconds to 2705.5792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P2_Medium_T3_1_Left", "images": ["Image/P22_D2_P2_Medium_T3_1_Left/1.png", "Image/P22_D2_P2_Medium_T3_1_Left/2.png", "Image/P22_D2_P2_Medium_T3_1_Left/3.png", "Image/P22_D2_P2_Medium_T3_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 666.9843999999999 seconds to 677.3120999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P4_Low_T1_27_Right", "images": ["Image/P29_D1_P4_Low_T1_27_Right/1.png", "Image/P29_D1_P4_Low_T1_27_Right/2.png", "Image/P29_D1_P4_Low_T1_27_Right/3.png", "Image/P29_D1_P4_Low_T1_27_Right/4.png", "Image/P29_D1_P4_Low_T1_27_Right/5.png", "Image/P29_D1_P4_Low_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2918.9559 seconds to 2930.5067999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P6_Medium_T2_1_Right", "images": ["Image/P4_D2_P6_Medium_T2_1_Right/1.png", "Image/P4_D2_P6_Medium_T2_1_Right/2.png", "Image/P4_D2_P6_Medium_T2_1_Right/3.png", "Image/P4_D2_P6_Medium_T2_1_Right/4.png", "Image/P4_D2_P6_Medium_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the right bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1910.2804 seconds to 1922.3602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P2_Low_T3_5_Mid", "images": ["Image/P21_D1_P2_Low_T3_5_Mid/1.png", "Image/P21_D1_P2_Low_T3_5_Mid/2.png", "Image/P21_D1_P2_Low_T3_5_Mid/3.png", "Image/P21_D1_P2_Low_T3_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1229.7025 seconds to 1240.4749 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P8_High_T2_5_Left", "images": ["Image/P12_D3_P8_High_T2_5_Left/1.png", "Image/P12_D3_P8_High_T2_5_Left/2.png", "Image/P12_D3_P8_High_T2_5_Left/3.png", "Image/P12_D3_P8_High_T2_5_Left/4.png", "Image/P12_D3_P8_High_T2_5_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that plant on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2693.16469 seconds to 2703.4223899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P5_Low_T1_19_Mid", "images": ["Image/P22_D2_P5_Low_T1_19_Mid/1.png", "Image/P22_D2_P5_Low_T1_19_Mid/2.png", "Image/P22_D2_P5_Low_T1_19_Mid/3.png", "Image/P22_D2_P5_Low_T1_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put the vase right there. Finish.\"\nThe frames roughly correspond to the speech interval from 934.1348 seconds to 943.8103 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P2_Low_T1_6_Right", "images": ["Image/P18_D1_P2_Low_T1_6_Right/1.png", "Image/P18_D1_P2_Low_T1_6_Right/2.png", "Image/P18_D1_P2_Low_T1_6_Right/3.png", "Image/P18_D1_P2_Low_T1_6_Right/4.png", "Image/P18_D1_P2_Low_T1_6_Right/5.png", "Image/P18_D1_P2_Low_T1_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1062.4603 seconds to 1073.8322 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P6_Low_T2_15_Right", "images": ["Image/P30_D1_P6_Low_T2_15_Right/1.png", "Image/P30_D1_P6_Low_T2_15_Right/2.png", "Image/P30_D1_P6_Low_T2_15_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 923.9531 seconds to 933.0558 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P4_High_T1_1_Right", "images": ["Image/P19_D3_P4_High_T1_1_Right/1.png", "Image/P19_D3_P4_High_T1_1_Right/2.png", "Image/P19_D3_P4_High_T1_1_Right/3.png", "Image/P19_D3_P4_High_T1_1_Right/4.png", "Image/P19_D3_P4_High_T1_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please move this vase over here. Finish.\"\nThe frames roughly correspond to the speech interval from 2127.87462 seconds to 2138.87081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P7_Low_T2_6_Right", "images": ["Image/P26_D1_P7_Low_T2_6_Right/1.png", "Image/P26_D1_P7_Low_T2_6_Right/2.png", "Image/P26_D1_P7_Low_T2_6_Right/3.png", "Image/P26_D1_P7_Low_T2_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 150.27659999999997 seconds to 160.2145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P7_High_T3_12_Right", "images": ["Image/P21_D1_P7_High_T3_12_Right/1.png", "Image/P21_D1_P7_High_T3_12_Right/2.png", "Image/P21_D1_P7_High_T3_12_Right/3.png", "Image/P21_D1_P7_High_T3_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 852.4165399999999 seconds to 864.0946099999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D1_P6_Medium_T1_3_Mid", "images": ["Image/P10_D1_P6_Medium_T1_3_Mid/1.png", "Image/P10_D1_P6_Medium_T1_3_Mid/2.png", "Image/P10_D1_P6_Medium_T1_3_Mid/3.png", "Image/P10_D1_P6_Medium_T1_3_Mid/4.png", "Image/P10_D1_P6_Medium_T1_3_Mid/5.png", "Image/P10_D1_P6_Medium_T1_3_Mid/6.png", "Image/P10_D1_P6_Medium_T1_3_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2359.0712 seconds to 2373.9788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P7_High_T2_5_Mid", "images": ["Image/P8_D3_P7_High_T2_5_Mid/1.png", "Image/P8_D3_P7_High_T2_5_Mid/2.png", "Image/P8_D3_P7_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 1575.2511 seconds to 1586.7487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P3_Medium_T3_19_Left", "images": ["Image/P18_D3_P3_Medium_T3_19_Left/1.png", "Image/P18_D3_P3_Medium_T3_19_Left/2.png", "Image/P18_D3_P3_Medium_T3_19_Left/3.png", "Image/P18_D3_P3_Medium_T3_19_Left/4.png", "Image/P18_D3_P3_Medium_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1270.7085000000002 seconds to 1281.7589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P9_Low_T2_6_Left", "images": ["Image/P11_D2_P9_Low_T2_6_Left/1.png", "Image/P11_D2_P9_Low_T2_6_Left/2.png", "Image/P11_D2_P9_Low_T2_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant. Please finish.\"\nThe frames roughly correspond to the speech interval from 595.774 seconds to 605.3833 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P6_High_T3_7_Left", "images": ["Image/P1_D1_P6_High_T3_7_Left/1.png", "Image/P1_D1_P6_High_T3_7_Left/2.png", "Image/P1_D1_P6_High_T3_7_Left/3.png", "Image/P1_D1_P6_High_T3_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 192.4017 seconds to 201.7979 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P9_Low_T2_7_Mid", "images": ["Image/P19_D3_P9_Low_T2_7_Mid/1.png", "Image/P19_D3_P9_Low_T2_7_Mid/2.png", "Image/P19_D3_P9_Low_T2_7_Mid/3.png", "Image/P19_D3_P9_Low_T2_7_Mid/4.png", "Image/P19_D3_P9_Low_T2_7_Mid/5.png", "Image/P19_D3_P9_Low_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 2406.7758 seconds to 2420.3871 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P5_High_T2_20_Mid", "images": ["Image/P14_D1_P5_High_T2_20_Mid/1.png", "Image/P14_D1_P5_High_T2_20_Mid/2.png", "Image/P14_D1_P5_High_T2_20_Mid/3.png", "Image/P14_D1_P5_High_T2_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 481.0875 seconds to 490.5241 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P3_Medium_T3_4_Left", "images": ["Image/P1_D3_P3_Medium_T3_4_Left/1.png", "Image/P1_D3_P3_Medium_T3_4_Left/2.png", "Image/P1_D3_P3_Medium_T3_4_Left/3.png", "Image/P1_D3_P3_Medium_T3_4_Left/4.png", "Image/P1_D3_P3_Medium_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the left plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1350.06884 seconds to 1360.91367 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P6_Low_T1_10_Mid", "images": ["Image/P3_D2_P6_Low_T1_10_Mid/1.png", "Image/P3_D2_P6_Low_T1_10_Mid/2.png", "Image/P3_D2_P6_Low_T1_10_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 2407.1854000000003 seconds to 2418.8235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P9_Low_T3_26_Mid", "images": ["Image/P4_D3_P9_Low_T3_26_Mid/1.png", "Image/P4_D3_P9_Low_T3_26_Mid/2.png", "Image/P4_D3_P9_Low_T3_26_Mid/3.png", "Image/P4_D3_P9_Low_T3_26_Mid/4.png", "Image/P4_D3_P9_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the white bottle? Finish\"\nThe frames roughly correspond to the speech interval from 815.7847999999999 seconds to 827.8959 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P7_Low_T1_20_Mid", "images": ["Image/P25_D3_P7_Low_T1_20_Mid/1.png", "Image/P25_D3_P7_Low_T1_20_Mid/2.png", "Image/P25_D3_P7_Low_T1_20_Mid/3.png", "Image/P25_D3_P7_Low_T1_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1725.38356 seconds to 1734.41628 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P7_High_T2_10_Left", "images": ["Image/P30_D3_P7_High_T2_10_Left/1.png", "Image/P30_D3_P7_High_T2_10_Left/2.png", "Image/P30_D3_P7_High_T2_10_Left/3.png", "Image/P30_D3_P7_High_T2_10_Left/4.png", "Image/P30_D3_P7_High_T2_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1421.1382 seconds to 1431.6061 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P4_Medium_T2_23_Right", "images": ["Image/P3_D3_P4_Medium_T2_23_Right/1.png", "Image/P3_D3_P4_Medium_T2_23_Right/2.png", "Image/P3_D3_P4_Medium_T2_23_Right/3.png", "Image/P3_D3_P4_Medium_T2_23_Right/4.png", "Image/P3_D3_P4_Medium_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 747.7130999999999 seconds to 758.5298 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P4_Medium_T3_14_Left", "images": ["Image/P12_D1_P4_Medium_T3_14_Left/1.png", "Image/P12_D1_P4_Medium_T3_14_Left/2.png", "Image/P12_D1_P4_Medium_T3_14_Left/3.png", "Image/P12_D1_P4_Medium_T3_14_Left/4.png", "Image/P12_D1_P4_Medium_T3_14_Left/5.png", "Image/P12_D1_P4_Medium_T3_14_Left/6.png", "Image/P12_D1_P4_Medium_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1783.6613 seconds to 1795.9369000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P2_Low_T3_8_Right", "images": ["Image/P25_D3_P2_Low_T3_8_Right/1.png", "Image/P25_D3_P2_Low_T3_8_Right/2.png", "Image/P25_D3_P2_Low_T3_8_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1835.1736 seconds to 1845.6536 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P1_Medium_T1_1_Mid", "images": ["Image/P5_D1_P1_Medium_T1_1_Mid/1.png", "Image/P5_D1_P1_Medium_T1_1_Mid/2.png", "Image/P5_D1_P1_Medium_T1_1_Mid/3.png", "Image/P5_D1_P1_Medium_T1_1_Mid/4.png", "Image/P5_D1_P1_Medium_T1_1_Mid/5.png", "Image/P5_D1_P1_Medium_T1_1_Mid/6.png", "Image/P5_D1_P1_Medium_T1_1_Mid/7.png", "Image/P5_D1_P1_Medium_T1_1_Mid/8.png", "Image/P5_D1_P1_Medium_T1_1_Mid/9.png", "Image/P5_D1_P1_Medium_T1_1_Mid/10.png", "Image/P5_D1_P1_Medium_T1_1_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the middle bottle into the bucket to my left. Finished.\"\nThe frames roughly correspond to the speech interval from 2045.3733 seconds to 2061.2817999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P9_Low_T3_5_Right", "images": ["Image/P4_D2_P9_Low_T3_5_Right/1.png", "Image/P4_D2_P9_Low_T3_5_Right/2.png", "Image/P4_D2_P9_Low_T3_5_Right/3.png", "Image/P4_D2_P9_Low_T3_5_Right/4.png", "Image/P4_D2_P9_Low_T3_5_Right/5.png", "Image/P4_D2_P9_Low_T3_5_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 1649.2834999999998 seconds to 1663.3947999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P1_Medium_T3_18_Left", "images": ["Image/P5_D2_P1_Medium_T3_18_Left/1.png", "Image/P5_D2_P1_Medium_T3_18_Left/2.png", "Image/P5_D2_P1_Medium_T3_18_Left/3.png", "Image/P5_D2_P1_Medium_T3_18_Left/4.png", "Image/P5_D2_P1_Medium_T3_18_Left/5.png", "Image/P5_D2_P1_Medium_T3_18_Left/6.png", "Image/P5_D2_P1_Medium_T3_18_Left/7.png", "Image/P5_D2_P1_Medium_T3_18_Left/8.png", "Image/P5_D2_P1_Medium_T3_18_Left/9.png", "Image/P5_D2_P1_Medium_T3_18_Left/10.png", "Image/P5_D2_P1_Medium_T3_18_Left/11.png", "Image/P5_D2_P1_Medium_T3_18_Left/12.png", "Image/P5_D2_P1_Medium_T3_18_Left/13.png", "Image/P5_D2_P1_Medium_T3_18_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are three cups on the shelf in front of you. Clean the one that is closer to the middle cup. Finish.\"\nThe frames roughly correspond to the speech interval from 548.6424 seconds to 569.5939000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P8_Low_T1_23_Mid", "images": ["Image/P23_D1_P8_Low_T1_23_Mid/1.png", "Image/P23_D1_P8_Low_T1_23_Mid/2.png", "Image/P23_D1_P8_Low_T1_23_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1719.814 seconds to 1729.0144 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P9_High_T2_27_Right", "images": ["Image/P29_D1_P9_High_T2_27_Right/1.png", "Image/P29_D1_P9_High_T2_27_Right/2.png", "Image/P29_D1_P9_High_T2_27_Right/3.png", "Image/P29_D1_P9_High_T2_27_Right/4.png", "Image/P29_D1_P9_High_T2_27_Right/5.png", "Image/P29_D1_P9_High_T2_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2861.2742 seconds to 2873.2684999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P1_Low_T2_18_Mid", "images": ["Image/P27_D1_P1_Low_T2_18_Mid/1.png", "Image/P27_D1_P1_Low_T2_18_Mid/2.png", "Image/P27_D1_P1_Low_T2_18_Mid/3.png", "Image/P27_D1_P1_Low_T2_18_Mid/4.png", "Image/P27_D1_P1_Low_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 977.3125 seconds to 989.0332 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P6_High_T3_21_Mid", "images": ["Image/P18_D2_P6_High_T3_21_Mid/1.png", "Image/P18_D2_P6_High_T3_21_Mid/2.png", "Image/P18_D2_P6_High_T3_21_Mid/3.png", "Image/P18_D2_P6_High_T3_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the white vase. Finish.\"\nThe frames roughly correspond to the speech interval from 353.97310000000004 seconds to 364.0362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P9_Medium_T2_3_Right", "images": ["Image/P20_D1_P9_Medium_T2_3_Right/1.png", "Image/P20_D1_P9_Medium_T2_3_Right/2.png", "Image/P20_D1_P9_Medium_T2_3_Right/3.png", "Image/P20_D1_P9_Medium_T2_3_Right/4.png", "Image/P20_D1_P9_Medium_T2_3_Right/5.png", "Image/P20_D1_P9_Medium_T2_3_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour some water into a vessel over there\"\nThe frames roughly correspond to the speech interval from 802.3855000000001 seconds to 814.5091 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P2_High_T3_2_Right", "images": ["Image/P30_D1_P2_High_T3_2_Right/1.png", "Image/P30_D1_P2_High_T3_2_Right/2.png", "Image/P30_D1_P2_High_T3_2_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 907.0131 seconds to 915.5877 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P7_Low_T3_26_Mid", "images": ["Image/P1_D1_P7_Low_T3_26_Mid/1.png", "Image/P1_D1_P7_Low_T3_26_Mid/2.png", "Image/P1_D1_P7_Low_T3_26_Mid/3.png", "Image/P1_D1_P7_Low_T3_26_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 141.64181 seconds to 151.42804999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P1_Low_T1_3_Right", "images": ["Image/P20_D3_P1_Low_T1_3_Right/1.png", "Image/P20_D3_P1_Low_T1_3_Right/2.png", "Image/P20_D3_P1_Low_T1_3_Right/3.png", "Image/P20_D3_P1_Low_T1_3_Right/4.png", "Image/P20_D3_P1_Low_T1_3_Right/5.png", "Image/P20_D3_P1_Low_T1_3_Right/6.png", "Image/P20_D3_P1_Low_T1_3_Right/7.png", "Image/P20_D3_P1_Low_T1_3_Right/8.png", "Image/P20_D3_P1_Low_T1_3_Right/9.png", "Image/P20_D3_P1_Low_T1_3_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green bottle to the bottom of that table? Finished.\"\nThe frames roughly correspond to the speech interval from 1791.3584 seconds to 1808.6045 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P8_High_T3_20_Left", "images": ["Image/P25_D2_P8_High_T3_20_Left/1.png", "Image/P25_D2_P8_High_T3_20_Left/2.png", "Image/P25_D2_P8_High_T3_20_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 939.3144 seconds to 947.7079 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P9_Low_T2_17_Left", "images": ["Image/P11_D1_P9_Low_T2_17_Left/1.png", "Image/P11_D1_P9_Low_T2_17_Left/2.png", "Image/P11_D1_P9_Low_T2_17_Left/3.png", "Image/P11_D1_P9_Low_T2_17_Left/4.png", "Image/P11_D1_P9_Low_T2_17_Left/5.png", "Image/P11_D1_P9_Low_T2_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a glass which is in the basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2566.4602999999997 seconds to 2578.7519999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P5_Medium_T2_3_Mid", "images": ["Image/P1_D2_P5_Medium_T2_3_Mid/1.png", "Image/P1_D2_P5_Medium_T2_3_Mid/2.png", "Image/P1_D2_P5_Medium_T2_3_Mid/3.png", "Image/P1_D2_P5_Medium_T2_3_Mid/4.png", "Image/P1_D2_P5_Medium_T2_3_Mid/5.png", "Image/P1_D2_P5_Medium_T2_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water to the middle bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 862.9969000000001 seconds to 874.4954 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P6_Low_T3_25_Left", "images": ["Image/P12_D1_P6_Low_T3_25_Left/1.png", "Image/P12_D1_P6_Low_T3_25_Left/2.png", "Image/P12_D1_P6_Low_T3_25_Left/3.png", "Image/P12_D1_P6_Low_T3_25_Left/4.png", "Image/P12_D1_P6_Low_T3_25_Left/5.png", "Image/P12_D1_P6_Low_T3_25_Left/6.png", "Image/P12_D1_P6_Low_T3_25_Left/7.png", "Image/P12_D1_P6_Low_T3_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1641.8581 seconds to 1656.6698 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P9_Medium_T2_17_Right", "images": ["Image/P28_D3_P9_Medium_T2_17_Right/1.png", "Image/P28_D3_P9_Medium_T2_17_Right/2.png", "Image/P28_D3_P9_Medium_T2_17_Right/3.png", "Image/P28_D3_P9_Medium_T2_17_Right/4.png", "Image/P28_D3_P9_Medium_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 127.09767 seconds to 137.58118000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P2_Medium_T2_25_Mid", "images": ["Image/P1_D2_P2_Medium_T2_25_Mid/1.png", "Image/P1_D2_P2_Medium_T2_25_Mid/2.png", "Image/P1_D2_P2_Medium_T2_25_Mid/3.png", "Image/P1_D2_P2_Medium_T2_25_Mid/4.png", "Image/P1_D2_P2_Medium_T2_25_Mid/5.png", "Image/P1_D2_P2_Medium_T2_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water in the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1023.9762 seconds to 1035.6003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P2_Low_T1_20_Mid", "images": ["Image/P3_D3_P2_Low_T1_20_Mid/1.png", "Image/P3_D3_P2_Low_T1_20_Mid/2.png", "Image/P3_D3_P2_Low_T1_20_Mid/3.png", "Image/P3_D3_P2_Low_T1_20_Mid/4.png", "Image/P3_D3_P2_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the bottle to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 846.1139 seconds to 857.1535999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P7_Medium_T3_22_Mid", "images": ["Image/P28_D3_P7_Medium_T3_22_Mid/1.png", "Image/P28_D3_P7_Medium_T3_22_Mid/2.png", "Image/P28_D3_P7_Medium_T3_22_Mid/3.png", "Image/P28_D3_P7_Medium_T3_22_Mid/4.png", "Image/P28_D3_P7_Medium_T3_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the middle plant on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 393.5807 seconds to 408.5307 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P3_High_T2_9_Mid", "images": ["Image/P12_D1_P3_High_T2_9_Mid/1.png", "Image/P12_D1_P3_High_T2_9_Mid/2.png", "Image/P12_D1_P3_High_T2_9_Mid/3.png", "Image/P12_D1_P3_High_T2_9_Mid/4.png", "Image/P12_D1_P3_High_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1574.39 seconds to 1587.4738 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P7_Medium_T3_21_Mid", "images": ["Image/P27_D2_P7_Medium_T3_21_Mid/1.png", "Image/P27_D2_P7_Medium_T3_21_Mid/2.png", "Image/P27_D2_P7_Medium_T3_21_Mid/3.png", "Image/P27_D2_P7_Medium_T3_21_Mid/4.png", "Image/P27_D2_P7_Medium_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that middle vase? Finish\"\nThe frames roughly correspond to the speech interval from 1516.4072 seconds to 1527.0124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P3_Low_T3_22_Left", "images": ["Image/P25_D2_P3_Low_T3_22_Left/1.png", "Image/P25_D2_P3_Low_T3_22_Left/2.png", "Image/P25_D2_P3_Low_T3_22_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1002.6639 seconds to 1019.1024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P9_Medium_T1_13_Left", "images": ["Image/P7_D1_P9_Medium_T1_13_Left/1.png", "Image/P7_D1_P9_Medium_T1_13_Left/2.png", "Image/P7_D1_P9_Medium_T1_13_Left/3.png", "Image/P7_D1_P9_Medium_T1_13_Left/4.png", "Image/P7_D1_P9_Medium_T1_13_Left/5.png", "Image/P7_D1_P9_Medium_T1_13_Left/6.png", "Image/P7_D1_P9_Medium_T1_13_Left/7.png", "Image/P7_D1_P9_Medium_T1_13_Left/8.png", "Image/P7_D1_P9_Medium_T1_13_Left/9.png", "Image/P7_D1_P9_Medium_T1_13_Left/10.png", "Image/P7_D1_P9_Medium_T1_13_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 593.6795 seconds to 612.4564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P1_Low_T1_18_Mid", "images": ["Image/P14_D2_P1_Low_T1_18_Mid/1.png", "Image/P14_D2_P1_Low_T1_18_Mid/2.png", "Image/P14_D2_P1_Low_T1_18_Mid/3.png", "Image/P14_D2_P1_Low_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup to the bottom. Finish.\"\nThe frames roughly correspond to the speech interval from 1683.6990999999998 seconds to 1692.7602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P8_Medium_T1_12_Right", "images": ["Image/P11_D3_P8_Medium_T1_12_Right/1.png", "Image/P11_D3_P8_Medium_T1_12_Right/2.png", "Image/P11_D3_P8_Medium_T1_12_Right/3.png", "Image/P11_D3_P8_Medium_T1_12_Right/4.png", "Image/P11_D3_P8_Medium_T1_12_Right/5.png", "Image/P11_D3_P8_Medium_T1_12_Right/6.png", "Image/P11_D3_P8_Medium_T1_12_Right/7.png", "Image/P11_D3_P8_Medium_T1_12_Right/8.png", "Image/P11_D3_P8_Medium_T1_12_Right/9.png", "Image/P11_D3_P8_Medium_T1_12_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first red bottle from the right side to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1809.2549 seconds to 1825.3594 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P4_High_T3_21_Mid", "images": ["Image/P9_D3_P4_High_T3_21_Mid/1.png", "Image/P9_D3_P4_High_T3_21_Mid/2.png", "Image/P9_D3_P4_High_T3_21_Mid/3.png", "Image/P9_D3_P4_High_T3_21_Mid/4.png", "Image/P9_D3_P4_High_T3_21_Mid/5.png", "Image/P9_D3_P4_High_T3_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the bottle on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 840.0231 seconds to 852.3541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P5_High_T1_12_Mid", "images": ["Image/P27_D2_P5_High_T1_12_Mid/1.png", "Image/P27_D2_P5_High_T1_12_Mid/2.png", "Image/P27_D2_P5_High_T1_12_Mid/3.png", "Image/P27_D2_P5_High_T1_12_Mid/4.png", "Image/P27_D2_P5_High_T1_12_Mid/5.png", "Image/P27_D2_P5_High_T1_12_Mid/6.png", "Image/P27_D2_P5_High_T1_12_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that red vase right there on that cabinet? Finish\"\nThe frames roughly correspond to the speech interval from 1429.1558 seconds to 1442.0582 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P3_Medium_T2_22_Right", "images": ["Image/P12_D1_P3_Medium_T2_22_Right/1.png", "Image/P12_D1_P3_Medium_T2_22_Right/2.png", "Image/P12_D1_P3_Medium_T2_22_Right/3.png", "Image/P12_D1_P3_Medium_T2_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1414.5744 seconds to 1424.9579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P2_Low_T1_27_Right", "images": ["Image/P21_D2_P2_Low_T1_27_Right/1.png", "Image/P21_D2_P2_Low_T1_27_Right/2.png", "Image/P21_D2_P2_Low_T1_27_Right/3.png", "Image/P21_D2_P2_Low_T1_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1371.9140900000002 seconds to 1383.86991 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P3_Medium_T1_2_Left", "images": ["Image/P17_D1_P3_Medium_T1_2_Left/1.png", "Image/P17_D1_P3_Medium_T1_2_Left/2.png", "Image/P17_D1_P3_Medium_T1_2_Left/3.png", "Image/P17_D1_P3_Medium_T1_2_Left/4.png", "Image/P17_D1_P3_Medium_T1_2_Left/5.png", "Image/P17_D1_P3_Medium_T1_2_Left/6.png", "Image/P17_D1_P3_Medium_T1_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green object right here and put it in the basket on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 1497.04165 seconds to 1510.6270700000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P5_Medium_T2_11_Left", "images": ["Image/P17_D1_P5_Medium_T2_11_Left/1.png", "Image/P17_D1_P5_Medium_T2_11_Left/2.png", "Image/P17_D1_P5_Medium_T2_11_Left/3.png", "Image/P17_D1_P5_Medium_T2_11_Left/4.png", "Image/P17_D1_P5_Medium_T2_11_Left/5.png", "Image/P17_D1_P5_Medium_T2_11_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in this brown vase right here? Finish\"\nThe frames roughly correspond to the speech interval from 1625.0217000000002 seconds to 1637.4926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P4_High_T1_9_Left", "images": ["Image/P23_D3_P4_High_T1_9_Left/1.png", "Image/P23_D3_P4_High_T1_9_Left/2.png", "Image/P23_D3_P4_High_T1_9_Left/3.png", "Image/P23_D3_P4_High_T1_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 884.6781000000001 seconds to 894.3803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P2_Medium_T3_11_Left", "images": ["Image/P3_D1_P2_Medium_T3_11_Left/1.png", "Image/P3_D1_P2_Medium_T3_11_Left/2.png", "Image/P3_D1_P2_Medium_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1575.8361 seconds to 1585.0231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P1_High_T3_10_Left", "images": ["Image/P23_D2_P1_High_T3_10_Left/1.png", "Image/P23_D2_P1_High_T3_10_Left/2.png", "Image/P23_D2_P1_High_T3_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 83.41482000000002 seconds to 93.84086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P9_High_T3_17_Left", "images": ["Image/P8_D3_P9_High_T3_17_Left/1.png", "Image/P8_D3_P9_High_T3_17_Left/2.png", "Image/P8_D3_P9_High_T3_17_Left/3.png", "Image/P8_D3_P9_High_T3_17_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean that brown glass? Finish.\"\nThe frames roughly correspond to the speech interval from 1824.1387 seconds to 1834.147 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P5_Medium_T1_24_Mid", "images": ["Image/P12_D1_P5_Medium_T1_24_Mid/1.png", "Image/P12_D1_P5_Medium_T1_24_Mid/2.png", "Image/P12_D1_P5_Medium_T1_24_Mid/3.png", "Image/P12_D1_P5_Medium_T1_24_Mid/4.png", "Image/P12_D1_P5_Medium_T1_24_Mid/5.png", "Image/P12_D1_P5_Medium_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1474.1949 seconds to 1485.5358 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P3_High_T2_3_Right", "images": ["Image/P10_D3_P3_High_T2_3_Right/1.png", "Image/P10_D3_P3_High_T2_3_Right/2.png", "Image/P10_D3_P3_High_T2_3_Right/3.png", "Image/P10_D3_P3_High_T2_3_Right/4.png", "Image/P10_D3_P3_High_T2_3_Right/5.png", "Image/P10_D3_P3_High_T2_3_Right/6.png", "Image/P10_D3_P3_High_T2_3_Right/7.png", "Image/P10_D3_P3_High_T2_3_Right/8.png", "Image/P10_D3_P3_High_T2_3_Right/9.png", "Image/P10_D3_P3_High_T2_3_Right/10.png", "Image/P10_D3_P3_High_T2_3_Right/11.png", "Image/P10_D3_P3_High_T2_3_Right/12.png", "Image/P10_D3_P3_High_T2_3_Right/13.png", "Image/P10_D3_P3_High_T2_3_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 929.93139 seconds to 949.37629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D2_P8_Low_T1_23_Right", "images": ["Image/P15_D2_P8_Low_T1_23_Right/1.png", "Image/P15_D2_P8_Low_T1_23_Right/2.png", "Image/P15_D2_P8_Low_T1_23_Right/3.png", "Image/P15_D2_P8_Low_T1_23_Right/4.png", "Image/P15_D2_P8_Low_T1_23_Right/5.png", "Image/P15_D2_P8_Low_T1_23_Right/6.png", "Image/P15_D2_P8_Low_T1_23_Right/7.png", "Image/P15_D2_P8_Low_T1_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the floor into the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2922.3689 seconds to 2935.313 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P5_Medium_T1_4_Left", "images": ["Image/P24_D2_P5_Medium_T1_4_Left/1.png", "Image/P24_D2_P5_Medium_T1_4_Left/2.png", "Image/P24_D2_P5_Medium_T1_4_Left/3.png", "Image/P24_D2_P5_Medium_T1_4_Left/4.png", "Image/P24_D2_P5_Medium_T1_4_Left/5.png", "Image/P24_D2_P5_Medium_T1_4_Left/6.png", "Image/P24_D2_P5_Medium_T1_4_Left/7.png", "Image/P24_D2_P5_Medium_T1_4_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shell? Please finish.\"\nThe frames roughly correspond to the speech interval from 616.9355 seconds to 630.5784 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P6_Medium_T2_24_Left", "images": ["Image/P1_D2_P6_Medium_T2_24_Left/1.png", "Image/P1_D2_P6_Medium_T2_24_Left/2.png", "Image/P1_D2_P6_Medium_T2_24_Left/3.png", "Image/P1_D2_P6_Medium_T2_24_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1091.5116 seconds to 1101.3542 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P8_Medium_T1_3_Left", "images": ["Image/P21_D3_P8_Medium_T1_3_Left/1.png", "Image/P21_D3_P8_Medium_T1_3_Left/2.png", "Image/P21_D3_P8_Medium_T1_3_Left/3.png", "Image/P21_D3_P8_Medium_T1_3_Left/4.png", "Image/P21_D3_P8_Medium_T1_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 214.64807 seconds to 225.90822999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P7_High_T2_26_Mid", "images": ["Image/P18_D2_P7_High_T2_26_Mid/1.png", "Image/P18_D2_P7_High_T2_26_Mid/2.png", "Image/P18_D2_P7_High_T2_26_Mid/3.png", "Image/P18_D2_P7_High_T2_26_Mid/4.png", "Image/P18_D2_P7_High_T2_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Pour the water out of the white cup. Finish.\"\nThe frames roughly correspond to the speech interval from 218.1135 seconds to 229.9181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P4_Low_T3_19_Right", "images": ["Image/P29_D3_P4_Low_T3_19_Right/1.png", "Image/P29_D3_P4_Low_T3_19_Right/2.png", "Image/P29_D3_P4_Low_T3_19_Right/3.png", "Image/P29_D3_P4_Low_T3_19_Right/4.png", "Image/P29_D3_P4_Low_T3_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddie\"\nThe frames roughly correspond to the speech interval from 1475.4314999999997 seconds to 1485.6065999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P1_Low_T3_13_Right", "images": ["Image/P4_D2_P1_Low_T3_13_Right/1.png", "Image/P4_D2_P1_Low_T3_13_Right/2.png", "Image/P4_D2_P1_Low_T3_13_Right/3.png", "Image/P4_D2_P1_Low_T3_13_Right/4.png", "Image/P4_D2_P1_Low_T3_13_Right/5.png", "Image/P4_D2_P1_Low_T3_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant on the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 1847.7378 seconds to 1862.0179 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P2_Low_T1_4_Mid", "images": ["Image/P13_D3_P2_Low_T1_4_Mid/1.png", "Image/P13_D3_P2_Low_T1_4_Mid/2.png", "Image/P13_D3_P2_Low_T1_4_Mid/3.png", "Image/P13_D3_P2_Low_T1_4_Mid/4.png", "Image/P13_D3_P2_Low_T1_4_Mid/5.png", "Image/P13_D3_P2_Low_T1_4_Mid/6.png", "Image/P13_D3_P2_Low_T1_4_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from the top shelf to the floor down here\"\nThe frames roughly correspond to the speech interval from 1810.0521 seconds to 1822.2441 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P8_Low_T1_27_Right", "images": ["Image/P8_D2_P8_Low_T1_27_Right/1.png", "Image/P8_D2_P8_Low_T1_27_Right/2.png", "Image/P8_D2_P8_Low_T1_27_Right/3.png", "Image/P8_D2_P8_Low_T1_27_Right/4.png", "Image/P8_D2_P8_Low_T1_27_Right/5.png", "Image/P8_D2_P8_Low_T1_27_Right/6.png", "Image/P8_D2_P8_Low_T1_27_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Could you move that white glass on the top of this shelf? Just on it\"\nThe frames roughly correspond to the speech interval from 2155.865 seconds to 2168.2824 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P7_High_T1_9_Right", "images": ["Image/P5_D2_P7_High_T1_9_Right/1.png", "Image/P5_D2_P7_High_T1_9_Right/2.png", "Image/P5_D2_P7_High_T1_9_Right/3.png", "Image/P5_D2_P7_High_T1_9_Right/4.png", "Image/P5_D2_P7_High_T1_9_Right/5.png", "Image/P5_D2_P7_High_T1_9_Right/6.png", "Image/P5_D2_P7_High_T1_9_Right/7.png", "Image/P5_D2_P7_High_T1_9_Right/8.png", "Image/P5_D2_P7_High_T1_9_Right/9.png", "Image/P5_D2_P7_High_T1_9_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Move the cup\"\nThe frames roughly correspond to the speech interval from 680.1091999999999 seconds to 696.2013 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P5_Low_T1_11_Right", "images": ["Image/P16_D3_P5_Low_T1_11_Right/1.png", "Image/P16_D3_P5_Low_T1_11_Right/2.png", "Image/P16_D3_P5_Low_T1_11_Right/3.png", "Image/P16_D3_P5_Low_T1_11_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 265.293 seconds to 274.8414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P1_Medium_T1_20_Mid", "images": ["Image/P20_D2_P1_Medium_T1_20_Mid/1.png", "Image/P20_D2_P1_Medium_T1_20_Mid/2.png", "Image/P20_D2_P1_Medium_T1_20_Mid/3.png", "Image/P20_D2_P1_Medium_T1_20_Mid/4.png", "Image/P20_D2_P1_Medium_T1_20_Mid/5.png", "Image/P20_D2_P1_Medium_T1_20_Mid/6.png", "Image/P20_D2_P1_Medium_T1_20_Mid/7.png", "Image/P20_D2_P1_Medium_T1_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. He moves that middle bottle to the bottom of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3232.1205000000004 seconds to 3250.5489 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P1_High_T3_20_Mid", "images": ["Image/P1_D2_P1_High_T3_20_Mid/1.png", "Image/P1_D2_P1_High_T3_20_Mid/2.png", "Image/P1_D2_P1_High_T3_20_Mid/3.png", "Image/P1_D2_P1_High_T3_20_Mid/4.png", "Image/P1_D2_P1_High_T3_20_Mid/5.png", "Image/P1_D2_P1_High_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 752.7892 seconds to 763.7176000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P1_Low_T1_5_Mid", "images": ["Image/P8_D3_P1_Low_T1_5_Mid/1.png", "Image/P8_D3_P1_Low_T1_5_Mid/2.png", "Image/P8_D3_P1_Low_T1_5_Mid/3.png", "Image/P8_D3_P1_Low_T1_5_Mid/4.png", "Image/P8_D3_P1_Low_T1_5_Mid/5.png", "Image/P8_D3_P1_Low_T1_5_Mid/6.png", "Image/P8_D3_P1_Low_T1_5_Mid/7.png", "Image/P8_D3_P1_Low_T1_5_Mid/8.png", "Image/P8_D3_P1_Low_T1_5_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the planet beneath the desk that's next to the green chair? That is fine. Finish.\"\nThe frames roughly correspond to the speech interval from 1868.8495 seconds to 1883.1316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P1_Low_T2_6_Left", "images": ["Image/P13_D2_P1_Low_T2_6_Left/1.png", "Image/P13_D2_P1_Low_T2_6_Left/2.png", "Image/P13_D2_P1_Low_T2_6_Left/3.png", "Image/P13_D2_P1_Low_T2_6_Left/4.png", "Image/P13_D2_P1_Low_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the plant over there\"\nThe frames roughly correspond to the speech interval from 1078.15976 seconds to 1088.04266 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P7_Medium_T1_2_Right", "images": ["Image/P3_D1_P7_Medium_T1_2_Right/1.png", "Image/P3_D1_P7_Medium_T1_2_Right/2.png", "Image/P3_D1_P7_Medium_T1_2_Right/3.png", "Image/P3_D1_P7_Medium_T1_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this way? Finish.\"\nThe frames roughly correspond to the speech interval from 1483.9089 seconds to 1493.9036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P7_Low_T2_21_Mid", "images": ["Image/P22_D1_P7_Low_T2_21_Mid/1.png", "Image/P22_D1_P7_Low_T2_21_Mid/2.png", "Image/P22_D1_P7_Low_T2_21_Mid/3.png", "Image/P22_D1_P7_Low_T2_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Put water in that vase. Finish\"\nThe frames roughly correspond to the speech interval from 1767.8983 seconds to 1777.1275 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_Medium_T3_3_Left", "images": ["Image/P8_D3_P3_Medium_T3_3_Left/1.png", "Image/P8_D3_P3_Medium_T3_3_Left/2.png", "Image/P8_D3_P3_Medium_T3_3_Left/3.png", "Image/P8_D3_P3_Medium_T3_3_Left/4.png", "Image/P8_D3_P3_Medium_T3_3_Left/5.png", "Image/P8_D3_P3_Medium_T3_3_Left/6.png", "Image/P8_D3_P3_Medium_T3_3_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean that green glass that is on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1804.3438 seconds to 1817.1949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P5_Low_T1_13_Mid", "images": ["Image/P5_D2_P5_Low_T1_13_Mid/1.png", "Image/P5_D2_P5_Low_T1_13_Mid/2.png", "Image/P5_D2_P5_Low_T1_13_Mid/3.png", "Image/P5_D2_P5_Low_T1_13_Mid/4.png", "Image/P5_D2_P5_Low_T1_13_Mid/5.png", "Image/P5_D2_P5_Low_T1_13_Mid/6.png", "Image/P5_D2_P5_Low_T1_13_Mid/7.png", "Image/P5_D2_P5_Low_T1_13_Mid/8.png", "Image/P5_D2_P5_Low_T1_13_Mid/9.png", "Image/P5_D2_P5_Low_T1_13_Mid/10.png", "Image/P5_D2_P5_Low_T1_13_Mid/11.png", "Image/P5_D2_P5_Low_T1_13_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move the plant in front of me to the shelf that is close to the kitchen\"\nThe frames roughly correspond to the speech interval from 585.7999 seconds to 606.8351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P6_Low_T1_16_Right", "images": ["Image/P11_D1_P6_Low_T1_16_Right/1.png", "Image/P11_D1_P6_Low_T1_16_Right/2.png", "Image/P11_D1_P6_Low_T1_16_Right/3.png", "Image/P11_D1_P6_Low_T1_16_Right/4.png", "Image/P11_D1_P6_Low_T1_16_Right/5.png", "Image/P11_D1_P6_Low_T1_16_Right/6.png", "Image/P11_D1_P6_Low_T1_16_Right/7.png", "Image/P11_D1_P6_Low_T1_16_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the bottle to the left side of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2442.1125999999995 seconds to 2455.3502 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P2_High_T2_1_Mid", "images": ["Image/P28_D1_P2_High_T2_1_Mid/1.png", "Image/P28_D1_P2_High_T2_1_Mid/2.png", "Image/P28_D1_P2_High_T2_1_Mid/3.png", "Image/P28_D1_P2_High_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1786.0022 seconds to 1799.4906 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P5_Low_T2_15_Left", "images": ["Image/P2_D2_P5_Low_T2_15_Left/1.png", "Image/P2_D2_P5_Low_T2_15_Left/2.png", "Image/P2_D2_P5_Low_T2_15_Left/3.png", "Image/P2_D2_P5_Low_T2_15_Left/4.png", "Image/P2_D2_P5_Low_T2_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please water that plant for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2756.9431999999997 seconds to 2767.6057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P7_Medium_T1_5_Mid", "images": ["Image/P10_D2_P7_Medium_T1_5_Mid/1.png", "Image/P10_D2_P7_Medium_T1_5_Mid/2.png", "Image/P10_D2_P7_Medium_T1_5_Mid/3.png", "Image/P10_D2_P7_Medium_T1_5_Mid/4.png", "Image/P10_D2_P7_Medium_T1_5_Mid/5.png", "Image/P10_D2_P7_Medium_T1_5_Mid/6.png", "Image/P10_D2_P7_Medium_T1_5_Mid/7.png", "Image/P10_D2_P7_Medium_T1_5_Mid/8.png", "Image/P10_D2_P7_Medium_T1_5_Mid/9.png", "Image/P10_D2_P7_Medium_T1_5_Mid/10.png", "Image/P10_D2_P7_Medium_T1_5_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1671.54778 seconds to 1688.0565800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P4_Low_T1_12_Right", "images": ["Image/P6_D1_P4_Low_T1_12_Right/1.png", "Image/P6_D1_P4_Low_T1_12_Right/2.png", "Image/P6_D1_P4_Low_T1_12_Right/3.png", "Image/P6_D1_P4_Low_T1_12_Right/4.png", "Image/P6_D1_P4_Low_T1_12_Right/5.png", "Image/P6_D1_P4_Low_T1_12_Right/6.png", "Image/P6_D1_P4_Low_T1_12_Right/7.png", "Image/P6_D1_P4_Low_T1_12_Right/8.png", "Image/P6_D1_P4_Low_T1_12_Right/9.png", "Image/P6_D1_P4_Low_T1_12_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2118.4335 seconds to 2138.7669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P3_Medium_T2_11_Mid", "images": ["Image/P14_D1_P3_Medium_T2_11_Mid/1.png", "Image/P14_D1_P3_Medium_T2_11_Mid/2.png", "Image/P14_D1_P3_Medium_T2_11_Mid/3.png", "Image/P14_D1_P3_Medium_T2_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Fill that bottle up\"\nThe frames roughly correspond to the speech interval from 254.6097 seconds to 264.4924 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P9_Low_T1_22_Left", "images": ["Image/P10_D3_P9_Low_T1_22_Left/1.png", "Image/P10_D3_P9_Low_T1_22_Left/2.png", "Image/P10_D3_P9_Low_T1_22_Left/3.png", "Image/P10_D3_P9_Low_T1_22_Left/4.png", "Image/P10_D3_P9_Low_T1_22_Left/5.png", "Image/P10_D3_P9_Low_T1_22_Left/6.png", "Image/P10_D3_P9_Low_T1_22_Left/7.png", "Image/P10_D3_P9_Low_T1_22_Left/8.png", "Image/P10_D3_P9_Low_T1_22_Left/9.png", "Image/P10_D3_P9_Low_T1_22_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 591.64919 seconds to 607.01599 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P9_High_T2_5_Right", "images": ["Image/P18_D1_P9_High_T2_5_Right/1.png", "Image/P18_D1_P9_High_T2_5_Right/2.png", "Image/P18_D1_P9_High_T2_5_Right/3.png", "Image/P18_D1_P9_High_T2_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 637.41601 seconds to 647.34027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P2_High_T1_2_Left", "images": ["Image/P14_D3_P2_High_T1_2_Left/1.png", "Image/P14_D3_P2_High_T1_2_Left/2.png", "Image/P14_D3_P2_High_T1_2_Left/3.png", "Image/P14_D3_P2_High_T1_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1372.13536 seconds to 1381.38436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P7_Low_T3_19_Left", "images": ["Image/P30_D3_P7_Low_T3_19_Left/1.png", "Image/P30_D3_P7_Low_T3_19_Left/2.png", "Image/P30_D3_P7_Low_T3_19_Left/3.png", "Image/P30_D3_P7_Low_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1385.2097 seconds to 1394.5377 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P8_Low_T3_17_Right", "images": ["Image/P4_D3_P8_Low_T3_17_Right/1.png", "Image/P4_D3_P8_Low_T3_17_Right/2.png", "Image/P4_D3_P8_Low_T3_17_Right/3.png", "Image/P4_D3_P8_Low_T3_17_Right/4.png", "Image/P4_D3_P8_Low_T3_17_Right/5.png", "Image/P4_D3_P8_Low_T3_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the iron on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 302.7208999999999 seconds to 316.84809999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P9_High_T1_26_Right", "images": ["Image/P9_D2_P9_High_T1_26_Right/1.png", "Image/P9_D2_P9_High_T1_26_Right/2.png", "Image/P9_D2_P9_High_T1_26_Right/3.png", "Image/P9_D2_P9_High_T1_26_Right/4.png", "Image/P9_D2_P9_High_T1_26_Right/5.png", "Image/P9_D2_P9_High_T1_26_Right/6.png", "Image/P9_D2_P9_High_T1_26_Right/7.png", "Image/P9_D2_P9_High_T1_26_Right/8.png", "Image/P9_D2_P9_High_T1_26_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the white cup to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 3154.1448 seconds to 3169.0033 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P6_High_T2_4_Mid", "images": ["Image/P4_D2_P6_High_T2_4_Mid/1.png", "Image/P4_D2_P6_High_T2_4_Mid/2.png", "Image/P4_D2_P6_High_T2_4_Mid/3.png", "Image/P4_D2_P6_High_T2_4_Mid/4.png", "Image/P4_D2_P6_High_T2_4_Mid/5.png", "Image/P4_D2_P6_High_T2_4_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1574.9509999999998 seconds to 1588.9103999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P2_Low_T2_14_Left", "images": ["Image/P22_D1_P2_Low_T2_14_Left/1.png", "Image/P22_D1_P2_Low_T2_14_Left/2.png", "Image/P22_D1_P2_Low_T2_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1311.54778 seconds to 1320.35832 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P2_Medium_T1_6_Left", "images": ["Image/P19_D3_P2_Medium_T1_6_Left/1.png", "Image/P19_D3_P2_Medium_T1_6_Left/2.png", "Image/P19_D3_P2_Medium_T1_6_Left/3.png", "Image/P19_D3_P2_Medium_T1_6_Left/4.png", "Image/P19_D3_P2_Medium_T1_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that flowerpot down here. Finish.\"\nThe frames roughly correspond to the speech interval from 2250.2025999999996 seconds to 2262.7306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P1_Low_T2_21_Left", "images": ["Image/P13_D3_P1_Low_T2_21_Left/1.png", "Image/P13_D3_P1_Low_T2_21_Left/2.png", "Image/P13_D3_P1_Low_T2_21_Left/3.png", "Image/P13_D3_P1_Low_T2_21_Left/4.png", "Image/P13_D3_P1_Low_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the white vase up there\"\nThe frames roughly correspond to the speech interval from 1446.3786 seconds to 1456.8196 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P6_Medium_T3_6_Right", "images": ["Image/P20_D3_P6_Medium_T3_6_Right/1.png", "Image/P20_D3_P6_Medium_T3_6_Right/2.png", "Image/P20_D3_P6_Medium_T3_6_Right/3.png", "Image/P20_D3_P6_Medium_T3_6_Right/4.png", "Image/P20_D3_P6_Medium_T3_6_Right/5.png", "Image/P20_D3_P6_Medium_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move? Um\"\nThe frames roughly correspond to the speech interval from 1163.23567 seconds to 1180.38438 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P8_High_T2_10_Left", "images": ["Image/P13_D2_P8_High_T2_10_Left/1.png", "Image/P13_D2_P8_High_T2_10_Left/2.png", "Image/P13_D2_P8_High_T2_10_Left/3.png", "Image/P13_D2_P8_High_T2_10_Left/4.png", "Image/P13_D2_P8_High_T2_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the brown bottle down here\"\nThe frames roughly correspond to the speech interval from 1284.82986 seconds to 1295.05966 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P1_Medium_T2_5_Mid", "images": ["Image/P26_D2_P1_Medium_T2_5_Mid/1.png", "Image/P26_D2_P1_Medium_T2_5_Mid/2.png", "Image/P26_D2_P1_Medium_T2_5_Mid/3.png", "Image/P26_D2_P1_Medium_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the middle plant please? Finish.\"\nThe frames roughly correspond to the speech interval from 1220.56272 seconds to 1234.3419199999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P9_Low_T2_2_Left", "images": ["Image/P16_D1_P9_Low_T2_2_Left/1.png", "Image/P16_D1_P9_Low_T2_2_Left/2.png", "Image/P16_D1_P9_Low_T2_2_Left/3.png", "Image/P16_D1_P9_Low_T2_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into this mug. Finish.\"\nThe frames roughly correspond to the speech interval from 1501.1914000000002 seconds to 1511.4915 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P1_Medium_T3_10_Right", "images": ["Image/P18_D1_P1_Medium_T3_10_Right/1.png", "Image/P18_D1_P1_Medium_T3_10_Right/2.png", "Image/P18_D1_P1_Medium_T3_10_Right/3.png", "Image/P18_D1_P1_Medium_T3_10_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1080.2351 seconds to 1090.4101 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P4_Low_T1_1_Right", "images": ["Image/P30_D3_P4_Low_T1_1_Right/1.png", "Image/P30_D3_P4_Low_T1_1_Right/2.png", "Image/P30_D3_P4_Low_T1_1_Right/3.png", "Image/P30_D3_P4_Low_T1_1_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase over to the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1579.6435 seconds to 1593.0764 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P1_Medium_T1_23_Mid", "images": ["Image/P28_D3_P1_Medium_T1_23_Mid/1.png", "Image/P28_D3_P1_Medium_T1_23_Mid/2.png", "Image/P28_D3_P1_Medium_T1_23_Mid/3.png", "Image/P28_D3_P1_Medium_T1_23_Mid/4.png", "Image/P28_D3_P1_Medium_T1_23_Mid/5.png", "Image/P28_D3_P1_Medium_T1_23_Mid/6.png", "Image/P28_D3_P1_Medium_T1_23_Mid/7.png", "Image/P28_D3_P1_Medium_T1_23_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant in the middle underneath the desk on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 145.9455 seconds to 162.0786 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P9_High_T2_11_Mid", "images": ["Image/P16_D2_P9_High_T2_11_Mid/1.png", "Image/P16_D2_P9_High_T2_11_Mid/2.png", "Image/P16_D2_P9_High_T2_11_Mid/3.png", "Image/P16_D2_P9_High_T2_11_Mid/4.png", "Image/P16_D2_P9_High_T2_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 684.46225 seconds to 697.4905000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P7_Low_T3_17_Right", "images": ["Image/P5_D1_P7_Low_T3_17_Right/1.png", "Image/P5_D1_P7_Low_T3_17_Right/2.png", "Image/P5_D1_P7_Low_T3_17_Right/3.png", "Image/P5_D1_P7_Low_T3_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup in the bucket. Finished.\"\nThe frames roughly correspond to the speech interval from 1876.7601 seconds to 1886.308 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P5_Low_T2_20_Left", "images": ["Image/P4_D3_P5_Low_T2_20_Left/1.png", "Image/P4_D3_P5_Low_T2_20_Left/2.png", "Image/P4_D3_P5_Low_T2_20_Left/3.png", "Image/P4_D3_P5_Low_T2_20_Left/4.png", "Image/P4_D3_P5_Low_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you fill in the white bottle? Finish\"\nThe frames roughly correspond to the speech interval from 768.8973999999998 seconds to 781.0209 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P4_High_T2_15_Mid", "images": ["Image/P22_D1_P4_High_T2_15_Mid/1.png", "Image/P22_D1_P4_High_T2_15_Mid/2.png", "Image/P22_D1_P4_High_T2_15_Mid/3.png", "Image/P22_D1_P4_High_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water this. Finish.\"\nThe frames roughly correspond to the speech interval from 1600.902 seconds to 1610.1027 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P5_High_T1_11_Right", "images": ["Image/P9_D2_P5_High_T1_11_Right/1.png", "Image/P9_D2_P5_High_T1_11_Right/2.png", "Image/P9_D2_P5_High_T1_11_Right/3.png", "Image/P9_D2_P5_High_T1_11_Right/4.png", "Image/P9_D2_P5_High_T1_11_Right/5.png", "Image/P9_D2_P5_High_T1_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the right bottle on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 2750.3683 seconds to 2764.9622 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P3_Low_T3_10_Left", "images": ["Image/P20_D1_P3_Low_T3_10_Left/1.png", "Image/P20_D1_P3_Low_T3_10_Left/2.png", "Image/P20_D1_P3_Low_T3_10_Left/3.png", "Image/P20_D1_P3_Low_T3_10_Left/4.png", "Image/P20_D1_P3_Low_T3_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this bottle? Thanks. Finish.\"\nThe frames roughly correspond to the speech interval from 158.81282 seconds to 171.39625999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P3_High_T2_27_Left", "images": ["Image/P6_D3_P3_High_T2_27_Left/1.png", "Image/P6_D3_P3_High_T2_27_Left/2.png", "Image/P6_D3_P3_High_T2_27_Left/3.png", "Image/P6_D3_P3_High_T2_27_Left/4.png", "Image/P6_D3_P3_High_T2_27_Left/5.png", "Image/P6_D3_P3_High_T2_27_Left/6.png", "Image/P6_D3_P3_High_T2_27_Left/7.png", "Image/P6_D3_P3_High_T2_27_Left/8.png", "Image/P6_D3_P3_High_T2_27_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4449.50241 seconds to 4465.3879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P5_Low_T1_14_Left", "images": ["Image/P15_D1_P5_Low_T1_14_Left/1.png", "Image/P15_D1_P5_Low_T1_14_Left/2.png", "Image/P15_D1_P5_Low_T1_14_Left/3.png", "Image/P15_D1_P5_Low_T1_14_Left/4.png", "Image/P15_D1_P5_Low_T1_14_Left/5.png", "Image/P15_D1_P5_Low_T1_14_Left/6.png", "Image/P15_D1_P5_Low_T1_14_Left/7.png", "Image/P15_D1_P5_Low_T1_14_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant At the right side of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2112.98247 seconds to 2126.67917 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P9_High_T1_8_Mid", "images": ["Image/P24_D1_P9_High_T1_8_Mid/1.png", "Image/P24_D1_P9_High_T1_8_Mid/2.png", "Image/P24_D1_P9_High_T1_8_Mid/3.png", "Image/P24_D1_P9_High_T1_8_Mid/4.png", "Image/P24_D1_P9_High_T1_8_Mid/5.png", "Image/P24_D1_P9_High_T1_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you move this one over here\"\nThe frames roughly correspond to the speech interval from 985.251 seconds to 997.6662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P8_Medium_T3_10_Mid", "images": ["Image/P3_D1_P8_Medium_T3_10_Mid/1.png", "Image/P3_D1_P8_Medium_T3_10_Mid/2.png", "Image/P3_D1_P8_Medium_T3_10_Mid/3.png", "Image/P3_D1_P8_Medium_T3_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 1384.129 seconds to 1393.6102 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P9_High_T3_15_Right", "images": ["Image/P4_D1_P9_High_T3_15_Right/1.png", "Image/P4_D1_P9_High_T3_15_Right/2.png", "Image/P4_D1_P9_High_T3_15_Right/3.png", "Image/P4_D1_P9_High_T3_15_Right/4.png", "Image/P4_D1_P9_High_T3_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 2115.55098 seconds to 2131.09601 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P9_Low_T1_7_Mid", "images": ["Image/P8_D3_P9_Low_T1_7_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Finish.\"\nThe frames roughly correspond to the speech interval from 1504.09183 seconds to 1516.18851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P3_Low_T1_12_Left", "images": ["Image/P14_D1_P3_Low_T1_12_Left/1.png", "Image/P14_D1_P3_Low_T1_12_Left/2.png", "Image/P14_D1_P3_Low_T1_12_Left/3.png", "Image/P14_D1_P3_Low_T1_12_Left/4.png", "Image/P14_D1_P3_Low_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that to the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 233.4096 seconds to 244.3232 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P6_High_T1_16_Mid", "images": ["Image/P16_D2_P6_High_T1_16_Mid/1.png", "Image/P16_D2_P6_High_T1_16_Mid/2.png", "Image/P16_D2_P6_High_T1_16_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 608.4317000000001 seconds to 617.61788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P9_High_T1_20_Left", "images": ["Image/P19_D3_P9_High_T1_20_Left/1.png", "Image/P19_D3_P9_High_T1_20_Left/2.png", "Image/P19_D3_P9_High_T1_20_Left/3.png", "Image/P19_D3_P9_High_T1_20_Left/4.png", "Image/P19_D3_P9_High_T1_20_Left/5.png", "Image/P19_D3_P9_High_T1_20_Left/6.png", "Image/P19_D3_P9_High_T1_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this container up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2187.3118999999997 seconds to 2200.3822 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P2_Low_T1_15_Mid", "images": ["Image/P16_D2_P2_Low_T1_15_Mid/1.png", "Image/P16_D2_P2_Low_T1_15_Mid/2.png", "Image/P16_D2_P2_Low_T1_15_Mid/3.png", "Image/P16_D2_P2_Low_T1_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please move that plant up there\"\nThe frames roughly correspond to the speech interval from 662.1201900000001 seconds to 677.1397400000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P3_Low_T2_9_Left", "images": ["Image/P17_D2_P3_Low_T2_9_Left/1.png", "Image/P17_D2_P3_Low_T2_9_Left/2.png", "Image/P17_D2_P3_Low_T2_9_Left/3.png", "Image/P17_D2_P3_Low_T2_9_Left/4.png", "Image/P17_D2_P3_Low_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that green cup? Finish.\"\nThe frames roughly correspond to the speech interval from 486.4967 seconds to 501.8362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P1_Low_T3_17_Left", "images": ["Image/P23_D3_P1_Low_T3_17_Left/1.png", "Image/P23_D3_P1_Low_T3_17_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 666.12228 seconds to 674.37552 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P1_High_T2_13_Right", "images": ["Image/P8_D1_P1_High_T2_13_Right/1.png", "Image/P8_D1_P1_High_T2_13_Right/2.png", "Image/P8_D1_P1_High_T2_13_Right/3.png", "Image/P8_D1_P1_High_T2_13_Right/4.png", "Image/P8_D1_P1_High_T2_13_Right/5.png", "Image/P8_D1_P1_High_T2_13_Right/6.png", "Image/P8_D1_P1_High_T2_13_Right/7.png", "Image/P8_D1_P1_High_T2_13_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. So could you water the planet just on the left side of me? Finish.\"\nThe frames roughly correspond to the speech interval from 615.4410999999999 seconds to 629.4796999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P9_Low_T1_20_Mid", "images": ["Image/P2_D1_P9_Low_T1_20_Mid/1.png", "Image/P2_D1_P9_Low_T1_20_Mid/2.png", "Image/P2_D1_P9_Low_T1_20_Mid/3.png", "Image/P2_D1_P9_Low_T1_20_Mid/4.png", "Image/P2_D1_P9_Low_T1_20_Mid/5.png", "Image/P2_D1_P9_Low_T1_20_Mid/6.png", "Image/P2_D1_P9_Low_T1_20_Mid/7.png", "Image/P2_D1_P9_Low_T1_20_Mid/8.png", "Image/P2_D1_P9_Low_T1_20_Mid/9.png", "Image/P2_D1_P9_Low_T1_20_Mid/10.png", "Image/P2_D1_P9_Low_T1_20_Mid/11.png", "Image/P2_D1_P9_Low_T1_20_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hello\"\nThe frames roughly correspond to the speech interval from 565.7004 seconds to 583.0835999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P8_Medium_T2_20_Right", "images": ["Image/P23_D1_P8_Medium_T2_20_Right/1.png", "Image/P23_D1_P8_Medium_T2_20_Right/2.png", "Image/P23_D1_P8_Medium_T2_20_Right/3.png", "Image/P23_D1_P8_Medium_T2_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the one on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 1617.1961 seconds to 1626.6341 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P7_High_T1_16_Mid", "images": ["Image/P1_D1_P7_High_T1_16_Mid/1.png", "Image/P1_D1_P7_High_T1_16_Mid/2.png", "Image/P1_D1_P7_High_T1_16_Mid/3.png", "Image/P1_D1_P7_High_T1_16_Mid/4.png", "Image/P1_D1_P7_High_T1_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 504.9224 seconds to 515.1696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P5_Low_T2_16_Right", "images": ["Image/P6_D3_P5_Low_T2_16_Right/1.png", "Image/P6_D3_P5_Low_T2_16_Right/2.png", "Image/P6_D3_P5_Low_T2_16_Right/3.png", "Image/P6_D3_P5_Low_T2_16_Right/4.png", "Image/P6_D3_P5_Low_T2_16_Right/5.png", "Image/P6_D3_P5_Low_T2_16_Right/6.png", "Image/P6_D3_P5_Low_T2_16_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3326.62154 seconds to 3340.06892 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P6_Medium_T1_2_Left", "images": ["Image/P25_D1_P6_Medium_T1_2_Left/1.png", "Image/P25_D1_P6_Medium_T1_2_Left/2.png", "Image/P25_D1_P6_Medium_T1_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this there. Finish.\"\nThe frames roughly correspond to the speech interval from 197.1719 seconds to 205.6068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P4_High_T3_4_Mid", "images": ["Image/P13_D1_P4_High_T3_4_Mid/1.png", "Image/P13_D1_P4_High_T3_4_Mid/2.png", "Image/P13_D1_P4_High_T3_4_Mid/3.png", "Image/P13_D1_P4_High_T3_4_Mid/4.png", "Image/P13_D1_P4_High_T3_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean this plant here please? Finish.\"\nThe frames roughly correspond to the speech interval from 677.0426 seconds to 687.8026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P7_High_T2_8_Left", "images": ["Image/P21_D3_P7_High_T2_8_Left/1.png", "Image/P21_D3_P7_High_T2_8_Left/2.png", "Image/P21_D3_P7_High_T2_8_Left/3.png", "Image/P21_D3_P7_High_T2_8_Left/4.png", "Image/P21_D3_P7_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill the green mug with water. Finish.\"\nThe frames roughly correspond to the speech interval from 496.5118 seconds to 520.3546 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D1_P8_Low_T3_8_Mid", "images": ["Image/P2_D1_P8_Low_T3_8_Mid/1.png", "Image/P2_D1_P8_Low_T3_8_Mid/2.png", "Image/P2_D1_P8_Low_T3_8_Mid/3.png", "Image/P2_D1_P8_Low_T3_8_Mid/4.png", "Image/P2_D1_P8_Low_T3_8_Mid/5.png", "Image/P2_D1_P8_Low_T3_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 338.98723999999993 seconds to 353.76886999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P6_High_T3_5_Left", "images": ["Image/P15_D3_P6_High_T3_5_Left/1.png", "Image/P15_D3_P6_High_T3_5_Left/2.png", "Image/P15_D3_P6_High_T3_5_Left/3.png", "Image/P15_D3_P6_High_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 751.0558 seconds to 761.6205 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P4_High_T3_10_Left", "images": ["Image/P4_D1_P4_High_T3_10_Left/1.png", "Image/P4_D1_P4_High_T3_10_Left/2.png", "Image/P4_D1_P4_High_T3_10_Left/3.png", "Image/P4_D1_P4_High_T3_10_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2412.4651 seconds to 2424.2239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P6_Low_T3_16_Mid", "images": ["Image/P15_D3_P6_Low_T3_16_Mid/1.png", "Image/P15_D3_P6_Low_T3_16_Mid/2.png", "Image/P15_D3_P6_Low_T3_16_Mid/3.png", "Image/P15_D3_P6_Low_T3_16_Mid/4.png", "Image/P15_D3_P6_Low_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 197.32180000000005 seconds to 209.22240000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P6_Medium_T3_23_Right", "images": ["Image/P9_D3_P6_Medium_T3_23_Right/1.png", "Image/P9_D3_P6_Medium_T3_23_Right/2.png", "Image/P9_D3_P6_Medium_T3_23_Right/3.png", "Image/P9_D3_P6_Medium_T3_23_Right/4.png", "Image/P9_D3_P6_Medium_T3_23_Right/5.png", "Image/P9_D3_P6_Medium_T3_23_Right/6.png", "Image/P9_D3_P6_Medium_T3_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right plant on the second layer? Finish.\"\nThe frames roughly correspond to the speech interval from 1037.7387 seconds to 1051.624 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P6_High_T3_10_Left", "images": ["Image/P29_D1_P6_High_T3_10_Left/1.png", "Image/P29_D1_P6_High_T3_10_Left/2.png", "Image/P29_D1_P6_High_T3_10_Left/3.png", "Image/P29_D1_P6_High_T3_10_Left/4.png", "Image/P29_D1_P6_High_T3_10_Left/5.png", "Image/P29_D1_P6_High_T3_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2970.3594 seconds to 2981.4113 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P8_Low_T2_9_Mid", "images": ["Image/P1_D3_P8_Low_T2_9_Mid/1.png", "Image/P1_D3_P8_Low_T2_9_Mid/2.png", "Image/P1_D3_P8_Low_T2_9_Mid/3.png", "Image/P1_D3_P8_Low_T2_9_Mid/4.png", "Image/P1_D3_P8_Low_T2_9_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the green cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1678.9210999999998 seconds to 1689.8608999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P4_High_T3_23_Mid", "images": ["Image/P17_D1_P4_High_T3_23_Mid/1.png", "Image/P17_D1_P4_High_T3_23_Mid/2.png", "Image/P17_D1_P4_High_T3_23_Mid/3.png", "Image/P17_D1_P4_High_T3_23_Mid/4.png", "Image/P17_D1_P4_High_T3_23_Mid/5.png", "Image/P17_D1_P4_High_T3_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this white pot with the plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1708.4157 seconds to 1720.4706 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P1_Medium_T2_7_Mid", "images": ["Image/P12_D3_P1_Medium_T2_7_Mid/1.png", "Image/P12_D3_P1_Medium_T2_7_Mid/2.png", "Image/P12_D3_P1_Medium_T2_7_Mid/3.png", "Image/P12_D3_P1_Medium_T2_7_Mid/4.png", "Image/P12_D3_P1_Medium_T2_7_Mid/5.png", "Image/P12_D3_P1_Medium_T2_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2550.63719 seconds to 2563.15179 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P8_High_T1_6_Mid", "images": ["Image/P24_D2_P8_High_T1_6_Mid/1.png", "Image/P24_D2_P8_High_T1_6_Mid/2.png", "Image/P24_D2_P8_High_T1_6_Mid/3.png", "Image/P24_D2_P8_High_T1_6_Mid/4.png", "Image/P24_D2_P8_High_T1_6_Mid/5.png", "Image/P24_D2_P8_High_T1_6_Mid/6.png", "Image/P24_D2_P8_High_T1_6_Mid/7.png", "Image/P24_D2_P8_High_T1_6_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this planet up to this shelf\"\nThe frames roughly correspond to the speech interval from 464.8329 seconds to 480.06139999999994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P1_Low_T3_17_Mid", "images": ["Image/P25_D3_P1_Low_T3_17_Mid/1.png", "Image/P25_D3_P1_Low_T3_17_Mid/2.png", "Image/P25_D3_P1_Low_T3_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1869.7672 seconds to 1877.6888 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P4_Medium_T1_8_Left", "images": ["Image/P20_D2_P4_Medium_T1_8_Left/1.png", "Image/P20_D2_P4_Medium_T1_8_Left/2.png", "Image/P20_D2_P4_Medium_T1_8_Left/3.png", "Image/P20_D2_P4_Medium_T1_8_Left/4.png", "Image/P20_D2_P4_Medium_T1_8_Left/5.png", "Image/P20_D2_P4_Medium_T1_8_Left/6.png", "Image/P20_D2_P4_Medium_T1_8_Left/7.png", "Image/P20_D2_P4_Medium_T1_8_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that left cup to that table. Finish\"\nThe frames roughly correspond to the speech interval from 3337.7871000000005 seconds to 3351.3163000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P1_Medium_T2_26_Right", "images": ["Image/P3_D3_P1_Medium_T2_26_Right/1.png", "Image/P3_D3_P1_Medium_T2_26_Right/2.png", "Image/P3_D3_P1_Medium_T2_26_Right/3.png", "Image/P3_D3_P1_Medium_T2_26_Right/4.png", "Image/P3_D3_P1_Medium_T2_26_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 976.0199 seconds to 987.2835 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P6_Low_T1_16_Mid", "images": ["Image/P5_D3_P6_Low_T1_16_Mid/1.png", "Image/P5_D3_P6_Low_T1_16_Mid/2.png", "Image/P5_D3_P6_Low_T1_16_Mid/3.png", "Image/P5_D3_P6_Low_T1_16_Mid/4.png", "Image/P5_D3_P6_Low_T1_16_Mid/5.png", "Image/P5_D3_P6_Low_T1_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Move the cup on the shelf to the table. Finished.\"\nThe frames roughly correspond to the speech interval from 2242.9853 seconds to 2255.4149 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P2_Medium_T2_12_Right", "images": ["Image/P26_D3_P2_Medium_T2_12_Right/1.png", "Image/P26_D3_P2_Medium_T2_12_Right/2.png", "Image/P26_D3_P2_Medium_T2_12_Right/3.png", "Image/P26_D3_P2_Medium_T2_12_Right/4.png", "Image/P26_D3_P2_Medium_T2_12_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour water into that vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 921.0174 seconds to 932.8494 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P3_High_T2_19_Mid", "images": ["Image/P21_D3_P3_High_T2_19_Mid/1.png", "Image/P21_D3_P3_High_T2_19_Mid/2.png", "Image/P21_D3_P3_High_T2_19_Mid/3.png", "Image/P21_D3_P3_High_T2_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 527.8471999999999 seconds to 539.1354 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P2_High_T2_14_Mid", "images": ["Image/P4_D3_P2_High_T2_14_Mid/1.png", "Image/P4_D3_P2_High_T2_14_Mid/2.png", "Image/P4_D3_P2_High_T2_14_Mid/3.png", "Image/P4_D3_P2_High_T2_14_Mid/4.png", "Image/P4_D3_P2_High_T2_14_Mid/5.png", "Image/P4_D3_P2_High_T2_14_Mid/6.png", "Image/P4_D3_P2_High_T2_14_Mid/7.png", "Image/P4_D3_P2_High_T2_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plants in the middle on the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 383.8718999999999 seconds to 397.6102 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P7_Low_T3_16_Mid", "images": ["Image/P29_D2_P7_Low_T3_16_Mid/1.png", "Image/P29_D2_P7_Low_T3_16_Mid/2.png", "Image/P29_D2_P7_Low_T3_16_Mid/3.png", "Image/P29_D2_P7_Low_T3_16_Mid/4.png", "Image/P29_D2_P7_Low_T3_16_Mid/5.png", "Image/P29_D2_P7_Low_T3_16_Mid/6.png", "Image/P29_D2_P7_Low_T3_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 512.5792 seconds to 524.6189 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P9_Low_T1_16_Left", "images": ["Image/P20_D2_P9_Low_T1_16_Left/1.png", "Image/P20_D2_P9_Low_T1_16_Left/2.png", "Image/P20_D2_P9_Low_T1_16_Left/3.png", "Image/P20_D2_P9_Low_T1_16_Left/4.png", "Image/P20_D2_P9_Low_T1_16_Left/5.png", "Image/P20_D2_P9_Low_T1_16_Left/6.png", "Image/P20_D2_P9_Low_T1_16_Left/7.png", "Image/P20_D2_P9_Low_T1_16_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that cup to the top of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 2854.793 seconds to 2868.5734 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P2_Low_T2_11_Right", "images": ["Image/P5_D3_P2_Low_T2_11_Right/1.png", "Image/P5_D3_P2_Low_T2_11_Right/2.png", "Image/P5_D3_P2_Low_T2_11_Right/3.png", "Image/P5_D3_P2_Low_T2_11_Right/4.png", "Image/P5_D3_P2_Low_T2_11_Right/5.png", "Image/P5_D3_P2_Low_T2_11_Right/6.png", "Image/P5_D3_P2_Low_T2_11_Right/7.png", "Image/P5_D3_P2_Low_T2_11_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Pour water into the Next\"\nThe frames roughly correspond to the speech interval from 2717.4595 seconds to 2733.9404999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D1_P3_Medium_T2_7_Left", "images": ["Image/P26_D1_P3_Medium_T2_7_Left/1.png", "Image/P26_D1_P3_Medium_T2_7_Left/2.png", "Image/P26_D1_P3_Medium_T2_7_Left/3.png", "Image/P26_D1_P3_Medium_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water just into this mug? Finish.\"\nThe frames roughly correspond to the speech interval from 555.3598999999999 seconds to 566.0491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P6_Low_T3_25_Mid", "images": ["Image/P22_D2_P6_Low_T3_25_Mid/1.png", "Image/P22_D2_P6_Low_T3_25_Mid/2.png", "Image/P22_D2_P6_Low_T3_25_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 916.4013 seconds to 927.1325 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P9_High_T1_1_Mid", "images": ["Image/P4_D2_P9_High_T1_1_Mid/1.png", "Image/P4_D2_P9_High_T1_1_Mid/2.png", "Image/P4_D2_P9_High_T1_1_Mid/3.png", "Image/P4_D2_P9_High_T1_1_Mid/4.png", "Image/P4_D2_P9_High_T1_1_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green bottle to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1417.09929 seconds to 1429.05264 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P1_High_T3_15_Mid", "images": ["Image/P21_D2_P1_High_T3_15_Mid/1.png", "Image/P21_D2_P1_High_T3_15_Mid/2.png", "Image/P21_D2_P1_High_T3_15_Mid/3.png", "Image/P21_D2_P1_High_T3_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1391.31752 seconds to 1401.22846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P4_Low_T1_19_Mid", "images": ["Image/P20_D3_P4_Low_T1_19_Mid/1.png", "Image/P20_D3_P4_Low_T1_19_Mid/2.png", "Image/P20_D3_P4_Low_T1_19_Mid/3.png", "Image/P20_D3_P4_Low_T1_19_Mid/4.png", "Image/P20_D3_P4_Low_T1_19_Mid/5.png", "Image/P20_D3_P4_Low_T1_19_Mid/6.png", "Image/P20_D3_P4_Low_T1_19_Mid/7.png", "Image/P20_D3_P4_Low_T1_19_Mid/8.png", "Image/P20_D3_P4_Low_T1_19_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey robot\"\nThe frames roughly correspond to the speech interval from 1861.5 seconds to 1876.9911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P7_High_T1_4_Left", "images": ["Image/P11_D3_P7_High_T1_4_Left/1.png", "Image/P11_D3_P7_High_T1_4_Left/2.png", "Image/P11_D3_P7_High_T1_4_Left/3.png", "Image/P11_D3_P7_High_T1_4_Left/4.png", "Image/P11_D3_P7_High_T1_4_Left/5.png", "Image/P11_D3_P7_High_T1_4_Left/6.png", "Image/P11_D3_P7_High_T1_4_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant to the top of the shelf on the same side. Finish.\"\nThe frames roughly correspond to the speech interval from 1837.4444 seconds to 1851.0559 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P8_High_T1_10_Mid", "images": ["Image/P16_D2_P8_High_T1_10_Mid/1.png", "Image/P16_D2_P8_High_T1_10_Mid/2.png", "Image/P16_D2_P8_High_T1_10_Mid/3.png", "Image/P16_D2_P8_High_T1_10_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that vase up there. Finish.\"\nThe frames roughly correspond to the speech interval from 990.5187 seconds to 999.5792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P9_High_T3_3_Left", "images": ["Image/P16_D3_P9_High_T3_3_Left/1.png", "Image/P16_D3_P9_High_T3_3_Left/2.png", "Image/P16_D3_P9_High_T3_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 483.29189999999994 seconds to 492.0054 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P3_Medium_T2_17_Mid", "images": ["Image/P24_D2_P3_Medium_T2_17_Mid/1.png", "Image/P24_D2_P3_Medium_T2_17_Mid/2.png", "Image/P24_D2_P3_Medium_T2_17_Mid/3.png", "Image/P24_D2_P3_Medium_T2_17_Mid/4.png", "Image/P24_D2_P3_Medium_T2_17_Mid/5.png", "Image/P24_D2_P3_Medium_T2_17_Mid/6.png", "Image/P24_D2_P3_Medium_T2_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you pour water in this cup\"\nThe frames roughly correspond to the speech interval from 395.289 seconds to 408.1644 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P2_Low_T1_5_Left", "images": ["Image/P7_D3_P2_Low_T1_5_Left/1.png", "Image/P7_D3_P2_Low_T1_5_Left/2.png", "Image/P7_D3_P2_Low_T1_5_Left/3.png", "Image/P7_D3_P2_Low_T1_5_Left/4.png", "Image/P7_D3_P2_Low_T1_5_Left/5.png", "Image/P7_D3_P2_Low_T1_5_Left/6.png", "Image/P7_D3_P2_Low_T1_5_Left/7.png", "Image/P7_D3_P2_Low_T1_5_Left/8.png", "Image/P7_D3_P2_Low_T1_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. If I remove the plant to the ground. Finish.\"\nThe frames roughly correspond to the speech interval from 1674.49508 seconds to 1690.2381 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P7_High_T2_6_Left", "images": ["Image/P10_D2_P7_High_T2_6_Left/1.png", "Image/P10_D2_P7_High_T2_6_Left/2.png", "Image/P10_D2_P7_High_T2_6_Left/3.png", "Image/P10_D2_P7_High_T2_6_Left/4.png", "Image/P10_D2_P7_High_T2_6_Left/5.png", "Image/P10_D2_P7_High_T2_6_Left/6.png", "Image/P10_D2_P7_High_T2_6_Left/7.png", "Image/P10_D2_P7_High_T2_6_Left/8.png", "Image/P10_D2_P7_High_T2_6_Left/9.png", "Image/P10_D2_P7_High_T2_6_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1561.5247800000002 seconds to 1578.6741800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P2_Low_T2_18_Right", "images": ["Image/P11_D3_P2_Low_T2_18_Right/1.png", "Image/P11_D3_P2_Low_T2_18_Right/2.png", "Image/P11_D3_P2_Low_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the right glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1858.3107 seconds to 1866.981 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P8_Medium_T2_27_Right", "images": ["Image/P18_D3_P8_Medium_T2_27_Right/1.png", "Image/P18_D3_P8_Medium_T2_27_Right/2.png", "Image/P18_D3_P8_Medium_T2_27_Right/3.png", "Image/P18_D3_P8_Medium_T2_27_Right/4.png", "Image/P18_D3_P8_Medium_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1425.9835 seconds to 1436.3971 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P8_Low_T1_7_Right", "images": ["Image/P22_D2_P8_Low_T1_7_Right/1.png", "Image/P22_D2_P8_Low_T1_7_Right/2.png", "Image/P22_D2_P8_Low_T1_7_Right/3.png", "Image/P22_D2_P8_Low_T1_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that cup up there. Finish.\"\nThe frames roughly correspond to the speech interval from 861.6670999999999 seconds to 870.4654999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P6_Medium_T2_8_Left", "images": ["Image/P13_D3_P6_Medium_T2_8_Left/1.png", "Image/P13_D3_P6_Medium_T2_8_Left/2.png", "Image/P13_D3_P6_Medium_T2_8_Left/3.png", "Image/P13_D3_P6_Medium_T2_8_Left/4.png", "Image/P13_D3_P6_Medium_T2_8_Left/5.png", "Image/P13_D3_P6_Medium_T2_8_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you water the left green cup over here\"\nThe frames roughly correspond to the speech interval from 1343.48517 seconds to 1354.53607 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P1_Medium_T2_8_Left", "images": ["Image/P23_D1_P1_Medium_T2_8_Left/1.png", "Image/P23_D1_P1_Medium_T2_8_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1601.5503 seconds to 1609.8053 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P8_Low_T3_6_Right", "images": ["Image/P3_D3_P8_Low_T3_6_Right/1.png", "Image/P3_D3_P8_Low_T3_6_Right/2.png", "Image/P3_D3_P8_Low_T3_6_Right/3.png", "Image/P3_D3_P8_Low_T3_6_Right/4.png", "Image/P3_D3_P8_Low_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 729.602 seconds to 740.4875 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P5_Medium_T3_24_Right", "images": ["Image/P10_D2_P5_Medium_T3_24_Right/1.png", "Image/P10_D2_P5_Medium_T3_24_Right/2.png", "Image/P10_D2_P5_Medium_T3_24_Right/3.png", "Image/P10_D2_P5_Medium_T3_24_Right/4.png", "Image/P10_D2_P5_Medium_T3_24_Right/5.png", "Image/P10_D2_P5_Medium_T3_24_Right/6.png", "Image/P10_D2_P5_Medium_T3_24_Right/7.png", "Image/P10_D2_P5_Medium_T3_24_Right/8.png", "Image/P10_D2_P5_Medium_T3_24_Right/9.png", "Image/P10_D2_P5_Medium_T3_24_Right/10.png", "Image/P10_D2_P5_Medium_T3_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1235.82758 seconds to 1255.7175800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D3_P5_High_T3_1_Left", "images": ["Image/P12_D3_P5_High_T3_1_Left/1.png", "Image/P12_D3_P5_High_T3_1_Left/2.png", "Image/P12_D3_P5_High_T3_1_Left/3.png", "Image/P12_D3_P5_High_T3_1_Left/4.png", "Image/P12_D3_P5_High_T3_1_Left/5.png", "Image/P12_D3_P5_High_T3_1_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2318.17549 seconds to 2333.48739 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P2_Medium_T3_1_Right", "images": ["Image/P8_D1_P2_Medium_T3_1_Right/1.png", "Image/P8_D1_P2_Medium_T3_1_Right/2.png", "Image/P8_D1_P2_Medium_T3_1_Right/3.png", "Image/P8_D1_P2_Medium_T3_1_Right/4.png", "Image/P8_D1_P2_Medium_T3_1_Right/5.png", "Image/P8_D1_P2_Medium_T3_1_Right/6.png", "Image/P8_D1_P2_Medium_T3_1_Right/7.png", "Image/P8_D1_P2_Medium_T3_1_Right/8.png", "Image/P8_D1_P2_Medium_T3_1_Right/9.png", "Image/P8_D1_P2_Medium_T3_1_Right/10.png", "Image/P8_D1_P2_Medium_T3_1_Right/11.png", "Image/P8_D1_P2_Medium_T3_1_Right/12.png", "Image/P8_D1_P2_Medium_T3_1_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Dark. Hi. The bottle that is in front of me is a little bit dirty\"\nThe frames roughly correspond to the speech interval from 292.2893999999999 seconds to 311.7707999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P9_Medium_T3_25_Mid", "images": ["Image/P22_D3_P9_Medium_T3_25_Mid/1.png", "Image/P22_D3_P9_Medium_T3_25_Mid/2.png", "Image/P22_D3_P9_Medium_T3_25_Mid/3.png", "Image/P22_D3_P9_Medium_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 370.0316 seconds to 379.5518 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P9_Medium_T3_9_Left", "images": ["Image/P29_D3_P9_Medium_T3_9_Left/1.png", "Image/P29_D3_P9_Medium_T3_9_Left/2.png", "Image/P29_D3_P9_Medium_T3_9_Left/3.png", "Image/P29_D3_P9_Medium_T3_9_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1670.9371 seconds to 1680.4851999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P2_High_T3_12_Left", "images": ["Image/P8_D3_P2_High_T3_12_Left/1.png", "Image/P8_D3_P2_High_T3_12_Left/2.png", "Image/P8_D3_P2_High_T3_12_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 1036.86476 seconds to 1051.12544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P7_Low_T2_5_Left", "images": ["Image/P11_D3_P7_Low_T2_5_Left/1.png", "Image/P11_D3_P7_Low_T2_5_Left/2.png", "Image/P11_D3_P7_Low_T2_5_Left/3.png", "Image/P11_D3_P7_Low_T2_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant under the table. Finish.\"\nThe frames roughly correspond to the speech interval from 1578.4504 seconds to 1587.6786 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P5_Medium_T3_3_Left", "images": ["Image/P3_D1_P5_Medium_T3_3_Left/1.png", "Image/P3_D1_P5_Medium_T3_3_Left/2.png", "Image/P3_D1_P5_Medium_T3_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean this item? Finish\"\nThe frames roughly correspond to the speech interval from 1288.1649 seconds to 1297.6037 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P3_Low_T2_2_Left", "images": ["Image/P20_D3_P3_Low_T2_2_Left/1.png", "Image/P20_D3_P3_Low_T2_2_Left/2.png", "Image/P20_D3_P3_Low_T2_2_Left/3.png", "Image/P20_D3_P3_Low_T2_2_Left/4.png", "Image/P20_D3_P3_Low_T2_2_Left/5.png", "Image/P20_D3_P3_Low_T2_2_Left/6.png", "Image/P20_D3_P3_Low_T2_2_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Add some water into that blue-green bottle over there. Yeah.\"\nThe frames roughly correspond to the speech interval from 1280.0793 seconds to 1295.1122 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P6_Medium_T2_24_Mid", "images": ["Image/P5_D3_P6_Medium_T2_24_Mid/1.png", "Image/P5_D3_P6_Medium_T2_24_Mid/2.png", "Image/P5_D3_P6_Medium_T2_24_Mid/3.png", "Image/P5_D3_P6_Medium_T2_24_Mid/4.png", "Image/P5_D3_P6_Medium_T2_24_Mid/5.png", "Image/P5_D3_P6_Medium_T2_24_Mid/6.png", "Image/P5_D3_P6_Medium_T2_24_Mid/7.png", "Image/P5_D3_P6_Medium_T2_24_Mid/8.png", "Image/P5_D3_P6_Medium_T2_24_Mid/9.png", "Image/P5_D3_P6_Medium_T2_24_Mid/10.png", "Image/P5_D3_P6_Medium_T2_24_Mid/11.png", "Image/P5_D3_P6_Medium_T2_24_Mid/12.png", "Image/P5_D3_P6_Medium_T2_24_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"What are the plants on the shelf in the living room? The shelf is the biggest one among all the shelves in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2661.2373 seconds to 2685.1928 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P5_Medium_T2_24_Mid", "images": ["Image/P20_D2_P5_Medium_T2_24_Mid/1.png", "Image/P20_D2_P5_Medium_T2_24_Mid/2.png", "Image/P20_D2_P5_Medium_T2_24_Mid/3.png", "Image/P20_D2_P5_Medium_T2_24_Mid/4.png", "Image/P20_D2_P5_Medium_T2_24_Mid/5.png", "Image/P20_D2_P5_Medium_T2_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3150.7731000000003 seconds to 3162.5478000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P7_Low_T2_8_Right", "images": ["Image/P24_D2_P7_Low_T2_8_Right/1.png", "Image/P24_D2_P7_Low_T2_8_Right/2.png", "Image/P24_D2_P7_Low_T2_8_Right/3.png", "Image/P24_D2_P7_Low_T2_8_Right/4.png", "Image/P24_D2_P7_Low_T2_8_Right/5.png", "Image/P24_D2_P7_Low_T2_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you pour water in this cup\"\nThe frames roughly correspond to the speech interval from 256.89739999999995 seconds to 268.479 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P8_Medium_T2_23_Left", "images": ["Image/P1_D2_P8_Medium_T2_23_Left/1.png", "Image/P1_D2_P8_Medium_T2_23_Left/2.png", "Image/P1_D2_P8_Medium_T2_23_Left/3.png", "Image/P1_D2_P8_Medium_T2_23_Left/4.png", "Image/P1_D2_P8_Medium_T2_23_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 806.7927000000001 seconds to 817.8329000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P2_Low_T3_4_Mid", "images": ["Image/P11_D1_P2_Low_T3_4_Mid/1.png", "Image/P11_D1_P2_Low_T3_4_Mid/2.png", "Image/P11_D1_P2_Low_T3_4_Mid/3.png", "Image/P11_D1_P2_Low_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the plant in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2668.9824999999996 seconds to 2678.2256 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P4_Low_T3_23_Right", "images": ["Image/P22_D1_P4_Low_T3_23_Right/1.png", "Image/P22_D1_P4_Low_T3_23_Right/2.png", "Image/P22_D1_P4_Low_T3_23_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 1426.61082 seconds to 1436.28565 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P8_High_T2_5_Right", "images": ["Image/P10_D2_P8_High_T2_5_Right/1.png", "Image/P10_D2_P8_High_T2_5_Right/2.png", "Image/P10_D2_P8_High_T2_5_Right/3.png", "Image/P10_D2_P8_High_T2_5_Right/4.png", "Image/P10_D2_P8_High_T2_5_Right/5.png", "Image/P10_D2_P8_High_T2_5_Right/6.png", "Image/P10_D2_P8_High_T2_5_Right/7.png", "Image/P10_D2_P8_High_T2_5_Right/8.png", "Image/P10_D2_P8_High_T2_5_Right/9.png", "Image/P10_D2_P8_High_T2_5_Right/10.png", "Image/P10_D2_P8_High_T2_5_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1696.22748 seconds to 1712.90398 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P5_High_T3_19_Mid", "images": ["Image/P22_D1_P5_High_T3_19_Mid/1.png", "Image/P22_D1_P5_High_T3_19_Mid/2.png", "Image/P22_D1_P5_High_T3_19_Mid/3.png", "Image/P22_D1_P5_High_T3_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this vase. Finish.\"\nThe frames roughly correspond to the speech interval from 1295.52529 seconds to 1305.04589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P8_Medium_T1_18_Left", "images": ["Image/P29_D1_P8_Medium_T1_18_Left/1.png", "Image/P29_D1_P8_Medium_T1_18_Left/2.png", "Image/P29_D1_P8_Medium_T1_18_Left/3.png", "Image/P29_D1_P8_Medium_T1_18_Left/4.png", "Image/P29_D1_P8_Medium_T1_18_Left/5.png", "Image/P29_D1_P8_Medium_T1_18_Left/6.png", "Image/P29_D1_P8_Medium_T1_18_Left/7.png", "Image/P29_D1_P8_Medium_T1_18_Left/8.png", "Image/P29_D1_P8_Medium_T1_18_Left/9.png", "Image/P29_D1_P8_Medium_T1_18_Left/10.png", "Image/P29_D1_P8_Medium_T1_18_Left/11.png", "Image/P29_D1_P8_Medium_T1_18_Left/12.png", "Image/P29_D1_P8_Medium_T1_18_Left/13.png", "Image/P29_D1_P8_Medium_T1_18_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2834.7787999999996 seconds to 2853.6418 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P1_Low_T2_12_Left", "images": ["Image/P15_D1_P1_Low_T2_12_Left/1.png", "Image/P15_D1_P1_Low_T2_12_Left/2.png", "Image/P15_D1_P1_Low_T2_12_Left/3.png", "Image/P15_D1_P1_Low_T2_12_Left/4.png", "Image/P15_D1_P1_Low_T2_12_Left/5.png", "Image/P15_D1_P1_Low_T2_12_Left/6.png", "Image/P15_D1_P1_Low_T2_12_Left/7.png", "Image/P15_D1_P1_Low_T2_12_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into the brown jar at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1977.73847 seconds to 1991.25287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P6_Medium_T1_16_Mid", "images": ["Image/P17_D2_P6_Medium_T1_16_Mid/1.png", "Image/P17_D2_P6_Medium_T1_16_Mid/2.png", "Image/P17_D2_P6_Medium_T1_16_Mid/3.png", "Image/P17_D2_P6_Medium_T1_16_Mid/4.png", "Image/P17_D2_P6_Medium_T1_16_Mid/5.png", "Image/P17_D2_P6_Medium_T1_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that mug in the center to the shelf over here? Finish.\"\nThe frames roughly correspond to the speech interval from 239.16849999999997 seconds to 252.3084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P1_Low_T1_5_Right", "images": ["Image/P19_D1_P1_Low_T1_5_Right/1.png", "Image/P19_D1_P1_Low_T1_5_Right/2.png", "Image/P19_D1_P1_Low_T1_5_Right/3.png", "Image/P19_D1_P1_Low_T1_5_Right/4.png", "Image/P19_D1_P1_Low_T1_5_Right/5.png", "Image/P19_D1_P1_Low_T1_5_Right/6.png", "Image/P19_D1_P1_Low_T1_5_Right/7.png", "Image/P19_D1_P1_Low_T1_5_Right/8.png", "Image/P19_D1_P1_Low_T1_5_Right/9.png", "Image/P19_D1_P1_Low_T1_5_Right/10.png", "Image/P19_D1_P1_Low_T1_5_Right/11.png", "Image/P19_D1_P1_Low_T1_5_Right/12.png", "Image/P19_D1_P1_Low_T1_5_Right/13.png", "Image/P19_D1_P1_Low_T1_5_Right/14.png", "Image/P19_D1_P1_Low_T1_5_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this flowerpot to this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 726.8826 seconds to 744.5029999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P5_High_T3_19_Left", "images": ["Image/P10_D1_P5_High_T3_19_Left/1.png", "Image/P10_D1_P5_High_T3_19_Left/2.png", "Image/P10_D1_P5_High_T3_19_Left/3.png", "Image/P10_D1_P5_High_T3_19_Left/4.png", "Image/P10_D1_P5_High_T3_19_Left/5.png", "Image/P10_D1_P5_High_T3_19_Left/6.png", "Image/P10_D1_P5_High_T3_19_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2076.90052 seconds to 2091.79374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P6_High_T2_13_Mid", "images": ["Image/P23_D3_P6_High_T2_13_Mid/1.png", "Image/P23_D3_P6_High_T2_13_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 767.6532 seconds to 775.8667 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D1_P2_Low_T1_10_Left", "images": ["Image/P12_D1_P2_Low_T1_10_Left/1.png", "Image/P12_D1_P2_Low_T1_10_Left/2.png", "Image/P12_D1_P2_Low_T1_10_Left/3.png", "Image/P12_D1_P2_Low_T1_10_Left/4.png", "Image/P12_D1_P2_Low_T1_10_Left/5.png", "Image/P12_D1_P2_Low_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1455.8489 seconds to 1467.2746 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P6_Low_T2_26_Left", "images": ["Image/P9_D2_P6_Low_T2_26_Left/1.png", "Image/P9_D2_P6_Low_T2_26_Left/2.png", "Image/P9_D2_P6_Low_T2_26_Left/3.png", "Image/P9_D2_P6_Low_T2_26_Left/4.png", "Image/P9_D2_P6_Low_T2_26_Left/5.png", "Image/P9_D2_P6_Low_T2_26_Left/6.png", "Image/P9_D2_P6_Low_T2_26_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in that cup on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2776.5862 seconds to 2789.4443 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P8_Low_T2_6_Right", "images": ["Image/P4_D2_P8_Low_T2_6_Right/1.png", "Image/P4_D2_P8_Low_T2_6_Right/2.png", "Image/P4_D2_P8_Low_T2_6_Right/3.png", "Image/P4_D2_P8_Low_T2_6_Right/4.png", "Image/P4_D2_P8_Low_T2_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1740.1816 seconds to 1753.6929999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P5_High_T1_22_Mid", "images": ["Image/P29_D2_P5_High_T1_22_Mid/1.png", "Image/P29_D2_P5_High_T1_22_Mid/2.png", "Image/P29_D2_P5_High_T1_22_Mid/3.png", "Image/P29_D2_P5_High_T1_22_Mid/4.png", "Image/P29_D2_P5_High_T1_22_Mid/5.png", "Image/P29_D2_P5_High_T1_22_Mid/6.png", "Image/P29_D2_P5_High_T1_22_Mid/7.png", "Image/P29_D2_P5_High_T1_22_Mid/8.png", "Image/P29_D2_P5_High_T1_22_Mid/9.png", "Image/P29_D2_P5_High_T1_22_Mid/10.png", "Image/P29_D2_P5_High_T1_22_Mid/11.png", "Image/P29_D2_P5_High_T1_22_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 466.00340000000006 seconds to 484.1129 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D3_P5_Medium_T1_14_Mid", "images": ["Image/P18_D3_P5_Medium_T1_14_Mid/1.png", "Image/P18_D3_P5_Medium_T1_14_Mid/2.png", "Image/P18_D3_P5_Medium_T1_14_Mid/3.png", "Image/P18_D3_P5_Medium_T1_14_Mid/4.png", "Image/P18_D3_P5_Medium_T1_14_Mid/5.png", "Image/P18_D3_P5_Medium_T1_14_Mid/6.png", "Image/P18_D3_P5_Medium_T1_14_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that middle plant over here to the second level from the top. Finish.\"\nThe frames roughly correspond to the speech interval from 1601.069 seconds to 1613.8895 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P8_Low_T3_16_Mid", "images": ["Image/P7_D1_P8_Low_T3_16_Mid/1.png", "Image/P7_D1_P8_Low_T3_16_Mid/2.png", "Image/P7_D1_P8_Low_T3_16_Mid/3.png", "Image/P7_D1_P8_Low_T3_16_Mid/4.png", "Image/P7_D1_P8_Low_T3_16_Mid/5.png", "Image/P7_D1_P8_Low_T3_16_Mid/6.png", "Image/P7_D1_P8_Low_T3_16_Mid/7.png", "Image/P7_D1_P8_Low_T3_16_Mid/8.png", "Image/P7_D1_P8_Low_T3_16_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 124.45337 seconds to 141.2689 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P1_Medium_T3_7_Left", "images": ["Image/P20_D3_P1_Medium_T3_7_Left/1.png", "Image/P20_D3_P1_Medium_T3_7_Left/2.png", "Image/P20_D3_P1_Medium_T3_7_Left/3.png", "Image/P20_D3_P1_Medium_T3_7_Left/4.png", "Image/P20_D3_P1_Medium_T3_7_Left/5.png", "Image/P20_D3_P1_Medium_T3_7_Left/6.png", "Image/P20_D3_P1_Medium_T3_7_Left/7.png", "Image/P20_D3_P1_Medium_T3_7_Left/8.png", "Image/P20_D3_P1_Medium_T3_7_Left/9.png", "Image/P20_D3_P1_Medium_T3_7_Left/10.png", "Image/P20_D3_P1_Medium_T3_7_Left/11.png", "Image/P20_D3_P1_Medium_T3_7_Left/12.png", "Image/P20_D3_P1_Medium_T3_7_Left/13.png", "Image/P20_D3_P1_Medium_T3_7_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move water into the left bottle over there on the shelf? White cup? Green cup? Yeah. Finish.\"\nThe frames roughly correspond to the speech interval from 1224.1906 seconds to 1244.4587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P7_High_T3_2_Right", "images": ["Image/P8_D1_P7_High_T3_2_Right/1.png", "Image/P8_D1_P7_High_T3_2_Right/2.png", "Image/P8_D1_P7_High_T3_2_Right/3.png", "Image/P8_D1_P7_High_T3_2_Right/4.png", "Image/P8_D1_P7_High_T3_2_Right/5.png", "Image/P8_D1_P7_High_T3_2_Right/6.png", "Image/P8_D1_P7_High_T3_2_Right/7.png", "Image/P8_D1_P7_High_T3_2_Right/8.png", "Image/P8_D1_P7_High_T3_2_Right/9.png", "Image/P8_D1_P7_High_T3_2_Right/10.png", "Image/P8_D1_P7_High_T3_2_Right/11.png", "Image/P8_D1_P7_High_T3_2_Right/12.png", "Image/P8_D1_P7_High_T3_2_Right/13.png", "Image/P8_D1_P7_High_T3_2_Right/14.png", "Image/P8_D1_P7_High_T3_2_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hi\"\nThe frames roughly correspond to the speech interval from 352.7647999999999 seconds to 373.5514999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P6_High_T2_10_Right", "images": ["Image/P9_D3_P6_High_T2_10_Right/1.png", "Image/P9_D3_P6_High_T2_10_Right/2.png", "Image/P9_D3_P6_High_T2_10_Right/3.png", "Image/P9_D3_P6_High_T2_10_Right/4.png", "Image/P9_D3_P6_High_T2_10_Right/5.png", "Image/P9_D3_P6_High_T2_10_Right/6.png", "Image/P9_D3_P6_High_T2_10_Right/7.png", "Image/P9_D3_P6_High_T2_10_Right/8.png", "Image/P9_D3_P6_High_T2_10_Right/9.png", "Image/P9_D3_P6_High_T2_10_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you help me to pour some water in the red bottle\"\nThe frames roughly correspond to the speech interval from 645.4862 seconds to 661.845 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P7_Low_T3_9_Mid", "images": ["Image/P22_D2_P7_Low_T3_9_Mid/1.png", "Image/P22_D2_P7_Low_T3_9_Mid/2.png", "Image/P22_D2_P7_Low_T3_9_Mid/3.png", "Image/P22_D2_P7_Low_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that cup. Finish.\"\nThe frames roughly correspond to the speech interval from 844.5038999999999 seconds to 854.0671 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P8_High_T2_8_Left", "images": ["Image/P15_D3_P8_High_T2_8_Left/1.png", "Image/P15_D3_P8_High_T2_8_Left/2.png", "Image/P15_D3_P8_High_T2_8_Left/3.png", "Image/P15_D3_P8_High_T2_8_Left/4.png", "Image/P15_D3_P8_High_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 255.96560000000005 seconds to 267.1435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P4_High_T3_18_Right", "images": ["Image/P28_D2_P4_High_T3_18_Right/1.png", "Image/P28_D2_P4_High_T3_18_Right/2.png", "Image/P28_D2_P4_High_T3_18_Right/3.png", "Image/P28_D2_P4_High_T3_18_Right/4.png", "Image/P28_D2_P4_High_T3_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that orange vase. Finish.\"\nThe frames roughly correspond to the speech interval from 852.24866 seconds to 862.85528 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P4_Medium_T1_27_Right", "images": ["Image/P16_D1_P4_Medium_T1_27_Right/1.png", "Image/P16_D1_P4_Medium_T1_27_Right/2.png", "Image/P16_D1_P4_Medium_T1_27_Right/3.png", "Image/P16_D1_P4_Medium_T1_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this mug over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1218.50631 seconds to 1228.09652 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P3_High_T2_23_Right", "images": ["Image/P12_D2_P3_High_T2_23_Right/1.png", "Image/P12_D2_P3_High_T2_23_Right/2.png", "Image/P12_D2_P3_High_T2_23_Right/3.png", "Image/P12_D2_P3_High_T2_23_Right/4.png", "Image/P12_D2_P3_High_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the white plant. Finish.\"\nThe frames roughly correspond to the speech interval from 655.0482 seconds to 665.6409 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P4_High_T2_8_Left", "images": ["Image/P7_D3_P4_High_T2_8_Left/1.png", "Image/P7_D3_P4_High_T2_8_Left/2.png", "Image/P7_D3_P4_High_T2_8_Left/3.png", "Image/P7_D3_P4_High_T2_8_Left/4.png", "Image/P7_D3_P4_High_T2_8_Left/5.png", "Image/P7_D3_P4_High_T2_8_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2098.4406 seconds to 2110.9407 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P9_Low_T1_3_Mid", "images": ["Image/P4_D1_P9_Low_T1_3_Mid/1.png", "Image/P4_D1_P9_Low_T1_3_Mid/2.png", "Image/P4_D1_P9_Low_T1_3_Mid/3.png", "Image/P4_D1_P9_Low_T1_3_Mid/4.png", "Image/P4_D1_P9_Low_T1_3_Mid/5.png", "Image/P4_D1_P9_Low_T1_3_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that bottle up here? Finish.\"\nThe frames roughly correspond to the speech interval from 2143.1973799999996 seconds to 2157.1973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P4_Low_T3_19_Left", "images": ["Image/P27_D2_P4_Low_T3_19_Left/1.png", "Image/P27_D2_P4_Low_T3_19_Left/2.png", "Image/P27_D2_P4_Low_T3_19_Left/3.png", "Image/P27_D2_P4_Low_T3_19_Left/4.png", "Image/P27_D2_P4_Low_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that vase? Finish\"\nThe frames roughly correspond to the speech interval from 1532.8168999999998 seconds to 1545.8598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P5_High_T1_25_Left", "images": ["Image/P13_D2_P5_High_T1_25_Left/1.png", "Image/P13_D2_P5_High_T1_25_Left/2.png", "Image/P13_D2_P5_High_T1_25_Left/3.png", "Image/P13_D2_P5_High_T1_25_Left/4.png", "Image/P13_D2_P5_High_T1_25_Left/5.png", "Image/P13_D2_P5_High_T1_25_Left/6.png", "Image/P13_D2_P5_High_T1_25_Left/7.png", "Image/P13_D2_P5_High_T1_25_Left/8.png", "Image/P13_D2_P5_High_T1_25_Left/9.png", "Image/P13_D2_P5_High_T1_25_Left/10.png", "Image/P13_D2_P5_High_T1_25_Left/11.png", "Image/P13_D2_P5_High_T1_25_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup in front of me over to this table over here\"\nThe frames roughly correspond to the speech interval from 1114.49166 seconds to 1131.75126 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P9_Low_T1_15_Left", "images": ["Image/P26_D3_P9_Low_T1_15_Left/1.png", "Image/P26_D3_P9_Low_T1_15_Left/2.png", "Image/P26_D3_P9_Low_T1_15_Left/3.png", "Image/P26_D3_P9_Low_T1_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the plant to the top shelf of that cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 1089.4477000000002 seconds to 1099.9422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P2_High_T3_22_Left", "images": ["Image/P10_D1_P2_High_T3_22_Left/1.png", "Image/P10_D1_P2_High_T3_22_Left/2.png", "Image/P10_D1_P2_High_T3_22_Left/3.png", "Image/P10_D1_P2_High_T3_22_Left/4.png", "Image/P10_D1_P2_High_T3_22_Left/5.png", "Image/P10_D1_P2_High_T3_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2381.1195 seconds to 2393.7024 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P6_High_T3_22_Right", "images": ["Image/P5_D1_P6_High_T3_22_Right/1.png", "Image/P5_D1_P6_High_T3_22_Right/2.png", "Image/P5_D1_P6_High_T3_22_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Finished.\"\nThe frames roughly correspond to the speech interval from 1622.4448 seconds to 1631.2545 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P4_Medium_T1_24_Right", "images": ["Image/P21_D3_P4_Medium_T1_24_Right/1.png", "Image/P21_D3_P4_Medium_T1_24_Right/2.png", "Image/P21_D3_P4_Medium_T1_24_Right/3.png", "Image/P21_D3_P4_Medium_T1_24_Right/4.png", "Image/P21_D3_P4_Medium_T1_24_Right/5.png", "Image/P21_D3_P4_Medium_T1_24_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 677.7245 seconds to 689.9024999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D1_P9_Low_T1_17_Mid", "images": ["Image/P21_D1_P9_Low_T1_17_Mid/1.png", "Image/P21_D1_P9_Low_T1_17_Mid/2.png", "Image/P21_D1_P9_Low_T1_17_Mid/3.png", "Image/P21_D1_P9_Low_T1_17_Mid/4.png", "Image/P21_D1_P9_Low_T1_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 914.1925 seconds to 926.331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P6_High_T1_17_Right", "images": ["Image/P10_D3_P6_High_T1_17_Right/1.png", "Image/P10_D3_P6_High_T1_17_Right/2.png", "Image/P10_D3_P6_High_T1_17_Right/3.png", "Image/P10_D3_P6_High_T1_17_Right/4.png", "Image/P10_D3_P6_High_T1_17_Right/5.png", "Image/P10_D3_P6_High_T1_17_Right/6.png", "Image/P10_D3_P6_High_T1_17_Right/7.png", "Image/P10_D3_P6_High_T1_17_Right/8.png", "Image/P10_D3_P6_High_T1_17_Right/9.png", "Image/P10_D3_P6_High_T1_17_Right/10.png", "Image/P10_D3_P6_High_T1_17_Right/11.png", "Image/P10_D3_P6_High_T1_17_Right/12.png", "Image/P10_D3_P6_High_T1_17_Right/13.png", "Image/P10_D3_P6_High_T1_17_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 491.03649 seconds to 512.12389 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P9_High_T2_20_Left", "images": ["Image/P26_D3_P9_High_T2_20_Left/1.png", "Image/P26_D3_P9_High_T2_20_Left/2.png", "Image/P26_D3_P9_High_T2_20_Left/3.png", "Image/P26_D3_P9_High_T2_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour water into the white vase? Finish.\"\nThe frames roughly correspond to the speech interval from 846.9203 seconds to 856.1357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P3_High_T2_21_Mid", "images": ["Image/P23_D1_P3_High_T2_21_Mid/1.png", "Image/P23_D1_P3_High_T2_21_Mid/2.png", "Image/P23_D1_P3_High_T2_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Just pour water here. Finish.\"\nThe frames roughly correspond to the speech interval from 1632.5231 seconds to 1641.2911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P9_Low_T1_22_Mid", "images": ["Image/P19_D1_P9_Low_T1_22_Mid/1.png", "Image/P19_D1_P9_Low_T1_22_Mid/2.png", "Image/P19_D1_P9_Low_T1_22_Mid/3.png", "Image/P19_D1_P9_Low_T1_22_Mid/4.png", "Image/P19_D1_P9_Low_T1_22_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this flowerpot here.\"\nThe frames roughly correspond to the speech interval from 1206.3918 seconds to 1220.1855999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P2_High_T1_6_Mid", "images": ["Image/P8_D2_P2_High_T1_6_Mid/1.png", "Image/P8_D2_P2_High_T1_6_Mid/2.png", "Image/P8_D2_P2_High_T1_6_Mid/3.png", "Image/P8_D2_P2_High_T1_6_Mid/4.png", "Image/P8_D2_P2_High_T1_6_Mid/5.png", "Image/P8_D2_P2_High_T1_6_Mid/6.png", "Image/P8_D2_P2_High_T1_6_Mid/7.png", "Image/P8_D2_P2_High_T1_6_Mid/8.png", "Image/P8_D2_P2_High_T1_6_Mid/9.png", "Image/P8_D2_P2_High_T1_6_Mid/10.png", "Image/P8_D2_P2_High_T1_6_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you move that planet to the ground floor of the desk at the bottom? Just that desk. Finish.\"\nThe frames roughly correspond to the speech interval from 2209.8209 seconds to 2232.802 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P8_High_T2_3_Left", "images": ["Image/P8_D3_P8_High_T2_3_Left/1.png", "Image/P8_D3_P8_High_T2_3_Left/2.png", "Image/P8_D3_P8_High_T2_3_Left/3.png", "Image/P8_D3_P8_High_T2_3_Left/4.png", "Image/P8_D3_P8_High_T2_3_Left/5.png", "Image/P8_D3_P8_High_T2_3_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you pour some water to the green glass or the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1963.5776 seconds to 1975.8793 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P2_Low_T1_11_Left", "images": ["Image/P6_D2_P2_Low_T1_11_Left/1.png", "Image/P6_D2_P2_Low_T1_11_Left/2.png", "Image/P6_D2_P2_Low_T1_11_Left/3.png", "Image/P6_D2_P2_Low_T1_11_Left/4.png", "Image/P6_D2_P2_Low_T1_11_Left/5.png", "Image/P6_D2_P2_Low_T1_11_Left/6.png", "Image/P6_D2_P2_Low_T1_11_Left/7.png", "Image/P6_D2_P2_Low_T1_11_Left/8.png", "Image/P6_D2_P2_Low_T1_11_Left/9.png", "Image/P6_D2_P2_Low_T1_11_Left/10.png", "Image/P6_D2_P2_Low_T1_11_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1178.0869 seconds to 1195.0029 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P4_Low_T2_7_Left", "images": ["Image/P26_D1_P4_Low_T2_7_Left/1.png", "Image/P26_D1_P4_Low_T2_7_Left/2.png", "Image/P26_D1_P4_Low_T2_7_Left/3.png", "Image/P26_D1_P4_Low_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in here? Finish\"\nThe frames roughly correspond to the speech interval from 475.362 seconds to 485.46629999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P8_Medium_T3_8_Right", "images": ["Image/P26_D2_P8_Medium_T3_8_Right/1.png", "Image/P26_D2_P8_Medium_T3_8_Right/2.png", "Image/P26_D2_P8_Medium_T3_8_Right/3.png", "Image/P26_D2_P8_Medium_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean the vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1528.7489 seconds to 1539.6063 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P6_High_T1_15_Right", "images": ["Image/P13_D2_P6_High_T1_15_Right/1.png", "Image/P13_D2_P6_High_T1_15_Right/2.png", "Image/P13_D2_P6_High_T1_15_Right/3.png", "Image/P13_D2_P6_High_T1_15_Right/4.png", "Image/P13_D2_P6_High_T1_15_Right/5.png", "Image/P13_D2_P6_High_T1_15_Right/6.png", "Image/P13_D2_P6_High_T1_15_Right/7.png", "Image/P13_D2_P6_High_T1_15_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the plant from over there to the middle shelf right here\"\nThe frames roughly correspond to the speech interval from 938.65496 seconds to 951.64106 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P5_Low_T2_14_Left", "images": ["Image/P9_D1_P5_Low_T2_14_Left/1.png", "Image/P9_D1_P5_Low_T2_14_Left/2.png", "Image/P9_D1_P5_Low_T2_14_Left/3.png", "Image/P9_D1_P5_Low_T2_14_Left/4.png", "Image/P9_D1_P5_Low_T2_14_Left/5.png", "Image/P9_D1_P5_Low_T2_14_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1332.7587999999998 seconds to 1345.4654999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P9_Medium_T3_17_Mid", "images": ["Image/P27_D1_P9_Medium_T3_17_Mid/1.png", "Image/P27_D1_P9_Medium_T3_17_Mid/2.png", "Image/P27_D1_P9_Medium_T3_17_Mid/3.png", "Image/P27_D1_P9_Medium_T3_17_Mid/4.png", "Image/P27_D1_P9_Medium_T3_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 781.6689 seconds to 793.9308000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D1_P9_Medium_T1_8_Left", "images": ["Image/P29_D1_P9_Medium_T1_8_Left/1.png", "Image/P29_D1_P9_Medium_T1_8_Left/2.png", "Image/P29_D1_P9_Medium_T1_8_Left/3.png", "Image/P29_D1_P9_Medium_T1_8_Left/4.png", "Image/P29_D1_P9_Medium_T1_8_Left/5.png", "Image/P29_D1_P9_Medium_T1_8_Left/6.png", "Image/P29_D1_P9_Medium_T1_8_Left/7.png", "Image/P29_D1_P9_Medium_T1_8_Left/8.png", "Image/P29_D1_P9_Medium_T1_8_Left/9.png", "Image/P29_D1_P9_Medium_T1_8_Left/10.png", "Image/P29_D1_P9_Medium_T1_8_Left/11.png", "Image/P29_D1_P9_Medium_T1_8_Left/12.png", "Image/P29_D1_P9_Medium_T1_8_Left/13.png", "Image/P29_D1_P9_Medium_T1_8_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2542.5739 seconds to 2556.5352999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P8_High_T3_9_Right", "images": ["Image/P1_D2_P8_High_T3_9_Right/1.png", "Image/P1_D2_P8_High_T3_9_Right/2.png", "Image/P1_D2_P8_High_T3_9_Right/3.png", "Image/P1_D2_P8_High_T3_9_Right/4.png", "Image/P1_D2_P8_High_T3_9_Right/5.png", "Image/P1_D2_P8_High_T3_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean the left cup? Finish.\"\nThe frames roughly correspond to the speech interval from 880.0772000000001 seconds to 891.4515000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P2_Medium_T3_18_Left", "images": ["Image/P20_D2_P2_Medium_T3_18_Left/1.png", "Image/P20_D2_P2_Medium_T3_18_Left/2.png", "Image/P20_D2_P2_Medium_T3_18_Left/3.png", "Image/P20_D2_P2_Medium_T3_18_Left/4.png", "Image/P20_D2_P2_Medium_T3_18_Left/5.png", "Image/P20_D2_P2_Medium_T3_18_Left/6.png", "Image/P20_D2_P2_Medium_T3_18_Left/7.png", "Image/P20_D2_P2_Medium_T3_18_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. That left cup on the shelf\"\nThe frames roughly correspond to the speech interval from 3058.5396 seconds to 3071.3849000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P3_Low_T1_24_Left", "images": ["Image/P2_D3_P3_Low_T1_24_Left/1.png", "Image/P2_D3_P3_Low_T1_24_Left/2.png", "Image/P2_D3_P3_Low_T1_24_Left/3.png", "Image/P2_D3_P3_Low_T1_24_Left/4.png", "Image/P2_D3_P3_Low_T1_24_Left/5.png", "Image/P2_D3_P3_Low_T1_24_Left/6.png", "Image/P2_D3_P3_Low_T1_24_Left/7.png", "Image/P2_D3_P3_Low_T1_24_Left/8.png", "Image/P2_D3_P3_Low_T1_24_Left/9.png", "Image/P2_D3_P3_Low_T1_24_Left/10.png", "Image/P2_D3_P3_Low_T1_24_Left/11.png", "Image/P2_D3_P3_Low_T1_24_Left/12.png", "Image/P2_D3_P3_Low_T1_24_Left/13.png", "Image/P2_D3_P3_Low_T1_24_Left/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 1957.2408000000005 seconds to 1978.4522 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P8_High_T1_27_Mid", "images": ["Image/P19_D1_P8_High_T1_27_Mid/1.png", "Image/P19_D1_P8_High_T1_27_Mid/2.png", "Image/P19_D1_P8_High_T1_27_Mid/3.png", "Image/P19_D1_P8_High_T1_27_Mid/4.png", "Image/P19_D1_P8_High_T1_27_Mid/5.png", "Image/P19_D1_P8_High_T1_27_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Please move this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 628.4669 seconds to 641.3561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P2_High_T3_13_Right", "images": ["Image/P12_D2_P2_High_T3_13_Right/1.png", "Image/P12_D2_P2_High_T3_13_Right/2.png", "Image/P12_D2_P2_High_T3_13_Right/3.png", "Image/P12_D2_P2_High_T3_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 637.2597 seconds to 648.1994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P9_High_T3_7_Left", "images": ["Image/P23_D1_P9_High_T3_7_Left/1.png", "Image/P23_D1_P9_High_T3_7_Left/2.png", "Image/P23_D1_P9_High_T3_7_Left/3.png", "Image/P23_D1_P9_High_T3_7_Left/4.png", "Image/P23_D1_P9_High_T3_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1880.5326 seconds to 1890.847 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P4_Low_T2_8_Right", "images": ["Image/P5_D2_P4_Low_T2_8_Right/1.png", "Image/P5_D2_P4_Low_T2_8_Right/2.png", "Image/P5_D2_P4_Low_T2_8_Right/3.png", "Image/P5_D2_P4_Low_T2_8_Right/4.png", "Image/P5_D2_P4_Low_T2_8_Right/5.png", "Image/P5_D2_P4_Low_T2_8_Right/6.png", "Image/P5_D2_P4_Low_T2_8_Right/7.png", "Image/P5_D2_P4_Low_T2_8_Right/8.png", "Image/P5_D2_P4_Low_T2_8_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water to the cup on the left side of me. Finish.\"\nThe frames roughly correspond to the speech interval from 893.2125000000001 seconds to 907.4391 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P3_High_T1_25_Mid", "images": ["Image/P12_D3_P3_High_T1_25_Mid/1.png", "Image/P12_D3_P3_High_T1_25_Mid/2.png", "Image/P12_D3_P3_High_T1_25_Mid/3.png", "Image/P12_D3_P3_High_T1_25_Mid/4.png", "Image/P12_D3_P3_High_T1_25_Mid/5.png", "Image/P12_D3_P3_High_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that white mug to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2715.76989 seconds to 2727.23809 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P2_Low_T1_20_Left", "images": ["Image/P30_D1_P2_Low_T1_20_Left/1.png", "Image/P30_D1_P2_Low_T1_20_Left/2.png", "Image/P30_D1_P2_Low_T1_20_Left/3.png", "Image/P30_D1_P2_Low_T1_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 732.7908000000001 seconds to 741.9363000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P7_Medium_T3_16_Left", "images": ["Image/P25_D2_P7_Medium_T3_16_Left/1.png", "Image/P25_D2_P7_Medium_T3_16_Left/2.png", "Image/P25_D2_P7_Medium_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Lean the third cup. Finish.\"\nThe frames roughly correspond to the speech interval from 855.1543 seconds to 863.2564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P1_High_T2_6_Right", "images": ["Image/P7_D1_P1_High_T2_6_Right/1.png", "Image/P7_D1_P1_High_T2_6_Right/2.png", "Image/P7_D1_P1_High_T2_6_Right/3.png", "Image/P7_D1_P1_High_T2_6_Right/4.png", "Image/P7_D1_P1_High_T2_6_Right/5.png", "Image/P7_D1_P1_High_T2_6_Right/6.png", "Image/P7_D1_P1_High_T2_6_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 402.5869 seconds to 417.3843 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P7_Low_T3_8_Right", "images": ["Image/P26_D3_P7_Low_T3_8_Right/1.png", "Image/P26_D3_P7_Low_T3_8_Right/2.png", "Image/P26_D3_P7_Low_T3_8_Right/3.png", "Image/P26_D3_P7_Low_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that vase down there? Finish.\"\nThe frames roughly correspond to the speech interval from 942.0219 seconds to 953.1292 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P5_Medium_T2_26_Left", "images": ["Image/P15_D2_P5_Medium_T2_26_Left/1.png", "Image/P15_D2_P5_Medium_T2_26_Left/2.png", "Image/P15_D2_P5_Medium_T2_26_Left/3.png", "Image/P15_D2_P5_Medium_T2_26_Left/4.png", "Image/P15_D2_P5_Medium_T2_26_Left/5.png", "Image/P15_D2_P5_Medium_T2_26_Left/6.png", "Image/P15_D2_P5_Medium_T2_26_Left/7.png", "Image/P15_D2_P5_Medium_T2_26_Left/8.png", "Image/P15_D2_P5_Medium_T2_26_Left/9.png", "Image/P15_D2_P5_Medium_T2_26_Left/10.png", "Image/P15_D2_P5_Medium_T2_26_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2855.2314 seconds to 2871.2405 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P2_High_T3_21_Left", "images": ["Image/P21_D1_P2_High_T3_21_Left/1.png", "Image/P21_D1_P2_High_T3_21_Left/2.png", "Image/P21_D1_P2_High_T3_21_Left/3.png", "Image/P21_D1_P2_High_T3_21_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 971.5151 seconds to 986.4792 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P2_Medium_T1_17_Right", "images": ["Image/P13_D1_P2_Medium_T1_17_Right/1.png", "Image/P13_D1_P2_Medium_T1_17_Right/2.png", "Image/P13_D1_P2_Medium_T1_17_Right/3.png", "Image/P13_D1_P2_Medium_T1_17_Right/4.png", "Image/P13_D1_P2_Medium_T1_17_Right/5.png", "Image/P13_D1_P2_Medium_T1_17_Right/6.png", "Image/P13_D1_P2_Medium_T1_17_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the right round cup down to here\"\nThe frames roughly correspond to the speech interval from 656.9565 seconds to 669.1218 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P3_High_T3_16_Right", "images": ["Image/P25_D2_P3_High_T3_16_Right/1.png", "Image/P25_D2_P3_High_T3_16_Right/2.png", "Image/P25_D2_P3_High_T3_16_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 756.2682 seconds to 764.759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P5_Medium_T1_17_Left", "images": ["Image/P5_D1_P5_Medium_T1_17_Left/1.png", "Image/P5_D1_P5_Medium_T1_17_Left/2.png", "Image/P5_D1_P5_Medium_T1_17_Left/3.png", "Image/P5_D1_P5_Medium_T1_17_Left/4.png", "Image/P5_D1_P5_Medium_T1_17_Left/5.png", "Image/P5_D1_P5_Medium_T1_17_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup from here to here. Finished.\"\nThe frames roughly correspond to the speech interval from 1539.7182 seconds to 1551.2395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P3_Low_T2_4_Mid", "images": ["Image/P18_D2_P3_Low_T2_4_Mid/1.png", "Image/P18_D2_P3_Low_T2_4_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 193.4902 seconds to 203.12150000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D3_P5_Medium_T1_18_Left", "images": ["Image/P28_D3_P5_Medium_T1_18_Left/1.png", "Image/P28_D3_P5_Medium_T1_18_Left/2.png", "Image/P28_D3_P5_Medium_T1_18_Left/3.png", "Image/P28_D3_P5_Medium_T1_18_Left/4.png", "Image/P28_D3_P5_Medium_T1_18_Left/5.png", "Image/P28_D3_P5_Medium_T1_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 356.581 seconds to 367.9537 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P5_High_T2_7_Left", "images": ["Image/P11_D2_P5_High_T2_7_Left/1.png", "Image/P11_D2_P5_High_T2_7_Left/2.png", "Image/P11_D2_P5_High_T2_7_Left/3.png", "Image/P11_D2_P5_High_T2_7_Left/4.png", "Image/P11_D2_P5_High_T2_7_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Pour the water in a green glass\"\nThe frames roughly correspond to the speech interval from 786.5188 seconds to 798.0447 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P7_High_T1_26_Mid", "images": ["Image/P8_D2_P7_High_T1_26_Mid/1.png", "Image/P8_D2_P7_High_T1_26_Mid/2.png", "Image/P8_D2_P7_High_T1_26_Mid/3.png", "Image/P8_D2_P7_High_T1_26_Mid/4.png", "Image/P8_D2_P7_High_T1_26_Mid/5.png", "Image/P8_D2_P7_High_T1_26_Mid/6.png", "Image/P8_D2_P7_High_T1_26_Mid/7.png", "Image/P8_D2_P7_High_T1_26_Mid/8.png", "Image/P8_D2_P7_High_T1_26_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you move this white cup on this shelf on the top\"\nThe frames roughly correspond to the speech interval from 2491.2716 seconds to 2506.0768000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P6_High_T2_3_Mid", "images": ["Image/P25_D1_P6_High_T2_3_Mid/1.png", "Image/P25_D1_P6_High_T2_3_Mid/2.png", "Image/P25_D1_P6_High_T2_3_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 548.707 seconds to 557.2369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P3_High_T1_5_Right", "images": ["Image/P26_D2_P3_High_T1_5_Right/1.png", "Image/P26_D2_P3_High_T1_5_Right/2.png", "Image/P26_D2_P3_High_T1_5_Right/3.png", "Image/P26_D2_P3_High_T1_5_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to the bottom shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1275.8949699999998 seconds to 1286.54283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P6_Medium_T3_6_Right", "images": ["Image/P17_D1_P6_Medium_T3_6_Right/1.png", "Image/P17_D1_P6_Medium_T3_6_Right/2.png", "Image/P17_D1_P6_Medium_T3_6_Right/3.png", "Image/P17_D1_P6_Medium_T3_6_Right/4.png", "Image/P17_D1_P6_Medium_T3_6_Right/5.png", "Image/P17_D1_P6_Medium_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this plant right here? Finish\"\nThe frames roughly correspond to the speech interval from 2013.038 seconds to 2024.9538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P5_Low_T2_14_Left", "images": ["Image/P3_D1_P5_Low_T2_14_Left/1.png", "Image/P3_D1_P5_Low_T2_14_Left/2.png", "Image/P3_D1_P5_Low_T2_14_Left/3.png", "Image/P3_D1_P5_Low_T2_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1206.30299 seconds to 1215.39415 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P5_Low_T3_26_Left", "images": ["Image/P12_D3_P5_Low_T3_26_Left/1.png", "Image/P12_D3_P5_Low_T3_26_Left/2.png", "Image/P12_D3_P5_Low_T3_26_Left/3.png", "Image/P12_D3_P5_Low_T3_26_Left/4.png", "Image/P12_D3_P5_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 2751.31929 seconds to 2761.6768899999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P3_Medium_T2_27_Mid", "images": ["Image/P23_D1_P3_Medium_T2_27_Mid/1.png", "Image/P23_D1_P3_Medium_T2_27_Mid/2.png", "Image/P23_D1_P3_Medium_T2_27_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start pour water into the middle one\"\nThe frames roughly correspond to the speech interval from 1751.5652 seconds to 1761.3092 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P5_High_T3_18_Right", "images": ["Image/P11_D3_P5_High_T3_18_Right/1.png", "Image/P11_D3_P5_High_T3_18_Right/2.png", "Image/P11_D3_P5_High_T3_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the right glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1725.5826 seconds to 1734.0333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D3_P8_Low_T3_9_Mid", "images": ["Image/P8_D3_P8_Low_T3_9_Mid/1.png", "Image/P8_D3_P8_Low_T3_9_Mid/2.png", "Image/P8_D3_P8_Low_T3_9_Mid/3.png", "Image/P8_D3_P8_Low_T3_9_Mid/4.png", "Image/P8_D3_P8_Low_T3_9_Mid/5.png", "Image/P8_D3_P8_Low_T3_9_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hey\"\nThe frames roughly correspond to the speech interval from 1764.2412 seconds to 1776.0989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P4_High_T3_22_Left", "images": ["Image/P27_D1_P4_High_T3_22_Left/1.png", "Image/P27_D1_P4_High_T3_22_Left/2.png", "Image/P27_D1_P4_High_T3_22_Left/3.png", "Image/P27_D1_P4_High_T3_22_Left/4.png", "Image/P27_D1_P4_High_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1072.3155000000002 seconds to 1082.4066 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P2_Medium_T1_5_Mid", "images": ["Image/P20_D1_P2_Medium_T1_5_Mid/1.png", "Image/P20_D1_P2_Medium_T1_5_Mid/2.png", "Image/P20_D1_P2_Medium_T1_5_Mid/3.png", "Image/P20_D1_P2_Medium_T1_5_Mid/4.png", "Image/P20_D1_P2_Medium_T1_5_Mid/5.png", "Image/P20_D1_P2_Medium_T1_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move this middle plant into the basket\"\nThe frames roughly correspond to the speech interval from 515.2867 seconds to 528.8861 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P5_High_T1_2_Right", "images": ["Image/P7_D1_P5_High_T1_2_Right/1.png", "Image/P7_D1_P5_High_T1_2_Right/2.png", "Image/P7_D1_P5_High_T1_2_Right/3.png", "Image/P7_D1_P5_High_T1_2_Right/4.png", "Image/P7_D1_P5_High_T1_2_Right/5.png", "Image/P7_D1_P5_High_T1_2_Right/6.png", "Image/P7_D1_P5_High_T1_2_Right/7.png", "Image/P7_D1_P5_High_T1_2_Right/8.png", "Image/P7_D1_P5_High_T1_2_Right/9.png", "Image/P7_D1_P5_High_T1_2_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Before they move the green bottle from in front of me to my right hand side. Finish.\"\nThe frames roughly correspond to the speech interval from 736.6094 seconds to 753.4793 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P1_Low_T3_4_Right", "images": ["Image/P5_D1_P1_Low_T3_4_Right/1.png", "Image/P5_D1_P1_Low_T3_4_Right/2.png", "Image/P5_D1_P1_Low_T3_4_Right/3.png", "Image/P5_D1_P1_Low_T3_4_Right/4.png", "Image/P5_D1_P1_Low_T3_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant on my left-hand side. Finished.\"\nThe frames roughly correspond to the speech interval from 1666.6966 seconds to 1681.2388 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P5_Medium_T3_27_Right", "images": ["Image/P24_D3_P5_Medium_T3_27_Right/1.png", "Image/P24_D3_P5_Medium_T3_27_Right/2.png", "Image/P24_D3_P5_Medium_T3_27_Right/3.png", "Image/P24_D3_P5_Medium_T3_27_Right/4.png", "Image/P24_D3_P5_Medium_T3_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1889.8835 seconds to 1899.4731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P3_High_T2_18_Right", "images": ["Image/P23_D3_P3_High_T2_18_Right/1.png", "Image/P23_D3_P3_High_T2_18_Right/2.png", "Image/P23_D3_P3_High_T2_18_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 816.4014 seconds to 825.4206 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P9_Medium_T2_24_Right", "images": ["Image/P9_D1_P9_Medium_T2_24_Right/1.png", "Image/P9_D1_P9_Medium_T2_24_Right/2.png", "Image/P9_D1_P9_Medium_T2_24_Right/3.png", "Image/P9_D1_P9_Medium_T2_24_Right/4.png", "Image/P9_D1_P9_Medium_T2_24_Right/5.png", "Image/P9_D1_P9_Medium_T2_24_Right/6.png", "Image/P9_D1_P9_Medium_T2_24_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1607.3930999999998 seconds to 1620.4863 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P3_Low_T1_16_Left", "images": ["Image/P1_D3_P3_Low_T1_16_Left/1.png", "Image/P1_D3_P3_Low_T1_16_Left/2.png", "Image/P1_D3_P3_Low_T1_16_Left/3.png", "Image/P1_D3_P3_Low_T1_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1537.5704999999998 seconds to 1547.7331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P8_High_T3_22_Mid", "images": ["Image/P1_D1_P8_High_T3_22_Mid/1.png", "Image/P1_D1_P8_High_T3_22_Mid/2.png", "Image/P1_D1_P8_High_T3_22_Mid/3.png", "Image/P1_D1_P8_High_T3_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 411.7569 seconds to 420.5963 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P5_Low_T2_21_Right", "images": ["Image/P11_D1_P5_Low_T2_21_Right/1.png", "Image/P11_D1_P5_Low_T2_21_Right/2.png", "Image/P11_D1_P5_Low_T2_21_Right/3.png", "Image/P11_D1_P5_Low_T2_21_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 2851.8758 seconds to 2861.7578 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P5_High_T3_25_Right", "images": ["Image/P20_D1_P5_High_T3_25_Right/1.png", "Image/P20_D1_P5_High_T3_25_Right/2.png", "Image/P20_D1_P5_High_T3_25_Right/3.png", "Image/P20_D1_P5_High_T3_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the white cup on the table. Finish.\"\nThe frames roughly correspond to the speech interval from 313.7267 seconds to 324.3477 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P8_Low_T1_25_Left", "images": ["Image/P25_D3_P8_Low_T1_25_Left/1.png", "Image/P25_D3_P8_Low_T1_25_Left/2.png", "Image/P25_D3_P8_Low_T1_25_Left/3.png", "Image/P25_D3_P8_Low_T1_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2102.5713 seconds to 2111.2156999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P7_Low_T3_16_Left", "images": ["Image/P24_D3_P7_Low_T3_16_Left/1.png", "Image/P24_D3_P7_Low_T3_16_Left/2.png", "Image/P24_D3_P7_Low_T3_16_Left/3.png", "Image/P24_D3_P7_Low_T3_16_Left/4.png", "Image/P24_D3_P7_Low_T3_16_Left/5.png", "Image/P24_D3_P7_Low_T3_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1776.1568 seconds to 1787.2643 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P3_Low_T1_27_Left", "images": ["Image/P15_D1_P3_Low_T1_27_Left/1.png", "Image/P15_D1_P3_Low_T1_27_Left/2.png", "Image/P15_D1_P3_Low_T1_27_Left/3.png", "Image/P15_D1_P3_Low_T1_27_Left/4.png", "Image/P15_D1_P3_Low_T1_27_Left/5.png", "Image/P15_D1_P3_Low_T1_27_Left/6.png", "Image/P15_D1_P3_Low_T1_27_Left/7.png", "Image/P15_D1_P3_Low_T1_27_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white cup from the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 1612.82647 seconds to 1626.67707 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P4_Low_T3_22_Left", "images": ["Image/P24_D1_P4_Low_T3_22_Left/1.png", "Image/P24_D1_P4_Low_T3_22_Left/2.png", "Image/P24_D1_P4_Low_T3_22_Left/3.png", "Image/P24_D1_P4_Low_T3_22_Left/4.png", "Image/P24_D1_P4_Low_T3_22_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star\"\nThe frames roughly correspond to the speech interval from 1234.4172 seconds to 1244.7869999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P4_High_T1_4_Right", "images": ["Image/P7_D1_P4_High_T1_4_Right/1.png", "Image/P7_D1_P4_High_T1_4_Right/2.png", "Image/P7_D1_P4_High_T1_4_Right/3.png", "Image/P7_D1_P4_High_T1_4_Right/4.png", "Image/P7_D1_P4_High_T1_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Freddy\"\nThe frames roughly correspond to the speech interval from 465.673 seconds to 487.8038 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P8_High_T3_1_Mid", "images": ["Image/P15_D2_P8_High_T3_1_Mid/1.png", "Image/P15_D2_P8_High_T3_1_Mid/2.png", "Image/P15_D2_P8_High_T3_1_Mid/3.png", "Image/P15_D2_P8_High_T3_1_Mid/4.png", "Image/P15_D2_P8_High_T3_1_Mid/5.png", "Image/P15_D2_P8_High_T3_1_Mid/6.png", "Image/P15_D2_P8_High_T3_1_Mid/7.png", "Image/P15_D2_P8_High_T3_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the green container in the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3125.7242 seconds to 3139.0297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P7_Medium_T3_27_Left", "images": ["Image/P19_D1_P7_Medium_T3_27_Left/1.png", "Image/P19_D1_P7_Medium_T3_27_Left/2.png", "Image/P19_D1_P7_Medium_T3_27_Left/3.png", "Image/P19_D1_P7_Medium_T3_27_Left/4.png", "Image/P19_D1_P7_Medium_T3_27_Left/5.png", "Image/P19_D1_P7_Medium_T3_27_Left/6.png", "Image/P19_D1_P7_Medium_T3_27_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this glass here. Finish.\"\nThe frames roughly correspond to the speech interval from 950.1841 seconds to 964.2841 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D3_P7_Low_T2_27_Right", "images": ["Image/P16_D3_P7_Low_T2_27_Right/1.png", "Image/P16_D3_P7_Low_T2_27_Right/2.png", "Image/P16_D3_P7_Low_T2_27_Right/3.png", "Image/P16_D3_P7_Low_T2_27_Right/4.png", "Image/P16_D3_P7_Low_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you pour water into that cup? Finish\"\nThe frames roughly correspond to the speech interval from 111.1577 seconds to 121.4021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P6_Low_T3_25_Mid", "images": ["Image/P4_D3_P6_Low_T3_25_Mid/1.png", "Image/P4_D3_P6_Low_T3_25_Mid/2.png", "Image/P4_D3_P6_Low_T3_25_Mid/3.png", "Image/P4_D3_P6_Low_T3_25_Mid/4.png", "Image/P4_D3_P6_Low_T3_25_Mid/5.png", "Image/P4_D3_P6_Low_T3_25_Mid/6.png", "Image/P4_D3_P6_Low_T3_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the white bottle in the middle of that shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 442.1481999999999 seconds to 456.0259999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P5_High_T3_9_Right", "images": ["Image/P24_D1_P5_High_T3_9_Right/1.png", "Image/P24_D1_P5_High_T3_9_Right/2.png", "Image/P24_D1_P5_High_T3_9_Right/3.png", "Image/P24_D1_P5_High_T3_9_Right/4.png", "Image/P24_D1_P5_High_T3_9_Right/5.png", "Image/P24_D1_P5_High_T3_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1049.1012999999998 seconds to 1060.849 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P2_Medium_T3_8_Mid", "images": ["Image/P19_D1_P2_Medium_T3_8_Mid/1.png", "Image/P19_D1_P2_Medium_T3_8_Mid/2.png", "Image/P19_D1_P2_Medium_T3_8_Mid/3.png", "Image/P19_D1_P2_Medium_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this glass. Finish.\"\nThe frames roughly correspond to the speech interval from 582.6706 seconds to 597.2869000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P1_High_T1_24_Mid", "images": ["Image/P27_D1_P1_High_T1_24_Mid/1.png", "Image/P27_D1_P1_High_T1_24_Mid/2.png", "Image/P27_D1_P1_High_T1_24_Mid/3.png", "Image/P27_D1_P1_High_T1_24_Mid/4.png", "Image/P27_D1_P1_High_T1_24_Mid/5.png", "Image/P27_D1_P1_High_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant into the center of that basket? Finish.\"\nThe frames roughly correspond to the speech interval from 1054.4147 seconds to 1066.2333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P8_High_T2_23_Left", "images": ["Image/P18_D2_P8_High_T2_23_Left/1.png", "Image/P18_D2_P8_High_T2_23_Left/2.png", "Image/P18_D2_P8_High_T2_23_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 318.89390000000003 seconds to 327.4705 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P7_Low_T1_11_Right", "images": ["Image/P11_D1_P7_Low_T1_11_Right/1.png", "Image/P11_D1_P7_Low_T1_11_Right/2.png", "Image/P11_D1_P7_Low_T1_11_Right/3.png", "Image/P11_D1_P7_Low_T1_11_Right/4.png", "Image/P11_D1_P7_Low_T1_11_Right/5.png", "Image/P11_D1_P7_Low_T1_11_Right/6.png", "Image/P11_D1_P7_Low_T1_11_Right/7.png", "Image/P11_D1_P7_Low_T1_11_Right/8.png", "Image/P11_D1_P7_Low_T1_11_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the red bottle in the basket to the top of the shelf on the left side. Finish.\"\nThe frames roughly correspond to the speech interval from 2911.3677999999995 seconds to 2925.7745999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P2_High_T2_18_Left", "images": ["Image/P5_D1_P2_High_T2_18_Left/1.png", "Image/P5_D1_P2_High_T2_18_Left/2.png", "Image/P5_D1_P2_High_T2_18_Left/3.png", "Image/P5_D1_P2_High_T2_18_Left/4.png", "Image/P5_D1_P2_High_T2_18_Left/5.png", "Image/P5_D1_P2_High_T2_18_Left/6.png", "Image/P5_D1_P2_High_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill water into the cup which is in red color. Finished.\"\nThe frames roughly correspond to the speech interval from 1560.8355 seconds to 1573.3333 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P3_Medium_T2_10_Right", "images": ["Image/P5_D1_P3_Medium_T2_10_Right/1.png", "Image/P5_D1_P3_Medium_T2_10_Right/2.png", "Image/P5_D1_P3_Medium_T2_10_Right/3.png", "Image/P5_D1_P3_Medium_T2_10_Right/4.png", "Image/P5_D1_P3_Medium_T2_10_Right/5.png", "Image/P5_D1_P3_Medium_T2_10_Right/6.png", "Image/P5_D1_P3_Medium_T2_10_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle that is farthest to me and closest to you. Finished.\"\nThe frames roughly correspond to the speech interval from 2068.2163 seconds to 2081.8831 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P7_High_T3_7_Mid", "images": ["Image/P14_D3_P7_High_T3_7_Mid/1.png", "Image/P14_D3_P7_High_T3_7_Mid/2.png", "Image/P14_D3_P7_High_T3_7_Mid/3.png", "Image/P14_D3_P7_High_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that green mug up. Finish.\"\nThe frames roughly correspond to the speech interval from 1042.99827 seconds to 1052.92531 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P1_Medium_T3_23_Right", "images": ["Image/P11_D1_P1_Medium_T3_23_Right/1.png", "Image/P11_D1_P1_Medium_T3_23_Right/2.png", "Image/P11_D1_P1_Medium_T3_23_Right/3.png", "Image/P11_D1_P1_Medium_T3_23_Right/4.png", "Image/P11_D1_P1_Medium_T3_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the first plant from the right side. Finish.\"\nThe frames roughly correspond to the speech interval from 2932.5274 seconds to 2943.7879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P9_Low_T2_3_Right", "images": ["Image/P24_D2_P9_Low_T2_3_Right/1.png", "Image/P24_D2_P9_Low_T2_3_Right/2.png", "Image/P24_D2_P9_Low_T2_3_Right/3.png", "Image/P24_D2_P9_Low_T2_3_Right/4.png", "Image/P24_D2_P9_Low_T2_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 661.7446 seconds to 672.8374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P4_High_T2_3_Mid", "images": ["Image/P29_D1_P4_High_T2_3_Mid/1.png", "Image/P29_D1_P4_High_T2_3_Mid/2.png", "Image/P29_D1_P4_High_T2_3_Mid/3.png", "Image/P29_D1_P4_High_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2652.7634 seconds to 2662.9255999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P5_Medium_T2_5_Left", "images": ["Image/P28_D2_P5_Medium_T2_5_Left/1.png", "Image/P28_D2_P5_Medium_T2_5_Left/2.png", "Image/P28_D2_P5_Medium_T2_5_Left/3.png", "Image/P28_D2_P5_Medium_T2_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1181.3426 seconds to 1190.6006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P3_Low_T1_18_Left", "images": ["Image/P5_D1_P3_Low_T1_18_Left/1.png", "Image/P5_D1_P3_Low_T1_18_Left/2.png", "Image/P5_D1_P3_Low_T1_18_Left/3.png", "Image/P5_D1_P3_Low_T1_18_Left/4.png", "Image/P5_D1_P3_Low_T1_18_Left/5.png", "Image/P5_D1_P3_Low_T1_18_Left/6.png", "Image/P5_D1_P3_Low_T1_18_Left/7.png", "Image/P5_D1_P3_Low_T1_18_Left/8.png", "Image/P5_D1_P3_Low_T1_18_Left/9.png", "Image/P5_D1_P3_Low_T1_18_Left/10.png", "Image/P5_D1_P3_Low_T1_18_Left/11.png", "Image/P5_D1_P3_Low_T1_18_Left/12.png", "Image/P5_D1_P3_Low_T1_18_Left/13.png", "Image/P5_D1_P3_Low_T1_18_Left/14.png", "Image/P5_D1_P3_Low_T1_18_Left/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup on the shelf into the bucket which is the closest to you. Finished.\"\nThe frames roughly correspond to the speech interval from 1813.8019 seconds to 1834.554 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P9_High_T3_5_Left", "images": ["Image/P9_D3_P9_High_T3_5_Left/1.png", "Image/P9_D3_P9_High_T3_5_Left/2.png", "Image/P9_D3_P9_High_T3_5_Left/3.png", "Image/P9_D3_P9_High_T3_5_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you help me to clean the plants? Finish\"\nThe frames roughly correspond to the speech interval from 195.14874 seconds to 205.63158 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P1_High_T3_16_Left", "images": ["Image/P4_D3_P1_High_T3_16_Left/1.png", "Image/P4_D3_P1_High_T3_16_Left/2.png", "Image/P4_D3_P1_High_T3_16_Left/3.png", "Image/P4_D3_P1_High_T3_16_Left/4.png", "Image/P4_D3_P1_High_T3_16_Left/5.png", "Image/P4_D3_P1_High_T3_16_Left/6.png", "Image/P4_D3_P1_High_T3_16_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left red bottle on the shoulder? Finish.\"\nThe frames roughly correspond to the speech interval from 521.8384999999998 seconds to 537.3738999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P6_High_T3_8_Left", "images": ["Image/P2_D3_P6_High_T3_8_Left/1.png", "Image/P2_D3_P6_High_T3_8_Left/2.png", "Image/P2_D3_P6_High_T3_8_Left/3.png", "Image/P2_D3_P6_High_T3_8_Left/4.png", "Image/P2_D3_P6_High_T3_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that green mug for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1880.5966000000003 seconds to 1894.3483 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P4_Medium_T1_5_Right", "images": ["Image/P18_D2_P4_Medium_T1_5_Right/1.png", "Image/P18_D2_P4_Medium_T1_5_Right/2.png", "Image/P18_D2_P4_Medium_T1_5_Right/3.png", "Image/P18_D2_P4_Medium_T1_5_Right/4.png", "Image/P18_D2_P4_Medium_T1_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that first plant to the table in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 256.71400000000006 seconds to 267.7674 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P5_Low_T1_13_Left", "images": ["Image/P26_D3_P5_Low_T1_13_Left/1.png", "Image/P26_D3_P5_Low_T1_13_Left/2.png", "Image/P26_D3_P5_Low_T1_13_Left/3.png", "Image/P26_D3_P5_Low_T1_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please move that plant to that shelf there? Finish.\"\nThe frames roughly correspond to the speech interval from 1035.2275 seconds to 1044.8752 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P3_High_T2_25_Left", "images": ["Image/P26_D3_P3_High_T2_25_Left/1.png", "Image/P26_D3_P3_High_T2_25_Left/2.png", "Image/P26_D3_P3_High_T2_25_Left/3.png", "Image/P26_D3_P3_High_T2_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour water into that white vase over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1148.9951 seconds to 1160.7587 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P4_Medium_T2_12_Mid", "images": ["Image/P20_D1_P4_Medium_T2_12_Mid/1.png", "Image/P20_D1_P4_Medium_T2_12_Mid/2.png", "Image/P20_D1_P4_Medium_T2_12_Mid/3.png", "Image/P20_D1_P4_Medium_T2_12_Mid/4.png", "Image/P20_D1_P4_Medium_T2_12_Mid/5.png", "Image/P20_D1_P4_Medium_T2_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour some water into this thingy. Finish.\"\nThe frames roughly correspond to the speech interval from 388.47714 seconds to 401.33851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P9_Medium_T2_26_Left", "images": ["Image/P4_D2_P9_Medium_T2_26_Left/1.png", "Image/P4_D2_P9_Medium_T2_26_Left/2.png", "Image/P4_D2_P9_Medium_T2_26_Left/3.png", "Image/P4_D2_P9_Medium_T2_26_Left/4.png", "Image/P4_D2_P9_Medium_T2_26_Left/5.png", "Image/P4_D2_P9_Medium_T2_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in the left photo? Finish.\"\nThe frames roughly correspond to the speech interval from 1670.9563999999998 seconds to 1684.5116999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P6_Low_T1_25_Left", "images": ["Image/P19_D2_P6_Low_T1_25_Left/1.png", "Image/P19_D2_P6_Low_T1_25_Left/2.png", "Image/P19_D2_P6_Low_T1_25_Left/3.png", "Image/P19_D2_P6_Low_T1_25_Left/4.png", "Image/P19_D2_P6_Low_T1_25_Left/5.png", "Image/P19_D2_P6_Low_T1_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this glass over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1707.5376 seconds to 1722.0413 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D3_P3_High_T2_6_Mid", "images": ["Image/P3_D3_P3_High_T2_6_Mid/1.png", "Image/P3_D3_P3_High_T2_6_Mid/2.png", "Image/P3_D3_P3_High_T2_6_Mid/3.png", "Image/P3_D3_P3_High_T2_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1033.839 seconds to 1044.1951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P3_High_T3_2_Right", "images": ["Image/P10_D1_P3_High_T3_2_Right/1.png", "Image/P10_D1_P3_High_T3_2_Right/2.png", "Image/P10_D1_P3_High_T3_2_Right/3.png", "Image/P10_D1_P3_High_T3_2_Right/4.png", "Image/P10_D1_P3_High_T3_2_Right/5.png", "Image/P10_D1_P3_High_T3_2_Right/6.png", "Image/P10_D1_P3_High_T3_2_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2338.8032 seconds to 2350.8719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P9_Low_T3_17_Left", "images": ["Image/P12_D1_P9_Low_T3_17_Left/1.png", "Image/P12_D1_P9_Low_T3_17_Left/2.png", "Image/P12_D1_P9_Low_T3_17_Left/3.png", "Image/P12_D1_P9_Low_T3_17_Left/4.png", "Image/P12_D1_P9_Low_T3_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1437.4588 seconds to 1448.9562 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D2_P1_Low_T2_23_Mid", "images": ["Image/P26_D2_P1_Low_T2_23_Mid/1.png", "Image/P26_D2_P1_Low_T2_23_Mid/2.png", "Image/P26_D2_P1_Low_T2_23_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant over there? Finish\"\nThe frames roughly correspond to the speech interval from 1328.4132 seconds to 1338.6015 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P1_Medium_T1_12_Mid", "images": ["Image/P25_D1_P1_Medium_T1_12_Mid/1.png", "Image/P25_D1_P1_Medium_T1_12_Mid/2.png", "Image/P25_D1_P1_Medium_T1_12_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 513.5020999999999 seconds to 521.9232999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P9_Medium_T1_8_Right", "images": ["Image/P7_D3_P9_Medium_T1_8_Right/1.png", "Image/P7_D3_P9_Medium_T1_8_Right/2.png", "Image/P7_D3_P9_Medium_T1_8_Right/3.png", "Image/P7_D3_P9_Medium_T1_8_Right/4.png", "Image/P7_D3_P9_Medium_T1_8_Right/5.png", "Image/P7_D3_P9_Medium_T1_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1613.1081600000002 seconds to 1625.84541 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P3_Low_T2_4_Mid", "images": ["Image/P22_D3_P3_Low_T2_4_Mid/1.png", "Image/P22_D3_P3_Low_T2_4_Mid/2.png", "Image/P22_D3_P3_Low_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 405.1932000000001 seconds to 413.725 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P5_Low_T3_5_Mid", "images": ["Image/P13_D3_P5_Low_T3_5_Mid/1.png", "Image/P13_D3_P5_Low_T3_5_Mid/2.png", "Image/P13_D3_P5_Low_T3_5_Mid/3.png", "Image/P13_D3_P5_Low_T3_5_Mid/4.png", "Image/P13_D3_P5_Low_T3_5_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean the plant over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1556.4739 seconds to 1567.246 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P4_Low_T1_19_Left", "images": ["Image/P23_D1_P4_Low_T1_19_Left/1.png", "Image/P23_D1_P4_Low_T1_19_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1542.58574 seconds to 1551.18735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P1_Medium_T3_25_Right", "images": ["Image/P2_D2_P1_Medium_T3_25_Right/1.png", "Image/P2_D2_P1_Medium_T3_25_Right/2.png", "Image/P2_D2_P1_Medium_T3_25_Right/3.png", "Image/P2_D2_P1_Medium_T3_25_Right/4.png", "Image/P2_D2_P1_Medium_T3_25_Right/5.png", "Image/P2_D2_P1_Medium_T3_25_Right/6.png", "Image/P2_D2_P1_Medium_T3_25_Right/7.png", "Image/P2_D2_P1_Medium_T3_25_Right/8.png", "Image/P2_D2_P1_Medium_T3_25_Right/9.png", "Image/P2_D2_P1_Medium_T3_25_Right/10.png", "Image/P2_D2_P1_Medium_T3_25_Right/11.png", "Image/P2_D2_P1_Medium_T3_25_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please clean the mug on the very right side on top of the shelf? Yeah\"\nThe frames roughly correspond to the speech interval from 2866.6178999999997 seconds to 2884.1695999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P9_Medium_T2_17_Right", "images": ["Image/P6_D2_P9_Medium_T2_17_Right/1.png", "Image/P6_D2_P9_Medium_T2_17_Right/2.png", "Image/P6_D2_P9_Medium_T2_17_Right/3.png", "Image/P6_D2_P9_Medium_T2_17_Right/4.png", "Image/P6_D2_P9_Medium_T2_17_Right/5.png", "Image/P6_D2_P9_Medium_T2_17_Right/6.png", "Image/P6_D2_P9_Medium_T2_17_Right/7.png", "Image/P6_D2_P9_Medium_T2_17_Right/8.png", "Image/P6_D2_P9_Medium_T2_17_Right/9.png", "Image/P6_D2_P9_Medium_T2_17_Right/10.png", "Image/P6_D2_P9_Medium_T2_17_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 611.3224 seconds to 629.9078999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P7_High_T2_14_Mid", "images": ["Image/P3_D2_P7_High_T2_14_Mid/1.png", "Image/P3_D2_P7_High_T2_14_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2251.7192 seconds to 2262.0181 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P6_Medium_T1_1_Right", "images": ["Image/P11_D3_P6_Medium_T1_1_Right/1.png", "Image/P11_D3_P6_Medium_T1_1_Right/2.png", "Image/P11_D3_P6_Medium_T1_1_Right/3.png", "Image/P11_D3_P6_Medium_T1_1_Right/4.png", "Image/P11_D3_P6_Medium_T1_1_Right/5.png", "Image/P11_D3_P6_Medium_T1_1_Right/6.png", "Image/P11_D3_P6_Medium_T1_1_Right/7.png", "Image/P11_D3_P6_Medium_T1_1_Right/8.png", "Image/P11_D3_P6_Medium_T1_1_Right/9.png", "Image/P11_D3_P6_Medium_T1_1_Right/10.png", "Image/P11_D3_P6_Medium_T1_1_Right/11.png", "Image/P11_D3_P6_Medium_T1_1_Right/12.png", "Image/P11_D3_P6_Medium_T1_1_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first green bottle from the right side to the desk on my left side. Finish.\"\nThe frames roughly correspond to the speech interval from 1698.2999 seconds to 1717.0491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P5_Low_T2_26_Left", "images": ["Image/P26_D2_P5_Low_T2_26_Left/1.png", "Image/P26_D2_P5_Low_T2_26_Left/2.png", "Image/P26_D2_P5_Low_T2_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water in there? Finish.\"\nThe frames roughly correspond to the speech interval from 1611.0565 seconds to 1621.357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D1_P6_High_T2_13_Left", "images": ["Image/P15_D1_P6_High_T2_13_Left/1.png", "Image/P15_D1_P6_High_T2_13_Left/2.png", "Image/P15_D1_P6_High_T2_13_Left/3.png", "Image/P15_D1_P6_High_T2_13_Left/4.png", "Image/P15_D1_P6_High_T2_13_Left/5.png", "Image/P15_D1_P6_High_T2_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1696.21987 seconds to 1708.56547 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P7_High_T3_2_Mid", "images": ["Image/P9_D1_P7_High_T3_2_Mid/1.png", "Image/P9_D1_P7_High_T3_2_Mid/2.png", "Image/P9_D1_P7_High_T3_2_Mid/3.png", "Image/P9_D1_P7_High_T3_2_Mid/4.png", "Image/P9_D1_P7_High_T3_2_Mid/5.png", "Image/P9_D1_P7_High_T3_2_Mid/6.png", "Image/P9_D1_P7_High_T3_2_Mid/7.png", "Image/P9_D1_P7_High_T3_2_Mid/8.png", "Image/P9_D1_P7_High_T3_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the green bottle in this box? Finish.\"\nThe frames roughly correspond to the speech interval from 1433.6582 seconds to 1449.2238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P4_High_T3_8_Left", "images": ["Image/P18_D2_P4_High_T3_8_Left/1.png", "Image/P18_D2_P4_High_T3_8_Left/2.png", "Image/P18_D2_P4_High_T3_8_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 408.6634 seconds to 417.9351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D1_P8_Low_T3_7_Left", "images": ["Image/P17_D1_P8_Low_T3_7_Left/1.png", "Image/P17_D1_P8_Low_T3_7_Left/2.png", "Image/P17_D1_P8_Low_T3_7_Left/3.png", "Image/P17_D1_P8_Low_T3_7_Left/4.png", "Image/P17_D1_P8_Low_T3_7_Left/5.png", "Image/P17_D1_P8_Low_T3_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1924.7586 seconds to 1936.4664 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P2_Medium_T1_26_Mid", "images": ["Image/P28_D3_P2_Medium_T1_26_Mid/1.png", "Image/P28_D3_P2_Medium_T1_26_Mid/2.png", "Image/P28_D3_P2_Medium_T1_26_Mid/3.png", "Image/P28_D3_P2_Medium_T1_26_Mid/4.png", "Image/P28_D3_P2_Medium_T1_26_Mid/5.png", "Image/P28_D3_P2_Medium_T1_26_Mid/6.png", "Image/P28_D3_P2_Medium_T1_26_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move the middle white vase to the floor underneath the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 335.40930000000003 seconds to 348.3549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P2_Medium_T2_6_Mid", "images": ["Image/P24_D3_P2_Medium_T2_6_Mid/1.png", "Image/P24_D3_P2_Medium_T2_6_Mid/2.png", "Image/P24_D3_P2_Medium_T2_6_Mid/3.png", "Image/P24_D3_P2_Medium_T2_6_Mid/4.png", "Image/P24_D3_P2_Medium_T2_6_Mid/5.png", "Image/P24_D3_P2_Medium_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water into this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1631.3623 seconds to 1643.1931 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P6_High_T1_17_Left", "images": ["Image/P22_D2_P6_High_T1_17_Left/1.png", "Image/P22_D2_P6_High_T1_17_Left/2.png", "Image/P22_D2_P6_High_T1_17_Left/3.png", "Image/P22_D2_P6_High_T1_17_Left/4.png", "Image/P22_D2_P6_High_T1_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move the cup to the opposite shelf. Finish\"\nThe frames roughly correspond to the speech interval from 950.9632 seconds to 962.8088 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P5_Low_T2_4_Left", "images": ["Image/P4_D2_P5_Low_T2_4_Left/1.png", "Image/P4_D2_P5_Low_T2_4_Left/2.png", "Image/P4_D2_P5_Low_T2_4_Left/3.png", "Image/P4_D2_P5_Low_T2_4_Left/4.png", "Image/P4_D2_P5_Low_T2_4_Left/5.png", "Image/P4_D2_P5_Low_T2_4_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1946.7397 seconds to 1960.989 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P5_Low_T2_13_Left", "images": ["Image/P29_D3_P5_Low_T2_13_Left/1.png", "Image/P29_D3_P5_Low_T2_13_Left/2.png", "Image/P29_D3_P5_Low_T2_13_Left/3.png", "Image/P29_D3_P5_Low_T2_13_Left/4.png", "Image/P29_D3_P5_Low_T2_13_Left/5.png", "Image/P29_D3_P5_Low_T2_13_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 1275.58388 seconds to 1288.4999299999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P6_Medium_T3_4_Left", "images": ["Image/P26_D2_P6_Medium_T3_4_Left/1.png", "Image/P26_D2_P6_Medium_T3_4_Left/2.png", "Image/P26_D2_P6_Medium_T3_4_Left/3.png", "Image/P26_D2_P6_Medium_T3_4_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean the plant on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1662.5172 seconds to 1672.3057 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D3_P9_Medium_T3_13_Right", "images": ["Image/P25_D3_P9_Medium_T3_13_Right/1.png", "Image/P25_D3_P9_Medium_T3_13_Right/2.png", "Image/P25_D3_P9_Medium_T3_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1853.145 seconds to 1861.4829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P5_Low_T1_25_Mid", "images": ["Image/P30_D3_P5_Low_T1_25_Mid/1.png", "Image/P30_D3_P5_Low_T1_25_Mid/2.png", "Image/P30_D3_P5_Low_T1_25_Mid/3.png", "Image/P30_D3_P5_Low_T1_25_Mid/4.png", "Image/P30_D3_P5_Low_T1_25_Mid/5.png", "Image/P30_D3_P5_Low_T1_25_Mid/6.png", "Image/P30_D3_P5_Low_T1_25_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1225.6736 seconds to 1240.8324 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P5_High_T3_5_Mid", "images": ["Image/P7_D2_P5_High_T3_5_Mid/1.png", "Image/P7_D2_P5_High_T3_5_Mid/2.png", "Image/P7_D2_P5_High_T3_5_Mid/3.png", "Image/P7_D2_P5_High_T3_5_Mid/4.png", "Image/P7_D2_P5_High_T3_5_Mid/5.png", "Image/P7_D2_P5_High_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1185.7301000000002 seconds to 1198.9820000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P4_Low_T2_10_Right", "images": ["Image/P14_D2_P4_Low_T2_10_Right/1.png", "Image/P14_D2_P4_Low_T2_10_Right/2.png", "Image/P14_D2_P4_Low_T2_10_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1839.2990999999995 seconds to 1847.2969 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P9_Medium_T3_10_Mid", "images": ["Image/P8_D1_P9_Medium_T3_10_Mid/1.png", "Image/P8_D1_P9_Medium_T3_10_Mid/2.png", "Image/P8_D1_P9_Medium_T3_10_Mid/3.png", "Image/P8_D1_P9_Medium_T3_10_Mid/4.png", "Image/P8_D1_P9_Medium_T3_10_Mid/5.png", "Image/P8_D1_P9_Medium_T3_10_Mid/6.png", "Image/P8_D1_P9_Medium_T3_10_Mid/7.png", "Image/P8_D1_P9_Medium_T3_10_Mid/8.png", "Image/P8_D1_P9_Medium_T3_10_Mid/9.png", "Image/P8_D1_P9_Medium_T3_10_Mid/10.png", "Image/P8_D1_P9_Medium_T3_10_Mid/11.png", "Image/P8_D1_P9_Medium_T3_10_Mid/12.png", "Image/P8_D1_P9_Medium_T3_10_Mid/13.png", "Image/P8_D1_P9_Medium_T3_10_Mid/14.png", "Image/P8_D1_P9_Medium_T3_10_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. So you can see that there are three brown glass or three brown cap bottles on the right side. So could you clean the middle one? Finish.\"\nThe frames roughly correspond to the speech interval from 581.2963 seconds to 603.3865 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P6_Low_T1_23_Mid", "images": ["Image/P18_D3_P6_Low_T1_23_Mid/1.png", "Image/P18_D3_P6_Low_T1_23_Mid/2.png", "Image/P18_D3_P6_Low_T1_23_Mid/3.png", "Image/P18_D3_P6_Low_T1_23_Mid/4.png", "Image/P18_D3_P6_Low_T1_23_Mid/5.png", "Image/P18_D3_P6_Low_T1_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1247.7401 seconds to 1259.2098 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P2_Medium_T3_1_Right", "images": ["Image/P17_D1_P2_Medium_T3_1_Right/1.png", "Image/P17_D1_P2_Medium_T3_1_Right/2.png", "Image/P17_D1_P2_Medium_T3_1_Right/3.png", "Image/P17_D1_P2_Medium_T3_1_Right/4.png", "Image/P17_D1_P2_Medium_T3_1_Right/5.png", "Image/P17_D1_P2_Medium_T3_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this green vase right here? Finish\"\nThe frames roughly correspond to the speech interval from 1746.3331 seconds to 1758.0395 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P4_High_T3_13_Right", "images": ["Image/P28_D1_P4_High_T3_13_Right/1.png", "Image/P28_D1_P4_High_T3_13_Right/2.png", "Image/P28_D1_P4_High_T3_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1475.352 seconds to 1484.0932 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D1_P6_Medium_T1_23_Mid", "images": ["Image/P16_D1_P6_Medium_T1_23_Mid/1.png", "Image/P16_D1_P6_Medium_T1_23_Mid/2.png", "Image/P16_D1_P6_Medium_T1_23_Mid/3.png", "Image/P16_D1_P6_Medium_T1_23_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you please move this plant over here? Finish\"\nThe frames roughly correspond to the speech interval from 1609.2248 seconds to 1618.953 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P2_High_T3_19_Right", "images": ["Image/P28_D2_P2_High_T3_19_Right/1.png", "Image/P28_D2_P2_High_T3_19_Right/2.png", "Image/P28_D2_P2_High_T3_19_Right/3.png", "Image/P28_D2_P2_High_T3_19_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that white base. Finish.\"\nThe frames roughly correspond to the speech interval from 1046.5433 seconds to 1055.5915 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D2_P3_Medium_T3_8_Mid", "images": ["Image/P11_D2_P3_Medium_T3_8_Mid/1.png", "Image/P11_D2_P3_Medium_T3_8_Mid/2.png", "Image/P11_D2_P3_Medium_T3_8_Mid/3.png", "Image/P11_D2_P3_Medium_T3_8_Mid/4.png", "Image/P11_D2_P3_Medium_T3_8_Mid/5.png", "Image/P11_D2_P3_Medium_T3_8_Mid/6.png", "Image/P11_D2_P3_Medium_T3_8_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the middle of the green glass. Please finish.\"\nThe frames roughly correspond to the speech interval from 537.6598 seconds to 551.1152 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P1_Low_T2_18_Right", "images": ["Image/P24_D3_P1_Low_T2_18_Right/1.png", "Image/P24_D3_P1_Low_T2_18_Right/2.png", "Image/P24_D3_P1_Low_T2_18_Right/3.png", "Image/P24_D3_P1_Low_T2_18_Right/4.png", "Image/P24_D3_P1_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1831.658 seconds to 1841.7769 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P2_Low_T3_15_Left", "images": ["Image/P24_D1_P2_Low_T3_15_Left/1.png", "Image/P24_D1_P2_Low_T3_15_Left/2.png", "Image/P24_D1_P2_Low_T3_15_Left/3.png", "Image/P24_D1_P2_Low_T3_15_Left/4.png", "Image/P24_D1_P2_Low_T3_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 1215.9738 seconds to 1225.6613 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P4_Medium_T2_27_Left", "images": ["Image/P25_D2_P4_Medium_T2_27_Left/1.png", "Image/P25_D2_P4_Medium_T2_27_Left/2.png", "Image/P25_D2_P4_Medium_T2_27_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 837.3917 seconds to 846.2592 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P6_Low_T2_14_Mid", "images": ["Image/P17_D2_P6_Low_T2_14_Mid/1.png", "Image/P17_D2_P6_Low_T2_14_Mid/2.png", "Image/P17_D2_P6_Low_T2_14_Mid/3.png", "Image/P17_D2_P6_Low_T2_14_Mid/4.png", "Image/P17_D2_P6_Low_T2_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that plant right there? Finish.\"\nThe frames roughly correspond to the speech interval from 332.66769999999997 seconds to 345.4166 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D3_P4_Medium_T3_9_Left", "images": ["Image/P20_D3_P4_Medium_T3_9_Left/1.png", "Image/P20_D3_P4_Medium_T3_9_Left/2.png", "Image/P20_D3_P4_Medium_T3_9_Left/3.png", "Image/P20_D3_P4_Medium_T3_9_Left/4.png", "Image/P20_D3_P4_Medium_T3_9_Left/5.png", "Image/P20_D3_P4_Medium_T3_9_Left/6.png", "Image/P20_D3_P4_Medium_T3_9_Left/7.png", "Image/P20_D3_P4_Medium_T3_9_Left/8.png", "Image/P20_D3_P4_Medium_T3_9_Left/9.png", "Image/P20_D3_P4_Medium_T3_9_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the... Cup on the left over there. Finished.\"\nThe frames roughly correspond to the speech interval from 1561.4425 seconds to 1578.4662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P1_Medium_T2_24_Right", "images": ["Image/P3_D1_P1_Medium_T2_24_Right/1.png", "Image/P3_D1_P1_Medium_T2_24_Right/2.png", "Image/P3_D1_P1_Medium_T2_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1525.2009 seconds to 1535.322 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P3_Low_T2_20_Mid", "images": ["Image/P12_D2_P3_Low_T2_20_Mid/1.png", "Image/P12_D2_P3_Low_T2_20_Mid/2.png", "Image/P12_D2_P3_Low_T2_20_Mid/3.png", "Image/P12_D2_P3_Low_T2_20_Mid/4.png", "Image/P12_D2_P3_Low_T2_20_Mid/5.png", "Image/P12_D2_P3_Low_T2_20_Mid/6.png", "Image/P12_D2_P3_Low_T2_20_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the white vase that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 477.24570000000006 seconds to 489.7732999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P7_Medium_T2_15_Right", "images": ["Image/P28_D2_P7_Medium_T2_15_Right/1.png", "Image/P28_D2_P7_Medium_T2_15_Right/2.png", "Image/P28_D2_P7_Medium_T2_15_Right/3.png", "Image/P28_D2_P7_Medium_T2_15_Right/4.png", "Image/P28_D2_P7_Medium_T2_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 994.8295 seconds to 1005.2834 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P5_Low_T2_15_Mid", "images": ["Image/P21_D3_P5_Low_T2_15_Mid/1.png", "Image/P21_D3_P5_Low_T2_15_Mid/2.png", "Image/P21_D3_P5_Low_T2_15_Mid/3.png", "Image/P21_D3_P5_Low_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 583.612 seconds to 594.6764000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D1_P1_Medium_T1_25_Mid", "images": ["Image/P8_D1_P1_Medium_T1_25_Mid/1.png", "Image/P8_D1_P1_Medium_T1_25_Mid/2.png", "Image/P8_D1_P1_Medium_T1_25_Mid/3.png", "Image/P8_D1_P1_Medium_T1_25_Mid/4.png", "Image/P8_D1_P1_Medium_T1_25_Mid/5.png", "Image/P8_D1_P1_Medium_T1_25_Mid/6.png", "Image/P8_D1_P1_Medium_T1_25_Mid/7.png", "Image/P8_D1_P1_Medium_T1_25_Mid/8.png", "Image/P8_D1_P1_Medium_T1_25_Mid/9.png", "Image/P8_D1_P1_Medium_T1_25_Mid/10.png", "Image/P8_D1_P1_Medium_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi\"\nThe frames roughly correspond to the speech interval from 321.7143999999999 seconds to 340.2104999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P5_High_T1_16_Right", "images": ["Image/P23_D2_P5_High_T1_16_Right/1.png", "Image/P23_D2_P5_High_T1_16_Right/2.png", "Image/P23_D2_P5_High_T1_16_Right/3.png", "Image/P23_D2_P5_High_T1_16_Right/4.png", "Image/P23_D2_P5_High_T1_16_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start for the red one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 427.5019 seconds to 439.5153 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P9_Low_T1_20_Right", "images": ["Image/P17_D3_P9_Low_T1_20_Right/1.png", "Image/P17_D3_P9_Low_T1_20_Right/2.png", "Image/P17_D3_P9_Low_T1_20_Right/3.png", "Image/P17_D3_P9_Low_T1_20_Right/4.png", "Image/P17_D3_P9_Low_T1_20_Right/5.png", "Image/P17_D3_P9_Low_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that white vase up to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 808.03516 seconds to 822.3167500000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P1_Medium_T3_8_Right", "images": ["Image/P3_D2_P1_Medium_T3_8_Right/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2570.4095 seconds to 2579.0369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P7_Low_T3_3_Right", "images": ["Image/P21_D1_P7_Low_T3_3_Right/1.png", "Image/P21_D1_P7_Low_T3_3_Right/2.png", "Image/P21_D1_P7_Low_T3_3_Right/3.png", "Image/P21_D1_P7_Low_T3_3_Right/4.png", "Image/P21_D1_P7_Low_T3_3_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1025.3438999999998 seconds to 1035.5318 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P4_Low_T3_3_Right", "images": ["Image/P14_D1_P4_Low_T3_3_Right/1.png", "Image/P14_D1_P4_Low_T3_3_Right/2.png", "Image/P14_D1_P4_Low_T3_3_Right/3.png", "Image/P14_D1_P4_Low_T3_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that bottle up. Finished.\"\nThe frames roughly correspond to the speech interval from 535.1519000000001 seconds to 544.297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P16_D2_P7_Low_T2_23_Right", "images": ["Image/P16_D2_P7_Low_T2_23_Right/1.png", "Image/P16_D2_P7_Low_T2_23_Right/2.png", "Image/P16_D2_P7_Low_T2_23_Right/3.png", "Image/P16_D2_P7_Low_T2_23_Right/4.png", "Image/P16_D2_P7_Low_T2_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 885.1592 seconds to 898.2859000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D3_P3_Low_T2_2_Mid", "images": ["Image/P19_D3_P3_Low_T2_2_Mid/1.png", "Image/P19_D3_P3_Low_T2_2_Mid/2.png", "Image/P19_D3_P3_Low_T2_2_Mid/3.png", "Image/P19_D3_P3_Low_T2_2_Mid/4.png", "Image/P19_D3_P3_Low_T2_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that container right there.\"\nThe frames roughly correspond to the speech interval from 2546.7805 seconds to 2561.2006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P1_Low_T3_1_Right", "images": ["Image/P11_D2_P1_Low_T3_1_Right/1.png", "Image/P11_D2_P1_Low_T3_1_Right/2.png", "Image/P11_D2_P1_Low_T3_1_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the glass finish.\"\nThe frames roughly correspond to the speech interval from 1012.4395 seconds to 1021.1315 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P2_High_T2_15_Left", "images": ["Image/P15_D2_P2_High_T2_15_Left/1.png", "Image/P15_D2_P2_High_T2_15_Left/2.png", "Image/P15_D2_P2_High_T2_15_Left/3.png", "Image/P15_D2_P2_High_T2_15_Left/4.png", "Image/P15_D2_P2_High_T2_15_Left/5.png", "Image/P15_D2_P2_High_T2_15_Left/6.png", "Image/P15_D2_P2_High_T2_15_Left/7.png", "Image/P15_D2_P2_High_T2_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please water the plant at the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2587.09254 seconds to 2601.0261299999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P6_Medium_T1_6_Right", "images": ["Image/P6_D1_P6_Medium_T1_6_Right/1.png", "Image/P6_D1_P6_Medium_T1_6_Right/2.png", "Image/P6_D1_P6_Medium_T1_6_Right/3.png", "Image/P6_D1_P6_Medium_T1_6_Right/4.png", "Image/P6_D1_P6_Medium_T1_6_Right/5.png", "Image/P6_D1_P6_Medium_T1_6_Right/6.png", "Image/P6_D1_P6_Medium_T1_6_Right/7.png", "Image/P6_D1_P6_Medium_T1_6_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 1945.2701 seconds to 1962.6673000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P7_Low_T1_9_Mid", "images": ["Image/P18_D1_P7_Low_T1_9_Mid/1.png", "Image/P18_D1_P7_Low_T1_9_Mid/2.png", "Image/P18_D1_P7_Low_T1_9_Mid/3.png", "Image/P18_D1_P7_Low_T1_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this green cup to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 790.6453 seconds to 800.7513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P1_Medium_T2_18_Left", "images": ["Image/P8_D3_P1_Medium_T2_18_Left/1.png", "Image/P8_D3_P1_Medium_T2_18_Left/2.png", "Image/P8_D3_P1_Medium_T2_18_Left/3.png", "Image/P8_D3_P1_Medium_T2_18_Left/4.png", "Image/P8_D3_P1_Medium_T2_18_Left/5.png", "Image/P8_D3_P1_Medium_T2_18_Left/6.png", "Image/P8_D3_P1_Medium_T2_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1730.0967 seconds to 1742.8868 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P1_Low_T1_5_Right", "images": ["Image/P17_D1_P1_Low_T1_5_Right/1.png", "Image/P17_D1_P1_Low_T1_5_Right/2.png", "Image/P17_D1_P1_Low_T1_5_Right/3.png", "Image/P17_D1_P1_Low_T1_5_Right/4.png", "Image/P17_D1_P1_Low_T1_5_Right/5.png", "Image/P17_D1_P1_Low_T1_5_Right/6.png", "Image/P17_D1_P1_Low_T1_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green potted plant to this basket right here? Finish.\"\nThe frames roughly correspond to the speech interval from 1901.0811 seconds to 1914.2068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P9_Medium_T2_13_Right", "images": ["Image/P13_D3_P9_Medium_T2_13_Right/1.png", "Image/P13_D3_P9_Medium_T2_13_Right/2.png", "Image/P13_D3_P9_Medium_T2_13_Right/3.png", "Image/P13_D3_P9_Medium_T2_13_Right/4.png", "Image/P13_D3_P9_Medium_T2_13_Right/5.png", "Image/P13_D3_P9_Medium_T2_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1575.2776 seconds to 1586.5668999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P6_Low_T1_17_Mid", "images": ["Image/P9_D1_P6_Low_T1_17_Mid/1.png", "Image/P9_D1_P6_Low_T1_17_Mid/2.png", "Image/P9_D1_P6_Low_T1_17_Mid/3.png", "Image/P9_D1_P6_Low_T1_17_Mid/4.png", "Image/P9_D1_P6_Low_T1_17_Mid/5.png", "Image/P9_D1_P6_Low_T1_17_Mid/6.png", "Image/P9_D1_P6_Low_T1_17_Mid/7.png", "Image/P9_D1_P6_Low_T1_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to move this cup from the right to the left? Finish.\"\nThe frames roughly correspond to the speech interval from 1276.492 seconds to 1298.9309999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P8_Medium_T2_22_Left", "images": ["Image/P29_D2_P8_Medium_T2_22_Left/1.png", "Image/P29_D2_P8_Medium_T2_22_Left/2.png", "Image/P29_D2_P8_Medium_T2_22_Left/3.png", "Image/P29_D2_P8_Medium_T2_22_Left/4.png", "Image/P29_D2_P8_Medium_T2_22_Left/5.png", "Image/P29_D2_P8_Medium_T2_22_Left/6.png", "Image/P29_D2_P8_Medium_T2_22_Left/7.png", "Image/P29_D2_P8_Medium_T2_22_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 444.4964 seconds to 458.2076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D1_P7_Medium_T2_14_Mid", "images": ["Image/P20_D1_P7_Medium_T2_14_Mid/1.png", "Image/P20_D1_P7_Medium_T2_14_Mid/2.png", "Image/P20_D1_P7_Medium_T2_14_Mid/3.png", "Image/P20_D1_P7_Medium_T2_14_Mid/4.png", "Image/P20_D1_P7_Medium_T2_14_Mid/5.png", "Image/P20_D1_P7_Medium_T2_14_Mid/6.png", "Image/P20_D1_P7_Medium_T2_14_Mid/7.png", "Image/P20_D1_P7_Medium_T2_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you water the plant on the floor\"\nThe frames roughly correspond to the speech interval from 44.52692999999999 seconds to 60.06147 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P9_High_T1_20_Mid", "images": ["Image/P11_D3_P9_High_T1_20_Mid/1.png", "Image/P11_D3_P9_High_T1_20_Mid/2.png", "Image/P11_D3_P9_High_T1_20_Mid/3.png", "Image/P11_D3_P9_High_T1_20_Mid/4.png", "Image/P11_D3_P9_High_T1_20_Mid/5.png", "Image/P11_D3_P9_High_T1_20_Mid/6.png", "Image/P11_D3_P9_High_T1_20_Mid/7.png", "Image/P11_D3_P9_High_T1_20_Mid/8.png", "Image/P11_D3_P9_High_T1_20_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the white bottle from the bottom of the shelf to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1554.507 seconds to 1569.8331 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P3_Low_T2_1_Mid", "images": ["Image/P14_D3_P3_Low_T2_1_Mid/1.png", "Image/P14_D3_P3_Low_T2_1_Mid/2.png", "Image/P14_D3_P3_Low_T2_1_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1131.78656 seconds to 1140.31326 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P5_Medium_T2_22_Left", "images": ["Image/P7_D1_P5_Medium_T2_22_Left/1.png", "Image/P7_D1_P5_Medium_T2_22_Left/2.png", "Image/P7_D1_P5_Medium_T2_22_Left/3.png", "Image/P7_D1_P5_Medium_T2_22_Left/4.png", "Image/P7_D1_P5_Medium_T2_22_Left/5.png", "Image/P7_D1_P5_Medium_T2_22_Left/6.png", "Image/P7_D1_P5_Medium_T2_22_Left/7.png", "Image/P7_D1_P5_Medium_T2_22_Left/8.png", "Image/P7_D1_P5_Medium_T2_22_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 377.2671 seconds to 394.402 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P1_High_T2_13_Mid", "images": ["Image/P29_D1_P1_High_T2_13_Mid/1.png", "Image/P29_D1_P1_High_T2_13_Mid/2.png", "Image/P29_D1_P1_High_T2_13_Mid/3.png", "Image/P29_D1_P1_High_T2_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2936.2140999999997 seconds to 2946.0548999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P15_D3_P6_Medium_T1_5_Left", "images": ["Image/P15_D3_P6_Medium_T1_5_Left/1.png", "Image/P15_D3_P6_Medium_T1_5_Left/2.png", "Image/P15_D3_P6_Medium_T1_5_Left/3.png", "Image/P15_D3_P6_Medium_T1_5_Left/4.png", "Image/P15_D3_P6_Medium_T1_5_Left/5.png", "Image/P15_D3_P6_Medium_T1_5_Left/6.png", "Image/P15_D3_P6_Medium_T1_5_Left/7.png", "Image/P15_D3_P6_Medium_T1_5_Left/8.png", "Image/P15_D3_P6_Medium_T1_5_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant at the left side into the table disk. Finish.\"\nThe frames roughly correspond to the speech interval from 348.2687 seconds to 363.0371 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P6_High_T3_26_Left", "images": ["Image/P15_D2_P6_High_T3_26_Left/1.png", "Image/P15_D2_P6_High_T3_26_Left/2.png", "Image/P15_D2_P6_High_T3_26_Left/3.png", "Image/P15_D2_P6_High_T3_26_Left/4.png", "Image/P15_D2_P6_High_T3_26_Left/5.png", "Image/P15_D2_P6_High_T3_26_Left/6.png", "Image/P15_D2_P6_High_T3_26_Left/7.png", "Image/P15_D2_P6_High_T3_26_Left/8.png", "Image/P15_D2_P6_High_T3_26_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2615.61634 seconds to 2631.3155 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P5_Medium_T2_6_Left", "images": ["Image/P3_D3_P5_Medium_T2_6_Left/1.png", "Image/P3_D3_P5_Medium_T2_6_Left/2.png", "Image/P3_D3_P5_Medium_T2_6_Left/3.png", "Image/P3_D3_P5_Medium_T2_6_Left/4.png", "Image/P3_D3_P5_Medium_T2_6_Left/5.png", "Image/P3_D3_P5_Medium_T2_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the first item? Finish.\"\nThe frames roughly correspond to the speech interval from 588.40762 seconds to 602.8289599999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P4_High_T2_2_Mid", "images": ["Image/P1_D1_P4_High_T2_2_Mid/1.png", "Image/P1_D1_P4_High_T2_2_Mid/2.png", "Image/P1_D1_P4_High_T2_2_Mid/3.png", "Image/P1_D1_P4_High_T2_2_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 489.1194 seconds to 498.5736 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P6_Low_T3_11_Right", "images": ["Image/P2_D3_P6_Low_T3_11_Right/1.png", "Image/P2_D3_P6_Low_T3_11_Right/2.png", "Image/P2_D3_P6_Low_T3_11_Right/3.png", "Image/P2_D3_P6_Low_T3_11_Right/4.png", "Image/P2_D3_P6_Low_T3_11_Right/5.png", "Image/P2_D3_P6_Low_T3_11_Right/6.png", "Image/P2_D3_P6_Low_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that round bottle? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1754.7187000000004 seconds to 1772.9251000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P3_High_T3_11_Mid", "images": ["Image/P3_D2_P3_High_T3_11_Mid/1.png", "Image/P3_D2_P3_High_T3_11_Mid/2.png", "Image/P3_D2_P3_High_T3_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you move the item to this table?\"\nThe frames roughly correspond to the speech interval from 2506.2251 seconds to 2515.4589 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P3_Medium_T2_27_Mid", "images": ["Image/P1_D1_P3_Medium_T2_27_Mid/1.png", "Image/P1_D1_P3_Medium_T2_27_Mid/2.png", "Image/P1_D1_P3_Medium_T2_27_Mid/3.png", "Image/P1_D1_P3_Medium_T2_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water to this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 291.34479999999996 seconds to 301.0902 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P8_High_T3_7_Left", "images": ["Image/P6_D1_P8_High_T3_7_Left/1.png", "Image/P6_D1_P8_High_T3_7_Left/2.png", "Image/P6_D1_P8_High_T3_7_Left/3.png", "Image/P6_D1_P8_High_T3_7_Left/4.png", "Image/P6_D1_P8_High_T3_7_Left/5.png", "Image/P6_D1_P8_High_T3_7_Left/6.png", "Image/P6_D1_P8_High_T3_7_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2016.485 seconds to 2029.9423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P7_Low_T3_13_Mid", "images": ["Image/P14_D1_P7_Low_T3_13_Mid/1.png", "Image/P14_D1_P7_Low_T3_13_Mid/2.png", "Image/P14_D1_P7_Low_T3_13_Mid/3.png", "Image/P14_D1_P7_Low_T3_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 291.3438 seconds to 301.0473 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P1_High_T3_24_Right", "images": ["Image/P26_D1_P1_High_T3_24_Right/1.png", "Image/P26_D1_P1_High_T3_24_Right/2.png", "Image/P26_D1_P1_High_T3_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 427.9784 seconds to 446.8819 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P5_Medium_T1_3_Right", "images": ["Image/P24_D1_P5_Medium_T1_3_Right/1.png", "Image/P24_D1_P5_Medium_T1_3_Right/2.png", "Image/P24_D1_P5_Medium_T1_3_Right/3.png", "Image/P24_D1_P5_Medium_T1_3_Right/4.png", "Image/P24_D1_P5_Medium_T1_3_Right/5.png", "Image/P24_D1_P5_Medium_T1_3_Right/6.png", "Image/P24_D1_P5_Medium_T1_3_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you please move this bottle to this area\"\nThe frames roughly correspond to the speech interval from 1252.5407 seconds to 1265.1927 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P8_Medium_T1_1_Left", "images": ["Image/P28_D2_P8_Medium_T1_1_Left/1.png", "Image/P28_D2_P8_Medium_T1_1_Left/2.png", "Image/P28_D2_P8_Medium_T1_1_Left/3.png", "Image/P28_D2_P8_Medium_T1_1_Left/4.png", "Image/P28_D2_P8_Medium_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that green vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 1112.872 seconds to 1123.0748 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P3_Medium_T2_2_Left", "images": ["Image/P1_D2_P3_Medium_T2_2_Left/1.png", "Image/P1_D2_P3_Medium_T2_2_Left/2.png", "Image/P1_D2_P3_Medium_T2_2_Left/3.png", "Image/P1_D2_P3_Medium_T2_2_Left/4.png", "Image/P1_D2_P3_Medium_T2_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help put some water to the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 826.1307 seconds to 837.3358000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P2_High_T3_20_Left", "images": ["Image/P2_D3_P2_High_T3_20_Left/1.png", "Image/P2_D3_P2_High_T3_20_Left/2.png", "Image/P2_D3_P2_High_T3_20_Left/3.png", "Image/P2_D3_P2_High_T3_20_Left/4.png", "Image/P2_D3_P2_High_T3_20_Left/5.png", "Image/P2_D3_P2_High_T3_20_Left/6.png", "Image/P2_D3_P2_High_T3_20_Left/7.png", "Image/P2_D3_P2_High_T3_20_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 1986.0552 seconds to 1999.5675 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D2_P2_Low_T3_26_Mid", "images": ["Image/P24_D2_P2_Low_T3_26_Mid/1.png", "Image/P24_D2_P2_Low_T3_26_Mid/2.png", "Image/P24_D2_P2_Low_T3_26_Mid/3.png", "Image/P24_D2_P2_Low_T3_26_Mid/4.png", "Image/P24_D2_P2_Low_T3_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 189.1776 seconds to 199.659 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D1_P3_High_T1_8_Left", "images": ["Image/P4_D1_P3_High_T1_8_Left/1.png", "Image/P4_D1_P3_High_T1_8_Left/2.png", "Image/P4_D1_P3_High_T1_8_Left/3.png", "Image/P4_D1_P3_High_T1_8_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move it down to the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 2283.2614999999996 seconds to 2296.5227999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P4_High_T1_12_Left", "images": ["Image/P19_D2_P4_High_T1_12_Left/1.png", "Image/P19_D2_P4_High_T1_12_Left/2.png", "Image/P19_D2_P4_High_T1_12_Left/3.png", "Image/P19_D2_P4_High_T1_12_Left/4.png", "Image/P19_D2_P4_High_T1_12_Left/5.png", "Image/P19_D2_P4_High_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this space here. Finish.\"\nThe frames roughly correspond to the speech interval from 1757.8729999999998 seconds to 1770.971 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P7_Medium_T1_26_Left", "images": ["Image/P22_D1_P7_Medium_T1_26_Left/1.png", "Image/P22_D1_P7_Medium_T1_26_Left/2.png", "Image/P22_D1_P7_Medium_T1_26_Left/3.png", "Image/P22_D1_P7_Medium_T1_26_Left/4.png", "Image/P22_D1_P7_Medium_T1_26_Left/5.png", "Image/P22_D1_P7_Medium_T1_26_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that cup on the left up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1390.6698 seconds to 1401.4983 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P1_High_T1_14_Mid", "images": ["Image/P16_D3_P1_High_T1_14_Mid/1.png", "Image/P16_D3_P1_High_T1_14_Mid/2.png", "Image/P16_D3_P1_High_T1_14_Mid/3.png", "Image/P16_D3_P1_High_T1_14_Mid/4.png", "Image/P16_D3_P1_High_T1_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase down to the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 353.76789999999994 seconds to 363.6082 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P4_Medium_T1_24_Left", "images": ["Image/P11_D1_P4_Medium_T1_24_Left/1.png", "Image/P11_D1_P4_Medium_T1_24_Left/2.png", "Image/P11_D1_P4_Medium_T1_24_Left/3.png", "Image/P11_D1_P4_Medium_T1_24_Left/4.png", "Image/P11_D1_P4_Medium_T1_24_Left/5.png", "Image/P11_D1_P4_Medium_T1_24_Left/6.png", "Image/P11_D1_P4_Medium_T1_24_Left/7.png", "Image/P11_D1_P4_Medium_T1_24_Left/8.png", "Image/P11_D1_P4_Medium_T1_24_Left/9.png", "Image/P11_D1_P4_Medium_T1_24_Left/10.png", "Image/P11_D1_P4_Medium_T1_24_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the first plant from the left side to the middle of the shelf close to the wall. Finish.\"\nThe frames roughly correspond to the speech interval from 2620.0401999999995 seconds to 2636.784799999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P8_High_T1_6_Left", "images": ["Image/P9_D3_P8_High_T1_6_Left/1.png", "Image/P9_D3_P8_High_T1_6_Left/2.png", "Image/P9_D3_P8_High_T1_6_Left/3.png", "Image/P9_D3_P8_High_T1_6_Left/4.png", "Image/P9_D3_P8_High_T1_6_Left/5.png", "Image/P9_D3_P8_High_T1_6_Left/6.png", "Image/P9_D3_P8_High_T1_6_Left/7.png", "Image/P9_D3_P8_High_T1_6_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the floor to the top layer? Finish.\"\nThe frames roughly correspond to the speech interval from 712.7528 seconds to 727.4305 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P4_Low_T1_27_Mid", "images": ["Image/P14_D3_P4_Low_T1_27_Mid/1.png", "Image/P14_D3_P4_Low_T1_27_Mid/2.png", "Image/P14_D3_P4_Low_T1_27_Mid/3.png", "Image/P14_D3_P4_Low_T1_27_Mid/4.png", "Image/P14_D3_P4_Low_T1_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that over to that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1062.60482 seconds to 1072.2134 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P5_High_T3_1_Mid", "images": ["Image/P24_D2_P5_High_T3_1_Mid/1.png", "Image/P24_D2_P5_High_T3_1_Mid/2.png", "Image/P24_D2_P5_High_T3_1_Mid/3.png", "Image/P24_D2_P5_High_T3_1_Mid/4.png", "Image/P24_D2_P5_High_T3_1_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 725.6359 seconds to 736.1168 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P7_Medium_T2_22_Left", "images": ["Image/P13_D1_P7_Medium_T2_22_Left/1.png", "Image/P13_D1_P7_Medium_T2_22_Left/2.png", "Image/P13_D1_P7_Medium_T2_22_Left/3.png", "Image/P13_D1_P7_Medium_T2_22_Left/4.png", "Image/P13_D1_P7_Medium_T2_22_Left/5.png", "Image/P13_D1_P7_Medium_T2_22_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the far left plant down there\"\nThe frames roughly correspond to the speech interval from 243.605 seconds to 255.3527 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P5_Medium_T1_23_Right", "images": ["Image/P8_D3_P5_Medium_T1_23_Right/1.png", "Image/P8_D3_P5_Medium_T1_23_Right/2.png", "Image/P8_D3_P5_Medium_T1_23_Right/3.png", "Image/P8_D3_P5_Medium_T1_23_Right/4.png", "Image/P8_D3_P5_Medium_T1_23_Right/5.png", "Image/P8_D3_P5_Medium_T1_23_Right/6.png", "Image/P8_D3_P5_Medium_T1_23_Right/7.png", "Image/P8_D3_P5_Medium_T1_23_Right/8.png", "Image/P8_D3_P5_Medium_T1_23_Right/9.png", "Image/P8_D3_P5_Medium_T1_23_Right/10.png", "Image/P8_D3_P5_Medium_T1_23_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star. Hi\"\nThe frames roughly correspond to the speech interval from 1614.9356 seconds to 1631.2086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P5_High_T3_8_Mid", "images": ["Image/P16_D3_P5_High_T3_8_Mid/1.png", "Image/P16_D3_P5_High_T3_8_Mid/2.png", "Image/P16_D3_P5_High_T3_8_Mid/3.png", "Image/P16_D3_P5_High_T3_8_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that mug? Finish.\"\nThe frames roughly correspond to the speech interval from 215.0002 seconds to 224.563 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P3_Medium_T1_5_Right", "images": ["Image/P17_D3_P3_Medium_T1_5_Right/1.png", "Image/P17_D3_P3_Medium_T1_5_Right/2.png", "Image/P17_D3_P3_Medium_T1_5_Right/3.png", "Image/P17_D3_P3_Medium_T1_5_Right/4.png", "Image/P17_D3_P3_Medium_T1_5_Right/5.png", "Image/P17_D3_P3_Medium_T1_5_Right/6.png", "Image/P17_D3_P3_Medium_T1_5_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant on the top shelf on the right down to the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 904.3615 seconds to 917.8076 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P4_Medium_T2_24_Right", "images": ["Image/P2_D3_P4_Medium_T2_24_Right/1.png", "Image/P2_D3_P4_Medium_T2_24_Right/2.png", "Image/P2_D3_P4_Medium_T2_24_Right/3.png", "Image/P2_D3_P4_Medium_T2_24_Right/4.png", "Image/P2_D3_P4_Medium_T2_24_Right/5.png", "Image/P2_D3_P4_Medium_T2_24_Right/6.png", "Image/P2_D3_P4_Medium_T2_24_Right/7.png", "Image/P2_D3_P4_Medium_T2_24_Right/8.png", "Image/P2_D3_P4_Medium_T2_24_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please water the plant on the very right side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1651.0149700000002 seconds to 1678.5356400000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P1_Medium_T1_5_Left", "images": ["Image/P29_D1_P1_Medium_T1_5_Left/1.png", "Image/P29_D1_P1_Medium_T1_5_Left/2.png", "Image/P29_D1_P1_Medium_T1_5_Left/3.png", "Image/P29_D1_P1_Medium_T1_5_Left/4.png", "Image/P29_D1_P1_Medium_T1_5_Left/5.png", "Image/P29_D1_P1_Medium_T1_5_Left/6.png", "Image/P29_D1_P1_Medium_T1_5_Left/7.png", "Image/P29_D1_P1_Medium_T1_5_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Hey Friday\"\nThe frames roughly correspond to the speech interval from 2608.9302 seconds to 2624.8399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P9_High_T1_4_Left", "images": ["Image/P25_D2_P9_High_T1_4_Left/1.png", "Image/P25_D2_P9_High_T1_4_Left/2.png", "Image/P25_D2_P9_High_T1_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 922.1795 seconds to 930.9484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P2_High_T3_13_Mid", "images": ["Image/P19_D2_P2_High_T3_13_Mid/1.png", "Image/P19_D2_P2_High_T3_13_Mid/2.png", "Image/P19_D2_P2_High_T3_13_Mid/3.png", "Image/P19_D2_P2_High_T3_13_Mid/4.png", "Image/P19_D2_P2_High_T3_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this flowerpot here. Finish.\"\nThe frames roughly correspond to the speech interval from 1866.3782 seconds to 1879.6846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P1_High_T2_19_Right", "images": ["Image/P12_D1_P1_High_T2_19_Right/1.png", "Image/P12_D1_P1_High_T2_19_Right/2.png", "Image/P12_D1_P1_High_T2_19_Right/3.png", "Image/P12_D1_P1_High_T2_19_Right/4.png", "Image/P12_D1_P1_High_T2_19_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1392.9576 seconds to 1407.1694 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P3_High_T2_18_Mid", "images": ["Image/P7_D2_P3_High_T2_18_Mid/1.png", "Image/P7_D2_P3_High_T2_18_Mid/2.png", "Image/P7_D2_P3_High_T2_18_Mid/3.png", "Image/P7_D2_P3_High_T2_18_Mid/4.png", "Image/P7_D2_P3_High_T2_18_Mid/5.png", "Image/P7_D2_P3_High_T2_18_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1347.0764 seconds to 1361.7756000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P2_High_T1_9_Mid", "images": ["Image/P7_D1_P2_High_T1_9_Mid/1.png", "Image/P7_D1_P2_High_T1_9_Mid/2.png", "Image/P7_D1_P2_High_T1_9_Mid/3.png", "Image/P7_D1_P2_High_T1_9_Mid/4.png", "Image/P7_D1_P2_High_T1_9_Mid/5.png", "Image/P7_D1_P2_High_T1_9_Mid/6.png", "Image/P7_D1_P2_High_T1_9_Mid/7.png", "Image/P7_D1_P2_High_T1_9_Mid/8.png", "Image/P7_D1_P2_High_T1_9_Mid/9.png", "Image/P7_D1_P2_High_T1_9_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Freddy\"\nThe frames roughly correspond to the speech interval from 91.40905 seconds to 112.59305 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D1_P7_High_T1_6_Left", "images": ["Image/P12_D1_P7_High_T1_6_Left/1.png", "Image/P12_D1_P7_High_T1_6_Left/2.png", "Image/P12_D1_P7_High_T1_6_Left/3.png", "Image/P12_D1_P7_High_T1_6_Left/4.png", "Image/P12_D1_P7_High_T1_6_Left/5.png", "Image/P12_D1_P7_High_T1_6_Left/6.png", "Image/P12_D1_P7_High_T1_6_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1620.0049 seconds to 1634.7867 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P3_Low_T2_10_Mid", "images": ["Image/P25_D1_P3_Low_T2_10_Mid/1.png", "Image/P25_D1_P3_Low_T2_10_Mid/2.png", "Image/P25_D1_P3_Low_T2_10_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 213.9472 seconds to 222.4647 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P8_Medium_T1_8_Right", "images": ["Image/P30_D1_P8_Medium_T1_8_Right/1.png", "Image/P30_D1_P8_Medium_T1_8_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Move that cup up here. Finish.\"\nThe frames roughly correspond to the speech interval from 976.6251 seconds to 984.9782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P8_Low_T1_17_Right", "images": ["Image/P30_D3_P8_Low_T1_17_Right/1.png", "Image/P30_D3_P8_Low_T1_17_Right/2.png", "Image/P30_D3_P8_Low_T1_17_Right/3.png", "Image/P30_D3_P8_Low_T1_17_Right/4.png", "Image/P30_D3_P8_Low_T1_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1493.9944 seconds to 1505.4081 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P4_Medium_T2_1_Mid", "images": ["Image/P14_D2_P4_Medium_T2_1_Mid/1.png", "Image/P14_D2_P4_Medium_T2_1_Mid/2.png", "Image/P14_D2_P4_Medium_T2_1_Mid/3.png", "Image/P14_D2_P4_Medium_T2_1_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Water that middle bottle\"\nThe frames roughly correspond to the speech interval from 1492.0574399999998 seconds to 1501.1748999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P6_High_T3_1_Mid", "images": ["Image/P6_D2_P6_High_T3_1_Mid/1.png", "Image/P6_D2_P6_High_T3_1_Mid/2.png", "Image/P6_D2_P6_High_T3_1_Mid/3.png", "Image/P6_D2_P6_High_T3_1_Mid/4.png", "Image/P6_D2_P6_High_T3_1_Mid/5.png", "Image/P6_D2_P6_High_T3_1_Mid/6.png", "Image/P6_D2_P6_High_T3_1_Mid/7.png", "Image/P6_D2_P6_High_T3_1_Mid/8.png", "Image/P6_D2_P6_High_T3_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 719.5655999999999 seconds to 735.6868999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P3_High_T2_2_Mid", "images": ["Image/P28_D3_P3_High_T2_2_Mid/1.png", "Image/P28_D3_P3_High_T2_2_Mid/2.png", "Image/P28_D3_P3_High_T2_2_Mid/3.png", "Image/P28_D3_P3_High_T2_2_Mid/4.png", "Image/P28_D3_P3_High_T2_2_Mid/5.png", "Image/P28_D3_P3_High_T2_2_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 294.9708 seconds to 305.94120000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D1_P5_Low_T2_13_Right", "images": ["Image/P13_D1_P5_Low_T2_13_Right/1.png", "Image/P13_D1_P5_Low_T2_13_Right/2.png", "Image/P13_D1_P5_Low_T2_13_Right/3.png", "Image/P13_D1_P5_Low_T2_13_Right/4.png", "Image/P13_D1_P5_Low_T2_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you please pour water into the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 335.7407 seconds to 346.8484 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P3_High_T1_6_Mid", "images": ["Image/P18_D2_P3_High_T1_6_Mid/1.png", "Image/P18_D2_P3_High_T1_6_Mid/2.png", "Image/P18_D2_P3_High_T1_6_Mid/3.png", "Image/P18_D2_P3_High_T1_6_Mid/4.png", "Image/P18_D2_P3_High_T1_6_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the green plant to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 80.80873000000003 seconds to 92.54184000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P6_Medium_T3_27_Left", "images": ["Image/P13_D1_P6_Medium_T3_27_Left/1.png", "Image/P13_D1_P6_Medium_T3_27_Left/2.png", "Image/P13_D1_P6_Medium_T3_27_Left/3.png", "Image/P13_D1_P6_Medium_T3_27_Left/4.png", "Image/P13_D1_P6_Medium_T3_27_Left/5.png", "Image/P13_D1_P6_Medium_T3_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday\"\nThe frames roughly correspond to the speech interval from 530.7873 seconds to 542.0313 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P1_High_T1_2_Left", "images": ["Image/P24_D1_P1_High_T1_2_Left/1.png", "Image/P24_D1_P1_High_T1_2_Left/2.png", "Image/P24_D1_P1_High_T1_2_Left/3.png", "Image/P24_D1_P1_High_T1_2_Left/4.png", "Image/P24_D1_P1_High_T1_2_Left/5.png", "Image/P24_D1_P1_High_T1_2_Left/6.png", "Image/P24_D1_P1_High_T1_2_Left/7.png", "Image/P24_D1_P1_High_T1_2_Left/8.png", "Image/P24_D1_P1_High_T1_2_Left/9.png", "Image/P24_D1_P1_High_T1_2_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you please move this bottle over there\"\nThe frames roughly correspond to the speech interval from 1097.0412 seconds to 1112.0328 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D2_P6_Low_T3_27_Right", "images": ["Image/P5_D2_P6_Low_T3_27_Right/1.png", "Image/P5_D2_P6_Low_T3_27_Right/2.png", "Image/P5_D2_P6_Low_T3_27_Right/3.png", "Image/P5_D2_P6_Low_T3_27_Right/4.png", "Image/P5_D2_P6_Low_T3_27_Right/5.png", "Image/P5_D2_P6_Low_T3_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup to the rest of you. Finish.\"\nThe frames roughly correspond to the speech interval from 757.8649 seconds to 771.1287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P7_Medium_T3_26_Right", "images": ["Image/P1_D1_P7_Medium_T3_26_Right/1.png", "Image/P1_D1_P7_Medium_T3_26_Right/2.png", "Image/P1_D1_P7_Medium_T3_26_Right/3.png", "Image/P1_D1_P7_Medium_T3_26_Right/4.png", "Image/P1_D1_P7_Medium_T3_26_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 521.4066 seconds to 531.3183 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P3_Low_T1_15_Right", "images": ["Image/P9_D3_P3_Low_T1_15_Right/1.png", "Image/P9_D3_P3_Low_T1_15_Right/2.png", "Image/P9_D3_P3_Low_T1_15_Right/3.png", "Image/P9_D3_P3_Low_T1_15_Right/4.png", "Image/P9_D3_P3_Low_T1_15_Right/5.png", "Image/P9_D3_P3_Low_T1_15_Right/6.png", "Image/P9_D3_P3_Low_T1_15_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the top to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 301.28290000000004 seconds to 315.7928 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P6_Low_T1_25_Left", "images": ["Image/P20_D2_P6_Low_T1_25_Left/1.png", "Image/P20_D2_P6_Low_T1_25_Left/2.png", "Image/P20_D2_P6_Low_T1_25_Left/3.png", "Image/P20_D2_P6_Low_T1_25_Left/4.png", "Image/P20_D2_P6_Low_T1_25_Left/5.png", "Image/P20_D2_P6_Low_T1_25_Left/6.png", "Image/P20_D2_P6_Low_T1_25_Left/7.png", "Image/P20_D2_P6_Low_T1_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move that white cup to the middle layer of that shelf over there. Finish\"\nThe frames roughly correspond to the speech interval from 3315.4317 seconds to 3330.0746000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P9_Medium_T1_25_Left", "images": ["Image/P10_D2_P9_Medium_T1_25_Left/1.png", "Image/P10_D2_P9_Medium_T1_25_Left/2.png", "Image/P10_D2_P9_Medium_T1_25_Left/3.png", "Image/P10_D2_P9_Medium_T1_25_Left/4.png", "Image/P10_D2_P9_Medium_T1_25_Left/5.png", "Image/P10_D2_P9_Medium_T1_25_Left/6.png", "Image/P10_D2_P9_Medium_T1_25_Left/7.png", "Image/P10_D2_P9_Medium_T1_25_Left/8.png", "Image/P10_D2_P9_Medium_T1_25_Left/9.png", "Image/P10_D2_P9_Medium_T1_25_Left/10.png", "Image/P10_D2_P9_Medium_T1_25_Left/11.png", "Image/P10_D2_P9_Medium_T1_25_Left/12.png", "Image/P10_D2_P9_Medium_T1_25_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1741.5097800000003 seconds to 1759.36798 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D1_P3_Low_T3_26_Left", "images": ["Image/P10_D1_P3_Low_T3_26_Left/1.png", "Image/P10_D1_P3_Low_T3_26_Left/2.png", "Image/P10_D1_P3_Low_T3_26_Left/3.png", "Image/P10_D1_P3_Low_T3_26_Left/4.png", "Image/P10_D1_P3_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2591.9653 seconds to 2602.5297 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P7_High_T1_21_Right", "images": ["Image/P26_D3_P7_High_T1_21_Right/1.png", "Image/P26_D3_P7_High_T1_21_Right/2.png", "Image/P26_D3_P7_High_T1_21_Right/3.png", "Image/P26_D3_P7_High_T1_21_Right/4.png", "Image/P26_D3_P7_High_T1_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the white face to the top shelf of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 650.1909400000001 seconds to 661.5206900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P5_Low_T3_4_Right", "images": ["Image/P27_D1_P5_Low_T3_4_Right/1.png", "Image/P27_D1_P5_Low_T3_4_Right/2.png", "Image/P27_D1_P5_Low_T3_4_Right/3.png", "Image/P27_D1_P5_Low_T3_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1127.3971 seconds to 1137.4879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P8_Low_T3_7_Right", "images": ["Image/P18_D2_P8_Low_T3_7_Right/1.png", "Image/P18_D2_P8_Low_T3_7_Right/2.png", "Image/P18_D2_P8_Low_T3_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that green cup. Finish.\"\nThe frames roughly correspond to the speech interval from 479.7387 seconds to 488.7036 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P1_Low_T3_2_Mid", "images": ["Image/P9_D1_P1_Low_T3_2_Mid/1.png", "Image/P9_D1_P1_Low_T3_2_Mid/2.png", "Image/P9_D1_P1_Low_T3_2_Mid/3.png", "Image/P9_D1_P1_Low_T3_2_Mid/4.png", "Image/P9_D1_P1_Low_T3_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1387.5838 seconds to 1420.5075 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P1_Medium_T1_17_Right", "images": ["Image/P23_D2_P1_Medium_T1_17_Right/1.png", "Image/P23_D2_P1_Medium_T1_17_Right/2.png", "Image/P23_D2_P1_Medium_T1_17_Right/3.png", "Image/P23_D2_P1_Medium_T1_17_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the one on the right over there. Finish.\"\nThe frames roughly correspond to the speech interval from 344.0806 seconds to 354.284 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P5_Medium_T2_17_Right", "images": ["Image/P30_D3_P5_Medium_T2_17_Right/1.png", "Image/P30_D3_P5_Medium_T2_17_Right/2.png", "Image/P30_D3_P5_Medium_T2_17_Right/3.png", "Image/P30_D3_P5_Medium_T2_17_Right/4.png", "Image/P30_D3_P5_Medium_T2_17_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1454.7823 seconds to 1468.2835 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P5_Low_T2_1_Right", "images": ["Image/P24_D2_P5_Low_T2_1_Right/1.png", "Image/P24_D2_P5_Low_T2_1_Right/2.png", "Image/P24_D2_P5_Low_T2_1_Right/3.png", "Image/P24_D2_P5_Low_T2_1_Right/4.png", "Image/P24_D2_P5_Low_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Can you pour water in this bottle\"\nThe frames roughly correspond to the speech interval from 554.5321 seconds to 565.2517 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P3_Low_T1_13_Right", "images": ["Image/P21_D3_P3_Low_T1_13_Right/1.png", "Image/P21_D3_P3_Low_T1_13_Right/2.png", "Image/P21_D3_P3_Low_T1_13_Right/3.png", "Image/P21_D3_P3_Low_T1_13_Right/4.png", "Image/P21_D3_P3_Low_T1_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 643.0211999999999 seconds to 654.8387 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P9_Low_T3_17_Mid", "images": ["Image/P17_D1_P9_Low_T3_17_Mid/1.png", "Image/P17_D1_P9_Low_T3_17_Mid/2.png", "Image/P17_D1_P9_Low_T3_17_Mid/3.png", "Image/P17_D1_P9_Low_T3_17_Mid/4.png", "Image/P17_D1_P9_Low_T3_17_Mid/5.png", "Image/P17_D1_P9_Low_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the brown mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1839.6378 seconds to 1850.7736 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P6_High_T3_4_Right", "images": ["Image/P5_D3_P6_High_T3_4_Right/1.png", "Image/P5_D3_P6_High_T3_4_Right/2.png", "Image/P5_D3_P6_High_T3_4_Right/3.png", "Image/P5_D3_P6_High_T3_4_Right/4.png", "Image/P5_D3_P6_High_T3_4_Right/5.png", "Image/P5_D3_P6_High_T3_4_Right/6.png", "Image/P5_D3_P6_High_T3_4_Right/7.png", "Image/P5_D3_P6_High_T3_4_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plants on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2696.8714 seconds to 2710.4012 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P8_Low_T1_5_Left", "images": ["Image/P29_D3_P8_Low_T1_5_Left/1.png", "Image/P29_D3_P8_Low_T1_5_Left/2.png", "Image/P29_D3_P8_Low_T1_5_Left/3.png", "Image/P29_D3_P8_Low_T1_5_Left/4.png", "Image/P29_D3_P8_Low_T1_5_Left/5.png", "Image/P29_D3_P8_Low_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1652.9525 seconds to 1663.8928999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P8_Medium_T3_16_Right", "images": ["Image/P1_D1_P8_Medium_T3_16_Right/1.png", "Image/P1_D1_P8_Medium_T3_16_Right/2.png", "Image/P1_D1_P8_Medium_T3_16_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this one? Finish.\"\nThe frames roughly correspond to the speech interval from 274.45730000000003 seconds to 284.04859999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P8_Medium_T3_19_Right", "images": ["Image/P17_D2_P8_Medium_T3_19_Right/1.png", "Image/P17_D2_P8_Medium_T3_19_Right/2.png", "Image/P17_D2_P8_Medium_T3_19_Right/3.png", "Image/P17_D2_P8_Medium_T3_19_Right/4.png", "Image/P17_D2_P8_Medium_T3_19_Right/5.png", "Image/P17_D2_P8_Medium_T3_19_Right/6.png", "Image/P17_D2_P8_Medium_T3_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that bottle to the right down there? Finish.\"\nThe frames roughly correspond to the speech interval from 211.1191 seconds to 224.7051 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P9_High_T2_16_Left", "images": ["Image/P30_D1_P9_High_T2_16_Left/1.png", "Image/P30_D1_P9_High_T2_16_Left/2.png", "Image/P30_D1_P9_High_T2_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1044.7231000000002 seconds to 1053.0463 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P9_Low_T1_7_Right", "images": ["Image/P5_D1_P9_Low_T1_7_Right/1.png", "Image/P5_D1_P9_Low_T1_7_Right/2.png", "Image/P5_D1_P9_Low_T1_7_Right/3.png", "Image/P5_D1_P9_Low_T1_7_Right/4.png", "Image/P5_D1_P9_Low_T1_7_Right/5.png", "Image/P5_D1_P9_Low_T1_7_Right/6.png", "Image/P5_D1_P9_Low_T1_7_Right/7.png", "Image/P5_D1_P9_Low_T1_7_Right/8.png", "Image/P5_D1_P9_Low_T1_7_Right/9.png", "Image/P5_D1_P9_Low_T1_7_Right/10.png", "Image/P5_D1_P9_Low_T1_7_Right/11.png", "Image/P5_D1_P9_Low_T1_7_Right/12.png", "Image/P5_D1_P9_Low_T1_7_Right/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. To my right\"\nThe frames roughly correspond to the speech interval from 1512.08618 seconds to 1530.6959 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P9_Low_T3_20_Right", "images": ["Image/P29_D3_P9_Low_T3_20_Right/1.png", "Image/P29_D3_P9_Low_T3_20_Right/2.png", "Image/P29_D3_P9_Low_T3_20_Right/3.png", "Image/P29_D3_P9_Low_T3_20_Right/4.png", "Image/P29_D3_P9_Low_T3_20_Right/5.png", "Image/P29_D3_P9_Low_T3_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1616.5521 seconds to 1628.1740999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P9_High_T2_24_Right", "images": ["Image/P28_D1_P9_High_T2_24_Right/1.png", "Image/P28_D1_P9_High_T2_24_Right/2.png", "Image/P28_D1_P9_High_T2_24_Right/3.png", "Image/P28_D1_P9_High_T2_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1329.08234 seconds to 1338.9369 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P8_Low_T1_26_Right", "images": ["Image/P13_D2_P8_Low_T1_26_Right/1.png", "Image/P13_D2_P8_Low_T1_26_Right/2.png", "Image/P13_D2_P8_Low_T1_26_Right/3.png", "Image/P13_D2_P8_Low_T1_26_Right/4.png", "Image/P13_D2_P8_Low_T1_26_Right/5.png", "Image/P13_D2_P8_Low_T1_26_Right/6.png", "Image/P13_D2_P8_Low_T1_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move the white cup from the bottom all the way up to the top shelf here\"\nThe frames roughly correspond to the speech interval from 1246.45126 seconds to 1258.77136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P9_Low_T3_16_Left", "images": ["Image/P21_D2_P9_Low_T3_16_Left/1.png", "Image/P21_D2_P9_Low_T3_16_Left/2.png", "Image/P21_D2_P9_Low_T3_16_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1478.7922 seconds to 1488.938 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P6_Medium_T3_10_Left", "images": ["Image/P29_D3_P6_Medium_T3_10_Left/1.png", "Image/P29_D3_P6_Medium_T3_10_Left/2.png", "Image/P29_D3_P6_Medium_T3_10_Left/3.png", "Image/P29_D3_P6_Medium_T3_10_Left/4.png", "Image/P29_D3_P6_Medium_T3_10_Left/5.png", "Image/P29_D3_P6_Medium_T3_10_Left/6.png", "Image/P29_D3_P6_Medium_T3_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1439.3371999999997 seconds to 1451.5993999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D2_P2_Low_T3_5_Right", "images": ["Image/P20_D2_P2_Low_T3_5_Right/1.png", "Image/P20_D2_P2_Low_T3_5_Right/2.png", "Image/P20_D2_P2_Low_T3_5_Right/3.png", "Image/P20_D2_P2_Low_T3_5_Right/4.png", "Image/P20_D2_P2_Low_T3_5_Right/5.png", "Image/P20_D2_P2_Low_T3_5_Right/6.png", "Image/P20_D2_P2_Low_T3_5_Right/7.png", "Image/P20_D2_P2_Low_T3_5_Right/8.png", "Image/P20_D2_P2_Low_T3_5_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that plant on the top of the shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 3128.1119000000003 seconds to 3143.8427 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P2_Medium_T2_18_Mid", "images": ["Image/P30_D1_P2_Medium_T2_18_Mid/1.png", "Image/P30_D1_P2_Medium_T2_18_Mid/2.png", "Image/P30_D1_P2_Medium_T2_18_Mid/3.png", "Image/P30_D1_P2_Medium_T2_18_Mid/4.png", "Image/P30_D1_P2_Medium_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 623.3826300000001 seconds to 636.1590100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P1_Low_T1_6_Left", "images": ["Image/P21_D2_P1_Low_T1_6_Left/1.png", "Image/P21_D2_P1_Low_T1_6_Left/2.png", "Image/P21_D2_P1_Low_T1_6_Left/3.png", "Image/P21_D2_P1_Low_T1_6_Left/4.png", "Image/P21_D2_P1_Low_T1_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1602.3845 seconds to 1614.3010000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P8_Medium_T2_12_Left", "images": ["Image/P6_D3_P8_Medium_T2_12_Left/1.png", "Image/P6_D3_P8_Medium_T2_12_Left/2.png", "Image/P6_D3_P8_Medium_T2_12_Left/3.png", "Image/P6_D3_P8_Medium_T2_12_Left/4.png", "Image/P6_D3_P8_Medium_T2_12_Left/5.png", "Image/P6_D3_P8_Medium_T2_12_Left/6.png", "Image/P6_D3_P8_Medium_T2_12_Left/7.png", "Image/P6_D3_P8_Medium_T2_12_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 3303.52956 seconds to 3319.04908 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P8_Low_T2_17_Right", "images": ["Image/P19_D1_P8_Low_T2_17_Right/1.png", "Image/P19_D1_P8_Low_T2_17_Right/2.png", "Image/P19_D1_P8_Low_T2_17_Right/3.png", "Image/P19_D1_P8_Low_T2_17_Right/4.png", "Image/P19_D1_P8_Low_T2_17_Right/5.png", "Image/P19_D1_P8_Low_T2_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please pour into this container. Finish.\"\nThe frames roughly correspond to the speech interval from 657.9487 seconds to 672.2167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P8_Medium_T3_18_Left", "images": ["Image/P17_D1_P8_Medium_T3_18_Left/1.png", "Image/P17_D1_P8_Medium_T3_18_Left/2.png", "Image/P17_D1_P8_Medium_T3_18_Left/3.png", "Image/P17_D1_P8_Medium_T3_18_Left/4.png", "Image/P17_D1_P8_Medium_T3_18_Left/5.png", "Image/P17_D1_P8_Medium_T3_18_Left/6.png", "Image/P17_D1_P8_Medium_T3_18_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this brown mug in the center of the basket on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 2032.5273 seconds to 2045.7085 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P6_High_T2_17_Mid", "images": ["Image/P3_D2_P6_High_T2_17_Mid/1.png", "Image/P3_D2_P6_High_T2_17_Mid/2.png", "Image/P3_D2_P6_High_T2_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2536.4251 seconds to 2546.2926 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P2_High_T1_23_Right", "images": ["Image/P1_D3_P2_High_T1_23_Right/1.png", "Image/P1_D3_P2_High_T1_23_Right/2.png", "Image/P1_D3_P2_High_T1_23_Right/3.png", "Image/P1_D3_P2_High_T1_23_Right/4.png", "Image/P1_D3_P2_High_T1_23_Right/5.png", "Image/P1_D3_P2_High_T1_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1712.0005 seconds to 1723.2345999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P6_High_T3_21_Mid", "images": ["Image/P19_D3_P6_High_T3_21_Mid/1.png", "Image/P19_D3_P6_High_T3_21_Mid/2.png", "Image/P19_D3_P6_High_T3_21_Mid/3.png", "Image/P19_D3_P6_High_T3_21_Mid/4.png", "Image/P19_D3_P6_High_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 2146.8338 seconds to 2158.1359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
