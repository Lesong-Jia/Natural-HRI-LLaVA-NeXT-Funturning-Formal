{"id": "P28_D1_P6_High_T1_9_Left", "images": ["Image/P28_D1_P6_High_T1_9_Left/1.png", "Image/P28_D1_P6_High_T1_9_Left/2.png", "Image/P28_D1_P6_High_T1_9_Left/3.png", "Image/P28_D1_P6_High_T1_9_Left/4.png", "Image/P28_D1_P6_High_T1_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this green cup over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1588.5343 seconds to 1599.5167 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P1_Medium_T1_18_Mid", "images": ["Image/P9_D3_P1_Medium_T1_18_Mid/1.png", "Image/P9_D3_P1_Medium_T1_18_Mid/2.png", "Image/P9_D3_P1_Medium_T1_18_Mid/3.png", "Image/P9_D3_P1_Medium_T1_18_Mid/4.png", "Image/P9_D3_P1_Medium_T1_18_Mid/5.png", "Image/P9_D3_P1_Medium_T1_18_Mid/6.png", "Image/P9_D3_P1_Medium_T1_18_Mid/7.png", "Image/P9_D3_P1_Medium_T1_18_Mid/8.png", "Image/P9_D3_P1_Medium_T1_18_Mid/9.png", "Image/P9_D3_P1_Medium_T1_18_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the middle cup to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 681.1903 seconds to 698.1445 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D3_P3_Low_T1_10_Left", "images": ["Image/P4_D3_P3_Low_T1_10_Left/1.png", "Image/P4_D3_P3_Low_T1_10_Left/2.png", "Image/P4_D3_P3_Low_T1_10_Left/3.png", "Image/P4_D3_P3_Low_T1_10_Left/4.png", "Image/P4_D3_P3_Low_T1_10_Left/5.png", "Image/P4_D3_P3_Low_T1_10_Left/6.png", "Image/P4_D3_P3_Low_T1_10_Left/7.png", "Image/P4_D3_P3_Low_T1_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that right bottle to the bottom of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 551.5753 seconds to 566.0379999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D2_P4_Medium_T3_7_Left", "images": ["Image/P24_D2_P4_Medium_T3_7_Left/1.png", "Image/P24_D2_P4_Medium_T3_7_Left/2.png", "Image/P24_D2_P4_Medium_T3_7_Left/3.png", "Image/P24_D2_P4_Medium_T3_7_Left/4.png", "Image/P24_D2_P4_Medium_T3_7_Left/5.png", "Image/P24_D2_P4_Medium_T3_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 102.49911 seconds to 114.30138999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D3_P4_High_T2_16_Right", "images": ["Image/P18_D3_P4_High_T2_16_Right/1.png", "Image/P18_D3_P4_High_T2_16_Right/2.png", "Image/P18_D3_P4_High_T2_16_Right/3.png", "Image/P18_D3_P4_High_T2_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1193.42435 seconds to 1202.7508400000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P6_Low_T2_17_Mid", "images": ["Image/P3_D3_P6_Low_T2_17_Mid/1.png", "Image/P3_D3_P6_Low_T2_17_Mid/2.png", "Image/P3_D3_P6_Low_T2_17_Mid/3.png", "Image/P3_D3_P6_Low_T2_17_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you give some water to this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 868.4024999999999 seconds to 878.6770999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P9_High_T1_15_Mid", "images": ["Image/P11_D2_P9_High_T1_15_Mid/1.png", "Image/P11_D2_P9_High_T1_15_Mid/2.png", "Image/P11_D2_P9_High_T1_15_Mid/3.png", "Image/P11_D2_P9_High_T1_15_Mid/4.png", "Image/P11_D2_P9_High_T1_15_Mid/5.png", "Image/P11_D2_P9_High_T1_15_Mid/6.png", "Image/P11_D2_P9_High_T1_15_Mid/7.png", "Image/P11_D2_P9_High_T1_15_Mid/8.png", "Image/P11_D2_P9_High_T1_15_Mid/9.png", "Image/P11_D2_P9_High_T1_15_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Move the plant to the Top of the shelf on my right side\"\nThe frames roughly correspond to the speech interval from 852.3414 seconds to 870.6287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D3_P9_High_T2_24_Right", "images": ["Image/P5_D3_P9_High_T2_24_Right/1.png", "Image/P5_D3_P9_High_T2_24_Right/2.png", "Image/P5_D3_P9_High_T2_24_Right/3.png", "Image/P5_D3_P9_High_T2_24_Right/4.png", "Image/P5_D3_P9_High_T2_24_Right/5.png", "Image/P5_D3_P9_High_T2_24_Right/6.png", "Image/P5_D3_P9_High_T2_24_Right/7.png", "Image/P5_D3_P9_High_T2_24_Right/8.png", "Image/P5_D3_P9_High_T2_24_Right/9.png", "Image/P5_D3_P9_High_T2_24_Right/10.png", "Image/P5_D3_P9_High_T2_24_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" water the plant\"\nThe frames roughly correspond to the speech interval from 2409.7743 seconds to 2426.9082 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P7_Low_T1_2_Left", "images": ["Image/P27_D3_P7_Low_T1_2_Left/1.png", "Image/P27_D3_P7_Low_T1_2_Left/2.png", "Image/P27_D3_P7_Low_T1_2_Left/3.png", "Image/P27_D3_P7_Low_T1_2_Left/4.png", "Image/P27_D3_P7_Low_T1_2_Left/5.png", "Image/P27_D3_P7_Low_T1_2_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that glass to the top shelf? Finish\"\nThe frames roughly correspond to the speech interval from 566.115 seconds to 577.1388 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D3_P5_High_T1_14_Right", "images": ["Image/P15_D3_P5_High_T1_14_Right/1.png", "Image/P15_D3_P5_High_T1_14_Right/2.png", "Image/P15_D3_P5_High_T1_14_Right/3.png", "Image/P15_D3_P5_High_T1_14_Right/4.png", "Image/P15_D3_P5_High_T1_14_Right/5.png", "Image/P15_D3_P5_High_T1_14_Right/6.png", "Image/P15_D3_P5_High_T1_14_Right/7.png", "Image/P15_D3_P5_High_T1_14_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant to the brown shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 460.2268000000001 seconds to 474.2157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P2_Low_T1_13_Right", "images": ["Image/P28_D2_P2_Low_T1_13_Right/1.png", "Image/P28_D2_P2_Low_T1_13_Right/2.png", "Image/P28_D2_P2_Low_T1_13_Right/3.png", "Image/P28_D2_P2_Low_T1_13_Right/4.png", "Image/P28_D2_P2_Low_T1_13_Right/5.png", "Image/P28_D2_P2_Low_T1_13_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to the bottom shelf. Finish\"\nThe frames roughly correspond to the speech interval from 889.3166 seconds to 900.6613 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P4_Low_T1_19_Mid", "images": ["Image/P11_D3_P4_Low_T1_19_Mid/1.png", "Image/P11_D3_P4_Low_T1_19_Mid/2.png", "Image/P11_D3_P4_Low_T1_19_Mid/3.png", "Image/P11_D3_P4_Low_T1_19_Mid/4.png", "Image/P11_D3_P4_Low_T1_19_Mid/5.png", "Image/P11_D3_P4_Low_T1_19_Mid/6.png", "Image/P11_D3_P4_Low_T1_19_Mid/7.png", "Image/P11_D3_P4_Low_T1_19_Mid/8.png", "Image/P11_D3_P4_Low_T1_19_Mid/9.png", "Image/P11_D3_P4_Low_T1_19_Mid/10.png", "Image/P11_D3_P4_Low_T1_19_Mid/11.png", "Image/P11_D3_P4_Low_T1_19_Mid/12.png", "Image/P11_D3_P4_Low_T1_19_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the bottle on the desk to the third layer of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 1900.8192 seconds to 1921.561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P8_Low_T2_24_Right", "images": ["Image/P10_D2_P8_Low_T2_24_Right/1.png", "Image/P10_D2_P8_Low_T2_24_Right/2.png", "Image/P10_D2_P8_Low_T2_24_Right/3.png", "Image/P10_D2_P8_Low_T2_24_Right/4.png", "Image/P10_D2_P8_Low_T2_24_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1720.3793800000003 seconds to 1731.3192800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P9_Low_T3_22_Mid", "images": ["Image/P3_D2_P9_Low_T3_22_Mid/1.png", "Image/P3_D2_P9_Low_T3_22_Mid/2.png", "Image/P3_D2_P9_Low_T3_22_Mid/3.png", "Image/P3_D2_P9_Low_T3_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean this item? Finished.\"\nThe frames roughly correspond to the speech interval from 2457.0262 seconds to 2466.1021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P12_D1_P8_High_T2_8_Right", "images": ["Image/P12_D1_P8_High_T2_8_Right/1.png", "Image/P12_D1_P8_High_T2_8_Right/2.png", "Image/P12_D1_P8_High_T2_8_Right/3.png", "Image/P12_D1_P8_High_T2_8_Right/4.png", "Image/P12_D1_P8_High_T2_8_Right/5.png", "Image/P12_D1_P8_High_T2_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean the green mug that I'm pointing at. Finish.\"\nThe frames roughly correspond to the speech interval from 1266.73042 seconds to 1279.31452 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P4_High_T1_14_Right", "images": ["Image/P27_D3_P4_High_T1_14_Right/1.png", "Image/P27_D3_P4_High_T1_14_Right/2.png", "Image/P27_D3_P4_High_T1_14_Right/3.png", "Image/P27_D3_P4_High_T1_14_Right/4.png", "Image/P27_D3_P4_High_T1_14_Right/5.png", "Image/P27_D3_P4_High_T1_14_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that plant to the middle shelf right there? Finish\"\nThe frames roughly correspond to the speech interval from 433.2917000000001 seconds to 445.1362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P4_Medium_T3_19_Right", "images": ["Image/P1_D2_P4_Medium_T3_19_Right/1.png", "Image/P1_D2_P4_Medium_T3_19_Right/2.png", "Image/P1_D2_P4_Medium_T3_19_Right/3.png", "Image/P1_D2_P4_Medium_T3_19_Right/4.png", "Image/P1_D2_P4_Medium_T3_19_Right/5.png", "Image/P1_D2_P4_Medium_T3_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 704.07677 seconds to 715.1014100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P8_High_T1_27_Left", "images": ["Image/P20_D3_P8_High_T1_27_Left/1.png", "Image/P20_D3_P8_High_T1_27_Left/2.png", "Image/P20_D3_P8_High_T1_27_Left/3.png", "Image/P20_D3_P8_High_T1_27_Left/4.png", "Image/P20_D3_P8_High_T1_27_Left/5.png", "Image/P20_D3_P8_High_T1_27_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white cup to the top of the shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1743.0973 seconds to 1756.3620999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D3_P6_Medium_T3_14_Left", "images": ["Image/P8_D3_P6_Medium_T3_14_Left/1.png", "Image/P8_D3_P6_Medium_T3_14_Left/2.png", "Image/P8_D3_P6_Medium_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 1594.5437 seconds to 1607.1398 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D3_P3_Medium_T1_11_Right", "images": ["Image/P15_D3_P3_Medium_T1_11_Right/1.png", "Image/P15_D3_P3_Medium_T1_11_Right/2.png", "Image/P15_D3_P3_Medium_T1_11_Right/3.png", "Image/P15_D3_P3_Medium_T1_11_Right/4.png", "Image/P15_D3_P3_Medium_T1_11_Right/5.png", "Image/P15_D3_P3_Medium_T1_11_Right/6.png", "Image/P15_D3_P3_Medium_T1_11_Right/7.png", "Image/P15_D3_P3_Medium_T1_11_Right/8.png", "Image/P15_D3_P3_Medium_T1_11_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the right side pot into the lowest level of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 169.2726 seconds to 184.3337 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P7_High_T3_24_Mid", "images": ["Image/P20_D1_P7_High_T3_24_Mid/1.png", "Image/P20_D1_P7_High_T3_24_Mid/2.png", "Image/P20_D1_P7_High_T3_24_Mid/3.png", "Image/P20_D1_P7_High_T3_24_Mid/4.png", "Image/P20_D1_P7_High_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean a plant in a basket. Finish.\"\nThe frames roughly correspond to the speech interval from 489.9668 seconds to 500.2396 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P8_High_T1_12_Right", "images": ["Image/P4_D3_P8_High_T1_12_Right/1.png", "Image/P4_D3_P8_High_T1_12_Right/2.png", "Image/P4_D3_P8_High_T1_12_Right/3.png", "Image/P4_D3_P8_High_T1_12_Right/4.png", "Image/P4_D3_P8_High_T1_12_Right/5.png", "Image/P4_D3_P8_High_T1_12_Right/6.png", "Image/P4_D3_P8_High_T1_12_Right/7.png", "Image/P4_D3_P8_High_T1_12_Right/8.png", "Image/P4_D3_P8_High_T1_12_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right item next to the plants to the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 413.3578999999999 seconds to 431.1759 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P7_High_T2_20_Left", "images": ["Image/P28_D2_P7_High_T2_20_Left/1.png", "Image/P28_D2_P7_High_T2_20_Left/2.png", "Image/P28_D2_P7_High_T2_20_Left/3.png", "Image/P28_D2_P7_High_T2_20_Left/4.png", "Image/P28_D2_P7_High_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that white vase. Finish.\"\nThe frames roughly correspond to the speech interval from 926.5659 seconds to 937.3544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P8_High_T1_18_Left", "images": ["Image/P2_D3_P8_High_T1_18_Left/1.png", "Image/P2_D3_P8_High_T1_18_Left/2.png", "Image/P2_D3_P8_High_T1_18_Left/3.png", "Image/P2_D3_P8_High_T1_18_Left/4.png", "Image/P2_D3_P8_High_T1_18_Left/5.png", "Image/P2_D3_P8_High_T1_18_Left/6.png", "Image/P2_D3_P8_High_T1_18_Left/7.png", "Image/P2_D3_P8_High_T1_18_Left/8.png", "Image/P2_D3_P8_High_T1_18_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Could you please move that brown cup all the way to the top of the shelf? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2201.8966 seconds to 2218.7669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P1_High_T1_15_Left", "images": ["Image/P26_D3_P1_High_T1_15_Left/1.png", "Image/P26_D3_P1_High_T1_15_Left/2.png", "Image/P26_D3_P1_High_T1_15_Left/3.png", "Image/P26_D3_P1_High_T1_15_Left/4.png", "Image/P26_D3_P1_High_T1_15_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant down on the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 732.9582 seconds to 743.3838000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P1_High_T1_6_Left", "images": ["Image/P4_D2_P1_High_T1_6_Left/1.png", "Image/P4_D2_P1_High_T1_6_Left/2.png", "Image/P4_D2_P1_High_T1_6_Left/3.png", "Image/P4_D2_P1_High_T1_6_Left/4.png", "Image/P4_D2_P1_High_T1_6_Left/5.png", "Image/P4_D2_P1_High_T1_6_Left/6.png", "Image/P4_D2_P1_High_T1_6_Left/7.png", "Image/P4_D2_P1_High_T1_6_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant in the green bottle to the ground? Finished.\"\nThe frames roughly correspond to the speech interval from 1696.6808 seconds to 1713.0888999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P2_High_T2_13_Left", "images": ["Image/P4_D1_P2_High_T2_13_Left/1.png", "Image/P4_D1_P2_High_T2_13_Left/2.png", "Image/P4_D1_P2_High_T2_13_Left/3.png", "Image/P4_D1_P2_High_T2_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water this plant? Finish\"\nThe frames roughly correspond to the speech interval from 2626.5815999999995 seconds to 2638.8569 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D3_P9_Low_T3_18_Right", "images": ["Image/P30_D3_P9_Low_T3_18_Right/1.png", "Image/P30_D3_P9_Low_T3_18_Right/2.png", "Image/P30_D3_P9_Low_T3_18_Right/3.png", "Image/P30_D3_P9_Low_T3_18_Right/4.png", "Image/P30_D3_P9_Low_T3_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1207.0902999999998 seconds to 1217.4189 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P4_High_T2_27_Right", "images": ["Image/P11_D1_P4_High_T2_27_Right/1.png", "Image/P11_D1_P4_High_T2_27_Right/2.png", "Image/P11_D1_P4_High_T2_27_Right/3.png", "Image/P11_D1_P4_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 2835.5053 seconds to 2845.5545999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D3_P9_High_T3_25_Right", "images": ["Image/P24_D3_P9_High_T3_25_Right/1.png", "Image/P24_D3_P9_High_T3_25_Right/2.png", "Image/P24_D3_P9_High_T3_25_Right/3.png", "Image/P24_D3_P9_High_T3_25_Right/4.png", "Image/P24_D3_P9_High_T3_25_Right/5.png", "Image/P24_D3_P9_High_T3_25_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1756.6134 seconds to 1767.5254 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D1_P4_Medium_T1_20_Right", "images": ["Image/P19_D1_P4_Medium_T1_20_Right/1.png", "Image/P19_D1_P4_Medium_T1_20_Right/2.png", "Image/P19_D1_P4_Medium_T1_20_Right/3.png", "Image/P19_D1_P4_Medium_T1_20_Right/4.png", "Image/P19_D1_P4_Medium_T1_20_Right/5.png", "Image/P19_D1_P4_Medium_T1_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1252.6050999999998 seconds to 1268.5425999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P6_Medium_T3_4_Mid", "images": ["Image/P5_D1_P6_Medium_T3_4_Mid/1.png", "Image/P5_D1_P6_Medium_T3_4_Mid/2.png", "Image/P5_D1_P6_Medium_T3_4_Mid/3.png", "Image/P5_D1_P6_Medium_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" On my right hand side\"\nThe frames roughly correspond to the speech interval from 1486.62788 seconds to 1496.47919 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P4_High_T1_25_Mid", "images": ["Image/P14_D2_P4_High_T1_25_Mid/1.png", "Image/P14_D2_P4_High_T1_25_Mid/2.png", "Image/P14_D2_P4_High_T1_25_Mid/3.png", "Image/P14_D2_P4_High_T1_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that white cup to the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 1777.4197 seconds to 1786.3629999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D2_P1_Medium_T2_20_Right", "images": ["Image/P12_D2_P1_Medium_T2_20_Right/1.png", "Image/P12_D2_P1_Medium_T2_20_Right/2.png", "Image/P12_D2_P1_Medium_T2_20_Right/3.png", "Image/P12_D2_P1_Medium_T2_20_Right/4.png", "Image/P12_D2_P1_Medium_T2_20_Right/5.png", "Image/P12_D2_P1_Medium_T2_20_Right/6.png", "Image/P12_D2_P1_Medium_T2_20_Right/7.png", "Image/P12_D2_P1_Medium_T2_20_Right/8.png", "Image/P12_D2_P1_Medium_T2_20_Right/9.png", "Image/P12_D2_P1_Medium_T2_20_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the middle white vase on the shelf that I'm pointing at. Finish\"\nThe frames roughly correspond to the speech interval from 122.95588 seconds to 139.92427 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P8_Low_T3_16_Mid", "images": ["Image/P10_D1_P8_Low_T3_16_Mid/1.png", "Image/P10_D1_P8_Low_T3_16_Mid/2.png", "Image/P10_D1_P8_Low_T3_16_Mid/3.png", "Image/P10_D1_P8_Low_T3_16_Mid/4.png", "Image/P10_D1_P8_Low_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey\"\nThe frames roughly correspond to the speech interval from 2403.1973 seconds to 2414.9174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P1_Medium_T2_21_Left", "images": ["Image/P2_D3_P1_Medium_T2_21_Left/1.png", "Image/P2_D3_P1_Medium_T2_21_Left/2.png", "Image/P2_D3_P1_Medium_T2_21_Left/3.png", "Image/P2_D3_P1_Medium_T2_21_Left/4.png", "Image/P2_D3_P1_Medium_T2_21_Left/5.png", "Image/P2_D3_P1_Medium_T2_21_Left/6.png", "Image/P2_D3_P1_Medium_T2_21_Left/7.png", "Image/P2_D3_P1_Medium_T2_21_Left/8.png", "Image/P2_D3_P1_Medium_T2_21_Left/9.png", "Image/P2_D3_P1_Medium_T2_21_Left/10.png", "Image/P2_D3_P1_Medium_T2_21_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the wide vessel on the very left side for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2249.0575000000003 seconds to 2270.5079000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P1_Low_T1_12_Left", "images": ["Image/P3_D3_P1_Low_T1_12_Left/1.png", "Image/P3_D3_P1_Low_T1_12_Left/2.png", "Image/P3_D3_P1_Low_T1_12_Left/3.png", "Image/P3_D3_P1_Low_T1_12_Left/4.png", "Image/P3_D3_P1_Low_T1_12_Left/5.png", "Image/P3_D3_P1_Low_T1_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 621.6907199999999 seconds to 633.95579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P8_Medium_T2_17_Mid", "images": ["Image/P16_D1_P8_Medium_T2_17_Mid/1.png", "Image/P16_D1_P8_Medium_T2_17_Mid/2.png", "Image/P16_D1_P8_Medium_T2_17_Mid/3.png", "Image/P16_D1_P8_Medium_T2_17_Mid/4.png", "Image/P16_D1_P8_Medium_T2_17_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish\"\nThe frames roughly correspond to the speech interval from 1432.5671 seconds to 1443.7444 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P1_Low_T2_21_Right", "images": ["Image/P4_D3_P1_Low_T2_21_Right/1.png", "Image/P4_D3_P1_Low_T2_21_Right/2.png", "Image/P4_D3_P1_Low_T2_21_Right/3.png", "Image/P4_D3_P1_Low_T2_21_Right/4.png", "Image/P4_D3_P1_Low_T2_21_Right/5.png", "Image/P4_D3_P1_Low_T2_21_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in that white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 578.7787999999999 seconds to 591.7538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D2_P3_Medium_T3_15_Mid", "images": ["Image/P9_D2_P3_Medium_T3_15_Mid/1.png", "Image/P9_D2_P3_Medium_T3_15_Mid/2.png", "Image/P9_D2_P3_Medium_T3_15_Mid/3.png", "Image/P9_D2_P3_Medium_T3_15_Mid/4.png", "Image/P9_D2_P3_Medium_T3_15_Mid/5.png", "Image/P9_D2_P3_Medium_T3_15_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the middle plant on the top? Finish.\"\nThe frames roughly correspond to the speech interval from 2823.1466 seconds to 2839.1151 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P6_Low_T2_11_Left", "images": ["Image/P22_D1_P6_Low_T2_11_Left/1.png", "Image/P22_D1_P6_Low_T2_11_Left/2.png", "Image/P22_D1_P6_Low_T2_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Put water in that. Finish\"\nThe frames roughly correspond to the speech interval from 1637.483 seconds to 1647.4203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P2_Medium_T2_1_Left", "images": ["Image/P13_D3_P2_Medium_T2_1_Left/1.png", "Image/P13_D3_P2_Medium_T2_1_Left/2.png", "Image/P13_D3_P2_Medium_T2_1_Left/3.png", "Image/P13_D3_P2_Medium_T2_1_Left/4.png", "Image/P13_D3_P2_Medium_T2_1_Left/5.png", "Image/P13_D3_P2_Medium_T2_1_Left/6.png", "Image/P13_D3_P2_Medium_T2_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you fill up the left green bottle with water\"\nThe frames roughly correspond to the speech interval from 1673.3856 seconds to 1686.2197 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P2_Low_T1_5_Mid", "images": ["Image/P26_D2_P2_Low_T1_5_Mid/1.png", "Image/P26_D2_P2_Low_T1_5_Mid/2.png", "Image/P26_D2_P2_Low_T1_5_Mid/3.png", "Image/P26_D2_P2_Low_T1_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this plant to the floor here? Finish.\"\nThe frames roughly correspond to the speech interval from 1593.5743 seconds to 1604.8217 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P3_High_T3_20_Right", "images": ["Image/P16_D2_P3_High_T3_20_Right/1.png", "Image/P16_D2_P3_High_T3_20_Right/2.png", "Image/P16_D2_P3_High_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish\"\nThe frames roughly correspond to the speech interval from 832.7230000000001 seconds to 842.6607000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P4_Medium_T3_6_Left", "images": ["Image/P21_D2_P4_Medium_T3_6_Left/1.png", "Image/P21_D2_P4_Medium_T3_6_Left/2.png", "Image/P21_D2_P4_Medium_T3_6_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1408.11887 seconds to 1418.1973 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P1_High_T2_23_Right", "images": ["Image/P27_D2_P1_High_T2_23_Right/1.png", "Image/P27_D2_P1_High_T2_23_Right/2.png", "Image/P27_D2_P1_High_T2_23_Right/3.png", "Image/P27_D2_P1_High_T2_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you water that plant? Finish\"\nThe frames roughly correspond to the speech interval from 1314.4418 seconds to 1323.9487 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P6_Medium_T2_6_Left", "images": ["Image/P23_D2_P6_Medium_T2_6_Left/1.png", "Image/P23_D2_P6_Medium_T2_6_Left/2.png", "Image/P23_D2_P6_Medium_T2_6_Left/3.png", "Image/P23_D2_P6_Medium_T2_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant to the left. Finish.\"\nThe frames roughly correspond to the speech interval from 159.29290000000003 seconds to 169.4674 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P8_Low_T1_10_Mid", "images": ["Image/P5_D3_P8_Low_T1_10_Mid/1.png", "Image/P5_D3_P8_Low_T1_10_Mid/2.png", "Image/P5_D3_P8_Low_T1_10_Mid/3.png", "Image/P5_D3_P8_Low_T1_10_Mid/4.png", "Image/P5_D3_P8_Low_T1_10_Mid/5.png", "Image/P5_D3_P8_Low_T1_10_Mid/6.png", "Image/P5_D3_P8_Low_T1_10_Mid/7.png", "Image/P5_D3_P8_Low_T1_10_Mid/8.png", "Image/P5_D3_P8_Low_T1_10_Mid/9.png", "Image/P5_D3_P8_Low_T1_10_Mid/10.png", "Image/P5_D3_P8_Low_T1_10_Mid/11.png", "Image/P5_D3_P8_Low_T1_10_Mid/12.png", "Image/P5_D3_P8_Low_T1_10_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. There's a bottle on the ground underneath the wooden shelf in front of me. Move the bottle to the top level of the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 2513.5467 seconds to 2533.8558 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D3_P7_Low_T1_21_Mid", "images": ["Image/P14_D3_P7_Low_T1_21_Mid/1.png", "Image/P14_D3_P7_Low_T1_21_Mid/2.png", "Image/P14_D3_P7_Low_T1_21_Mid/3.png", "Image/P14_D3_P7_Low_T1_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1163.21276 seconds to 1172.15486 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D3_P2_Low_T3_25_Right", "images": ["Image/P26_D3_P2_Low_T3_25_Right/1.png", "Image/P26_D3_P2_Low_T3_25_Right/2.png", "Image/P26_D3_P2_Low_T3_25_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean that vase up there? Finish.\"\nThe frames roughly correspond to the speech interval from 1015.9375 seconds to 1026.0143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P8_Low_T3_20_Mid", "images": ["Image/P27_D3_P8_Low_T3_20_Mid/1.png", "Image/P27_D3_P8_Low_T3_20_Mid/2.png", "Image/P27_D3_P8_Low_T3_20_Mid/3.png", "Image/P27_D3_P8_Low_T3_20_Mid/4.png", "Image/P27_D3_P8_Low_T3_20_Mid/5.png", "Image/P27_D3_P8_Low_T3_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that white vase at the bottom? Finish.\"\nThe frames roughly correspond to the speech interval from 143.22851000000003 seconds to 154.83660000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P3_Low_T1_1_Right", "images": ["Image/P8_D2_P3_Low_T1_1_Right/1.png", "Image/P8_D2_P3_Low_T1_1_Right/2.png", "Image/P8_D2_P3_Low_T1_1_Right/3.png", "Image/P8_D2_P3_Low_T1_1_Right/4.png", "Image/P8_D2_P3_Low_T1_1_Right/5.png", "Image/P8_D2_P3_Low_T1_1_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move that bottle here? There. Never mind. Finish.\"\nThe frames roughly correspond to the speech interval from 2351.9289 seconds to 2364.1362 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P4_High_T3_17_Right", "images": ["Image/P6_D3_P4_High_T3_17_Right/1.png", "Image/P6_D3_P4_High_T3_17_Right/2.png", "Image/P6_D3_P4_High_T3_17_Right/3.png", "Image/P6_D3_P4_High_T3_17_Right/4.png", "Image/P6_D3_P4_High_T3_17_Right/5.png", "Image/P6_D3_P4_High_T3_17_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Freddy\"\nThe frames roughly correspond to the speech interval from 4543.3075 seconds to 4577.9735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P2_High_T1_1_Mid", "images": ["Image/P29_D2_P2_High_T1_1_Mid/1.png", "Image/P29_D2_P2_High_T1_1_Mid/2.png", "Image/P29_D2_P2_High_T1_1_Mid/3.png", "Image/P29_D2_P2_High_T1_1_Mid/4.png", "Image/P29_D2_P2_High_T1_1_Mid/5.png", "Image/P29_D2_P2_High_T1_1_Mid/6.png", "Image/P29_D2_P2_High_T1_1_Mid/7.png", "Image/P29_D2_P2_High_T1_1_Mid/8.png", "Image/P29_D2_P2_High_T1_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 364.304 seconds to 378.8359 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P4_Medium_T2_12_Left", "images": ["Image/P19_D2_P4_Medium_T2_12_Left/1.png", "Image/P19_D2_P4_Medium_T2_12_Left/2.png", "Image/P19_D2_P4_Medium_T2_12_Left/3.png", "Image/P19_D2_P4_Medium_T2_12_Left/4.png", "Image/P19_D2_P4_Medium_T2_12_Left/5.png", "Image/P19_D2_P4_Medium_T2_12_Left/6.png", "Image/P19_D2_P4_Medium_T2_12_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1517.9616999999998 seconds to 1531.5897 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P4_Medium_T1_18_Right", "images": ["Image/P22_D2_P4_Medium_T1_18_Right/1.png", "Image/P22_D2_P4_Medium_T1_18_Right/2.png", "Image/P22_D2_P4_Medium_T1_18_Right/3.png", "Image/P22_D2_P4_Medium_T1_18_Right/4.png", "Image/P22_D2_P4_Medium_T1_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put that cup over there. Finish.\"\nThe frames roughly correspond to the speech interval from 701.2136999999999 seconds to 718.1393999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P3_Medium_T1_15_Left", "images": ["Image/P29_D1_P3_Medium_T1_15_Left/1.png", "Image/P29_D1_P3_Medium_T1_15_Left/2.png", "Image/P29_D1_P3_Medium_T1_15_Left/3.png", "Image/P29_D1_P3_Medium_T1_15_Left/4.png", "Image/P29_D1_P3_Medium_T1_15_Left/5.png", "Image/P29_D1_P3_Medium_T1_15_Left/6.png", "Image/P29_D1_P3_Medium_T1_15_Left/7.png", "Image/P29_D1_P3_Medium_T1_15_Left/8.png", "Image/P29_D1_P3_Medium_T1_15_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2793.1308 seconds to 2807.2582 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D3_P6_Low_T2_13_Left", "images": ["Image/P22_D3_P6_Low_T2_13_Left/1.png", "Image/P22_D3_P6_Low_T2_13_Left/2.png", "Image/P22_D3_P6_Low_T2_13_Left/3.png", "Image/P22_D3_P6_Low_T2_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Water that plant. Finish\"\nThe frames roughly correspond to the speech interval from 176.1687 seconds to 185.3143 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P2_High_T1_23_Right", "images": ["Image/P26_D3_P2_High_T1_23_Right/1.png", "Image/P26_D3_P2_High_T1_23_Right/2.png", "Image/P26_D3_P2_High_T1_23_Right/3.png", "Image/P26_D3_P2_High_T1_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant to the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 903.8956 seconds to 913.3459 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P9_Medium_T1_10_Left", "images": ["Image/P6_D3_P9_Medium_T1_10_Left/1.png", "Image/P6_D3_P9_Medium_T1_10_Left/2.png", "Image/P6_D3_P9_Medium_T1_10_Left/3.png", "Image/P6_D3_P9_Medium_T1_10_Left/4.png", "Image/P6_D3_P9_Medium_T1_10_Left/5.png", "Image/P6_D3_P9_Medium_T1_10_Left/6.png", "Image/P6_D3_P9_Medium_T1_10_Left/7.png", "Image/P6_D3_P9_Medium_T1_10_Left/8.png", "Image/P6_D3_P9_Medium_T1_10_Left/9.png", "Image/P6_D3_P9_Medium_T1_10_Left/10.png", "Image/P6_D3_P9_Medium_T1_10_Left/11.png", "Image/P6_D3_P9_Medium_T1_10_Left/12.png", "Image/P6_D3_P9_Medium_T1_10_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3654.9518 seconds to 3675.1629 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D2_P8_Low_T1_9_Right", "images": ["Image/P20_D2_P8_Low_T1_9_Right/1.png", "Image/P20_D2_P8_Low_T1_9_Right/2.png", "Image/P20_D2_P8_Low_T1_9_Right/3.png", "Image/P20_D2_P8_Low_T1_9_Right/4.png", "Image/P20_D2_P8_Low_T1_9_Right/5.png", "Image/P20_D2_P8_Low_T1_9_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that green cup to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3033.1650000000004 seconds to 3044.9786000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D1_P8_Low_T2_19_Right", "images": ["Image/P25_D1_P8_Low_T2_19_Right/1.png", "Image/P25_D1_P8_Low_T2_19_Right/2.png", "Image/P25_D1_P8_Low_T2_19_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 179.425 seconds to 187.7205 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D3_P2_Low_T3_26_Right", "images": ["Image/P21_D3_P2_Low_T3_26_Right/1.png", "Image/P21_D3_P2_Low_T3_26_Right/2.png", "Image/P21_D3_P2_Low_T3_26_Right/3.png", "Image/P21_D3_P2_Low_T3_26_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 478.7082 seconds to 488.3404 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P1_Low_T1_2_Right", "images": ["Image/P7_D2_P1_Low_T1_2_Right/1.png", "Image/P7_D2_P1_Low_T1_2_Right/2.png", "Image/P7_D2_P1_Low_T1_2_Right/3.png", "Image/P7_D2_P1_Low_T1_2_Right/4.png", "Image/P7_D2_P1_Low_T1_2_Right/5.png", "Image/P7_D2_P1_Low_T1_2_Right/6.png", "Image/P7_D2_P1_Low_T1_2_Right/7.png", "Image/P7_D2_P1_Low_T1_2_Right/8.png", "Image/P7_D2_P1_Low_T1_2_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1467.024 seconds to 1483.3804 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D3_P9_High_T1_2_Mid", "images": ["Image/P13_D3_P9_High_T1_2_Mid/1.png", "Image/P13_D3_P9_High_T1_2_Mid/2.png", "Image/P13_D3_P9_High_T1_2_Mid/3.png", "Image/P13_D3_P9_High_T1_2_Mid/4.png", "Image/P13_D3_P9_High_T1_2_Mid/5.png", "Image/P13_D3_P9_High_T1_2_Mid/6.png", "Image/P13_D3_P9_High_T1_2_Mid/7.png", "Image/P13_D3_P9_High_T1_2_Mid/8.png", "Image/P13_D3_P9_High_T1_2_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move the green bottle in the middle on the floor from the bottom shelf to the top shelf please? Finish.\"\nThe frames roughly correspond to the speech interval from 1534.005 seconds to 1548.9414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P7_Medium_T2_19_Left", "images": ["Image/P25_D3_P7_Medium_T2_19_Left/1.png", "Image/P25_D3_P7_Medium_T2_19_Left/2.png", "Image/P25_D3_P7_Medium_T2_19_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 1821.152 seconds to 1829.477 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P7_Low_T2_9_Mid", "images": ["Image/P2_D2_P7_Low_T2_9_Mid/1.png", "Image/P2_D2_P7_Low_T2_9_Mid/2.png", "Image/P2_D2_P7_Low_T2_9_Mid/3.png", "Image/P2_D2_P7_Low_T2_9_Mid/4.png", "Image/P2_D2_P7_Low_T2_9_Mid/5.png", "Image/P2_D2_P7_Low_T2_9_Mid/6.png", "Image/P2_D2_P7_Low_T2_9_Mid/7.png", "Image/P2_D2_P7_Low_T2_9_Mid/8.png", "Image/P2_D2_P7_Low_T2_9_Mid/9.png", "Image/P2_D2_P7_Low_T2_9_Mid/10.png", "Image/P2_D2_P7_Low_T2_9_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please add some water into the green cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2896.4482 seconds to 2912.4988999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P5_High_T2_1_Mid", "images": ["Image/P18_D3_P5_High_T2_1_Mid/1.png", "Image/P18_D3_P5_High_T2_1_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1178.1268300000002 seconds to 1186.61801 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P3_High_T3_8_Left", "images": ["Image/P11_D2_P3_High_T3_8_Left/1.png", "Image/P11_D2_P3_High_T3_8_Left/2.png", "Image/P11_D2_P3_High_T3_8_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the green glass. Finish.\"\nThe frames roughly correspond to the speech interval from 992.1501 seconds to 1001.6761 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P7_Medium_T3_19_Mid", "images": ["Image/P2_D2_P7_Medium_T3_19_Mid/1.png", "Image/P2_D2_P7_Medium_T3_19_Mid/2.png", "Image/P2_D2_P7_Medium_T3_19_Mid/3.png", "Image/P2_D2_P7_Medium_T3_19_Mid/4.png", "Image/P2_D2_P7_Medium_T3_19_Mid/5.png", "Image/P2_D2_P7_Medium_T3_19_Mid/6.png", "Image/P2_D2_P7_Medium_T3_19_Mid/7.png", "Image/P2_D2_P7_Medium_T3_19_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you please clean the white bottle in the very middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2841.4924 seconds to 2857.3741999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P25_D2_P7_High_T2_21_Left", "images": ["Image/P25_D2_P7_High_T2_21_Left/1.png", "Image/P25_D2_P7_High_T2_21_Left/2.png", "Image/P25_D2_P7_High_T2_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 789.6613 seconds to 798.0972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P7_Low_T1_13_Mid", "images": ["Image/P11_D2_P7_Low_T1_13_Mid/1.png", "Image/P11_D2_P7_Low_T1_13_Mid/2.png", "Image/P11_D2_P7_Low_T1_13_Mid/3.png", "Image/P11_D2_P7_Low_T1_13_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the top of the shelf please. Finish.\"\nThe frames roughly correspond to the speech interval from 716.4757 seconds to 727.3905 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P6_Low_T3_4_Right", "images": ["Image/P16_D3_P6_Low_T3_4_Right/1.png", "Image/P16_D3_P6_Low_T3_4_Right/2.png", "Image/P16_D3_P6_Low_T3_4_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 433.5432 seconds to 442.3126999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D1_P1_High_T1_7_Mid", "images": ["Image/P20_D1_P1_High_T1_7_Mid/1.png", "Image/P20_D1_P1_High_T1_7_Mid/2.png", "Image/P20_D1_P1_High_T1_7_Mid/3.png", "Image/P20_D1_P1_High_T1_7_Mid/4.png", "Image/P20_D1_P1_High_T1_7_Mid/5.png", "Image/P20_D1_P1_High_T1_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Move this club\"\nThe frames roughly correspond to the speech interval from 731.5748000000001 seconds to 746.8314 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P9_Medium_T2_4_Mid", "images": ["Image/P13_D2_P9_Medium_T2_4_Mid/1.png", "Image/P13_D2_P9_Medium_T2_4_Mid/2.png", "Image/P13_D2_P9_Medium_T2_4_Mid/3.png", "Image/P13_D2_P9_Medium_T2_4_Mid/4.png", "Image/P13_D2_P9_Medium_T2_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you water the middle plant down there\"\nThe frames roughly correspond to the speech interval from 919.16616 seconds to 929.95426 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P9_Medium_T3_1_Right", "images": ["Image/P27_D2_P9_Medium_T3_1_Right/1.png", "Image/P27_D2_P9_Medium_T3_1_Right/2.png", "Image/P27_D2_P9_Medium_T3_1_Right/3.png", "Image/P27_D2_P9_Medium_T3_1_Right/4.png", "Image/P27_D2_P9_Medium_T3_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the vase on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1261.2822 seconds to 1271.03859 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P5_Medium_T2_11_Left", "images": ["Image/P5_D3_P5_Medium_T2_11_Left/1.png", "Image/P5_D3_P5_Medium_T2_11_Left/2.png", "Image/P5_D3_P5_Medium_T2_11_Left/3.png", "Image/P5_D3_P5_Medium_T2_11_Left/4.png", "Image/P5_D3_P5_Medium_T2_11_Left/5.png", "Image/P5_D3_P5_Medium_T2_11_Left/6.png", "Image/P5_D3_P5_Medium_T2_11_Left/7.png", "Image/P5_D3_P5_Medium_T2_11_Left/8.png", "Image/P5_D3_P5_Medium_T2_11_Left/9.png", "Image/P5_D3_P5_Medium_T2_11_Left/10.png", "Image/P5_D3_P5_Medium_T2_11_Left/11.png", "Image/P5_D3_P5_Medium_T2_11_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the bottle On-the-shelf and the lemon bottle. Finished.\"\nThe frames roughly correspond to the speech interval from 2779.1391999999996 seconds to 2796.5654 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P1_Medium_T1_12_Mid", "images": ["Image/P27_D2_P1_Medium_T1_12_Mid/1.png", "Image/P27_D2_P1_Medium_T1_12_Mid/2.png", "Image/P27_D2_P1_Medium_T1_12_Mid/3.png", "Image/P27_D2_P1_Medium_T1_12_Mid/4.png", "Image/P27_D2_P1_Medium_T1_12_Mid/5.png", "Image/P27_D2_P1_Medium_T1_12_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the middle vase to the bottom of that shelf? Finish\"\nThe frames roughly correspond to the speech interval from 1552.2082 seconds to 1564.192 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P1_Low_T2_8_Mid", "images": ["Image/P10_D2_P1_Low_T2_8_Mid/1.png", "Image/P10_D2_P1_Low_T2_8_Mid/2.png", "Image/P10_D2_P1_Low_T2_8_Mid/3.png", "Image/P10_D2_P1_Low_T2_8_Mid/4.png", "Image/P10_D2_P1_Low_T2_8_Mid/5.png", "Image/P10_D2_P1_Low_T2_8_Mid/6.png", "Image/P10_D2_P1_Low_T2_8_Mid/7.png", "Image/P10_D2_P1_Low_T2_8_Mid/8.png", "Image/P10_D2_P1_Low_T2_8_Mid/9.png", "Image/P10_D2_P1_Low_T2_8_Mid/10.png", "Image/P10_D2_P1_Low_T2_8_Mid/11.png", "Image/P10_D2_P1_Low_T2_8_Mid/12.png", "Image/P10_D2_P1_Low_T2_8_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1209.2945800000002 seconds to 1228.0183800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P9_High_T3_3_Right", "images": ["Image/P3_D1_P9_High_T3_3_Right/1.png", "Image/P3_D1_P9_High_T3_3_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Would you clean this item?\"\nThe frames roughly correspond to the speech interval from 1447.1265 seconds to 1456.1064 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D2_P9_Low_T2_26_Mid", "images": ["Image/P7_D2_P9_Low_T2_26_Mid/1.png", "Image/P7_D2_P9_Low_T2_26_Mid/2.png", "Image/P7_D2_P9_Low_T2_26_Mid/3.png", "Image/P7_D2_P9_Low_T2_26_Mid/4.png", "Image/P7_D2_P9_Low_T2_26_Mid/5.png", "Image/P7_D2_P9_Low_T2_26_Mid/6.png", "Image/P7_D2_P9_Low_T2_26_Mid/7.png", "Image/P7_D2_P9_Low_T2_26_Mid/8.png", "Image/P7_D2_P9_Low_T2_26_Mid/9.png", "Image/P7_D2_P9_Low_T2_26_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Frederick\"\nThe frames roughly correspond to the speech interval from 1275.3188000000002 seconds to 1292.469 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P9_High_T2_25_Right", "images": ["Image/P19_D2_P9_High_T2_25_Right/1.png", "Image/P19_D2_P9_High_T2_25_Right/2.png", "Image/P19_D2_P9_High_T2_25_Right/3.png", "Image/P19_D2_P9_High_T2_25_Right/4.png", "Image/P19_D2_P9_High_T2_25_Right/5.png", "Image/P19_D2_P9_High_T2_25_Right/6.png", "Image/P19_D2_P9_High_T2_25_Right/7.png", "Image/P19_D2_P9_High_T2_25_Right/8.png", "Image/P19_D2_P9_High_T2_25_Right/9.png", "Image/P19_D2_P9_High_T2_25_Right/10.png", "Image/P19_D2_P9_High_T2_25_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1683.4991 seconds to 1698.6419999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P7_Low_T3_11_Right", "images": ["Image/P18_D2_P7_Low_T3_11_Right/1.png", "Image/P18_D2_P7_Low_T3_11_Right/2.png", "Image/P18_D2_P7_Low_T3_11_Right/3.png", "Image/P18_D2_P7_Low_T3_11_Right/4.png", "Image/P18_D2_P7_Low_T3_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that orange bottle. Finish.\"\nThe frames roughly correspond to the speech interval from 462.5755 seconds to 473.3913 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P8_Medium_T3_21_Left", "images": ["Image/P19_D2_P8_Medium_T3_21_Left/1.png", "Image/P19_D2_P8_Medium_T3_21_Left/2.png", "Image/P19_D2_P8_Medium_T3_21_Left/3.png", "Image/P19_D2_P8_Medium_T3_21_Left/4.png", "Image/P19_D2_P8_Medium_T3_21_Left/5.png", "Image/P19_D2_P8_Medium_T3_21_Left/6.png", "Image/P19_D2_P8_Medium_T3_21_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please clean this item down here. Okay.\"\nThe frames roughly correspond to the speech interval from 1404.1948599999998 seconds to 1419.4219999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D3_P1_High_T2_19_Mid", "images": ["Image/P11_D3_P1_High_T2_19_Mid/1.png", "Image/P11_D3_P1_High_T2_19_Mid/2.png", "Image/P11_D3_P1_High_T2_19_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in a white glass. Finish.\"\nThe frames roughly correspond to the speech interval from 1875.0119 seconds to 1884.6306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P9_High_T3_24_Mid", "images": ["Image/P18_D3_P9_High_T3_24_Mid/1.png", "Image/P18_D3_P9_High_T3_24_Mid/2.png", "Image/P18_D3_P9_High_T3_24_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1569.081 seconds to 1576.9316 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P7_Low_T1_1_Left", "images": ["Image/P19_D2_P7_Low_T1_1_Left/1.png", "Image/P19_D2_P7_Low_T1_1_Left/2.png", "Image/P19_D2_P7_Low_T1_1_Left/3.png", "Image/P19_D2_P7_Low_T1_1_Left/4.png", "Image/P19_D2_P7_Low_T1_1_Left/5.png", "Image/P19_D2_P7_Low_T1_1_Left/6.png", "Image/P19_D2_P7_Low_T1_1_Left/7.png", "Image/P19_D2_P7_Low_T1_1_Left/8.png", "Image/P19_D2_P7_Low_T1_1_Left/9.png", "Image/P19_D2_P7_Low_T1_1_Left/10.png", "Image/P19_D2_P7_Low_T1_1_Left/11.png", "Image/P19_D2_P7_Low_T1_1_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start. Please move this container up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1329.44391 seconds to 1345.31253 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P3_Medium_T1_10_Mid", "images": ["Image/P11_D1_P3_Medium_T1_10_Mid/1.png", "Image/P11_D1_P3_Medium_T1_10_Mid/2.png", "Image/P11_D1_P3_Medium_T1_10_Mid/3.png", "Image/P11_D1_P3_Medium_T1_10_Mid/4.png", "Image/P11_D1_P3_Medium_T1_10_Mid/5.png", "Image/P11_D1_P3_Medium_T1_10_Mid/6.png", "Image/P11_D1_P3_Medium_T1_10_Mid/7.png", "Image/P11_D1_P3_Medium_T1_10_Mid/8.png", "Image/P11_D1_P3_Medium_T1_10_Mid/9.png", "Image/P11_D1_P3_Medium_T1_10_Mid/10.png", "Image/P11_D1_P3_Medium_T1_10_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the second red glass to the right basket on the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2750.5101999999997 seconds to 2769.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D1_P4_Low_T1_23_Right", "images": ["Image/P3_D1_P4_Low_T1_23_Right/1.png", "Image/P3_D1_P4_Low_T1_23_Right/2.png", "Image/P3_D1_P4_Low_T1_23_Right/3.png", "Image/P3_D1_P4_Low_T1_23_Right/4.png", "Image/P3_D1_P4_Low_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this place? Finish.\"\nThe frames roughly correspond to the speech interval from 1344.1591 seconds to 1356.4936 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D1_P2_Medium_T1_13_Left", "images": ["Image/P28_D1_P2_Medium_T1_13_Left/1.png", "Image/P28_D1_P2_Medium_T1_13_Left/2.png", "Image/P28_D1_P2_Medium_T1_13_Left/3.png", "Image/P28_D1_P2_Medium_T1_13_Left/4.png", "Image/P28_D1_P2_Medium_T1_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1684.3455 seconds to 1695.2173 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P2_Medium_T3_14_Right", "images": ["Image/P24_D1_P2_Medium_T3_14_Right/1.png", "Image/P24_D1_P2_Medium_T3_14_Right/2.png", "Image/P24_D1_P2_Medium_T3_14_Right/3.png", "Image/P24_D1_P2_Medium_T3_14_Right/4.png", "Image/P24_D1_P2_Medium_T3_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1337.536 seconds to 1347.7103 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D1_P6_Low_T1_15_Mid", "images": ["Image/P1_D1_P6_Low_T1_15_Mid/1.png", "Image/P1_D1_P6_Low_T1_15_Mid/2.png", "Image/P1_D1_P6_Low_T1_15_Mid/3.png", "Image/P1_D1_P6_Low_T1_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 426.8892 seconds to 436.3005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P8_Medium_T2_21_Left", "images": ["Image/P28_D3_P8_Medium_T2_21_Left/1.png", "Image/P28_D3_P8_Medium_T2_21_Left/2.png", "Image/P28_D3_P8_Medium_T2_21_Left/3.png", "Image/P28_D3_P8_Medium_T2_21_Left/4.png", "Image/P28_D3_P8_Medium_T2_21_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 375.2899 seconds to 385.8549 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D1_P4_Low_T3_2_Right", "images": ["Image/P5_D1_P4_Low_T3_2_Right/1.png", "Image/P5_D1_P4_Low_T3_2_Right/2.png", "Image/P5_D1_P4_Low_T3_2_Right/3.png", "Image/P5_D1_P4_Low_T3_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle. Finished.\"\nThe frames roughly correspond to the speech interval from 2005.1879 seconds to 2015.2766 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P9_Medium_T3_14_Left", "images": ["Image/P5_D2_P9_Medium_T3_14_Left/1.png", "Image/P5_D2_P9_Medium_T3_14_Left/2.png", "Image/P5_D2_P9_Medium_T3_14_Left/3.png", "Image/P5_D2_P9_Medium_T3_14_Left/4.png", "Image/P5_D2_P9_Medium_T3_14_Left/5.png", "Image/P5_D2_P9_Medium_T3_14_Left/6.png", "Image/P5_D2_P9_Medium_T3_14_Left/7.png", "Image/P5_D2_P9_Medium_T3_14_Left/8.png", "Image/P5_D2_P9_Medium_T3_14_Left/9.png", "Image/P5_D2_P9_Medium_T3_14_Left/10.png", "Image/P5_D2_P9_Medium_T3_14_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"There are three plants to the right side of me and clean the plant on the leftmost finish.\"\nThe frames roughly correspond to the speech interval from 476.7928 seconds to 495.5561 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D3_P2_High_T3_6_Mid", "images": ["Image/P22_D3_P2_High_T3_6_Mid/1.png", "Image/P22_D3_P2_High_T3_6_Mid/2.png", "Image/P22_D3_P2_High_T3_6_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Finish.\"\nThe frames roughly correspond to the speech interval from 437.8912 seconds to 446.3683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P9_Medium_T1_7_Left", "images": ["Image/P18_D1_P9_Medium_T1_7_Left/1.png", "Image/P18_D1_P9_Medium_T1_7_Left/2.png", "Image/P18_D1_P9_Medium_T1_7_Left/3.png", "Image/P18_D1_P9_Medium_T1_7_Left/4.png", "Image/P18_D1_P9_Medium_T1_7_Left/5.png", "Image/P18_D1_P9_Medium_T1_7_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this cup on the left up to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 746.2133 seconds to 761.0504000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P1_Medium_T1_27_Mid", "images": ["Image/P9_D2_P1_Medium_T1_27_Mid/1.png", "Image/P9_D2_P1_Medium_T1_27_Mid/2.png", "Image/P9_D2_P1_Medium_T1_27_Mid/3.png", "Image/P9_D2_P1_Medium_T1_27_Mid/4.png", "Image/P9_D2_P1_Medium_T1_27_Mid/5.png", "Image/P9_D2_P1_Medium_T1_27_Mid/6.png", "Image/P9_D2_P1_Medium_T1_27_Mid/7.png", "Image/P9_D2_P1_Medium_T1_27_Mid/8.png", "Image/P9_D2_P1_Medium_T1_27_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the middle cup from the top to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3054.8293 seconds to 3073.9229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P3_Medium_T3_16_Mid", "images": ["Image/P27_D3_P3_Medium_T3_16_Mid/1.png", "Image/P27_D3_P3_Medium_T3_16_Mid/2.png", "Image/P27_D3_P3_Medium_T3_16_Mid/3.png", "Image/P27_D3_P3_Medium_T3_16_Mid/4.png", "Image/P27_D3_P3_Medium_T3_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the middle cup over there? Finish\"\nThe frames roughly correspond to the speech interval from 392.2553 seconds to 406.3423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P3_Low_T2_13_Right", "images": ["Image/P30_D3_P3_Low_T2_13_Right/1.png", "Image/P30_D3_P3_Low_T2_13_Right/2.png", "Image/P30_D3_P3_Low_T2_13_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1561.951 seconds to 1569.8021 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P6_High_T1_22_Right", "images": ["Image/P3_D3_P6_High_T1_22_Right/1.png", "Image/P3_D3_P6_High_T1_22_Right/2.png", "Image/P3_D3_P6_High_T1_22_Right/3.png", "Image/P3_D3_P6_High_T1_22_Right/4.png", "Image/P3_D3_P6_High_T1_22_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you move the item to this table? Finish.\"\nThe frames roughly correspond to the speech interval from 905.9226 seconds to 916.7953 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P7_High_T3_4_Left", "images": ["Image/P9_D2_P7_High_T3_4_Left/1.png", "Image/P9_D2_P7_High_T3_4_Left/2.png", "Image/P9_D2_P7_High_T3_4_Left/3.png", "Image/P9_D2_P7_High_T3_4_Left/4.png", "Image/P9_D2_P7_High_T3_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the plant on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 2709.38909 seconds to 2720.77628 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P4_Medium_T2_15_Mid", "images": ["Image/P8_D1_P4_Medium_T2_15_Mid/1.png", "Image/P8_D1_P4_Medium_T2_15_Mid/2.png", "Image/P8_D1_P4_Medium_T2_15_Mid/3.png", "Image/P8_D1_P4_Medium_T2_15_Mid/4.png", "Image/P8_D1_P4_Medium_T2_15_Mid/5.png", "Image/P8_D1_P4_Medium_T2_15_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 837.1936 seconds to 850.5941999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P2_Medium_T3_9_Left", "images": ["Image/P18_D2_P2_Medium_T3_9_Left/1.png", "Image/P18_D2_P2_Medium_T3_9_Left/2.png", "Image/P18_D2_P2_Medium_T3_9_Left/3.png", "Image/P18_D2_P2_Medium_T3_9_Left/4.png", "Image/P18_D2_P2_Medium_T3_9_Left/5.png", "Image/P18_D2_P2_Medium_T3_9_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that first cup that's separate from the first two. Finish.\"\nThe frames roughly correspond to the speech interval from 297.5847 seconds to 310.2778 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P8_Medium_T1_15_Left", "images": ["Image/P7_D1_P8_Medium_T1_15_Left/1.png", "Image/P7_D1_P8_Medium_T1_15_Left/2.png", "Image/P7_D1_P8_Medium_T1_15_Left/3.png", "Image/P7_D1_P8_Medium_T1_15_Left/4.png", "Image/P7_D1_P8_Medium_T1_15_Left/5.png", "Image/P7_D1_P8_Medium_T1_15_Left/6.png", "Image/P7_D1_P8_Medium_T1_15_Left/7.png", "Image/P7_D1_P8_Medium_T1_15_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 496.4903 seconds to 511.8165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P6_Medium_T3_25_Mid", "images": ["Image/P3_D2_P6_Medium_T3_25_Mid/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you move the item to this space? Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2371.9391 seconds to 2381.165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P20_D3_P9_Medium_T2_26_Mid", "images": ["Image/P20_D3_P9_Medium_T2_26_Mid/1.png", "Image/P20_D3_P9_Medium_T2_26_Mid/2.png", "Image/P20_D3_P9_Medium_T2_26_Mid/3.png", "Image/P20_D3_P9_Medium_T2_26_Mid/4.png", "Image/P20_D3_P9_Medium_T2_26_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you add some water into the middle cup over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1670.2545 seconds to 1682.406 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P1_Low_T2_7_Right", "images": ["Image/P10_D1_P1_Low_T2_7_Right/1.png", "Image/P10_D1_P1_Low_T2_7_Right/2.png", "Image/P10_D1_P1_Low_T2_7_Right/3.png", "Image/P10_D1_P1_Low_T2_7_Right/4.png", "Image/P10_D1_P1_Low_T2_7_Right/5.png", "Image/P10_D1_P1_Low_T2_7_Right/6.png", "Image/P10_D1_P1_Low_T2_7_Right/7.png", "Image/P10_D1_P1_Low_T2_7_Right/8.png", "Image/P10_D1_P1_Low_T2_7_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2521.1268 seconds to 2535.9091 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P4_Low_T1_5_Left", "images": ["Image/P18_D3_P4_Low_T1_5_Left/1.png", "Image/P18_D3_P4_Low_T1_5_Left/2.png", "Image/P18_D3_P4_Low_T1_5_Left/3.png", "Image/P18_D3_P4_Low_T1_5_Left/4.png", "Image/P18_D3_P4_Low_T1_5_Left/5.png", "Image/P18_D3_P4_Low_T1_5_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1159.47388 seconds to 1171.93235 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P7_Medium_T1_25_Right", "images": ["Image/P24_D1_P7_Medium_T1_25_Right/1.png", "Image/P24_D1_P7_Medium_T1_25_Right/2.png", "Image/P24_D1_P7_Medium_T1_25_Right/3.png", "Image/P24_D1_P7_Medium_T1_25_Right/4.png", "Image/P24_D1_P7_Medium_T1_25_Right/5.png", "Image/P24_D1_P7_Medium_T1_25_Right/6.png", "Image/P24_D1_P7_Medium_T1_25_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you move this cup over here\"\nThe frames roughly correspond to the speech interval from 1155.0037 seconds to 1168.3948 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P1_Low_T3_10_Right", "images": ["Image/P22_D1_P1_Low_T3_10_Right/1.png", "Image/P22_D1_P1_Low_T3_10_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\" In that\"\nThe frames roughly correspond to the speech interval from 1784.269 seconds to 1791.9381 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P7_High_T3_2_Left", "images": ["Image/P23_D2_P7_High_T3_2_Left/1.png", "Image/P23_D2_P7_High_T3_2_Left/2.png", "Image/P23_D2_P7_High_T3_2_Left/3.png", "Image/P23_D2_P7_High_T3_2_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the thing on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 288.72092 seconds to 299.0084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P5_Low_T3_14_Left", "images": ["Image/P18_D1_P5_Low_T3_14_Left/1.png", "Image/P18_D1_P5_Low_T3_14_Left/2.png", "Image/P18_D1_P5_Low_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 848.6773000000001 seconds to 858.2817 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P7_Medium_T1_17_Mid", "images": ["Image/P4_D2_P7_Medium_T1_17_Mid/1.png", "Image/P4_D2_P7_Medium_T1_17_Mid/2.png", "Image/P4_D2_P7_Medium_T1_17_Mid/3.png", "Image/P4_D2_P7_Medium_T1_17_Mid/4.png", "Image/P4_D2_P7_Medium_T1_17_Mid/5.png", "Image/P4_D2_P7_Medium_T1_17_Mid/6.png", "Image/P4_D2_P7_Medium_T1_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the cap in the middle to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1599.4071 seconds to 1616.0126999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P3_Low_T1_23_Right", "images": ["Image/P27_D1_P3_Low_T1_23_Right/1.png", "Image/P27_D1_P3_Low_T1_23_Right/2.png", "Image/P27_D1_P3_Low_T1_23_Right/3.png", "Image/P27_D1_P3_Low_T1_23_Right/4.png", "Image/P27_D1_P3_Low_T1_23_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant into that basket? Finish\"\nThe frames roughly correspond to the speech interval from 705.8738800000001 seconds to 716.8292100000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P1_Low_T3_23_Mid", "images": ["Image/P10_D3_P1_Low_T3_23_Mid/1.png", "Image/P10_D3_P1_Low_T3_23_Mid/2.png", "Image/P10_D3_P1_Low_T3_23_Mid/3.png", "Image/P10_D3_P1_Low_T3_23_Mid/4.png", "Image/P10_D3_P1_Low_T3_23_Mid/5.png", "Image/P10_D3_P1_Low_T3_23_Mid/6.png", "Image/P10_D3_P1_Low_T3_23_Mid/7.png", "Image/P10_D3_P1_Low_T3_23_Mid/8.png", "Image/P10_D3_P1_Low_T3_23_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 805.6959899999999 seconds to 823.91689 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P8_Low_T1_27_Right", "images": ["Image/P19_D3_P8_Low_T1_27_Right/1.png", "Image/P19_D3_P8_Low_T1_27_Right/2.png", "Image/P19_D3_P8_Low_T1_27_Right/3.png", "Image/P19_D3_P8_Low_T1_27_Right/4.png", "Image/P19_D3_P8_Low_T1_27_Right/5.png", "Image/P19_D3_P8_Low_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this glass up there. Finish.\"\nThe frames roughly correspond to the speech interval from 2164.8601999999996 seconds to 2178.723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P4_Low_T1_14_Left", "images": ["Image/P25_D3_P4_Low_T1_14_Left/1.png", "Image/P25_D3_P4_Low_T1_14_Left/2.png", "Image/P25_D3_P4_Low_T1_14_Left/3.png", "Image/P25_D3_P4_Low_T1_14_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1917.4924 seconds to 1926.1922 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P3_Medium_T1_9_Mid", "images": ["Image/P13_D2_P3_Medium_T1_9_Mid/1.png", "Image/P13_D2_P3_Medium_T1_9_Mid/2.png", "Image/P13_D2_P3_Medium_T1_9_Mid/3.png", "Image/P13_D2_P3_Medium_T1_9_Mid/4.png", "Image/P13_D2_P3_Medium_T1_9_Mid/5.png", "Image/P13_D2_P3_Medium_T1_9_Mid/6.png", "Image/P13_D2_P3_Medium_T1_9_Mid/7.png", "Image/P13_D2_P3_Medium_T1_9_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you move the middle green glass down to all the way to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 879.09046 seconds to 892.18976 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P2_Low_T3_8_Mid", "images": ["Image/P29_D1_P2_Low_T3_8_Mid/1.png", "Image/P29_D1_P2_Low_T3_8_Mid/2.png", "Image/P29_D1_P2_Low_T3_8_Mid/3.png", "Image/P29_D1_P2_Low_T3_8_Mid/4.png", "Image/P29_D1_P2_Low_T3_8_Mid/5.png", "Image/P29_D1_P2_Low_T3_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hey Friday\"\nThe frames roughly correspond to the speech interval from 2900.7218 seconds to 2912.6063 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P7_High_T3_8_Mid", "images": ["Image/P13_D2_P7_High_T3_8_Mid/1.png", "Image/P13_D2_P7_High_T3_8_Mid/2.png", "Image/P13_D2_P7_High_T3_8_Mid/3.png", "Image/P13_D2_P7_High_T3_8_Mid/4.png", "Image/P13_D2_P7_High_T3_8_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the green glass down there\"\nThe frames roughly correspond to the speech interval from 959.96526 seconds to 970.51666 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D1_P4_Medium_T1_10_Left", "images": ["Image/P9_D1_P4_Medium_T1_10_Left/1.png", "Image/P9_D1_P4_Medium_T1_10_Left/2.png", "Image/P9_D1_P4_Medium_T1_10_Left/3.png", "Image/P9_D1_P4_Medium_T1_10_Left/4.png", "Image/P9_D1_P4_Medium_T1_10_Left/5.png", "Image/P9_D1_P4_Medium_T1_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this bottle here? Finish.\"\nThe frames roughly correspond to the speech interval from 1856.225 seconds to 1870.8186 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P9_High_T2_23_Left", "images": ["Image/P9_D1_P9_High_T2_23_Left/1.png", "Image/P9_D1_P9_High_T2_23_Left/2.png", "Image/P9_D1_P9_High_T2_23_Left/3.png", "Image/P9_D1_P9_High_T2_23_Left/4.png", "Image/P9_D1_P9_High_T2_23_Left/5.png", "Image/P9_D1_P9_High_T2_23_Left/6.png", "Image/P9_D1_P9_High_T2_23_Left/7.png", "Image/P9_D1_P9_High_T2_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Would you help me to pour some water in this plant in the box? Finish.\"\nThe frames roughly correspond to the speech interval from 1742.8971 seconds to 1761.032 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P7_Low_T1_18_Left", "images": ["Image/P12_D3_P7_Low_T1_18_Left/1.png", "Image/P12_D3_P7_Low_T1_18_Left/2.png", "Image/P12_D3_P7_Low_T1_18_Left/3.png", "Image/P12_D3_P7_Low_T1_18_Left/4.png", "Image/P12_D3_P7_Low_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2359.0300899999997 seconds to 2370.62439 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D2_P6_High_T2_15_Mid", "images": ["Image/P28_D2_P6_High_T2_15_Mid/1.png", "Image/P28_D2_P6_High_T2_15_Mid/2.png", "Image/P28_D2_P6_High_T2_15_Mid/3.png", "Image/P28_D2_P6_High_T2_15_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1213.2886 seconds to 1222.2117 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P3_High_T1_22_Mid", "images": ["Image/P28_D2_P3_High_T1_22_Mid/1.png", "Image/P28_D2_P3_High_T1_22_Mid/2.png", "Image/P28_D2_P3_High_T1_22_Mid/3.png", "Image/P28_D2_P3_High_T1_22_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1130.4108 seconds to 1140.4743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P1_Low_T2_7_Mid", "images": ["Image/P6_D1_P1_Low_T2_7_Mid/1.png", "Image/P6_D1_P1_Low_T2_7_Mid/2.png", "Image/P6_D1_P1_Low_T2_7_Mid/3.png", "Image/P6_D1_P1_Low_T2_7_Mid/4.png", "Image/P6_D1_P1_Low_T2_7_Mid/5.png", "Image/P6_D1_P1_Low_T2_7_Mid/6.png", "Image/P6_D1_P1_Low_T2_7_Mid/7.png", "Image/P6_D1_P1_Low_T2_7_Mid/8.png", "Image/P6_D1_P1_Low_T2_7_Mid/9.png", "Image/P6_D1_P1_Low_T2_7_Mid/10.png", "Image/P6_D1_P1_Low_T2_7_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 2309.7908 seconds to 2327.8705 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P2_Low_T2_18_Right", "images": ["Image/P8_D3_P2_Low_T2_18_Right/1.png", "Image/P8_D3_P2_Low_T2_18_Right/2.png", "Image/P8_D3_P2_Low_T2_18_Right/3.png", "Image/P8_D3_P2_Low_T2_18_Right/4.png", "Image/P8_D3_P2_Low_T2_18_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1523.32849 seconds to 1534.70158 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D2_P5_Medium_T3_13_Right", "images": ["Image/P3_D2_P5_Medium_T3_13_Right/1.png", "Image/P3_D2_P5_Medium_T3_13_Right/2.png", "Image/P3_D2_P5_Medium_T3_13_Right/3.png", "Image/P3_D2_P5_Medium_T3_13_Right/4.png", "Image/P3_D2_P5_Medium_T3_13_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move this item to this table? Finish.\"\nThe frames roughly correspond to the speech interval from 2424.6284 seconds to 2433.4712 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P1_Medium_T3_1_Right", "images": ["Image/P29_D2_P1_Medium_T3_1_Right/1.png", "Image/P29_D2_P1_Medium_T3_1_Right/2.png", "Image/P29_D2_P1_Medium_T3_1_Right/3.png", "Image/P29_D2_P1_Medium_T3_1_Right/4.png", "Image/P29_D2_P1_Medium_T3_1_Right/5.png", "Image/P29_D2_P1_Medium_T3_1_Right/6.png", "Image/P29_D2_P1_Medium_T3_1_Right/7.png", "Image/P29_D2_P1_Medium_T3_1_Right/8.png", "Image/P29_D2_P1_Medium_T3_1_Right/9.png", "Image/P29_D2_P1_Medium_T3_1_Right/10.png", "Image/P29_D2_P1_Medium_T3_1_Right/11.png", "Image/P29_D2_P1_Medium_T3_1_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 320.12199999999996 seconds to 337.6883 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P8_Low_T3_9_Mid", "images": ["Image/P21_D2_P8_Low_T3_9_Mid/1.png", "Image/P21_D2_P8_Low_T3_9_Mid/2.png", "Image/P21_D2_P8_Low_T3_9_Mid/3.png", "Image/P21_D2_P8_Low_T3_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1829.1821 seconds to 1842.4194 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P2_Medium_T2_7_Mid", "images": ["Image/P8_D2_P2_Medium_T2_7_Mid/1.png", "Image/P8_D2_P2_Medium_T2_7_Mid/2.png", "Image/P8_D2_P2_Medium_T2_7_Mid/3.png", "Image/P8_D2_P2_Medium_T2_7_Mid/4.png", "Image/P8_D2_P2_Medium_T2_7_Mid/5.png", "Image/P8_D2_P2_Medium_T2_7_Mid/6.png", "Image/P8_D2_P2_Medium_T2_7_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the green glass in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2277.0136 seconds to 2291.7949000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D3_P3_Medium_T1_27_Right", "images": ["Image/P29_D3_P3_Medium_T1_27_Right/1.png", "Image/P29_D3_P3_Medium_T1_27_Right/2.png", "Image/P29_D3_P3_Medium_T1_27_Right/3.png", "Image/P29_D3_P3_Medium_T1_27_Right/4.png", "Image/P29_D3_P3_Medium_T1_27_Right/5.png", "Image/P29_D3_P3_Medium_T1_27_Right/6.png", "Image/P29_D3_P3_Medium_T1_27_Right/7.png", "Image/P29_D3_P3_Medium_T1_27_Right/8.png", "Image/P29_D3_P3_Medium_T1_27_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Bridie\"\nThe frames roughly correspond to the speech interval from 1357.9761 seconds to 1373.1478999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P3_Low_T1_22_Right", "images": ["Image/P11_D3_P3_Low_T1_22_Right/1.png", "Image/P11_D3_P3_Low_T1_22_Right/2.png", "Image/P11_D3_P3_Low_T1_22_Right/3.png", "Image/P11_D3_P3_Low_T1_22_Right/4.png", "Image/P11_D3_P3_Low_T1_22_Right/5.png", "Image/P11_D3_P3_Low_T1_22_Right/6.png", "Image/P11_D3_P3_Low_T1_22_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from top of the shelf to the bottom of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1431.71972 seconds to 1444.77669 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P6_Medium_T2_20_Mid", "images": ["Image/P24_D1_P6_Medium_T2_20_Mid/1.png", "Image/P24_D1_P6_Medium_T2_20_Mid/2.png", "Image/P24_D1_P6_Medium_T2_20_Mid/3.png", "Image/P24_D1_P6_Medium_T2_20_Mid/4.png", "Image/P24_D1_P6_Medium_T2_20_Mid/5.png", "Image/P24_D1_P6_Medium_T2_20_Mid/6.png", "Image/P24_D1_P6_Medium_T2_20_Mid/7.png", "Image/P24_D1_P6_Medium_T2_20_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this bottle? And finish.\"\nThe frames roughly correspond to the speech interval from 1273.7819 seconds to 1286.8515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P8_High_T3_19_Mid", "images": ["Image/P6_D3_P8_High_T3_19_Mid/1.png", "Image/P6_D3_P8_High_T3_19_Mid/2.png", "Image/P6_D3_P8_High_T3_19_Mid/3.png", "Image/P6_D3_P8_High_T3_19_Mid/4.png", "Image/P6_D3_P8_High_T3_19_Mid/5.png", "Image/P6_D3_P8_High_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 4518.2629 seconds to 4531.683 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P8_Medium_T3_6_Left", "images": ["Image/P13_D2_P8_Medium_T3_6_Left/1.png", "Image/P13_D2_P8_Medium_T3_6_Left/2.png", "Image/P13_D2_P8_Medium_T3_6_Left/3.png", "Image/P13_D2_P8_Medium_T3_6_Left/4.png", "Image/P13_D2_P8_Medium_T3_6_Left/5.png", "Image/P13_D2_P8_Medium_T3_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the left green plant\"\nThe frames roughly correspond to the speech interval from 1059.00596 seconds to 1070.19726 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P8_Low_T2_5_Left", "images": ["Image/P30_D1_P8_Low_T2_5_Left/1.png", "Image/P30_D1_P8_Low_T2_5_Left/2.png", "Image/P30_D1_P8_Low_T2_5_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 843.9983000000001 seconds to 852.4455 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P8_Medium_T1_27_Mid", "images": ["Image/P14_D2_P8_Medium_T1_27_Mid/1.png", "Image/P14_D2_P8_Medium_T1_27_Mid/2.png", "Image/P14_D2_P8_Medium_T1_27_Mid/3.png", "Image/P14_D2_P8_Medium_T1_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1745.0368 seconds to 1754.2156999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D2_P2_Medium_T2_2_Right", "images": ["Image/P29_D2_P2_Medium_T2_2_Right/1.png", "Image/P29_D2_P2_Medium_T2_2_Right/2.png", "Image/P29_D2_P2_Medium_T2_2_Right/3.png", "Image/P29_D2_P2_Medium_T2_2_Right/4.png", "Image/P29_D2_P2_Medium_T2_2_Right/5.png", "Image/P29_D2_P2_Medium_T2_2_Right/6.png", "Image/P29_D2_P2_Medium_T2_2_Right/7.png", "Image/P29_D2_P2_Medium_T2_2_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 111.38022 seconds to 125.48083 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_High_T3_16_Left", "images": ["Image/P8_D3_P3_High_T3_16_Left/1.png", "Image/P8_D3_P3_High_T3_16_Left/2.png", "Image/P8_D3_P3_High_T3_16_Left/3.png", "Image/P8_D3_P3_High_T3_16_Left/4.png", "Image/P8_D3_P3_High_T3_16_Left/5.png", "Image/P8_D3_P3_High_T3_16_Left/6.png", "Image/P8_D3_P3_High_T3_16_Left/7.png", "Image/P8_D3_P3_High_T3_16_Left/8.png", "Image/P8_D3_P3_High_T3_16_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Hey\"\nThe frames roughly correspond to the speech interval from 1638.6703 seconds to 1653.1593 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P9_Medium_T1_12_Right", "images": ["Image/P3_D1_P9_Medium_T1_12_Right/1.png", "Image/P3_D1_P9_Medium_T1_12_Right/2.png", "Image/P3_D1_P9_Medium_T1_12_Right/3.png", "Image/P3_D1_P9_Medium_T1_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Would you move this item to this way?\"\nThe frames roughly correspond to the speech interval from 1425.3941 seconds to 1436.754 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P4_High_T1_6_Right", "images": ["Image/P20_D1_P4_High_T1_6_Right/1.png", "Image/P20_D1_P4_High_T1_6_Right/2.png", "Image/P20_D1_P4_High_T1_6_Right/3.png", "Image/P20_D1_P4_High_T1_6_Right/4.png", "Image/P20_D1_P4_High_T1_6_Right/5.png", "Image/P20_D1_P4_High_T1_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant over here? Thanks. Finish.\"\nThe frames roughly correspond to the speech interval from 212.0009 seconds to 223.8045 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P9_Low_T3_17_Mid", "images": ["Image/P2_D3_P9_Low_T3_17_Mid/1.png", "Image/P2_D3_P9_Low_T3_17_Mid/2.png", "Image/P2_D3_P9_Low_T3_17_Mid/3.png", "Image/P2_D3_P9_Low_T3_17_Mid/4.png", "Image/P2_D3_P9_Low_T3_17_Mid/5.png", "Image/P2_D3_P9_Low_T3_17_Mid/6.png", "Image/P2_D3_P9_Low_T3_17_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please claim that brown mug for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2226.2019 seconds to 2241.8032000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P9_High_T1_14_Mid", "images": ["Image/P8_D2_P9_High_T1_14_Mid/1.png", "Image/P8_D2_P9_High_T1_14_Mid/2.png", "Image/P8_D2_P9_High_T1_14_Mid/3.png", "Image/P8_D2_P9_High_T1_14_Mid/4.png", "Image/P8_D2_P9_High_T1_14_Mid/5.png", "Image/P8_D2_P9_High_T1_14_Mid/6.png", "Image/P8_D2_P9_High_T1_14_Mid/7.png", "Image/P8_D2_P9_High_T1_14_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Dar\"\nThe frames roughly correspond to the speech interval from 2097.05413 seconds to 2110.9881 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P7_Medium_T2_9_Right", "images": ["Image/P3_D2_P7_Medium_T2_9_Right/1.png", "Image/P3_D2_P7_Medium_T2_9_Right/2.png", "Image/P3_D2_P7_Medium_T2_9_Right/3.png", "Image/P3_D2_P7_Medium_T2_9_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2200.05502 seconds to 2210.5496700000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P8_High_T3_14_Left", "images": ["Image/P23_D1_P8_High_T3_14_Left/1.png", "Image/P23_D1_P8_High_T3_14_Left/2.png", "Image/P23_D1_P8_High_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1735.0843 seconds to 1744.1312 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P7_Medium_T3_23_Mid", "images": ["Image/P27_D1_P7_Medium_T3_23_Mid/1.png", "Image/P27_D1_P7_Medium_T3_23_Mid/2.png", "Image/P27_D1_P7_Medium_T3_23_Mid/3.png", "Image/P27_D1_P7_Medium_T3_23_Mid/4.png", "Image/P27_D1_P7_Medium_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off the plant in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 957.7842 seconds to 970.0731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P6_Medium_T2_1_Left", "images": ["Image/P26_D1_P6_Medium_T2_1_Left/1.png", "Image/P26_D1_P6_Medium_T2_1_Left/2.png", "Image/P26_D1_P6_Medium_T2_1_Left/3.png", "Image/P26_D1_P6_Medium_T2_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water in this leftmost jug? Finish.\"\nThe frames roughly correspond to the speech interval from 235.5353 seconds to 247.0173 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P5_Low_T1_20_Left", "images": ["Image/P16_D2_P5_Low_T1_20_Left/1.png", "Image/P16_D2_P5_Low_T1_20_Left/2.png", "Image/P16_D2_P5_Low_T1_20_Left/3.png", "Image/P16_D2_P5_Low_T1_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase over there. Finish.\"\nThe frames roughly correspond to the speech interval from 867.3009000000001 seconds to 877.7813000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P5_Low_T3_21_Right", "images": ["Image/P9_D2_P5_Low_T3_21_Right/1.png", "Image/P9_D2_P5_Low_T3_21_Right/2.png", "Image/P9_D2_P5_Low_T3_21_Right/3.png", "Image/P9_D2_P5_Low_T3_21_Right/4.png", "Image/P9_D2_P5_Low_T3_21_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the bottle on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2679.72853 seconds to 2699.44538 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D1_P7_Medium_T3_6_Right", "images": ["Image/P14_D1_P7_Medium_T3_6_Right/1.png", "Image/P14_D1_P7_Medium_T3_6_Right/2.png", "Image/P14_D1_P7_Medium_T3_6_Right/3.png", "Image/P14_D1_P7_Medium_T3_6_Right/4.png", "Image/P14_D1_P7_Medium_T3_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean that plant up\"\nThe frames roughly correspond to the speech interval from 115.2713 seconds to 126.51917 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P6_Medium_T2_20_Mid", "images": ["Image/P28_D3_P6_Medium_T2_20_Mid/1.png", "Image/P28_D3_P6_Medium_T2_20_Mid/2.png", "Image/P28_D3_P6_Medium_T2_20_Mid/3.png", "Image/P28_D3_P6_Medium_T2_20_Mid/4.png", "Image/P28_D3_P6_Medium_T2_20_Mid/5.png", "Image/P28_D3_P6_Medium_T2_20_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water in the middle vase on that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 234.4214 seconds to 246.0431 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P6_Medium_T2_27_Right", "images": ["Image/P3_D1_P6_Medium_T2_27_Right/1.png", "Image/P3_D1_P6_Medium_T2_27_Right/2.png", "Image/P3_D1_P6_Medium_T2_27_Right/3.png", "Image/P3_D1_P6_Medium_T2_27_Right/4.png", "Image/P3_D1_P6_Medium_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1401.908 seconds to 1412.7524 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D3_P6_Medium_T3_8_Right", "images": ["Image/P27_D3_P6_Medium_T3_8_Right/1.png", "Image/P27_D3_P6_Medium_T3_8_Right/2.png", "Image/P27_D3_P6_Medium_T3_8_Right/3.png", "Image/P27_D3_P6_Medium_T3_8_Right/4.png", "Image/P27_D3_P6_Medium_T3_8_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that third glass over there? Finish.\"\nThe frames roughly correspond to the speech interval from 163.25841000000003 seconds to 173.92108000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P1_Medium_T1_25_Mid", "images": ["Image/P6_D2_P1_Medium_T1_25_Mid/1.png", "Image/P6_D2_P1_Medium_T1_25_Mid/2.png", "Image/P6_D2_P1_Medium_T1_25_Mid/3.png", "Image/P6_D2_P1_Medium_T1_25_Mid/4.png", "Image/P6_D2_P1_Medium_T1_25_Mid/5.png", "Image/P6_D2_P1_Medium_T1_25_Mid/6.png", "Image/P6_D2_P1_Medium_T1_25_Mid/7.png", "Image/P6_D2_P1_Medium_T1_25_Mid/8.png", "Image/P6_D2_P1_Medium_T1_25_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1114.0609 seconds to 1128.9556 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P4_High_T3_1_Left", "images": ["Image/P22_D2_P4_High_T3_1_Left/1.png", "Image/P22_D2_P4_High_T3_1_Left/2.png", "Image/P22_D2_P4_High_T3_1_Left/3.png", "Image/P22_D2_P4_High_T3_1_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 740.8393999999998 seconds to 750.1414 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P3_High_T3_11_Left", "images": ["Image/P26_D1_P3_High_T3_11_Left/1.png", "Image/P26_D1_P3_High_T3_11_Left/2.png", "Image/P26_D1_P3_High_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean this vase? Finish.\"\nThe frames roughly correspond to the speech interval from 42.522829999999985 seconds to 54.27024999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P7_High_T2_23_Right", "images": ["Image/P4_D3_P7_High_T2_23_Right/1.png", "Image/P4_D3_P7_High_T2_23_Right/2.png", "Image/P4_D3_P7_High_T2_23_Right/3.png", "Image/P4_D3_P7_High_T2_23_Right/4.png", "Image/P4_D3_P7_High_T2_23_Right/5.png", "Image/P4_D3_P7_High_T2_23_Right/6.png", "Image/P4_D3_P7_High_T2_23_Right/7.png", "Image/P4_D3_P7_High_T2_23_Right/8.png", "Image/P4_D3_P7_High_T2_23_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start? Can you water the right plant on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 601.5553 seconds to 616.9938999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D1_P3_Medium_T1_26_Right", "images": ["Image/P6_D1_P3_Medium_T1_26_Right/1.png", "Image/P6_D1_P3_Medium_T1_26_Right/2.png", "Image/P6_D1_P3_Medium_T1_26_Right/3.png", "Image/P6_D1_P3_Medium_T1_26_Right/4.png", "Image/P6_D1_P3_Medium_T1_26_Right/5.png", "Image/P6_D1_P3_Medium_T1_26_Right/6.png", "Image/P6_D1_P3_Medium_T1_26_Right/7.png", "Image/P6_D1_P3_Medium_T1_26_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1889.14305 seconds to 1904.71984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P8_High_T3_25_Mid", "images": ["Image/P27_D1_P8_High_T3_25_Mid/1.png", "Image/P27_D1_P8_High_T3_25_Mid/2.png", "Image/P27_D1_P8_High_T3_25_Mid/3.png", "Image/P27_D1_P8_High_T3_25_Mid/4.png", "Image/P27_D1_P8_High_T3_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that white cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1035.8604 seconds to 1046.6756 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D2_P8_High_T3_24_Left", "images": ["Image/P4_D2_P8_High_T3_24_Left/1.png", "Image/P4_D2_P8_High_T3_24_Left/2.png", "Image/P4_D2_P8_High_T3_24_Left/3.png", "Image/P4_D2_P8_High_T3_24_Left/4.png", "Image/P4_D2_P8_High_T3_24_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1763.2036999999998 seconds to 1776.357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P8_High_T1_20_Left", "images": ["Image/P17_D3_P8_High_T1_20_Left/1.png", "Image/P17_D3_P8_High_T1_20_Left/2.png", "Image/P17_D3_P8_High_T1_20_Left/3.png", "Image/P17_D3_P8_High_T1_20_Left/4.png", "Image/P17_D3_P8_High_T1_20_Left/5.png", "Image/P17_D3_P8_High_T1_20_Left/6.png", "Image/P17_D3_P8_High_T1_20_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the white vase on the floor to the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1300.8419 seconds to 1313.7581 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P7_Low_T1_13_Mid", "images": ["Image/P2_D3_P7_Low_T1_13_Mid/1.png", "Image/P2_D3_P7_Low_T1_13_Mid/2.png", "Image/P2_D3_P7_Low_T1_13_Mid/3.png", "Image/P2_D3_P7_Low_T1_13_Mid/4.png", "Image/P2_D3_P7_Low_T1_13_Mid/5.png", "Image/P2_D3_P7_Low_T1_13_Mid/6.png", "Image/P2_D3_P7_Low_T1_13_Mid/7.png", "Image/P2_D3_P7_Low_T1_13_Mid/8.png", "Image/P2_D3_P7_Low_T1_13_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Could you please move that plant\"\nThe frames roughly correspond to the speech interval from 2307.6050000000005 seconds to 2324.8634 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D3_P8_Medium_T2_12_Right", "images": ["Image/P29_D3_P8_Medium_T2_12_Right/1.png", "Image/P29_D3_P8_Medium_T2_12_Right/2.png", "Image/P29_D3_P8_Medium_T2_12_Right/3.png", "Image/P29_D3_P8_Medium_T2_12_Right/4.png", "Image/P29_D3_P8_Medium_T2_12_Right/5.png", "Image/P29_D3_P8_Medium_T2_12_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1404.5372999999995 seconds to 1415.8970999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D2_P6_High_T2_26_Mid", "images": ["Image/P19_D2_P6_High_T2_26_Mid/1.png", "Image/P19_D2_P6_High_T2_26_Mid/2.png", "Image/P19_D2_P6_High_T2_26_Mid/3.png", "Image/P19_D2_P6_High_T2_26_Mid/4.png", "Image/P19_D2_P6_High_T2_26_Mid/5.png", "Image/P19_D2_P6_High_T2_26_Mid/6.png", "Image/P19_D2_P6_High_T2_26_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here. Start.\"\nThe frames roughly correspond to the speech interval from 1801.3015999999998 seconds to 1816.2513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P6_Low_T2_6_Left", "images": ["Image/P1_D3_P6_Low_T2_6_Left/1.png", "Image/P1_D3_P6_Low_T2_6_Left/2.png", "Image/P1_D3_P6_Low_T2_6_Left/3.png", "Image/P1_D3_P6_Low_T2_6_Left/4.png", "Image/P1_D3_P6_Low_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help watering this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1406.1749099999995 seconds to 1416.8937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P7_Medium_T3_20_Left", "images": ["Image/P22_D3_P7_Medium_T3_20_Left/1.png", "Image/P22_D3_P7_Medium_T3_20_Left/2.png", "Image/P22_D3_P7_Medium_T3_20_Left/3.png", "Image/P22_D3_P7_Medium_T3_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 248.0374 seconds to 258.6874 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P5_Medium_T2_10_Mid", "images": ["Image/P2_D3_P5_Medium_T2_10_Mid/1.png", "Image/P2_D3_P5_Medium_T2_10_Mid/2.png", "Image/P2_D3_P5_Medium_T2_10_Mid/3.png", "Image/P2_D3_P5_Medium_T2_10_Mid/4.png", "Image/P2_D3_P5_Medium_T2_10_Mid/5.png", "Image/P2_D3_P5_Medium_T2_10_Mid/6.png", "Image/P2_D3_P5_Medium_T2_10_Mid/7.png", "Image/P2_D3_P5_Medium_T2_10_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into the brown vessel in the very middle for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 2279.0815 seconds to 2293.6136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P8_Low_T3_9_Left", "images": ["Image/P3_D1_P8_Low_T3_9_Left/1.png", "Image/P3_D1_P8_Low_T3_9_Left/2.png", "Image/P3_D1_P8_Low_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1188.03814 seconds to 1198.64666 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D3_P8_Medium_T3_14_Left", "images": ["Image/P19_D3_P8_Medium_T3_14_Left/1.png", "Image/P19_D3_P8_Medium_T3_14_Left/2.png", "Image/P19_D3_P8_Medium_T3_14_Left/3.png", "Image/P19_D3_P8_Medium_T3_14_Left/4.png", "Image/P19_D3_P8_Medium_T3_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2384.6141 seconds to 2398.4357 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P6_High_T1_27_Right", "images": ["Image/P21_D3_P6_High_T1_27_Right/1.png", "Image/P21_D3_P6_High_T1_27_Right/2.png", "Image/P21_D3_P6_High_T1_27_Right/3.png", "Image/P21_D3_P6_High_T1_27_Right/4.png", "Image/P21_D3_P6_High_T1_27_Right/5.png", "Image/P21_D3_P6_High_T1_27_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 546.3743 seconds to 559.5424 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D3_P1_Medium_T2_20_Left", "images": ["Image/P1_D3_P1_Medium_T2_20_Left/1.png", "Image/P1_D3_P1_Medium_T2_20_Left/2.png", "Image/P1_D3_P1_Medium_T2_20_Left/3.png", "Image/P1_D3_P1_Medium_T2_20_Left/4.png", "Image/P1_D3_P1_Medium_T2_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water into the left bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1626.3638 seconds to 1637.2938999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D2_P1_Medium_T3_21_Left", "images": ["Image/P14_D2_P1_Medium_T3_21_Left/1.png", "Image/P14_D2_P1_Medium_T3_21_Left/2.png", "Image/P14_D2_P1_Medium_T3_21_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1572.78968 seconds to 1581.3142999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D3_P8_Medium_T3_18_Right", "images": ["Image/P7_D3_P8_Medium_T3_18_Right/1.png", "Image/P7_D3_P8_Medium_T3_18_Right/2.png", "Image/P7_D3_P8_Medium_T3_18_Right/3.png", "Image/P7_D3_P8_Medium_T3_18_Right/4.png", "Image/P7_D3_P8_Medium_T3_18_Right/5.png", "Image/P7_D3_P8_Medium_T3_18_Right/6.png", "Image/P7_D3_P8_Medium_T3_18_Right/7.png", "Image/P7_D3_P8_Medium_T3_18_Right/8.png", "Image/P7_D3_P8_Medium_T3_18_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 2182.8641 seconds to 2199.5820000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P1_High_T2_16_Mid", "images": ["Image/P3_D3_P1_High_T2_16_Mid/1.png", "Image/P3_D3_P1_High_T2_16_Mid/2.png", "Image/P3_D3_P1_High_T2_16_Mid/3.png", "Image/P3_D3_P1_High_T2_16_Mid/4.png", "Image/P3_D3_P1_High_T2_16_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Star\"\nThe frames roughly correspond to the speech interval from 1052.9526 seconds to 1063.7145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D1_P7_High_T3_19_Left", "images": ["Image/P30_D1_P7_High_T3_19_Left/1.png", "Image/P30_D1_P7_High_T3_19_Left/2.png", "Image/P30_D1_P7_High_T3_19_Left/3.png", "Image/P30_D1_P7_High_T3_19_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1025.8194 seconds to 1034.3242 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D1_P9_High_T1_23_Right", "images": ["Image/P13_D1_P9_High_T1_23_Right/1.png", "Image/P13_D1_P9_High_T1_23_Right/2.png", "Image/P13_D1_P9_High_T1_23_Right/3.png", "Image/P13_D1_P9_High_T1_23_Right/4.png", "Image/P13_D1_P9_High_T1_23_Right/5.png", "Image/P13_D1_P9_High_T1_23_Right/6.png", "Image/P13_D1_P9_High_T1_23_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this plant down here up to here\"\nThe frames roughly correspond to the speech interval from 593.6483000000001 seconds to 605.6753 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P7_High_T2_10_Left", "images": ["Image/P2_D3_P7_High_T2_10_Left/1.png", "Image/P2_D3_P7_High_T2_10_Left/2.png", "Image/P2_D3_P7_High_T2_10_Left/3.png", "Image/P2_D3_P7_High_T2_10_Left/4.png", "Image/P2_D3_P7_High_T2_10_Left/5.png", "Image/P2_D3_P7_High_T2_10_Left/6.png", "Image/P2_D3_P7_High_T2_10_Left/7.png", "Image/P2_D3_P7_High_T2_10_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour some water into that brown bottle? Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 2054.2478 seconds to 2067.6918 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D2_P1_High_T1_15_Right", "images": ["Image/P17_D2_P1_High_T1_15_Right/1.png", "Image/P17_D2_P1_High_T1_15_Right/2.png", "Image/P17_D2_P1_High_T1_15_Right/3.png", "Image/P17_D2_P1_High_T1_15_Right/4.png", "Image/P17_D2_P1_High_T1_15_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant down to the bottom shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 444.2913 seconds to 456.1793 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P8_High_T2_19_Right", "images": ["Image/P8_D2_P8_High_T2_19_Right/1.png", "Image/P8_D2_P8_High_T2_19_Right/2.png", "Image/P8_D2_P8_High_T2_19_Right/3.png", "Image/P8_D2_P8_High_T2_19_Right/4.png", "Image/P8_D2_P8_High_T2_19_Right/5.png", "Image/P8_D2_P8_High_T2_19_Right/6.png", "Image/P8_D2_P8_High_T2_19_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the right bottom under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2316.3196000000003 seconds to 2329.5619 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P9_High_T2_3_Left", "images": ["Image/P6_D2_P9_High_T2_3_Left/1.png", "Image/P6_D2_P9_High_T2_3_Left/2.png", "Image/P6_D2_P9_High_T2_3_Left/3.png", "Image/P6_D2_P9_High_T2_3_Left/4.png", "Image/P6_D2_P9_High_T2_3_Left/5.png", "Image/P6_D2_P9_High_T2_3_Left/6.png", "Image/P6_D2_P9_High_T2_3_Left/7.png", "Image/P6_D2_P9_High_T2_3_Left/8.png", "Image/P6_D2_P9_High_T2_3_Left/9.png", "Image/P6_D2_P9_High_T2_3_Left/10.png", "Image/P6_D2_P9_High_T2_3_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1086.9128 seconds to 1104.1053 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P6_Low_T2_12_Right", "images": ["Image/P16_D2_P6_Low_T2_12_Right/1.png", "Image/P16_D2_P6_Low_T2_12_Right/2.png", "Image/P16_D2_P6_Low_T2_12_Right/3.png", "Image/P16_D2_P6_Low_T2_12_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that mug. Finish\"\nThe frames roughly correspond to the speech interval from 851.1115 seconds to 860.9939 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P3_High_T1_24_Right", "images": ["Image/P16_D3_P3_High_T1_24_Right/1.png", "Image/P16_D3_P3_High_T1_24_Right/2.png", "Image/P16_D3_P3_High_T1_24_Right/3.png", "Image/P16_D3_P3_High_T1_24_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that vase down there. Finish.\"\nThe frames roughly correspond to the speech interval from 499.6341 seconds to 509.0719 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D2_P1_Low_T3_20_Mid", "images": ["Image/P22_D2_P1_Low_T3_20_Mid/1.png", "Image/P22_D2_P1_Low_T3_20_Mid/2.png", "Image/P22_D2_P1_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 724.9465999999999 seconds to 734.4264999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P2_High_T1_9_Left", "images": ["Image/P15_D1_P2_High_T1_9_Left/1.png", "Image/P15_D1_P2_High_T1_9_Left/2.png", "Image/P15_D1_P2_High_T1_9_Left/3.png", "Image/P15_D1_P2_High_T1_9_Left/4.png", "Image/P15_D1_P2_High_T1_9_Left/5.png", "Image/P15_D1_P2_High_T1_9_Left/6.png", "Image/P15_D1_P2_High_T1_9_Left/7.png", "Image/P15_D1_P2_High_T1_9_Left/8.png", "Image/P15_D1_P2_High_T1_9_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1639.4692699999998 seconds to 1653.86187 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P2_Medium_T2_13_Right", "images": ["Image/P17_D2_P2_Medium_T2_13_Right/1.png", "Image/P17_D2_P2_Medium_T2_13_Right/2.png", "Image/P17_D2_P2_Medium_T2_13_Right/3.png", "Image/P17_D2_P2_Medium_T2_13_Right/4.png", "Image/P17_D2_P2_Medium_T2_13_Right/5.png", "Image/P17_D2_P2_Medium_T2_13_Right/6.png", "Image/P17_D2_P2_Medium_T2_13_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in the right-nose plant on the top shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 283.37710000000004 seconds to 297.3963 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D3_P3_High_T2_21_Mid", "images": ["Image/P11_D3_P3_High_T2_21_Mid/1.png", "Image/P11_D3_P3_High_T2_21_Mid/2.png", "Image/P11_D3_P3_High_T2_21_Mid/3.png", "Image/P11_D3_P3_High_T2_21_Mid/4.png", "Image/P11_D3_P3_High_T2_21_Mid/5.png", "Image/P11_D3_P3_High_T2_21_Mid/6.png", "Image/P11_D3_P3_High_T2_21_Mid/7.png", "Image/P11_D3_P3_High_T2_21_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the white glass on the top of the shell. Finish.\"\nThe frames roughly correspond to the speech interval from 1532.8619 seconds to 1546.2803 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P9_Low_T1_2_Mid", "images": ["Image/P1_D3_P9_Low_T1_2_Mid/1.png", "Image/P1_D3_P9_Low_T1_2_Mid/2.png", "Image/P1_D3_P9_Low_T1_2_Mid/3.png", "Image/P1_D3_P9_Low_T1_2_Mid/4.png", "Image/P1_D3_P9_Low_T1_2_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move the bottle to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1769.1072 seconds to 1779.9112999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P1_Low_T3_8_Right", "images": ["Image/P9_D3_P1_Low_T3_8_Right/1.png", "Image/P9_D3_P1_Low_T3_8_Right/2.png", "Image/P9_D3_P1_Low_T3_8_Right/3.png", "Image/P9_D3_P1_Low_T3_8_Right/4.png", "Image/P9_D3_P1_Low_T3_8_Right/5.png", "Image/P9_D3_P1_Low_T3_8_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the green cup? Finish.\"\nThe frames roughly correspond to the speech interval from 440.3557 seconds to 452.8816 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D2_P2_Medium_T1_3_Mid", "images": ["Image/P30_D2_P2_Medium_T1_3_Mid/1.png", "Image/P30_D2_P2_Medium_T1_3_Mid/2.png", "Image/P30_D2_P2_Medium_T1_3_Mid/3.png", "Image/P30_D2_P2_Medium_T1_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 520.4847 seconds to 533.4579 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P5_High_T3_19_Right", "images": ["Image/P27_D1_P5_High_T3_19_Right/1.png", "Image/P27_D1_P5_High_T3_19_Right/2.png", "Image/P27_D1_P5_High_T3_19_Right/3.png", "Image/P27_D1_P5_High_T3_19_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this glass? Finish.\"\nThe frames roughly correspond to the speech interval from 722.9256 seconds to 733.1984600000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P1_High_T1_20_Right", "images": ["Image/P21_D3_P1_High_T1_20_Right/1.png", "Image/P21_D3_P1_High_T1_20_Right/2.png", "Image/P21_D3_P1_High_T1_20_Right/3.png", "Image/P21_D3_P1_High_T1_20_Right/4.png", "Image/P21_D3_P1_High_T1_20_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 383.3849 seconds to 394.7849 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D1_P2_Low_T3_20_Right", "images": ["Image/P9_D1_P2_Low_T3_20_Right/1.png", "Image/P9_D1_P2_Low_T3_20_Right/2.png", "Image/P9_D1_P2_Low_T3_20_Right/3.png", "Image/P9_D1_P2_Low_T3_20_Right/4.png", "Image/P9_D1_P2_Low_T3_20_Right/5.png", "Image/P9_D1_P2_Low_T3_20_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1582.7448 seconds to 1595.2421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D1_P3_Low_T2_15_Right", "images": ["Image/P3_D1_P3_Low_T2_15_Right/1.png", "Image/P3_D1_P3_Low_T2_15_Right/2.png", "Image/P3_D1_P3_Low_T2_15_Right/3.png", "Image/P3_D1_P3_Low_T2_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to this item? Finish.\"\nThe frames roughly correspond to the speech interval from 1674.2921 seconds to 1685.3735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P7_Medium_T1_17_Right", "images": ["Image/P29_D1_P7_Medium_T1_17_Right/1.png", "Image/P29_D1_P7_Medium_T1_17_Right/2.png", "Image/P29_D1_P7_Medium_T1_17_Right/3.png", "Image/P29_D1_P7_Medium_T1_17_Right/4.png", "Image/P29_D1_P7_Medium_T1_17_Right/5.png", "Image/P29_D1_P7_Medium_T1_17_Right/6.png", "Image/P29_D1_P7_Medium_T1_17_Right/7.png", "Image/P29_D1_P7_Medium_T1_17_Right/8.png", "Image/P29_D1_P7_Medium_T1_17_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2712.8134999999997 seconds to 2728.9896999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P6_High_T1_3_Right", "images": ["Image/P7_D3_P6_High_T1_3_Right/1.png", "Image/P7_D3_P6_High_T1_3_Right/2.png", "Image/P7_D3_P6_High_T1_3_Right/3.png", "Image/P7_D3_P6_High_T1_3_Right/4.png", "Image/P7_D3_P6_High_T1_3_Right/5.png", "Image/P7_D3_P6_High_T1_3_Right/6.png", "Image/P7_D3_P6_High_T1_3_Right/7.png", "Image/P7_D3_P6_High_T1_3_Right/8.png", "Image/P7_D3_P6_High_T1_3_Right/9.png", "Image/P7_D3_P6_High_T1_3_Right/10.png", "Image/P7_D3_P6_High_T1_3_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Say Friday. Move the green bottle to the disk on my left-hand side. Finish.\"\nThe frames roughly correspond to the speech interval from 1695.9311 seconds to 1716.5618000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P8_Medium_T1_9_Mid", "images": ["Image/P4_D2_P8_Medium_T1_9_Mid/1.png", "Image/P4_D2_P8_Medium_T1_9_Mid/2.png", "Image/P4_D2_P8_Medium_T1_9_Mid/3.png", "Image/P4_D2_P8_Medium_T1_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green cap to the top? Finish.\"\nThe frames roughly correspond to the speech interval from 1463.6741599999998 seconds to 1476.7852999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P3_Medium_T1_1_Left", "images": ["Image/P30_D2_P3_Medium_T1_1_Left/1.png", "Image/P30_D2_P3_Medium_T1_1_Left/2.png", "Image/P30_D2_P3_Medium_T1_1_Left/3.png", "Image/P30_D2_P3_Medium_T1_1_Left/4.png", "Image/P30_D2_P3_Medium_T1_1_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that base down there. Finish.\"\nThe frames roughly correspond to the speech interval from 302.3319 seconds to 315.76329999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D1_P3_Low_T3_18_Mid", "images": ["Image/P19_D1_P3_Low_T3_18_Mid/1.png", "Image/P19_D1_P3_Low_T3_18_Mid/2.png", "Image/P19_D1_P3_Low_T3_18_Mid/3.png", "Image/P19_D1_P3_Low_T3_18_Mid/4.png", "Image/P19_D1_P3_Low_T3_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean this vase here. Finish.\"\nThe frames roughly correspond to the speech interval from 758.2565999999999 seconds to 769.5174 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D2_P6_Low_T2_14_Right", "images": ["Image/P13_D2_P6_Low_T2_14_Right/1.png", "Image/P13_D2_P6_Low_T2_14_Right/2.png", "Image/P13_D2_P6_Low_T2_14_Right/3.png", "Image/P13_D2_P6_Low_T2_14_Right/4.png", "Image/P13_D2_P6_Low_T2_14_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you water the plant please over here? Finish.\"\nThe frames roughly correspond to the speech interval from 1175.31996 seconds to 1185.2868600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P6_Low_T3_22_Right", "images": ["Image/P21_D2_P6_Low_T3_22_Right/1.png", "Image/P21_D2_P6_Low_T3_22_Right/2.png", "Image/P21_D2_P6_Low_T3_22_Right/3.png", "Image/P21_D2_P6_Low_T3_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1758.4554 seconds to 1773.7949 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D2_P8_High_T2_3_Left", "images": ["Image/P23_D2_P8_High_T2_3_Left/1.png", "Image/P23_D2_P8_High_T2_3_Left/2.png", "Image/P23_D2_P8_High_T2_3_Left/3.png", "Image/P23_D2_P8_High_T2_3_Left/4.png", "Image/P23_D2_P8_High_T2_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 250.10687 seconds to 260.93708 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P5_Low_T3_3_Left", "images": ["Image/P23_D2_P5_Low_T3_3_Left/1.png", "Image/P23_D2_P5_Low_T3_3_Left/2.png", "Image/P23_D2_P5_Low_T3_3_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 360.3538999999999 seconds to 369.1648 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D1_P7_Medium_T2_4_Mid", "images": ["Image/P18_D1_P7_Medium_T2_4_Mid/1.png", "Image/P18_D1_P7_Medium_T2_4_Mid/2.png", "Image/P18_D1_P7_Medium_T2_4_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that middle plant. Finish.\"\nThe frames roughly correspond to the speech interval from 920.339 seconds to 928.9119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P7_Medium_T3_2_Left", "images": ["Image/P18_D3_P7_Medium_T3_2_Left/1.png", "Image/P18_D3_P7_Medium_T3_2_Left/2.png", "Image/P18_D3_P7_Medium_T3_2_Left/3.png", "Image/P18_D3_P7_Medium_T3_2_Left/4.png", "Image/P18_D3_P7_Medium_T3_2_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up the bottle on the far left-hand side of the room. Finish.\"\nThe frames roughly correspond to the speech interval from 1620.9462 seconds to 1632.3744000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D2_P8_High_T3_17_Right", "images": ["Image/P5_D2_P8_High_T3_17_Right/1.png", "Image/P5_D2_P8_High_T3_17_Right/2.png", "Image/P5_D2_P8_High_T3_17_Right/3.png", "Image/P5_D2_P8_High_T3_17_Right/4.png", "Image/P5_D2_P8_High_T3_17_Right/5.png", "Image/P5_D2_P8_High_T3_17_Right/6.png", "Image/P5_D2_P8_High_T3_17_Right/7.png", "Image/P5_D2_P8_High_T3_17_Right/8.png", "Image/P5_D2_P8_High_T3_17_Right/9.png", "Image/P5_D2_P8_High_T3_17_Right/10.png", "Image/P5_D2_P8_High_T3_17_Right/11.png", "Image/P5_D2_P8_High_T3_17_Right/12.png", "Image/P5_D2_P8_High_T3_17_Right/13.png", "Image/P5_D2_P8_High_T3_17_Right/14.png", "Image/P5_D2_P8_High_T3_17_Right/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Clean the cup in front of me\"\nThe frames roughly correspond to the speech interval from 515.409 seconds to 537.6569999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P9_Medium_T1_12_Right", "images": ["Image/P4_D3_P9_Medium_T1_12_Right/1.png", "Image/P4_D3_P9_Medium_T1_12_Right/2.png", "Image/P4_D3_P9_Medium_T1_12_Right/3.png", "Image/P4_D3_P9_Medium_T1_12_Right/4.png", "Image/P4_D3_P9_Medium_T1_12_Right/5.png", "Image/P4_D3_P9_Medium_T1_12_Right/6.png", "Image/P4_D3_P9_Medium_T1_12_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right item to the top of the shelter? Finish.\"\nThe frames roughly correspond to the speech interval from 471.04979999999995 seconds to 485.0938999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P2_Medium_T2_19_Left", "images": ["Image/P11_D2_P2_Medium_T2_19_Left/1.png", "Image/P11_D2_P2_Medium_T2_19_Left/2.png", "Image/P11_D2_P2_Medium_T2_19_Left/3.png", "Image/P11_D2_P2_Medium_T2_19_Left/4.png", "Image/P11_D2_P2_Medium_T2_19_Left/5.png", "Image/P11_D2_P2_Medium_T2_19_Left/6.png", "Image/P11_D2_P2_Medium_T2_19_Left/7.png", "Image/P11_D2_P2_Medium_T2_19_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour the water in the first white glass from left side. Finish.\"\nThe frames roughly correspond to the speech interval from 1055.611 seconds to 1072.1086 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P7_Low_T3_27_Left", "images": ["Image/P7_D3_P7_Low_T3_27_Left/1.png", "Image/P7_D3_P7_Low_T3_27_Left/2.png", "Image/P7_D3_P7_Low_T3_27_Left/3.png", "Image/P7_D3_P7_Low_T3_27_Left/4.png", "Image/P7_D3_P7_Low_T3_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey Friday\"\nThe frames roughly correspond to the speech interval from 1908.3361 seconds to 1921.435 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D2_P3_Medium_T1_13_Left", "images": ["Image/P29_D2_P3_Medium_T1_13_Left/1.png", "Image/P29_D2_P3_Medium_T1_13_Left/2.png", "Image/P29_D2_P3_Medium_T1_13_Left/3.png", "Image/P29_D2_P3_Medium_T1_13_Left/4.png", "Image/P29_D2_P3_Medium_T1_13_Left/5.png", "Image/P29_D2_P3_Medium_T1_13_Left/6.png", "Image/P29_D2_P3_Medium_T1_13_Left/7.png", "Image/P29_D2_P3_Medium_T1_13_Left/8.png", "Image/P29_D2_P3_Medium_T1_13_Left/9.png", "Image/P29_D2_P3_Medium_T1_13_Left/10.png", "Image/P29_D2_P3_Medium_T1_13_Left/11.png", "Image/P29_D2_P3_Medium_T1_13_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 597.3088 seconds to 614.7917 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P4_Low_T1_24_Mid", "images": ["Image/P24_D3_P4_Low_T1_24_Mid/1.png", "Image/P24_D3_P4_Low_T1_24_Mid/2.png", "Image/P24_D3_P4_Low_T1_24_Mid/3.png", "Image/P24_D3_P4_Low_T1_24_Mid/4.png", "Image/P24_D3_P4_Low_T1_24_Mid/5.png", "Image/P24_D3_P4_Low_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this planet to this shell? Finish.\"\nThe frames roughly correspond to the speech interval from 1850.1977 seconds to 1861.0967 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P1_High_T2_12_Right", "images": ["Image/P13_D1_P1_High_T2_12_Right/1.png", "Image/P13_D1_P1_High_T2_12_Right/2.png", "Image/P13_D1_P1_High_T2_12_Right/3.png", "Image/P13_D1_P1_High_T2_12_Right/4.png", "Image/P13_D1_P1_High_T2_12_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you pour water into the brown jug here\"\nThe frames roughly correspond to the speech interval from 311.6313 seconds to 322.837 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P3_High_T1_24_Left", "images": ["Image/P25_D1_P3_High_T1_24_Left/1.png", "Image/P25_D1_P3_High_T1_24_Left/2.png", "Image/P25_D1_P3_High_T1_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 318.4848 seconds to 326.8225 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P7_High_T2_22_Left", "images": ["Image/P21_D2_P7_High_T2_22_Left/1.png", "Image/P21_D2_P7_High_T2_22_Left/2.png", "Image/P21_D2_P7_High_T2_22_Left/3.png", "Image/P21_D2_P7_High_T2_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1742.3352 seconds to 1752.303 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P4_Low_T2_7_Left", "images": ["Image/P21_D1_P4_Low_T2_7_Left/1.png", "Image/P21_D1_P4_Low_T2_7_Left/2.png", "Image/P21_D1_P4_Low_T2_7_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1067.4926 seconds to 1077.1810999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P3_Medium_T2_4_Mid", "images": ["Image/P3_D3_P3_Medium_T2_4_Mid/1.png", "Image/P3_D3_P3_Medium_T2_4_Mid/2.png", "Image/P3_D3_P3_Medium_T2_4_Mid/3.png", "Image/P3_D3_P3_Medium_T2_4_Mid/4.png", "Image/P3_D3_P3_Medium_T2_4_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the second item? Finish.\"\nThe frames roughly correspond to the speech interval from 670.8774 seconds to 681.8191999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D2_P4_High_T1_15_Left", "images": ["Image/P16_D2_P4_High_T1_15_Left/1.png", "Image/P16_D2_P4_High_T1_15_Left/2.png", "Image/P16_D2_P4_High_T1_15_Left/3.png", "Image/P16_D2_P4_High_T1_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move that plant over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1025.4721 seconds to 1035.2984 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D2_P9_Medium_T1_12_Left", "images": ["Image/P18_D2_P9_Medium_T1_12_Left/1.png", "Image/P18_D2_P9_Medium_T1_12_Left/2.png", "Image/P18_D2_P9_Medium_T1_12_Left/3.png", "Image/P18_D2_P9_Medium_T1_12_Left/4.png", "Image/P18_D2_P9_Medium_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move those vases to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 38.534280000000024 seconds to 50.11516000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D2_P4_Medium_T1_25_Left", "images": ["Image/P23_D2_P4_Medium_T1_25_Left/1.png", "Image/P23_D2_P4_Medium_T1_25_Left/2.png", "Image/P23_D2_P4_Medium_T1_25_Left/3.png", "Image/P23_D2_P4_Medium_T1_25_Left/4.png", "Image/P23_D2_P4_Medium_T1_25_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the one on the left over there. Finish.\"\nThe frames roughly correspond to the speech interval from 324.73303999999996 seconds to 335.9104 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D2_P2_High_T1_14_Right", "images": ["Image/P9_D2_P2_High_T1_14_Right/1.png", "Image/P9_D2_P2_High_T1_14_Right/2.png", "Image/P9_D2_P2_High_T1_14_Right/3.png", "Image/P9_D2_P2_High_T1_14_Right/4.png", "Image/P9_D2_P2_High_T1_14_Right/5.png", "Image/P9_D2_P2_High_T1_14_Right/6.png", "Image/P9_D2_P2_High_T1_14_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move this plant to the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2920.8252 seconds to 2934.1009 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P2_Medium_T3_21_Left", "images": ["Image/P7_D2_P2_Medium_T3_21_Left/1.png", "Image/P7_D2_P2_Medium_T3_21_Left/2.png", "Image/P7_D2_P2_Medium_T3_21_Left/3.png", "Image/P7_D2_P2_Medium_T3_21_Left/4.png", "Image/P7_D2_P2_Medium_T3_21_Left/5.png", "Image/P7_D2_P2_Medium_T3_21_Left/6.png", "Image/P7_D2_P2_Medium_T3_21_Left/7.png", "Image/P7_D2_P2_Medium_T3_21_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 919.21951 seconds to 934.62851 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P2_Low_T3_15_Mid", "images": ["Image/P8_D2_P2_Low_T3_15_Mid/1.png", "Image/P8_D2_P2_Low_T3_15_Mid/2.png", "Image/P8_D2_P2_Low_T3_15_Mid/3.png", "Image/P8_D2_P2_Low_T3_15_Mid/4.png", "Image/P8_D2_P2_Low_T3_15_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Star. Could you help me with cleaning that planet? Finish.\"\nThe frames roughly correspond to the speech interval from 2137.7707 seconds to 2148.3483 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P6_Medium_T2_27_Left", "images": ["Image/P28_D1_P6_Medium_T2_27_Left/1.png", "Image/P28_D1_P6_Medium_T2_27_Left/2.png", "Image/P28_D1_P6_Medium_T2_27_Left/3.png", "Image/P28_D1_P6_Medium_T2_27_Left/4.png", "Image/P28_D1_P6_Medium_T2_27_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour water into this cup. Finish.\"\nThe frames roughly correspond to the speech interval from 1770.522 seconds to 1780.4898 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D3_P8_Medium_T2_14_Right", "images": ["Image/P22_D3_P8_Medium_T2_14_Right/1.png", "Image/P22_D3_P8_Medium_T2_14_Right/2.png", "Image/P22_D3_P8_Medium_T2_14_Right/3.png", "Image/P22_D3_P8_Medium_T2_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the little plant on the right. Finish.\"\nThe frames roughly correspond to the speech interval from 87.84753 seconds to 98.10577 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P8_High_T1_19_Mid", "images": ["Image/P18_D3_P8_High_T1_19_Mid/1.png", "Image/P18_D3_P8_High_T1_19_Mid/2.png", "Image/P18_D3_P8_High_T1_19_Mid/3.png", "Image/P18_D3_P8_High_T1_19_Mid/4.png", "Image/P18_D3_P8_High_T1_19_Mid/5.png", "Image/P18_D3_P8_High_T1_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1229.75535 seconds to 1240.76658 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P8_Medium_T1_10_Mid", "images": ["Image/P27_D3_P8_Medium_T1_10_Mid/1.png", "Image/P27_D3_P8_Medium_T1_10_Mid/2.png", "Image/P27_D3_P8_Medium_T1_10_Mid/3.png", "Image/P27_D3_P8_Medium_T1_10_Mid/4.png", "Image/P27_D3_P8_Medium_T1_10_Mid/5.png", "Image/P27_D3_P8_Medium_T1_10_Mid/6.png", "Image/P27_D3_P8_Medium_T1_10_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you get that middle red cup and put it on top of the cabinet? Finished.\"\nThe frames roughly correspond to the speech interval from 122.32064000000004 seconds to 135.12613000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P4_Medium_T2_25_Right", "images": ["Image/P26_D2_P4_Medium_T2_25_Right/1.png", "Image/P26_D2_P4_Medium_T2_25_Right/2.png", "Image/P26_D2_P4_Medium_T2_25_Right/3.png", "Image/P26_D2_P4_Medium_T2_25_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please pour water into the cup on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 1562.0863 seconds to 1571.4544 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D2_P1_Medium_T3_10_Mid", "images": ["Image/P7_D2_P1_Medium_T3_10_Mid/1.png", "Image/P7_D2_P1_Medium_T3_10_Mid/2.png", "Image/P7_D2_P1_Medium_T3_10_Mid/3.png", "Image/P7_D2_P1_Medium_T3_10_Mid/4.png", "Image/P7_D2_P1_Medium_T3_10_Mid/5.png", "Image/P7_D2_P1_Medium_T3_10_Mid/6.png", "Image/P7_D2_P1_Medium_T3_10_Mid/7.png", "Image/P7_D2_P1_Medium_T3_10_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Before they brush the brown bottle\"\nThe frames roughly correspond to the speech interval from 893.1066800000002 seconds to 909.51707 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D3_P9_Medium_T1_25_Mid", "images": ["Image/P27_D3_P9_Medium_T1_25_Mid/1.png", "Image/P27_D3_P9_Medium_T1_25_Mid/2.png", "Image/P27_D3_P9_Medium_T1_25_Mid/3.png", "Image/P27_D3_P9_Medium_T1_25_Mid/4.png", "Image/P27_D3_P9_Medium_T1_25_Mid/5.png", "Image/P27_D3_P9_Medium_T1_25_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the middle glass to the top of that cabinet? Finish\"\nThe frames roughly correspond to the speech interval from 205.25590000000005 seconds to 219.8001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D2_P3_Medium_T2_16_Right", "images": ["Image/P19_D2_P3_Medium_T2_16_Right/1.png", "Image/P19_D2_P3_Medium_T2_16_Right/2.png", "Image/P19_D2_P3_Medium_T2_16_Right/3.png", "Image/P19_D2_P3_Medium_T2_16_Right/4.png", "Image/P19_D2_P3_Medium_T2_16_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this container here. Finish.\"\nThe frames roughly correspond to the speech interval from 1496.1789 seconds to 1509.9287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P3_Low_T2_13_Left", "images": ["Image/P20_D2_P3_Low_T2_13_Left/1.png", "Image/P20_D2_P3_Low_T2_13_Left/2.png", "Image/P20_D2_P3_Low_T2_13_Left/3.png", "Image/P20_D2_P3_Low_T2_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 3257.8583000000003 seconds to 3267.3666000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P4_High_T2_5_Mid", "images": ["Image/P14_D3_P4_High_T2_5_Mid/1.png", "Image/P14_D3_P4_High_T2_5_Mid/2.png", "Image/P14_D3_P4_High_T2_5_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1147.50746 seconds to 1156.01856 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P6_High_T2_13_Left", "images": ["Image/P9_D1_P6_High_T2_13_Left/1.png", "Image/P9_D1_P6_High_T2_13_Left/2.png", "Image/P9_D1_P6_High_T2_13_Left/3.png", "Image/P9_D1_P6_High_T2_13_Left/4.png", "Image/P9_D1_P6_High_T2_13_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1461.9725 seconds to 1474.0805 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P9_High_T3_6_Mid", "images": ["Image/P14_D1_P9_High_T3_6_Mid/1.png", "Image/P14_D1_P9_High_T3_6_Mid/2.png", "Image/P14_D1_P9_High_T3_6_Mid/3.png", "Image/P14_D1_P9_High_T3_6_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that plant up for me. Finish.\"\nThe frames roughly correspond to the speech interval from 517.7943 seconds to 527.4680000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P3_Low_T1_11_Right", "images": ["Image/P19_D2_P3_Low_T1_11_Right/1.png", "Image/P19_D2_P3_Low_T1_11_Right/2.png", "Image/P19_D2_P3_Low_T1_11_Right/3.png", "Image/P19_D2_P3_Low_T1_11_Right/4.png", "Image/P19_D2_P3_Low_T1_11_Right/5.png", "Image/P19_D2_P3_Low_T1_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. And please move this container down here. Finish.\"\nThe frames roughly correspond to the speech interval from 1431.6165999999998 seconds to 1444.5767999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P5_Low_T2_14_Left", "images": ["Image/P24_D3_P5_Low_T2_14_Left/1.png", "Image/P24_D3_P5_Low_T2_14_Left/2.png", "Image/P24_D3_P5_Low_T2_14_Left/3.png", "Image/P24_D3_P5_Low_T2_14_Left/4.png", "Image/P24_D3_P5_Low_T2_14_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water to this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1923.9581 seconds to 1934.0501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P1_Medium_T1_12_Left", "images": ["Image/P22_D2_P1_Medium_T1_12_Left/1.png", "Image/P22_D2_P1_Medium_T1_12_Left/2.png", "Image/P22_D2_P1_Medium_T1_12_Left/3.png", "Image/P22_D2_P1_Medium_T1_12_Left/4.png", "Image/P22_D2_P1_Medium_T1_12_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that vase to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 602.6470699999999 seconds to 612.96084 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P9_D3_P7_Low_T1_11_Right", "images": ["Image/P9_D3_P7_Low_T1_11_Right/1.png", "Image/P9_D3_P7_Low_T1_11_Right/2.png", "Image/P9_D3_P7_Low_T1_11_Right/3.png", "Image/P9_D3_P7_Low_T1_11_Right/4.png", "Image/P9_D3_P7_Low_T1_11_Right/5.png", "Image/P9_D3_P7_Low_T1_11_Right/6.png", "Image/P9_D3_P7_Low_T1_11_Right/7.png", "Image/P9_D3_P7_Low_T1_11_Right/8.png", "Image/P9_D3_P7_Low_T1_11_Right/9.png", "Image/P9_D3_P7_Low_T1_11_Right/10.png", "Image/P9_D3_P7_Low_T1_11_Right/11.png", "Image/P9_D3_P7_Low_T1_11_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the red bottle from the floor to the top layer of the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1004.8549 seconds to 1024.9202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D2_P4_Low_T3_16_Mid", "images": ["Image/P4_D2_P4_Low_T3_16_Mid/1.png", "Image/P4_D2_P4_Low_T3_16_Mid/2.png", "Image/P4_D2_P4_Low_T3_16_Mid/3.png", "Image/P4_D2_P4_Low_T3_16_Mid/4.png", "Image/P4_D2_P4_Low_T3_16_Mid/5.png", "Image/P4_D2_P4_Low_T3_16_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean that bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1967.9933 seconds to 1981.717 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P3_Low_T2_2_Left", "images": ["Image/P14_D2_P3_Low_T2_2_Left/1.png", "Image/P14_D2_P3_Low_T2_2_Left/2.png", "Image/P14_D2_P3_Low_T2_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1883.1797 seconds to 1891.0933999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P3_High_T1_4_Left", "images": ["Image/P22_D2_P3_High_T1_4_Left/1.png", "Image/P22_D2_P3_High_T1_4_Left/2.png", "Image/P22_D2_P3_High_T1_4_Left/3.png", "Image/P22_D2_P3_High_T1_4_Left/4.png", "Image/P22_D2_P3_High_T1_4_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant to the bottom of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 636.2492899999999 seconds to 645.8528699999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P5_Low_T2_18_Mid", "images": ["Image/P28_D3_P5_Low_T2_18_Mid/1.png", "Image/P28_D3_P5_Low_T2_18_Mid/2.png", "Image/P28_D3_P5_Low_T2_18_Mid/3.png", "Image/P28_D3_P5_Low_T2_18_Mid/4.png", "Image/P28_D3_P5_Low_T2_18_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish\"\nThe frames roughly correspond to the speech interval from 508.6707 seconds to 518.7209 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D2_P2_Low_T1_20_Mid", "images": ["Image/P1_D2_P2_Low_T1_20_Mid/1.png", "Image/P1_D2_P2_Low_T1_20_Mid/2.png", "Image/P1_D2_P2_Low_T1_20_Mid/3.png", "Image/P1_D2_P2_Low_T1_20_Mid/4.png", "Image/P1_D2_P2_Low_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you help move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 986.0394 seconds to 996.7434 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P7_Low_T3_27_Mid", "images": ["Image/P27_D2_P7_Low_T3_27_Mid/1.png", "Image/P27_D2_P7_Low_T3_27_Mid/2.png", "Image/P27_D2_P7_Low_T3_27_Mid/3.png", "Image/P27_D2_P7_Low_T3_27_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that cup? Finish\"\nThe frames roughly correspond to the speech interval from 1670.861 seconds to 1680.7307999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P2_Medium_T2_4_Mid", "images": ["Image/P22_D1_P2_Medium_T2_4_Mid/1.png", "Image/P22_D1_P2_Medium_T2_4_Mid/2.png", "Image/P22_D1_P2_Medium_T2_4_Mid/3.png", "Image/P22_D1_P2_Medium_T2_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water this one. Finish.\"\nThe frames roughly correspond to the speech interval from 1567.2299 seconds to 1576.4723 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D2_P8_High_T1_2_Right", "images": ["Image/P29_D2_P8_High_T1_2_Right/1.png", "Image/P29_D2_P8_High_T1_2_Right/2.png", "Image/P29_D2_P8_High_T1_2_Right/3.png", "Image/P29_D2_P8_High_T1_2_Right/4.png", "Image/P29_D2_P8_High_T1_2_Right/5.png", "Image/P29_D2_P8_High_T1_2_Right/6.png", "Image/P29_D2_P8_High_T1_2_Right/7.png", "Image/P29_D2_P8_High_T1_2_Right/8.png", "Image/P29_D2_P8_High_T1_2_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 162.702 seconds to 177.2474 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D3_P4_Low_T3_19_Mid", "images": ["Image/P17_D3_P4_Low_T3_19_Mid/1.png", "Image/P17_D3_P4_Low_T3_19_Mid/2.png", "Image/P17_D3_P4_Low_T3_19_Mid/3.png", "Image/P17_D3_P4_Low_T3_19_Mid/4.png", "Image/P17_D3_P4_Low_T3_19_Mid/5.png", "Image/P17_D3_P4_Low_T3_19_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this white vase on the desk over there? Finish.\"\nThe frames roughly correspond to the speech interval from 1280.1855 seconds to 1293.2972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D3_P6_Low_T3_25_Mid", "images": ["Image/P21_D3_P6_Low_T3_25_Mid/1.png", "Image/P21_D3_P6_Low_T3_25_Mid/2.png", "Image/P21_D3_P6_Low_T3_25_Mid/3.png", "Image/P21_D3_P6_Low_T3_25_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 330.3642 seconds to 340.3026 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P4_High_T2_27_Left", "images": ["Image/P30_D1_P4_High_T2_27_Left/1.png", "Image/P30_D1_P4_High_T2_27_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 780.3138 seconds to 788.4972 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P7_Medium_T3_10_Left", "images": ["Image/P21_D2_P7_Medium_T3_10_Left/1.png", "Image/P21_D2_P7_Medium_T3_10_Left/2.png", "Image/P21_D2_P7_Medium_T3_10_Left/3.png", "Image/P21_D2_P7_Medium_T3_10_Left/4.png", "Image/P21_D2_P7_Medium_T3_10_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the bottle on the left. Finish.\"\nThe frames roughly correspond to the speech interval from 1495.6334000000002 seconds to 1506.3374 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P6_Medium_T1_11_Right", "images": ["Image/P26_D3_P6_Medium_T1_11_Right/1.png", "Image/P26_D3_P6_Medium_T1_11_Right/2.png", "Image/P26_D3_P6_Medium_T1_11_Right/3.png", "Image/P26_D3_P6_Medium_T1_11_Right/4.png", "Image/P26_D3_P6_Medium_T1_11_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the right base to the middle of that desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1110.7305 seconds to 1122.9165 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P3_Medium_T1_1_Mid", "images": ["Image/P15_D2_P3_Medium_T1_1_Mid/1.png", "Image/P15_D2_P3_Medium_T1_1_Mid/2.png", "Image/P15_D2_P3_Medium_T1_1_Mid/3.png", "Image/P15_D2_P3_Medium_T1_1_Mid/4.png", "Image/P15_D2_P3_Medium_T1_1_Mid/5.png", "Image/P15_D2_P3_Medium_T1_1_Mid/6.png", "Image/P15_D2_P3_Medium_T1_1_Mid/7.png", "Image/P15_D2_P3_Medium_T1_1_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the middle container from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 3046.45 seconds to 3061.5951999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D2_P8_High_T2_13_Right", "images": ["Image/P21_D2_P8_High_T2_13_Right/1.png", "Image/P21_D2_P8_High_T2_13_Right/2.png", "Image/P21_D2_P8_High_T2_13_Right/3.png", "Image/P21_D2_P8_High_T2_13_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1813.593 seconds to 1823.7941 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D1_P7_High_T1_25_Mid", "images": ["Image/P10_D1_P7_High_T1_25_Mid/1.png", "Image/P10_D1_P7_High_T1_25_Mid/2.png", "Image/P10_D1_P7_High_T1_25_Mid/3.png", "Image/P10_D1_P7_High_T1_25_Mid/4.png", "Image/P10_D1_P7_High_T1_25_Mid/5.png", "Image/P10_D1_P7_High_T1_25_Mid/6.png", "Image/P10_D1_P7_High_T1_25_Mid/7.png", "Image/P10_D1_P7_High_T1_25_Mid/8.png", "Image/P10_D1_P7_High_T1_25_Mid/9.png", "Image/P10_D1_P7_High_T1_25_Mid/10.png", "Image/P10_D1_P7_High_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 2245.7077 seconds to 2263.0094 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D3_P2_Medium_T3_11_Mid", "images": ["Image/P11_D3_P2_Medium_T3_11_Mid/1.png", "Image/P11_D3_P2_Medium_T3_11_Mid/2.png", "Image/P11_D3_P2_Medium_T3_11_Mid/3.png", "Image/P11_D3_P2_Medium_T3_11_Mid/4.png", "Image/P11_D3_P2_Medium_T3_11_Mid/5.png", "Image/P11_D3_P2_Medium_T3_11_Mid/6.png", "Image/P11_D3_P2_Medium_T3_11_Mid/7.png", "Image/P11_D3_P2_Medium_T3_11_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the second red bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1491.6301 seconds to 1505.2996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P9_High_T1_2_Right", "images": ["Image/P27_D1_P9_High_T1_2_Right/1.png", "Image/P27_D1_P9_High_T1_2_Right/2.png", "Image/P27_D1_P9_High_T1_2_Right/3.png", "Image/P27_D1_P9_High_T1_2_Right/4.png", "Image/P27_D1_P9_High_T1_2_Right/5.png", "Image/P27_D1_P9_High_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that green glass and put it on this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 826.0593000000001 seconds to 837.7233000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D1_P6_Medium_T1_16_Right", "images": ["Image/P15_D1_P6_Medium_T1_16_Right/1.png", "Image/P15_D1_P6_Medium_T1_16_Right/2.png", "Image/P15_D1_P6_Medium_T1_16_Right/3.png", "Image/P15_D1_P6_Medium_T1_16_Right/4.png", "Image/P15_D1_P6_Medium_T1_16_Right/5.png", "Image/P15_D1_P6_Medium_T1_16_Right/6.png", "Image/P15_D1_P6_Medium_T1_16_Right/7.png", "Image/P15_D1_P6_Medium_T1_16_Right/8.png", "Image/P15_D1_P6_Medium_T1_16_Right/9.png", "Image/P15_D1_P6_Medium_T1_16_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the right side cup into the left of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1760.8090699999998 seconds to 1776.82937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D2_P5_Medium_T1_19_Left", "images": ["Image/P8_D2_P5_Medium_T1_19_Left/1.png", "Image/P8_D2_P5_Medium_T1_19_Left/2.png", "Image/P8_D2_P5_Medium_T1_19_Left/3.png", "Image/P8_D2_P5_Medium_T1_19_Left/4.png", "Image/P8_D2_P5_Medium_T1_19_Left/5.png", "Image/P8_D2_P5_Medium_T1_19_Left/6.png", "Image/P8_D2_P5_Medium_T1_19_Left/7.png", "Image/P8_D2_P5_Medium_T1_19_Left/8.png", "Image/P8_D2_P5_Medium_T1_19_Left/9.png", "Image/P8_D2_P5_Medium_T1_19_Left/10.png", "Image/P8_D2_P5_Medium_T1_19_Left/11.png", "Image/P8_D2_P5_Medium_T1_19_Left/12.png", "Image/P8_D2_P5_Medium_T1_19_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you move the left bottle here? Just on the right shelf. I mean the lower one. Just on it. Finish.\"\nThe frames roughly correspond to the speech interval from 2463.5957000000003 seconds to 2482.9283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P4_D1_P7_Low_T2_22_Left", "images": ["Image/P4_D1_P7_Low_T2_22_Left/1.png", "Image/P4_D1_P7_Low_T2_22_Left/2.png", "Image/P4_D1_P7_Low_T2_22_Left/3.png", "Image/P4_D1_P7_Low_T2_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2389.8581999999997 seconds to 2402.8864999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P5_Medium_T1_22_Right", "images": ["Image/P30_D2_P5_Medium_T1_22_Right/1.png", "Image/P30_D2_P5_Medium_T1_22_Right/2.png", "Image/P30_D2_P5_Medium_T1_22_Right/3.png", "Image/P30_D2_P5_Medium_T1_22_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant furthest on the right over here. Finish.\"\nThe frames roughly correspond to the speech interval from 358.5961 seconds to 368.8261 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P4_Medium_T2_13_Mid", "images": ["Image/P27_D3_P4_Medium_T2_13_Mid/1.png", "Image/P27_D3_P4_Medium_T2_13_Mid/2.png", "Image/P27_D3_P4_Medium_T2_13_Mid/3.png", "Image/P27_D3_P4_Medium_T2_13_Mid/4.png", "Image/P27_D3_P4_Medium_T2_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you water the plant in the middle? Finish\"\nThe frames roughly correspond to the speech interval from 547.4479 seconds to 558.0408 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D2_P4_High_T3_3_Right", "images": ["Image/P27_D2_P4_High_T3_3_Right/1.png", "Image/P27_D2_P4_High_T3_3_Right/2.png", "Image/P27_D2_P4_High_T3_3_Right/3.png", "Image/P27_D2_P4_High_T3_3_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off that vase? Finish\"\nThe frames roughly correspond to the speech interval from 1369.2156 seconds to 1379.3782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P5_Medium_T1_11_Mid", "images": ["Image/P6_D2_P5_Medium_T1_11_Mid/1.png", "Image/P6_D2_P5_Medium_T1_11_Mid/2.png", "Image/P6_D2_P5_Medium_T1_11_Mid/3.png", "Image/P6_D2_P5_Medium_T1_11_Mid/4.png", "Image/P6_D2_P5_Medium_T1_11_Mid/5.png", "Image/P6_D2_P5_Medium_T1_11_Mid/6.png", "Image/P6_D2_P5_Medium_T1_11_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Friday\"\nThe frames roughly correspond to the speech interval from 1031.7812 seconds to 1050.993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D1_P6_High_T2_27_Mid", "images": ["Image/P27_D1_P6_High_T2_27_Mid/1.png", "Image/P27_D1_P6_High_T2_27_Mid/2.png", "Image/P27_D1_P6_High_T2_27_Mid/3.png", "Image/P27_D1_P6_High_T2_27_Mid/4.png", "Image/P27_D1_P6_High_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you put some water in that cup? Finish\"\nThe frames roughly correspond to the speech interval from 1159.9124000000002 seconds to 1169.6145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P3_Medium_T1_23_Mid", "images": ["Image/P4_D2_P3_Medium_T1_23_Mid/1.png", "Image/P4_D2_P3_Medium_T1_23_Mid/2.png", "Image/P4_D2_P3_Medium_T1_23_Mid/3.png", "Image/P4_D2_P3_Medium_T1_23_Mid/4.png", "Image/P4_D2_P3_Medium_T1_23_Mid/5.png", "Image/P4_D2_P3_Medium_T1_23_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plant in the middle to the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 1827.2633 seconds to 1841.2238 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P8_Medium_T2_27_Mid", "images": ["Image/P11_D1_P8_Medium_T2_27_Mid/1.png", "Image/P11_D1_P8_Medium_T2_27_Mid/2.png", "Image/P11_D1_P8_Medium_T2_27_Mid/3.png", "Image/P11_D1_P8_Medium_T2_27_Mid/4.png", "Image/P11_D1_P8_Medium_T2_27_Mid/5.png", "Image/P11_D1_P8_Medium_T2_27_Mid/6.png", "Image/P11_D1_P8_Medium_T2_27_Mid/7.png", "Image/P11_D1_P8_Medium_T2_27_Mid/8.png", "Image/P11_D1_P8_Medium_T2_27_Mid/9.png", "Image/P11_D1_P8_Medium_T2_27_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Please pour the water in the In the white glass in the basket\"\nThe frames roughly correspond to the speech interval from 2518.6764 seconds to 2535.0303 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D1_P9_Medium_T2_1_Right", "images": ["Image/P1_D1_P9_Medium_T2_1_Right/1.png", "Image/P1_D1_P9_Medium_T2_1_Right/2.png", "Image/P1_D1_P9_Medium_T2_1_Right/3.png", "Image/P1_D1_P9_Medium_T2_1_Right/4.png", "Image/P1_D1_P9_Medium_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water to this bottle? Finished.\"\nThe frames roughly correspond to the speech interval from 240.2939 seconds to 249.8306 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P4_Low_T3_23_Left", "images": ["Image/P4_D3_P4_Low_T3_23_Left/1.png", "Image/P4_D3_P4_Low_T3_23_Left/2.png", "Image/P4_D3_P4_Low_T3_23_Left/3.png", "Image/P4_D3_P4_Low_T3_23_Left/4.png", "Image/P4_D3_P4_Low_T3_23_Left/5.png", "Image/P4_D3_P4_Low_T3_23_Left/6.png", "Image/P4_D3_P4_Low_T3_23_Left/7.png", "Image/P4_D3_P4_Low_T3_23_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the plants on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 650.4205 seconds to 665.6235999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P5_Medium_T1_6_Right", "images": ["Image/P28_D1_P5_Medium_T1_6_Right/1.png", "Image/P28_D1_P5_Medium_T1_6_Right/2.png", "Image/P28_D1_P5_Medium_T1_6_Right/3.png", "Image/P28_D1_P5_Medium_T1_6_Right/4.png", "Image/P28_D1_P5_Medium_T1_6_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1625.0322 seconds to 1635.1662 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D1_P8_Medium_T3_25_Left", "images": ["Image/P14_D1_P8_Medium_T3_25_Left/1.png", "Image/P14_D1_P8_Medium_T3_25_Left/2.png", "Image/P14_D1_P8_Medium_T3_25_Left/3.png", "Image/P14_D1_P8_Medium_T3_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that mug up. Finish.\"\nThe frames roughly correspond to the speech interval from 330.2222 seconds to 339.5212 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P11_D1_P8_Low_T1_9_Mid", "images": ["Image/P11_D1_P8_Low_T1_9_Mid/1.png", "Image/P11_D1_P8_Low_T1_9_Mid/2.png", "Image/P11_D1_P8_Low_T1_9_Mid/3.png", "Image/P11_D1_P8_Low_T1_9_Mid/4.png", "Image/P11_D1_P8_Low_T1_9_Mid/5.png", "Image/P11_D1_P8_Low_T1_9_Mid/6.png", "Image/P11_D1_P8_Low_T1_9_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green glass in the basket on the floor to the top of the shelf in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2870.7933999999996 seconds to 2882.9862999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D2_P4_Low_T1_22_Left", "images": ["Image/P1_D2_P4_Low_T1_22_Left/1.png", "Image/P1_D2_P4_Low_T1_22_Left/2.png", "Image/P1_D2_P4_Low_T1_22_Left/3.png", "Image/P1_D2_P4_Low_T1_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this one to there? Finish.\"\nThe frames roughly correspond to the speech interval from 1041.4896 seconds to 1050.6513 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D2_P7_Medium_T3_21_Mid", "images": ["Image/P30_D2_P7_Medium_T3_21_Mid/1.png", "Image/P30_D2_P7_Medium_T3_21_Mid/2.png", "Image/P30_D2_P7_Medium_T3_21_Mid/3.png", "Image/P30_D2_P7_Medium_T3_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle base. Finish.\"\nThe frames roughly correspond to the speech interval from 561.3249 seconds to 570.2201 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P7_Low_T3_13_Mid", "images": ["Image/P17_D3_P7_Low_T3_13_Mid/1.png", "Image/P17_D3_P7_Low_T3_13_Mid/2.png", "Image/P17_D3_P7_Low_T3_13_Mid/3.png", "Image/P17_D3_P7_Low_T3_13_Mid/4.png", "Image/P17_D3_P7_Low_T3_13_Mid/5.png", "Image/P17_D3_P7_Low_T3_13_Mid/6.png", "Image/P17_D3_P7_Low_T3_13_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that plant on the floor under the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 1259.7244 seconds to 1272.5146 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D2_P2_Low_T1_1_Mid", "images": ["Image/P15_D2_P2_Low_T1_1_Mid/1.png", "Image/P15_D2_P2_Low_T1_1_Mid/2.png", "Image/P15_D2_P2_Low_T1_1_Mid/3.png", "Image/P15_D2_P2_Low_T1_1_Mid/4.png", "Image/P15_D2_P2_Low_T1_1_Mid/5.png", "Image/P15_D2_P2_Low_T1_1_Mid/6.png", "Image/P15_D2_P2_Low_T1_1_Mid/7.png", "Image/P15_D2_P2_Low_T1_1_Mid/8.png", "Image/P15_D2_P2_Low_T1_1_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the green container from the top of the shelf into the floor. Finish.\"\nThe frames roughly correspond to the speech interval from 2899.4843 seconds to 2913.7785 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P3_Low_T3_22_Left", "images": ["Image/P26_D2_P3_Low_T3_22_Left/1.png", "Image/P26_D2_P3_Low_T3_22_Left/2.png", "Image/P26_D2_P3_Low_T3_22_Left/3.png", "Image/P26_D2_P3_Low_T3_22_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean this plant? Finish.\"\nThe frames roughly correspond to the speech interval from 1363.7006 seconds to 1375.8103999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D2_P3_Low_T2_6_Left", "images": ["Image/P2_D2_P3_Low_T2_6_Left/1.png", "Image/P2_D2_P3_Low_T2_6_Left/2.png", "Image/P2_D2_P3_Low_T2_6_Left/3.png", "Image/P2_D2_P3_Low_T2_6_Left/4.png", "Image/P2_D2_P3_Low_T2_6_Left/5.png", "Image/P2_D2_P3_Low_T2_6_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 3036.7061 seconds to 3047.9246999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P20_D2_P7_Medium_T1_2_Right", "images": ["Image/P20_D2_P7_Medium_T1_2_Right/1.png", "Image/P20_D2_P7_Medium_T1_2_Right/2.png", "Image/P20_D2_P7_Medium_T1_2_Right/3.png", "Image/P20_D2_P7_Medium_T1_2_Right/4.png", "Image/P20_D2_P7_Medium_T1_2_Right/5.png", "Image/P20_D2_P7_Medium_T1_2_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the right bottle to the top of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 3274.7838 seconds to 3288.4528 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P2_High_T1_1_Left", "images": ["Image/P17_D1_P2_High_T1_1_Left/1.png", "Image/P17_D1_P2_High_T1_1_Left/2.png", "Image/P17_D1_P2_High_T1_1_Left/3.png", "Image/P17_D1_P2_High_T1_1_Left/4.png", "Image/P17_D1_P2_High_T1_1_Left/5.png", "Image/P17_D1_P2_High_T1_1_Left/6.png", "Image/P17_D1_P2_High_T1_1_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this green vase to the basket closest to the wall? Finish\"\nThe frames roughly correspond to the speech interval from 1604.6721000000002 seconds to 1617.7962000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P9_High_T3_25_Left", "images": ["Image/P7_D2_P9_High_T3_25_Left/1.png", "Image/P7_D2_P9_High_T3_25_Left/2.png", "Image/P7_D2_P9_High_T3_25_Left/3.png", "Image/P7_D2_P9_High_T3_25_Left/4.png", "Image/P7_D2_P9_High_T3_25_Left/5.png", "Image/P7_D2_P9_High_T3_25_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1512.7231000000002 seconds to 1528.5787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P5_Low_T2_17_Left", "images": ["Image/P10_D2_P5_Low_T2_17_Left/1.png", "Image/P10_D2_P5_Low_T2_17_Left/2.png", "Image/P10_D2_P5_Low_T2_17_Left/3.png", "Image/P10_D2_P5_Low_T2_17_Left/4.png", "Image/P10_D2_P5_Low_T2_17_Left/5.png", "Image/P10_D2_P5_Low_T2_17_Left/6.png", "Image/P10_D2_P5_Low_T2_17_Left/7.png", "Image/P10_D2_P5_Low_T2_17_Left/8.png", "Image/P10_D2_P5_Low_T2_17_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1506.6522800000002 seconds to 1524.0248800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D1_P8_Medium_T2_9_Left", "images": ["Image/P24_D1_P8_Medium_T2_9_Left/1.png", "Image/P24_D1_P8_Medium_T2_9_Left/2.png", "Image/P24_D1_P8_Medium_T2_9_Left/3.png", "Image/P24_D1_P8_Medium_T2_9_Left/4.png", "Image/P24_D1_P8_Medium_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please pour water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1295.9978 seconds to 1306.6323 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P4_Low_T3_20_Left", "images": ["Image/P13_D2_P4_Low_T3_20_Left/1.png", "Image/P13_D2_P4_Low_T3_20_Left/2.png", "Image/P13_D2_P4_Low_T3_20_Left/3.png", "Image/P13_D2_P4_Low_T3_20_Left/4.png", "Image/P13_D2_P4_Low_T3_20_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Friday. Can you clean the white bottle over there\"\nThe frames roughly correspond to the speech interval from 1096.10246 seconds to 1106.31956 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D1_P6_Low_T3_21_Mid", "images": ["Image/P8_D1_P6_Low_T3_21_Mid/1.png", "Image/P8_D1_P6_Low_T3_21_Mid/2.png", "Image/P8_D1_P6_Low_T3_21_Mid/3.png", "Image/P8_D1_P6_Low_T3_21_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you clean that? Finish.\"\nThe frames roughly correspond to the speech interval from 696.8272999999999 seconds to 708.3971999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P3_Medium_T2_8_Left", "images": ["Image/P28_D3_P3_Medium_T2_8_Left/1.png", "Image/P28_D3_P3_Medium_T2_8_Left/2.png", "Image/P28_D3_P3_Medium_T2_8_Left/3.png", "Image/P28_D3_P3_Medium_T2_8_Left/4.png", "Image/P28_D3_P3_Medium_T2_8_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water in that green vase. Finish.\"\nThe frames roughly correspond to the speech interval from 453.046 seconds to 462.8603 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D3_P5_Low_T2_1_Right", "images": ["Image/P18_D3_P5_Low_T2_1_Right/1.png", "Image/P18_D3_P5_Low_T2_1_Right/2.png", "Image/P18_D3_P5_Low_T2_1_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1535.4362 seconds to 1543.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P19_D1_P3_Medium_T2_25_Mid", "images": ["Image/P19_D1_P3_Medium_T2_25_Mid/1.png", "Image/P19_D1_P3_Medium_T2_25_Mid/2.png", "Image/P19_D1_P3_Medium_T2_25_Mid/3.png", "Image/P19_D1_P3_Medium_T2_25_Mid/4.png", "Image/P19_D1_P3_Medium_T2_25_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour from this cup here. Finish.\"\nThe frames roughly correspond to the speech interval from 892.6397 seconds to 904.3312 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D2_P4_High_T1_24_Right", "images": ["Image/P6_D2_P4_High_T1_24_Right/1.png", "Image/P6_D2_P4_High_T1_24_Right/2.png", "Image/P6_D2_P4_High_T1_24_Right/3.png", "Image/P6_D2_P4_High_T1_24_Right/4.png", "Image/P6_D2_P4_High_T1_24_Right/5.png", "Image/P6_D2_P4_High_T1_24_Right/6.png", "Image/P6_D2_P4_High_T1_24_Right/7.png", "Image/P6_D2_P4_High_T1_24_Right/8.png", "Image/P6_D2_P4_High_T1_24_Right/9.png", "Image/P6_D2_P4_High_T1_24_Right/10.png", "Image/P6_D2_P4_High_T1_24_Right/11.png", "Image/P6_D2_P4_High_T1_24_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday\"\nThe frames roughly correspond to the speech interval from 648.5504 seconds to 668.7928999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P2_Medium_T2_12_Left", "images": ["Image/P14_D2_P2_Medium_T2_12_Left/1.png", "Image/P14_D2_P2_Medium_T2_12_Left/2.png", "Image/P14_D2_P2_Medium_T2_12_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1601.9635999999998 seconds to 1610.448 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P9_High_T3_25_Left", "images": ["Image/P16_D1_P9_High_T3_25_Left/1.png", "Image/P16_D1_P9_High_T3_25_Left/2.png", "Image/P16_D1_P9_High_T3_25_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish\"\nThe frames roughly correspond to the speech interval from 1451.6782 seconds to 1460.6988 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P2_High_T1_20_Mid", "images": ["Image/P27_D2_P2_High_T1_20_Mid/1.png", "Image/P27_D2_P2_High_T1_20_Mid/2.png", "Image/P27_D2_P2_High_T1_20_Mid/3.png", "Image/P27_D2_P2_High_T1_20_Mid/4.png", "Image/P27_D2_P2_High_T1_20_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You move the white vase to the bottom of that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1585.2806 seconds to 1596.0829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P6_Medium_T3_24_Mid", "images": ["Image/P7_D1_P6_Medium_T3_24_Mid/1.png", "Image/P7_D1_P6_Medium_T3_24_Mid/2.png", "Image/P7_D1_P6_Medium_T3_24_Mid/3.png", "Image/P7_D1_P6_Medium_T3_24_Mid/4.png", "Image/P7_D1_P6_Medium_T3_24_Mid/5.png", "Image/P7_D1_P6_Medium_T3_24_Mid/6.png", "Image/P7_D1_P6_Medium_T3_24_Mid/7.png", "Image/P7_D1_P6_Medium_T3_24_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 569.9594 seconds to 584.8255 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D3_P7_Medium_T3_19_Mid", "images": ["Image/P26_D3_P7_Medium_T3_19_Mid/1.png", "Image/P26_D3_P7_Medium_T3_19_Mid/2.png", "Image/P26_D3_P7_Medium_T3_19_Mid/3.png", "Image/P26_D3_P7_Medium_T3_19_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the middle base on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1131.7504 seconds to 1141.8557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D1_P9_Medium_T3_27_Left", "images": ["Image/P21_D1_P9_Medium_T3_27_Left/1.png", "Image/P21_D1_P9_Medium_T3_27_Left/2.png", "Image/P21_D1_P9_Medium_T3_27_Left/3.png", "Image/P21_D1_P9_Medium_T3_27_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 896.5005699999999 seconds to 906.36965 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D1_P1_Medium_T1_9_Left", "images": ["Image/P6_D1_P1_Medium_T1_9_Left/1.png", "Image/P6_D1_P1_Medium_T1_9_Left/2.png", "Image/P6_D1_P1_Medium_T1_9_Left/3.png", "Image/P6_D1_P1_Medium_T1_9_Left/4.png", "Image/P6_D1_P1_Medium_T1_9_Left/5.png", "Image/P6_D1_P1_Medium_T1_9_Left/6.png", "Image/P6_D1_P1_Medium_T1_9_Left/7.png", "Image/P6_D1_P1_Medium_T1_9_Left/8.png", "Image/P6_D1_P1_Medium_T1_9_Left/9.png", "Image/P6_D1_P1_Medium_T1_9_Left/10.png", "Image/P6_D1_P1_Medium_T1_9_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Next Friday\"\nThe frames roughly correspond to the speech interval from 2557.6491 seconds to 2574.6151 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P2_Medium_T2_13_Right", "images": ["Image/P10_D3_P2_Medium_T2_13_Right/1.png", "Image/P10_D3_P2_Medium_T2_13_Right/2.png", "Image/P10_D3_P2_Medium_T2_13_Right/3.png", "Image/P10_D3_P2_Medium_T2_13_Right/4.png", "Image/P10_D3_P2_Medium_T2_13_Right/5.png", "Image/P10_D3_P2_Medium_T2_13_Right/6.png", "Image/P10_D3_P2_Medium_T2_13_Right/7.png", "Image/P10_D3_P2_Medium_T2_13_Right/8.png", "Image/P10_D3_P2_Medium_T2_13_Right/9.png", "Image/P10_D3_P2_Medium_T2_13_Right/10.png", "Image/P10_D3_P2_Medium_T2_13_Right/11.png", "Image/P10_D3_P2_Medium_T2_13_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 297.34157999999996 seconds to 315.77115 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P5_Low_T1_24_Mid", "images": ["Image/P17_D1_P5_Low_T1_24_Mid/1.png", "Image/P17_D1_P5_Low_T1_24_Mid/2.png", "Image/P17_D1_P5_Low_T1_24_Mid/3.png", "Image/P17_D1_P5_Low_T1_24_Mid/4.png", "Image/P17_D1_P5_Low_T1_24_Mid/5.png", "Image/P17_D1_P5_Low_T1_24_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this plant to the spot on the table? Finish\"\nThe frames roughly correspond to the speech interval from 1518.01816 seconds to 1530.99131 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P8_Low_T1_2_Left", "images": ["Image/P3_D2_P8_Low_T1_2_Left/1.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image>\nThe user's spoken instruction is: \"Could you give some water to this item?\"\nThe frames roughly correspond to the speech interval from 2286.5365 seconds to 2297.2693 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P5_Low_T3_8_Right", "images": ["Image/P22_D1_P5_Low_T3_8_Right/1.png", "Image/P22_D1_P5_Low_T3_8_Right/2.png", "Image/P22_D1_P5_Low_T3_8_Right/3.png", "Image/P22_D1_P5_Low_T3_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this. Finish.\"\nThe frames roughly correspond to the speech interval from 1582.4718 seconds to 1593.7602 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P8_High_T2_27_Right", "images": ["Image/P23_D3_P8_High_T2_27_Right/1.png", "Image/P23_D3_P8_High_T2_27_Right/2.png", "Image/P23_D3_P8_High_T2_27_Right/3.png", "Image/P23_D3_P8_High_T2_27_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 628.25926 seconds to 639.70055 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D2_P2_Low_T2_18_Right", "images": ["Image/P23_D2_P2_Low_T2_18_Right/1.png", "Image/P23_D2_P2_Low_T2_18_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 486.1618 seconds to 495.4879 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D2_P8_Medium_T1_17_Left", "images": ["Image/P11_D2_P8_Medium_T1_17_Left/1.png", "Image/P11_D2_P8_Medium_T1_17_Left/2.png", "Image/P11_D2_P8_Medium_T1_17_Left/3.png", "Image/P11_D2_P8_Medium_T1_17_Left/4.png", "Image/P11_D2_P8_Medium_T1_17_Left/5.png", "Image/P11_D2_P8_Medium_T1_17_Left/6.png", "Image/P11_D2_P8_Medium_T1_17_Left/7.png", "Image/P11_D2_P8_Medium_T1_17_Left/8.png", "Image/P11_D2_P8_Medium_T1_17_Left/9.png", "Image/P11_D2_P8_Medium_T1_17_Left/10.png", "Image/P11_D2_P8_Medium_T1_17_Left/11.png", "Image/P11_D2_P8_Medium_T1_17_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the first red glass to the shelf on the wall in front of me. Please finish.\"\nThe frames roughly correspond to the speech interval from 756.4135 seconds to 777.9787 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D2_P9_Low_T1_21_Left", "images": ["Image/P27_D2_P9_Low_T1_21_Left/1.png", "Image/P27_D2_P9_Low_T1_21_Left/2.png", "Image/P27_D2_P9_Low_T1_21_Left/3.png", "Image/P27_D2_P9_Low_T1_21_Left/4.png", "Image/P27_D2_P9_Low_T1_21_Left/5.png", "Image/P27_D2_P9_Low_T1_21_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you take that vase and put it on top of that cabinet? Finish.\"\nThe frames roughly correspond to the speech interval from 1634.336 seconds to 1645.6248 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P4_Low_T1_3_Left", "images": ["Image/P7_D3_P4_Low_T1_3_Left/1.png", "Image/P7_D3_P4_Low_T1_3_Left/2.png", "Image/P7_D3_P4_Low_T1_3_Left/3.png", "Image/P7_D3_P4_Low_T1_3_Left/4.png", "Image/P7_D3_P4_Low_T1_3_Left/5.png", "Image/P7_D3_P4_Low_T1_3_Left/6.png", "Image/P7_D3_P4_Low_T1_3_Left/7.png", "Image/P7_D3_P4_Low_T1_3_Left/8.png", "Image/P7_D3_P4_Low_T1_3_Left/9.png", "Image/P7_D3_P4_Low_T1_3_Left/10.png", "Image/P7_D3_P4_Low_T1_3_Left/11.png", "Image/P7_D3_P4_Low_T1_3_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1815.5596 seconds to 1835.048 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P5_High_T1_2_Mid", "images": ["Image/P20_D3_P5_High_T1_2_Mid/1.png", "Image/P20_D3_P5_High_T1_2_Mid/2.png", "Image/P20_D3_P5_High_T1_2_Mid/3.png", "Image/P20_D3_P5_High_T1_2_Mid/4.png", "Image/P20_D3_P5_High_T1_2_Mid/5.png", "Image/P20_D3_P5_High_T1_2_Mid/6.png", "Image/P20_D3_P5_High_T1_2_Mid/7.png", "Image/P20_D3_P5_High_T1_2_Mid/8.png", "Image/P20_D3_P5_High_T1_2_Mid/9.png", "Image/P20_D3_P5_High_T1_2_Mid/10.png", "Image/P20_D3_P5_High_T1_2_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Okay\"\nThe frames roughly correspond to the speech interval from 1467.2044 seconds to 1484.7283 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P4_Low_T1_16_Right", "images": ["Image/P3_D2_P4_Low_T1_16_Right/1.png", "Image/P3_D2_P4_Low_T1_16_Right/2.png", "Image/P3_D2_P4_Low_T1_16_Right/3.png", "Image/P3_D2_P4_Low_T1_16_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Could you move this item to this space?\"\nThe frames roughly correspond to the speech interval from 2553.4366 seconds to 2563.9465 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D3_P9_Medium_T1_4_Right", "images": ["Image/P23_D3_P9_Medium_T1_4_Right/1.png", "Image/P23_D3_P9_Medium_T1_4_Right/2.png", "Image/P23_D3_P9_Medium_T1_4_Right/3.png", "Image/P23_D3_P9_Medium_T1_4_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 750.4487 seconds to 759.9274 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P9_Medium_T3_23_Mid", "images": ["Image/P23_D1_P9_Medium_T3_23_Mid/1.png", "Image/P23_D1_P9_Medium_T3_23_Mid/2.png", "Image/P23_D1_P9_Medium_T3_23_Mid/3.png", "Image/P23_D1_P9_Medium_T3_23_Mid/4.png", "Image/P23_D1_P9_Medium_T3_23_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1703.0979 seconds to 1714.1351 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P8_Medium_T2_18_Left", "images": ["Image/P27_D2_P8_Medium_T2_18_Left/1.png", "Image/P27_D2_P8_Medium_T2_18_Left/2.png", "Image/P27_D2_P8_Medium_T2_18_Left/3.png", "Image/P27_D2_P8_Medium_T2_18_Left/4.png", "Image/P27_D2_P8_Medium_T2_18_Left/5.png", "Image/P27_D2_P8_Medium_T2_18_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. You put water in that first red cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1448.2132 seconds to 1459.501 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D3_P9_Medium_T3_13_Mid", "images": ["Image/P5_D3_P9_Medium_T3_13_Mid/1.png", "Image/P5_D3_P9_Medium_T3_13_Mid/2.png", "Image/P5_D3_P9_Medium_T3_13_Mid/3.png", "Image/P5_D3_P9_Medium_T3_13_Mid/4.png", "Image/P5_D3_P9_Medium_T3_13_Mid/5.png", "Image/P5_D3_P9_Medium_T3_13_Mid/6.png", "Image/P5_D3_P9_Medium_T3_13_Mid/7.png", "Image/P5_D3_P9_Medium_T3_13_Mid/8.png", "Image/P5_D3_P9_Medium_T3_13_Mid/9.png", "Image/P5_D3_P9_Medium_T3_13_Mid/10.png", "Image/P5_D3_P9_Medium_T3_13_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle plant on the shelf in the living room. Finished.\"\nThe frames roughly correspond to the speech interval from 2541.5531 seconds to 2557.8244 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P4_High_T2_27_Right", "images": ["Image/P28_D3_P4_High_T2_27_Right/1.png", "Image/P28_D3_P4_High_T2_27_Right/2.png", "Image/P28_D3_P4_High_T2_27_Right/3.png", "Image/P28_D3_P4_High_T2_27_Right/4.png", "Image/P28_D3_P4_High_T2_27_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please pour water into that vase. Finish\"\nThe frames roughly correspond to the speech interval from 526.0137 seconds to 537.2768000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D2_P2_Medium_T3_10_Left", "images": ["Image/P13_D2_P2_Medium_T3_10_Left/1.png", "Image/P13_D2_P2_Medium_T3_10_Left/2.png", "Image/P13_D2_P2_Medium_T3_10_Left/3.png", "Image/P13_D2_P2_Medium_T3_10_Left/4.png", "Image/P13_D2_P2_Medium_T3_10_Left/5.png", "Image/P13_D2_P2_Medium_T3_10_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Friday\"\nThe frames roughly correspond to the speech interval from 1022.28526 seconds to 1032.96226 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P2_High_T2_3_Mid", "images": ["Image/P2_D1_P2_High_T2_3_Mid/1.png", "Image/P2_D1_P2_High_T2_3_Mid/2.png", "Image/P2_D1_P2_High_T2_3_Mid/3.png", "Image/P2_D1_P2_High_T2_3_Mid/4.png", "Image/P2_D1_P2_High_T2_3_Mid/5.png", "Image/P2_D1_P2_High_T2_3_Mid/6.png", "Image/P2_D1_P2_High_T2_3_Mid/7.png", "Image/P2_D1_P2_High_T2_3_Mid/8.png", "Image/P2_D1_P2_High_T2_3_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 787.9176 seconds to 805.9699 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P8_Medium_T2_23_Right", "images": ["Image/P14_D3_P8_Medium_T2_23_Right/1.png", "Image/P14_D3_P8_Medium_T2_23_Right/2.png", "Image/P14_D3_P8_Medium_T2_23_Right/3.png", "Image/P14_D3_P8_Medium_T2_23_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1178.65306 seconds to 1187.43056 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P13_D3_P5_High_T3_21_Mid", "images": ["Image/P13_D3_P5_High_T3_21_Mid/1.png", "Image/P13_D3_P5_High_T3_21_Mid/2.png", "Image/P13_D3_P5_High_T3_21_Mid/3.png", "Image/P13_D3_P5_High_T3_21_Mid/4.png", "Image/P13_D3_P5_High_T3_21_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you clean the white vase in the middle\"\nThe frames roughly correspond to the speech interval from 1767.4705 seconds to 1778.2436 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D3_P5_High_T2_11_Left", "images": ["Image/P30_D3_P5_High_T2_11_Left/1.png", "Image/P30_D3_P5_High_T2_11_Left/2.png", "Image/P30_D3_P5_High_T2_11_Left/3.png", "Image/P30_D3_P5_High_T2_11_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1437.3964999999998 seconds to 1448.3366999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D3_P1_Low_T1_25_Mid", "images": ["Image/P2_D3_P1_Low_T1_25_Mid/1.png", "Image/P2_D3_P1_Low_T1_25_Mid/2.png", "Image/P2_D3_P1_Low_T1_25_Mid/3.png", "Image/P2_D3_P1_Low_T1_25_Mid/4.png", "Image/P2_D3_P1_Low_T1_25_Mid/5.png", "Image/P2_D3_P1_Low_T1_25_Mid/6.png", "Image/P2_D3_P1_Low_T1_25_Mid/7.png", "Image/P2_D3_P1_Low_T1_25_Mid/8.png", "Image/P2_D3_P1_Low_T1_25_Mid/9.png", "Image/P2_D3_P1_Low_T1_25_Mid/10.png", "Image/P2_D3_P1_Low_T1_25_Mid/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 1854.4971000000005 seconds to 1871.7019000000005 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P18_D1_P3_High_T3_9_Right", "images": ["Image/P18_D1_P3_High_T3_9_Right/1.png", "Image/P18_D1_P3_High_T3_9_Right/2.png", "Image/P18_D1_P3_High_T3_9_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1046.2435 seconds to 1055.3887 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P8_Medium_T2_14_Left", "images": ["Image/P4_D3_P8_Medium_T2_14_Left/1.png", "Image/P4_D3_P8_Medium_T2_14_Left/2.png", "Image/P4_D3_P8_Medium_T2_14_Left/3.png", "Image/P4_D3_P8_Medium_T2_14_Left/4.png", "Image/P4_D3_P8_Medium_T2_14_Left/5.png", "Image/P4_D3_P8_Medium_T2_14_Left/6.png", "Image/P4_D3_P8_Medium_T2_14_Left/7.png", "Image/P4_D3_P8_Medium_T2_14_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the left plants on the ground? Finish.\"\nThe frames roughly correspond to the speech interval from 326.59619999999995 seconds to 341.74069999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P2_D2_P5_Medium_T1_16_Right", "images": ["Image/P2_D2_P5_Medium_T1_16_Right/1.png", "Image/P2_D2_P5_Medium_T1_16_Right/2.png", "Image/P2_D2_P5_Medium_T1_16_Right/3.png", "Image/P2_D2_P5_Medium_T1_16_Right/4.png", "Image/P2_D2_P5_Medium_T1_16_Right/5.png", "Image/P2_D2_P5_Medium_T1_16_Right/6.png", "Image/P2_D2_P5_Medium_T1_16_Right/7.png", "Image/P2_D2_P5_Medium_T1_16_Right/8.png", "Image/P2_D2_P5_Medium_T1_16_Right/9.png", "Image/P2_D2_P5_Medium_T1_16_Right/10.png", "Image/P2_D2_P5_Medium_T1_16_Right/11.png", "Image/P2_D2_P5_Medium_T1_16_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2661.3556 seconds to 2678.1416999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D3_P2_Low_T3_14_Left", "images": ["Image/P2_D3_P2_Low_T3_14_Left/1.png", "Image/P2_D3_P2_Low_T3_14_Left/2.png", "Image/P2_D3_P2_Low_T3_14_Left/3.png", "Image/P2_D3_P2_Low_T3_14_Left/4.png", "Image/P2_D3_P2_Low_T3_14_Left/5.png", "Image/P2_D3_P2_Low_T3_14_Left/6.png", "Image/P2_D3_P2_Low_T3_14_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hello\"\nThe frames roughly correspond to the speech interval from 2032.4773 seconds to 2046.5199 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D2_P2_High_T2_8_Left", "images": ["Image/P6_D2_P2_High_T2_8_Left/1.png", "Image/P6_D2_P2_High_T2_8_Left/2.png", "Image/P6_D2_P2_High_T2_8_Left/3.png", "Image/P6_D2_P2_High_T2_8_Left/4.png", "Image/P6_D2_P2_High_T2_8_Left/5.png", "Image/P6_D2_P2_High_T2_8_Left/6.png", "Image/P6_D2_P2_High_T2_8_Left/7.png", "Image/P6_D2_P2_High_T2_8_Left/8.png", "Image/P6_D2_P2_High_T2_8_Left/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hi\"\nThe frames roughly correspond to the speech interval from 746.2962 seconds to 762.068 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P6_Low_T2_11_Mid", "images": ["Image/P25_D1_P6_Low_T2_11_Mid/1.png", "Image/P25_D1_P6_Low_T2_11_Mid/2.png", "Image/P25_D1_P6_Low_T2_11_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into this. Finish.\"\nThe frames roughly correspond to the speech interval from 441.9678 seconds to 450.3491 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P5_High_T2_1_Right", "images": ["Image/P1_D3_P5_High_T2_1_Right/1.png", "Image/P1_D3_P5_High_T2_1_Right/2.png", "Image/P1_D3_P5_High_T2_1_Right/3.png", "Image/P1_D3_P5_High_T2_1_Right/4.png", "Image/P1_D3_P5_High_T2_1_Right/5.png", "Image/P1_D3_P5_High_T2_1_Right/6.png", "Image/P1_D3_P5_High_T2_1_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you watering the green bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1660.3628 seconds to 1673.2124 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D1_P9_Medium_T2_20_Right", "images": ["Image/P14_D1_P9_Medium_T2_20_Right/1.png", "Image/P14_D1_P9_Medium_T2_20_Right/2.png", "Image/P14_D1_P9_Medium_T2_20_Right/3.png", "Image/P14_D1_P9_Medium_T2_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 446.2458 seconds to 455.8782 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D1_P5_Medium_T1_11_Right", "images": ["Image/P22_D1_P5_Medium_T1_11_Right/1.png", "Image/P22_D1_P5_Medium_T1_11_Right/2.png", "Image/P22_D1_P5_Medium_T1_11_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1707.348 seconds to 1718.9967 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P1_High_T1_17_Mid", "images": ["Image/P24_D3_P1_High_T1_17_Mid/1.png", "Image/P24_D3_P1_High_T1_17_Mid/2.png", "Image/P24_D3_P1_High_T1_17_Mid/3.png", "Image/P24_D3_P1_High_T1_17_Mid/4.png", "Image/P24_D3_P1_High_T1_17_Mid/5.png", "Image/P24_D3_P1_High_T1_17_Mid/6.png", "Image/P24_D3_P1_High_T1_17_Mid/7.png", "Image/P24_D3_P1_High_T1_17_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this hotel to this area? Finish.\"\nThe frames roughly correspond to the speech interval from 1713.2946 seconds to 1726.7829 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D1_P1_High_T3_15_Mid", "images": ["Image/P30_D1_P1_High_T3_15_Mid/1.png", "Image/P30_D1_P1_High_T3_15_Mid/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 717.6323000000001 seconds to 725.1626000000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P2_High_T1_20_Mid", "images": ["Image/P22_D1_P2_High_T1_20_Mid/1.png", "Image/P22_D1_P2_High_T1_20_Mid/2.png", "Image/P22_D1_P2_High_T1_20_Mid/3.png", "Image/P22_D1_P2_High_T1_20_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase to the basket. Finish.\"\nThe frames roughly correspond to the speech interval from 1478.5878 seconds to 1488.6233 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P1_D1_P1_Low_T2_7_Right", "images": ["Image/P1_D1_P1_Low_T2_7_Right/1.png", "Image/P1_D1_P1_Low_T2_7_Right/2.png", "Image/P1_D1_P1_Low_T2_7_Right/3.png", "Image/P1_D1_P1_Low_T2_7_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 222.7513 seconds to 231.3416 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D2_P7_Low_T1_12_Mid", "images": ["Image/P21_D2_P7_Low_T1_12_Mid/1.png", "Image/P21_D2_P7_Low_T1_12_Mid/2.png", "Image/P21_D2_P7_Low_T1_12_Mid/3.png", "Image/P21_D2_P7_Low_T1_12_Mid/4.png", "Image/P21_D2_P7_Low_T1_12_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase to the top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1568.1287000000002 seconds to 1579.4738000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P22_D1_P3_Low_T3_5_Right", "images": ["Image/P22_D1_P3_Low_T3_5_Right/1.png", "Image/P22_D1_P3_Low_T3_5_Right/2.png", "Image/P22_D1_P3_Low_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that. Finish.\"\nThe frames roughly correspond to the speech interval from 1460.35333 seconds to 1471.25338 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P30_D1_P8_High_T2_4_Left", "images": ["Image/P30_D1_P8_High_T2_4_Left/1.png", "Image/P30_D1_P8_High_T2_4_Left/2.png", "Image/P30_D1_P8_High_T2_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 940.2396 seconds to 951.2077 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D2_P9_Low_T3_21_Right", "images": ["Image/P26_D2_P9_Low_T3_21_Right/1.png", "Image/P26_D2_P9_Low_T3_21_Right/2.png", "Image/P26_D2_P9_Low_T3_21_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the white vase down there? Finish.\"\nThe frames roughly correspond to the speech interval from 1403.4139 seconds to 1412.3785 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P4_D3_P3_High_T3_19_Left", "images": ["Image/P4_D3_P3_High_T3_19_Left/1.png", "Image/P4_D3_P3_High_T3_19_Left/2.png", "Image/P4_D3_P3_High_T3_19_Left/3.png", "Image/P4_D3_P3_High_T3_19_Left/4.png", "Image/P4_D3_P3_High_T3_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 865.6533999999999 seconds to 877.6535999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P7_High_T2_24_Right", "images": ["Image/P23_D3_P7_High_T2_24_Right/1.png", "Image/P23_D3_P7_High_T2_24_Right/2.png", "Image/P23_D3_P7_High_T2_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 978.9147 seconds to 987.4913 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P21_D1_P1_High_T1_9_Mid", "images": ["Image/P21_D1_P1_High_T1_9_Mid/1.png", "Image/P21_D1_P1_High_T1_9_Mid/2.png", "Image/P21_D1_P1_High_T1_9_Mid/3.png", "Image/P21_D1_P1_High_T1_9_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1194.5824 seconds to 1206.4696 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D3_P2_High_T2_20_Right", "images": ["Image/P7_D3_P2_High_T2_20_Right/1.png", "Image/P7_D3_P2_High_T2_20_Right/2.png", "Image/P7_D3_P2_High_T2_20_Right/3.png", "Image/P7_D3_P2_High_T2_20_Right/4.png", "Image/P7_D3_P2_High_T2_20_Right/5.png", "Image/P7_D3_P2_High_T2_20_Right/6.png", "Image/P7_D3_P2_High_T2_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1749.6075 seconds to 1763.1924 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D3_P9_High_T2_4_Mid", "images": ["Image/P3_D3_P9_High_T2_4_Mid/1.png", "Image/P3_D3_P9_High_T2_4_Mid/2.png", "Image/P3_D3_P9_High_T2_4_Mid/3.png", "Image/P3_D3_P9_High_T2_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 956.2652 seconds to 966.9415 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P17_D1_P7_High_T3_16_Left", "images": ["Image/P17_D1_P7_High_T3_16_Left/1.png", "Image/P17_D1_P7_High_T3_16_Left/2.png", "Image/P17_D1_P7_High_T3_16_Left/3.png", "Image/P17_D1_P7_High_T3_16_Left/4.png", "Image/P17_D1_P7_High_T3_16_Left/5.png", "Image/P17_D1_P7_High_T3_16_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off this brown mug in the basket on the floor? Finish.\"\nThe frames roughly correspond to the speech interval from 1947.8512 seconds to 1960.6996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D3_P4_High_T1_21_Mid", "images": ["Image/P17_D3_P4_High_T1_21_Mid/1.png", "Image/P17_D3_P4_High_T1_21_Mid/2.png", "Image/P17_D3_P4_High_T1_21_Mid/3.png", "Image/P17_D3_P4_High_T1_21_Mid/4.png", "Image/P17_D3_P4_High_T1_21_Mid/5.png", "Image/P17_D3_P4_High_T1_21_Mid/6.png", "Image/P17_D3_P4_High_T1_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move this white object on the desk to the second shelf over here on the right? Finish\"\nThe frames roughly correspond to the speech interval from 1040.7483 seconds to 1055.3495 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P21_D3_P8_Low_T3_20_Left", "images": ["Image/P21_D3_P8_Low_T3_20_Left/1.png", "Image/P21_D3_P8_Low_T3_20_Left/2.png", "Image/P21_D3_P8_Low_T3_20_Left/3.png", "Image/P21_D3_P8_Low_T3_20_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 618.8137999999999 seconds to 634.6692 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P7_High_T2_16_Left", "images": ["Image/P26_D1_P7_High_T2_16_Left/1.png", "Image/P26_D1_P7_High_T2_16_Left/2.png", "Image/P26_D1_P7_High_T2_16_Left/3.png", "Image/P26_D1_P7_High_T2_16_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into that jug? Finish\"\nThe frames roughly correspond to the speech interval from 111.3151 seconds to 123.24366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D2_P6_Low_T1_9_Left", "images": ["Image/P4_D2_P6_Low_T1_9_Left/1.png", "Image/P4_D2_P6_Low_T1_9_Left/2.png", "Image/P4_D2_P6_Low_T1_9_Left/3.png", "Image/P4_D2_P6_Low_T1_9_Left/4.png", "Image/P4_D2_P6_Low_T1_9_Left/5.png", "Image/P4_D2_P6_Low_T1_9_Left/6.png", "Image/P4_D2_P6_Low_T1_9_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move the green cap to the left on the shelter? Finish\"\nThe frames roughly correspond to the speech interval from 1528.0829 seconds to 1544.4916999999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P2_D2_P6_High_T2_19_Right", "images": ["Image/P2_D2_P6_High_T2_19_Right/1.png", "Image/P2_D2_P6_High_T2_19_Right/2.png", "Image/P2_D2_P6_High_T2_19_Right/3.png", "Image/P2_D2_P6_High_T2_19_Right/4.png", "Image/P2_D2_P6_High_T2_19_Right/5.png", "Image/P2_D2_P6_High_T2_19_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 2686.4678 seconds to 2699.6202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D1_P4_Medium_T2_2_Mid", "images": ["Image/P28_D1_P4_Medium_T2_2_Mid/1.png", "Image/P28_D1_P4_Medium_T2_2_Mid/2.png", "Image/P28_D1_P4_Medium_T2_2_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1346.34243 seconds to 1356.5735 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D1_P4_Low_T3_6_Right", "images": ["Image/P16_D1_P4_Low_T3_6_Right/1.png", "Image/P16_D1_P4_Low_T3_6_Right/2.png", "Image/P16_D1_P4_Low_T3_6_Right/3.png", "Image/P16_D1_P4_Low_T3_6_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1570.6777 seconds to 1581.1184 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D1_P5_High_T3_20_Right", "images": ["Image/P23_D1_P5_High_T3_20_Right/1.png", "Image/P23_D1_P5_High_T3_20_Right/2.png", "Image/P23_D1_P5_High_T3_20_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1831.3681 seconds to 1840.3994 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D2_P3_Low_T3_26_Left", "images": ["Image/P27_D2_P3_Low_T3_26_Left/1.png", "Image/P27_D2_P3_Low_T3_26_Left/2.png", "Image/P27_D2_P3_Low_T3_26_Left/3.png", "Image/P27_D2_P3_Low_T3_26_Left/4.png", "Image/P27_D2_P3_Low_T3_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you clean off that cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1500.0794999999998 seconds to 1510.0157 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P7_D1_P1_Medium_T2_16_Mid", "images": ["Image/P7_D1_P1_Medium_T2_16_Mid/1.png", "Image/P7_D1_P1_Medium_T2_16_Mid/2.png", "Image/P7_D1_P1_Medium_T2_16_Mid/3.png", "Image/P7_D1_P1_Medium_T2_16_Mid/4.png", "Image/P7_D1_P1_Medium_T2_16_Mid/5.png", "Image/P7_D1_P1_Medium_T2_16_Mid/6.png", "Image/P7_D1_P1_Medium_T2_16_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 714.4489 seconds to 729.1882 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P1_Medium_T2_26_Right", "images": ["Image/P14_D3_P1_Medium_T2_26_Right/1.png", "Image/P14_D3_P1_Medium_T2_26_Right/2.png", "Image/P14_D3_P1_Medium_T2_26_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Fill up that mug. Finished.\"\nThe frames roughly correspond to the speech interval from 1341.16676 seconds to 1349.75136 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D2_P5_High_T2_10_Left", "images": ["Image/P12_D2_P5_High_T2_10_Left/1.png", "Image/P12_D2_P5_High_T2_10_Left/2.png", "Image/P12_D2_P5_High_T2_10_Left/3.png", "Image/P12_D2_P5_High_T2_10_Left/4.png", "Image/P12_D2_P5_High_T2_10_Left/5.png", "Image/P12_D2_P5_High_T2_10_Left/6.png", "Image/P12_D2_P5_High_T2_10_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please add water to the orange vase in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 148.16558 seconds to 161.63924 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P6_Low_T2_21_Mid", "images": ["Image/P30_D2_P6_Low_T2_21_Mid/1.png", "Image/P30_D2_P6_Low_T2_21_Mid/2.png", "Image/P30_D2_P6_Low_T2_21_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 411.2825 seconds to 420.1772 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P12_D3_P7_High_T3_2_Mid", "images": ["Image/P12_D3_P7_High_T3_2_Mid/1.png", "Image/P12_D3_P7_High_T3_2_Mid/2.png", "Image/P12_D3_P7_High_T3_2_Mid/3.png", "Image/P12_D3_P7_High_T3_2_Mid/4.png", "Image/P12_D3_P7_High_T3_2_Mid/5.png", "Image/P12_D3_P7_High_T3_2_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2273.17188 seconds to 2287.14703 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D3_P3_Medium_T3_5_Mid", "images": ["Image/P5_D3_P3_Medium_T3_5_Mid/1.png", "Image/P5_D3_P3_Medium_T3_5_Mid/2.png", "Image/P5_D3_P3_Medium_T3_5_Mid/3.png", "Image/P5_D3_P3_Medium_T3_5_Mid/4.png", "Image/P5_D3_P3_Medium_T3_5_Mid/5.png", "Image/P5_D3_P3_Medium_T3_5_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the middle plant on the shelf. Finished.\"\nThe frames roughly correspond to the speech interval from 2264.9927 seconds to 2276.6007999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P21_D2_P1_Medium_T3_4_Mid", "images": ["Image/P21_D2_P1_Medium_T3_4_Mid/1.png", "Image/P21_D2_P1_Medium_T3_4_Mid/2.png", "Image/P21_D2_P1_Medium_T3_4_Mid/3.png", "Image/P21_D2_P1_Medium_T3_4_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1708.2581 seconds to 1719.1731 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P9_Low_T2_3_Left", "images": ["Image/P13_D3_P9_Low_T2_3_Left/1.png", "Image/P13_D3_P9_Low_T2_3_Left/2.png", "Image/P13_D3_P9_Low_T2_3_Left/3.png", "Image/P13_D3_P9_Low_T2_3_Left/4.png", "Image/P13_D3_P9_Low_T2_3_Left/5.png", "Image/P13_D3_P9_Low_T2_3_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Friday\"\nThe frames roughly correspond to the speech interval from 1594.8072 seconds to 1606.3471 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P3_D1_P7_High_T2_26_Left", "images": ["Image/P3_D1_P7_High_T2_26_Left/1.png", "Image/P3_D1_P7_High_T2_26_Left/2.png", "Image/P3_D1_P7_High_T2_26_Left/3.png", "Image/P3_D1_P7_High_T2_26_Left/4.png", "Image/P3_D1_P7_High_T2_26_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the white bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 1694.7295 seconds to 1705.366 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P22_D2_P2_High_T2_23_Left", "images": ["Image/P22_D2_P2_High_T2_23_Left/1.png", "Image/P22_D2_P2_High_T2_23_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"What does that plant? Finish.\"\nThe frames roughly correspond to the speech interval from 825.0726 seconds to 837.6006 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P26_D3_P4_High_T1_14_Mid", "images": ["Image/P26_D3_P4_High_T1_14_Mid/1.png", "Image/P26_D3_P4_High_T1_14_Mid/2.png", "Image/P26_D3_P4_High_T1_14_Mid/3.png", "Image/P26_D3_P4_High_T1_14_Mid/4.png", "Image/P26_D3_P4_High_T1_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move the plant to that shelf on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 713.8048100000001 seconds to 725.3995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P5_D1_P4_High_T3_7_Left", "images": ["Image/P5_D1_P4_High_T3_7_Left/1.png", "Image/P5_D1_P4_High_T3_7_Left/2.png", "Image/P5_D1_P4_High_T3_7_Left/3.png", "Image/P5_D1_P4_High_T3_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this cup. Finished.\"\nThe frames roughly correspond to the speech interval from 1960.2652 seconds to 1969.8976 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D2_P1_High_T3_9_Left", "images": ["Image/P22_D2_P1_High_T3_9_Left/1.png", "Image/P22_D2_P1_High_T3_9_Left/2.png", "Image/P22_D2_P1_High_T3_9_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup. Finish.\"\nThe frames roughly correspond to the speech interval from 772.3991999999998 seconds to 787.3217999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P22_D1_P9_Low_T1_6_Right", "images": ["Image/P22_D1_P9_Low_T1_6_Right/1.png", "Image/P22_D1_P9_Low_T1_6_Right/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Move the plant up here. Finish.\"\nThe frames roughly correspond to the speech interval from 1672.5481 seconds to 1680.5656 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P29_D1_P5_High_T2_6_Left", "images": ["Image/P29_D1_P5_High_T2_6_Left/1.png", "Image/P29_D1_P5_High_T2_6_Left/2.png", "Image/P29_D1_P5_High_T2_6_Left/3.png", "Image/P29_D1_P5_High_T2_6_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start Hey Friday\"\nThe frames roughly correspond to the speech interval from 2631.4952 seconds to 2641.2508 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P11_D1_P1_High_T3_13_Left", "images": ["Image/P11_D1_P1_High_T3_13_Left/1.png", "Image/P11_D1_P1_High_T3_13_Left/2.png", "Image/P11_D1_P1_High_T3_13_Left/3.png", "Image/P11_D1_P1_High_T3_13_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean the plant in front of me. Finish.\"\nThe frames roughly correspond to the speech interval from 2705.9244999999996 seconds to 2717.9929999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P15_D1_P7_Low_T1_22_Mid", "images": ["Image/P15_D1_P7_Low_T1_22_Mid/1.png", "Image/P15_D1_P7_Low_T1_22_Mid/2.png", "Image/P15_D1_P7_Low_T1_22_Mid/3.png", "Image/P15_D1_P7_Low_T1_22_Mid/4.png", "Image/P15_D1_P7_Low_T1_22_Mid/5.png", "Image/P15_D1_P7_Low_T1_22_Mid/6.png", "Image/P15_D1_P7_Low_T1_22_Mid/7.png", "Image/P15_D1_P7_Low_T1_22_Mid/8.png", "Image/P15_D1_P7_Low_T1_22_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move the plant from the floor to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2007.23297 seconds to 2020.98557 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D3_P8_Medium_T3_26_Right", "images": ["Image/P24_D3_P8_Medium_T3_26_Right/1.png", "Image/P24_D3_P8_Medium_T3_26_Right/2.png", "Image/P24_D3_P8_Medium_T3_26_Right/3.png", "Image/P24_D3_P8_Medium_T3_26_Right/4.png", "Image/P24_D3_P8_Medium_T3_26_Right/5.png", "Image/P24_D3_P8_Medium_T3_26_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2025.6433 seconds to 2036.5564 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P9_Low_T2_4_Left", "images": ["Image/P8_D2_P9_Low_T2_4_Left/1.png", "Image/P8_D2_P9_Low_T2_4_Left/2.png", "Image/P8_D2_P9_Low_T2_4_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"water that planet. Finish\"\nThe frames roughly correspond to the speech interval from 2549.2007000000003 seconds to 2559.3634 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P4_High_T1_8_Mid", "images": ["Image/P8_D3_P4_High_T1_8_Mid/1.png", "Image/P8_D3_P4_High_T1_8_Mid/2.png", "Image/P8_D3_P4_High_T1_8_Mid/3.png", "Image/P8_D3_P4_High_T1_8_Mid/4.png", "Image/P8_D3_P4_High_T1_8_Mid/5.png", "Image/P8_D3_P4_High_T1_8_Mid/6.png", "Image/P8_D3_P4_High_T1_8_Mid/7.png", "Image/P8_D3_P4_High_T1_8_Mid/8.png", "Image/P8_D3_P4_High_T1_8_Mid/9.png", "Image/P8_D3_P4_High_T1_8_Mid/10.png", "Image/P8_D3_P4_High_T1_8_Mid/11.png", "Image/P8_D3_P4_High_T1_8_Mid/12.png", "Image/P8_D3_P4_High_T1_8_Mid/13.png", "Image/P8_D3_P4_High_T1_8_Mid/14.png", "Image/P8_D3_P4_High_T1_8_Mid/15.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Hi. Star. Hi. Could you move the green glass to the wooden shelf? I mean the right side. And it is the second level\"\nThe frames roughly correspond to the speech interval from 1899.1545 seconds to 1920.3958 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D1_P3_High_T1_16_Left", "images": ["Image/P13_D1_P3_High_T1_16_Left/1.png", "Image/P13_D1_P3_High_T1_16_Left/2.png", "Image/P13_D1_P3_High_T1_16_Left/3.png", "Image/P13_D1_P3_High_T1_16_Left/4.png", "Image/P13_D1_P3_High_T1_16_Left/5.png", "Image/P13_D1_P3_High_T1_16_Left/6.png", "Image/P13_D1_P3_High_T1_16_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you move this brown cup down here please? Finish.\"\nThe frames roughly correspond to the speech interval from 357.3167 seconds to 370.1914 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D1_P3_Medium_T1_18_Mid", "images": ["Image/P16_D1_P3_Medium_T1_18_Mid/1.png", "Image/P16_D1_P3_Medium_T1_18_Mid/2.png", "Image/P16_D1_P3_Medium_T1_18_Mid/3.png", "Image/P16_D1_P3_Medium_T1_18_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move this mug down here. Finish.\"\nThe frames roughly correspond to the speech interval from 1316.6142 seconds to 1326.0788 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P12_D3_P3_Low_T3_13_Mid", "images": ["Image/P12_D3_P3_Low_T3_13_Mid/1.png", "Image/P12_D3_P3_Low_T3_13_Mid/2.png", "Image/P12_D3_P3_Low_T3_13_Mid/3.png", "Image/P12_D3_P3_Low_T3_13_Mid/4.png", "Image/P12_D3_P3_Low_T3_13_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2455.8571899999997 seconds to 2466.17269 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P7_High_T3_11_Mid", "images": ["Image/P24_D3_P7_High_T3_11_Mid/1.png", "Image/P24_D3_P7_High_T3_11_Mid/2.png", "Image/P24_D3_P7_High_T3_11_Mid/3.png", "Image/P24_D3_P7_High_T3_11_Mid/4.png", "Image/P24_D3_P7_High_T3_11_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1672.8591 seconds to 1683.492 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P5_Medium_T3_22_Mid", "images": ["Image/P9_D2_P5_Medium_T3_22_Mid/1.png", "Image/P9_D2_P5_Medium_T3_22_Mid/2.png", "Image/P9_D2_P5_Medium_T3_22_Mid/3.png", "Image/P9_D2_P5_Medium_T3_22_Mid/4.png", "Image/P9_D2_P5_Medium_T3_22_Mid/5.png", "Image/P9_D2_P5_Medium_T3_22_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the middle plant on the desk? Finish.\"\nThe frames roughly correspond to the speech interval from 2990.3266 seconds to 3002.9202 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P7_Medium_T1_3_Mid", "images": ["Image/P28_D1_P7_Medium_T1_3_Mid/1.png", "Image/P28_D1_P7_Medium_T1_3_Mid/2.png", "Image/P28_D1_P7_Medium_T1_3_Mid/3.png", "Image/P28_D1_P7_Medium_T1_3_Mid/4.png", "Image/P28_D1_P7_Medium_T1_3_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that middle vase to this shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 1404.1104 seconds to 1415.3563 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P4_Low_T1_4_Right", "images": ["Image/P28_D3_P4_Low_T1_4_Right/1.png", "Image/P28_D3_P4_Low_T1_4_Right/2.png", "Image/P28_D3_P4_Low_T1_4_Right/3.png", "Image/P28_D3_P4_Low_T1_4_Right/4.png", "Image/P28_D3_P4_Low_T1_4_Right/5.png", "Image/P28_D3_P4_Low_T1_4_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move that plant to that shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 82.58232000000001 seconds to 93.92716 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P30_D3_P4_Medium_T2_7_Left", "images": ["Image/P30_D3_P4_Medium_T2_7_Left/1.png", "Image/P30_D3_P4_Medium_T2_7_Left/2.png", "Image/P30_D3_P4_Medium_T2_7_Left/3.png", "Image/P30_D3_P4_Medium_T2_7_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1326.649 seconds to 1336.2691 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P7_Medium_T3_7_Mid", "images": ["Image/P4_D1_P7_Medium_T3_7_Mid/1.png", "Image/P4_D1_P7_Medium_T3_7_Mid/2.png", "Image/P4_D1_P7_Medium_T3_7_Mid/3.png", "Image/P4_D1_P7_Medium_T3_7_Mid/4.png", "Image/P4_D1_P7_Medium_T3_7_Mid/5.png", "Image/P4_D1_P7_Medium_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean the cap in the middle? Finish\"\nThe frames roughly correspond to the speech interval from 2519.8437999999996 seconds to 2533.8300999999997 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P8_Low_T3_11_Mid", "images": ["Image/P14_D3_P8_Low_T3_11_Mid/1.png", "Image/P14_D3_P8_Low_T3_11_Mid/2.png", "Image/P14_D3_P8_Low_T3_11_Mid/3.png", "Image/P14_D3_P8_Low_T3_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean that up. Finish.\"\nThe frames roughly correspond to the speech interval from 1024.72799 seconds to 1033.80416 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D1_P2_Low_T1_1_Right", "images": ["Image/P10_D1_P2_Low_T1_1_Right/1.png", "Image/P10_D1_P2_Low_T1_1_Right/2.png", "Image/P10_D1_P2_Low_T1_1_Right/3.png", "Image/P10_D1_P2_Low_T1_1_Right/4.png", "Image/P10_D1_P2_Low_T1_1_Right/5.png", "Image/P10_D1_P2_Low_T1_1_Right/6.png", "Image/P10_D1_P2_Low_T1_1_Right/7.png", "Image/P10_D1_P2_Low_T1_1_Right/8.png", "Image/P10_D1_P2_Low_T1_1_Right/9.png", "Image/P10_D1_P2_Low_T1_1_Right/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 1988.4869 seconds to 2011.0789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P3_D2_P5_Low_T1_24_Left", "images": ["Image/P3_D2_P5_Low_T1_24_Left/1.png", "Image/P3_D2_P5_Low_T1_24_Left/2.png", "Image/P3_D2_P5_Low_T1_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Could you clean this item? Finished.\"\nThe frames roughly correspond to the speech interval from 2473.1482 seconds to 2482.8789 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P13_D2_P1_High_T1_20_Right", "images": ["Image/P13_D2_P1_High_T1_20_Right/1.png", "Image/P13_D2_P1_High_T1_20_Right/2.png", "Image/P13_D2_P1_High_T1_20_Right/3.png", "Image/P13_D2_P1_High_T1_20_Right/4.png", "Image/P13_D2_P1_High_T1_20_Right/5.png", "Image/P13_D2_P1_High_T1_20_Right/6.png", "Image/P13_D2_P1_High_T1_20_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start Friday. Can you move this white bottle down to the bottom shelf\"\nThe frames roughly correspond to the speech interval from 774.35856 seconds to 786.3562800000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P19_D3_P2_Low_T2_6_Mid", "images": ["Image/P19_D3_P2_Low_T2_6_Mid/1.png", "Image/P19_D3_P2_Low_T2_6_Mid/2.png", "Image/P19_D3_P2_Low_T2_6_Mid/3.png", "Image/P19_D3_P2_Low_T2_6_Mid/4.png", "Image/P19_D3_P2_Low_T2_6_Mid/5.png", "Image/P19_D3_P2_Low_T2_6_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into that flowerpot. Finish.\"\nThe frames roughly correspond to the speech interval from 2207.7187999999996 seconds to 2220.5951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P16_D3_P2_Low_T3_7_Mid", "images": ["Image/P16_D3_P2_Low_T3_7_Mid/1.png", "Image/P16_D3_P2_Low_T3_7_Mid/2.png", "Image/P16_D3_P2_Low_T3_7_Mid/3.png", "Image/P16_D3_P2_Low_T3_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that vase. Finish.\"\nThe frames roughly correspond to the speech interval from 464.529 seconds to 473.53469999999993 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P27_D1_P8_Medium_T2_7_Mid", "images": ["Image/P27_D1_P8_Medium_T2_7_Mid/1.png", "Image/P27_D1_P8_Medium_T2_7_Mid/2.png", "Image/P27_D1_P8_Medium_T2_7_Mid/3.png", "Image/P27_D1_P8_Medium_T2_7_Mid/4.png", "Image/P27_D1_P8_Medium_T2_7_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in the middle cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1179.888 seconds to 1190.0911 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P1_D3_P4_Medium_T3_14_Left", "images": ["Image/P1_D3_P4_Medium_T3_14_Left/1.png", "Image/P1_D3_P4_Medium_T3_14_Left/2.png", "Image/P1_D3_P4_Medium_T3_14_Left/3.png", "Image/P1_D3_P4_Medium_T3_14_Left/4.png", "Image/P1_D3_P4_Medium_T3_14_Left/5.png", "Image/P1_D3_P4_Medium_T3_14_Left/6.png", "Image/P1_D3_P4_Medium_T3_14_Left/7.png", "Image/P1_D3_P4_Medium_T3_14_Left/8.png", "Image/P1_D3_P4_Medium_T3_14_Left/9.png", "Image/P1_D3_P4_Medium_T3_14_Left/10.png", "Image/P1_D3_P4_Medium_T3_14_Left/11.png", "Image/P1_D3_P4_Medium_T3_14_Left/12.png", "Image/P1_D3_P4_Medium_T3_14_Left/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Can you help clean the... The left plant\"\nThe frames roughly correspond to the speech interval from 1472.5962999999997 seconds to 1492.1703999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P7_High_T3_7_Mid", "images": ["Image/P24_D1_P7_High_T3_7_Mid/1.png", "Image/P24_D1_P7_High_T3_7_Mid/2.png", "Image/P24_D1_P7_High_T3_7_Mid/3.png", "Image/P24_D1_P7_High_T3_7_Mid/4.png", "Image/P24_D1_P7_High_T3_7_Mid/5.png", "Image/P24_D1_P7_High_T3_7_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1356.7186 seconds to 1368.159 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P2_High_T2_18_Left", "images": ["Image/P3_D3_P2_High_T2_18_Left/1.png", "Image/P3_D3_P2_High_T2_18_Left/2.png", "Image/P3_D3_P2_High_T2_18_Left/3.png", "Image/P3_D3_P2_High_T2_18_Left/4.png", "Image/P3_D3_P2_High_T2_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you put some water into the first item? Finish.\"\nThe frames roughly correspond to the speech interval from 767.3424 seconds to 779.8304999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P3_High_T3_24_Left", "images": ["Image/P30_D2_P3_High_T3_24_Left/1.png", "Image/P30_D2_P3_High_T3_24_Left/2.png", "Image/P30_D2_P3_High_T3_24_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 504.8243 seconds to 513.0921 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P13_D3_P2_High_T3_6_Right", "images": ["Image/P13_D3_P2_High_T3_6_Right/1.png", "Image/P13_D3_P2_High_T3_6_Right/2.png", "Image/P13_D3_P2_High_T3_6_Right/3.png", "Image/P13_D3_P2_High_T3_6_Right/4.png", "Image/P13_D3_P2_High_T3_6_Right/5.png", "Image/P13_D3_P2_High_T3_6_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Friday. Can you clean the plant over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1693.9315 seconds to 1705.109 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P5_D1_P2_Medium_T2_16_Left", "images": ["Image/P5_D1_P2_Medium_T2_16_Left/1.png", "Image/P5_D1_P2_Medium_T2_16_Left/2.png", "Image/P5_D1_P2_Medium_T2_16_Left/3.png", "Image/P5_D1_P2_Medium_T2_16_Left/4.png", "Image/P5_D1_P2_Medium_T2_16_Left/5.png", "Image/P5_D1_P2_Medium_T2_16_Left/6.png", "Image/P5_D1_P2_Medium_T2_16_Left/7.png", "Image/P5_D1_P2_Medium_T2_16_Left/8.png", "Image/P5_D1_P2_Medium_T2_16_Left/9.png", "Image/P5_D1_P2_Medium_T2_16_Left/10.png", "Image/P5_D1_P2_Medium_T2_16_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the cup that's in the middle of the shelf or in front of me. Finished.\"\nThe frames roughly correspond to the speech interval from 1695.7323 seconds to 1716.3453 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D1_P6_High_T1_2_Right", "images": ["Image/P18_D1_P6_High_T1_2_Right/1.png", "Image/P18_D1_P6_High_T1_2_Right/2.png", "Image/P18_D1_P6_High_T1_2_Right/3.png", "Image/P18_D1_P6_High_T1_2_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1013.8926 seconds to 1023.7901 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P4_High_T3_26_Mid", "images": ["Image/P10_D3_P4_High_T3_26_Mid/1.png", "Image/P10_D3_P4_High_T3_26_Mid/2.png", "Image/P10_D3_P4_High_T3_26_Mid/3.png", "Image/P10_D3_P4_High_T3_26_Mid/4.png", "Image/P10_D3_P4_High_T3_26_Mid/5.png", "Image/P10_D3_P4_High_T3_26_Mid/6.png", "Image/P10_D3_P4_High_T3_26_Mid/7.png", "Image/P10_D3_P4_High_T3_26_Mid/8.png", "Image/P10_D3_P4_High_T3_26_Mid/9.png", "Image/P10_D3_P4_High_T3_26_Mid/10.png", "Image/P10_D3_P4_High_T3_26_Mid/11.png", "Image/P10_D3_P4_High_T3_26_Mid/12.png", "Image/P10_D3_P4_High_T3_26_Mid/13.png", "Image/P10_D3_P4_High_T3_26_Mid/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey\"\nThe frames roughly correspond to the speech interval from 899.76749 seconds to 921.34229 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P6_Low_T3_20_Mid", "images": ["Image/P14_D3_P6_Low_T3_20_Mid/1.png", "Image/P14_D3_P6_Low_T3_20_Mid/2.png", "Image/P14_D3_P6_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1388.71436 seconds to 1397.26886 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D3_P9_Medium_T2_19_Left", "images": ["Image/P10_D3_P9_Medium_T2_19_Left/1.png", "Image/P10_D3_P9_Medium_T2_19_Left/2.png", "Image/P10_D3_P9_Medium_T2_19_Left/3.png", "Image/P10_D3_P9_Medium_T2_19_Left/4.png", "Image/P10_D3_P9_Medium_T2_19_Left/5.png", "Image/P10_D3_P9_Medium_T2_19_Left/6.png", "Image/P10_D3_P9_Medium_T2_19_Left/7.png", "Image/P10_D3_P9_Medium_T2_19_Left/8.png", "Image/P10_D3_P9_Medium_T2_19_Left/9.png", "Image/P10_D3_P9_Medium_T2_19_Left/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 778.46819 seconds to 796.2994899999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D3_P1_High_T3_21_Right", "images": ["Image/P10_D3_P1_High_T3_21_Right/1.png", "Image/P10_D3_P1_High_T3_21_Right/2.png", "Image/P10_D3_P1_High_T3_21_Right/3.png", "Image/P10_D3_P1_High_T3_21_Right/4.png", "Image/P10_D3_P1_High_T3_21_Right/5.png", "Image/P10_D3_P1_High_T3_21_Right/6.png", "Image/P10_D3_P1_High_T3_21_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Hey\"\nThe frames roughly correspond to the speech interval from 431.55719 seconds to 443.61049 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P29_D3_P4_Medium_T1_4_Right", "images": ["Image/P29_D3_P4_Medium_T1_4_Right/1.png", "Image/P29_D3_P4_Medium_T1_4_Right/2.png", "Image/P29_D3_P4_Medium_T1_4_Right/3.png", "Image/P29_D3_P4_Medium_T1_4_Right/4.png", "Image/P29_D3_P4_Medium_T1_4_Right/5.png", "Image/P29_D3_P4_Medium_T1_4_Right/6.png", "Image/P29_D3_P4_Medium_T1_4_Right/7.png", "Image/P29_D3_P4_Medium_T1_4_Right/8.png", "Image/P29_D3_P4_Medium_T1_4_Right/9.png", "Image/P29_D3_P4_Medium_T1_4_Right/10.png", "Image/P29_D3_P4_Medium_T1_4_Right/11.png", "Image/P29_D3_P4_Medium_T1_4_Right/12.png", "Image/P29_D3_P4_Medium_T1_4_Right/13.png", "Image/P29_D3_P4_Medium_T1_4_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1527.3243999999995 seconds to 1546.4647999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D1_P5_Medium_T2_3_Mid", "images": ["Image/P26_D1_P5_Medium_T2_3_Mid/1.png", "Image/P26_D1_P5_Medium_T2_3_Mid/2.png", "Image/P26_D1_P5_Medium_T2_3_Mid/3.png", "Image/P26_D1_P5_Medium_T2_3_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour water into the middle canister? Finish.\"\nThe frames roughly correspond to the speech interval from 388.515 seconds to 399.5250999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P29_D1_P3_High_T2_9_Left", "images": ["Image/P29_D1_P3_High_T2_9_Left/1.png", "Image/P29_D1_P3_High_T2_9_Left/2.png", "Image/P29_D1_P3_High_T2_9_Left/3.png", "Image/P29_D1_P3_High_T2_9_Left/4.png", "Image/P29_D1_P3_High_T2_9_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 2481.8284299999996 seconds to 2492.4763999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D1_P1_High_T1_25_Left", "images": ["Image/P23_D1_P1_High_T1_25_Left/1.png", "Image/P23_D1_P1_High_T1_25_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Start. Pour water there. Finish.\"\nThe frames roughly correspond to the speech interval from 1509.06734 seconds to 1517.5853499999998 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P8_High_T1_27_Mid", "images": ["Image/P17_D1_P8_High_T1_27_Mid/1.png", "Image/P17_D1_P8_High_T1_27_Mid/2.png", "Image/P17_D1_P8_High_T1_27_Mid/3.png", "Image/P17_D1_P8_High_T1_27_Mid/4.png", "Image/P17_D1_P8_High_T1_27_Mid/5.png", "Image/P17_D1_P8_High_T1_27_Mid/6.png", "Image/P17_D1_P8_High_T1_27_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"start? Can you move this white mug to this top shelf right here in front of me? Finish.\"\nThe frames roughly correspond to the speech interval from 1788.7057000000002 seconds to 1805.8822 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D2_P7_High_T3_14_Right", "images": ["Image/P11_D2_P7_High_T3_14_Right/1.png", "Image/P11_D2_P7_High_T3_14_Right/2.png", "Image/P11_D2_P7_High_T3_14_Right/3.png", "Image/P11_D2_P7_High_T3_14_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the plant. Please finish.\"\nThe frames roughly correspond to the speech interval from 698.7991 seconds to 708.5598 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D2_P6_Medium_T3_11_Right", "images": ["Image/P9_D2_P6_Medium_T3_11_Right/1.png", "Image/P9_D2_P6_Medium_T3_11_Right/2.png", "Image/P9_D2_P6_Medium_T3_11_Right/3.png", "Image/P9_D2_P6_Medium_T3_11_Right/4.png", "Image/P9_D2_P6_Medium_T3_11_Right/5.png", "Image/P9_D2_P6_Medium_T3_11_Right/6.png", "Image/P9_D2_P6_Medium_T3_11_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the right bottle on that desk? Finish.\"\nThe frames roughly correspond to the speech interval from 3010.795 seconds to 3024.7651 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P9_Low_T1_14_Mid", "images": ["Image/P28_D1_P9_Low_T1_14_Mid/1.png", "Image/P28_D1_P9_Low_T1_14_Mid/2.png", "Image/P28_D1_P9_Low_T1_14_Mid/3.png", "Image/P28_D1_P9_Low_T1_14_Mid/4.png", "Image/P28_D1_P9_Low_T1_14_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1752.7894 seconds to 1762.6864 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D1_P2_Low_T2_25_Right", "images": ["Image/P7_D1_P2_Low_T2_25_Right/1.png", "Image/P7_D1_P2_Low_T2_25_Right/2.png", "Image/P7_D1_P2_Low_T2_25_Right/3.png", "Image/P7_D1_P2_Low_T2_25_Right/4.png", "Image/P7_D1_P2_Low_T2_25_Right/5.png", "Image/P7_D1_P2_Low_T2_25_Right/6.png", "Image/P7_D1_P2_Low_T2_25_Right/7.png", "Image/P7_D1_P2_Low_T2_25_Right/8.png", "Image/P7_D1_P2_Low_T2_25_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 332.1658 seconds to 348.4239 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D1_P4_Low_T2_27_Mid", "images": ["Image/P9_D1_P4_Low_T2_27_Mid/1.png", "Image/P9_D1_P4_Low_T2_27_Mid/2.png", "Image/P9_D1_P4_Low_T2_27_Mid/3.png", "Image/P9_D1_P4_Low_T2_27_Mid/4.png", "Image/P9_D1_P4_Low_T2_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water in this cup? Finish.\"\nThe frames roughly correspond to the speech interval from 1915.048 seconds to 1926.4065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P5_High_T2_1_Right", "images": ["Image/P30_D2_P5_High_T2_1_Right/1.png", "Image/P30_D2_P5_High_T2_1_Right/2.png", "Image/P30_D2_P5_High_T2_1_Right/3.png", "Image/P30_D2_P5_High_T2_1_Right/4.png", "Image/P30_D2_P5_High_T2_1_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put some water in the green base. Finish.\"\nThe frames roughly correspond to the speech interval from 184.1092 seconds to 195.8855 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P7_Low_T1_9_Mid", "images": ["Image/P25_D2_P7_Low_T1_9_Mid/1.png", "Image/P25_D2_P7_Low_T1_9_Mid/2.png", "Image/P25_D2_P7_Low_T1_9_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move this over here. Finish.\"\nThe frames roughly correspond to the speech interval from 692.94548 seconds to 701.5064 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D3_P6_Medium_T3_11_Right", "images": ["Image/P10_D3_P6_Medium_T3_11_Right/1.png", "Image/P10_D3_P6_Medium_T3_11_Right/2.png", "Image/P10_D3_P6_Medium_T3_11_Right/3.png", "Image/P10_D3_P6_Medium_T3_11_Right/4.png", "Image/P10_D3_P6_Medium_T3_11_Right/5.png", "Image/P10_D3_P6_Medium_T3_11_Right/6.png", "Image/P10_D3_P6_Medium_T3_11_Right/7.png", "Image/P10_D3_P6_Medium_T3_11_Right/8.png", "Image/P10_D3_P6_Medium_T3_11_Right/9.png", "Image/P10_D3_P6_Medium_T3_11_Right/10.png", "Image/P10_D3_P6_Medium_T3_11_Right/11.png", "Image/P10_D3_P6_Medium_T3_11_Right/12.png", "Image/P10_D3_P6_Medium_T3_11_Right/13.png", "Image/P10_D3_P6_Medium_T3_11_Right/14.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 832.5748899999999 seconds to 852.2706900000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D3_P7_Medium_T1_10_Mid", "images": ["Image/P24_D3_P7_Medium_T1_10_Mid/1.png", "Image/P24_D3_P7_Medium_T1_10_Mid/2.png", "Image/P24_D3_P7_Medium_T1_10_Mid/3.png", "Image/P24_D3_P7_Medium_T1_10_Mid/4.png", "Image/P24_D3_P7_Medium_T1_10_Mid/5.png", "Image/P24_D3_P7_Medium_T1_10_Mid/6.png", "Image/P24_D3_P7_Medium_T1_10_Mid/7.png", "Image/P24_D3_P7_Medium_T1_10_Mid/8.png", "Image/P24_D3_P7_Medium_T1_10_Mid/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please move this bottle to this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 1868.8931 seconds to 1883.3687 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D2_P2_Medium_T2_10_Left", "images": ["Image/P25_D2_P2_Medium_T2_10_Left/1.png", "Image/P25_D2_P2_Medium_T2_10_Left/2.png", "Image/P25_D2_P2_Medium_T2_10_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 868.7544 seconds to 877.2473 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P28_D2_P1_High_T3_25_Right", "images": ["Image/P28_D2_P1_High_T3_25_Right/1.png", "Image/P28_D2_P1_High_T3_25_Right/2.png", "Image/P28_D2_P1_High_T3_25_Right/3.png", "Image/P28_D2_P1_High_T3_25_Right/4.png", "Image/P28_D2_P1_High_T3_25_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that white vase over there. Finish\"\nThe frames roughly correspond to the speech interval from 907.511 seconds to 918.9945 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D3_P1_High_T3_9_Mid", "images": ["Image/P2_D3_P1_High_T3_9_Mid/1.png", "Image/P2_D3_P1_High_T3_9_Mid/2.png", "Image/P2_D3_P1_High_T3_9_Mid/3.png", "Image/P2_D3_P1_High_T3_9_Mid/4.png", "Image/P2_D3_P1_High_T3_9_Mid/5.png", "Image/P2_D3_P1_High_T3_9_Mid/6.png", "Image/P2_D3_P1_High_T3_9_Mid/7.png", "Image/P2_D3_P1_High_T3_9_Mid/8.png", "Image/P2_D3_P1_High_T3_9_Mid/9.png", "Image/P2_D3_P1_High_T3_9_Mid/10.png", "Image/P2_D3_P1_High_T3_9_Mid/11.png", "Image/P2_D3_P1_High_T3_9_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you please clean that ring cup for me? Thank you. Finish.\"\nThe frames roughly correspond to the speech interval from 1807.9777000000004 seconds to 1825.6815 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P19_D2_P7_High_T2_11_Right", "images": ["Image/P19_D2_P7_High_T2_11_Right/1.png", "Image/P19_D2_P7_High_T2_11_Right/2.png", "Image/P19_D2_P7_High_T2_11_Right/3.png", "Image/P19_D2_P7_High_T2_11_Right/4.png", "Image/P19_D2_P7_High_T2_11_Right/5.png", "Image/P19_D2_P7_High_T2_11_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please pour into this glass here.\"\nThe frames roughly correspond to the speech interval from 1908.43 seconds to 1921.3757 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P30_D2_P1_Medium_T1_13_Mid", "images": ["Image/P30_D2_P1_Medium_T1_13_Mid/1.png", "Image/P30_D2_P1_Medium_T1_13_Mid/2.png", "Image/P30_D2_P1_Medium_T1_13_Mid/3.png", "Image/P30_D2_P1_Medium_T1_13_Mid/4.png", "Image/P30_D2_P1_Medium_T1_13_Mid/5.png", "Image/P30_D2_P1_Medium_T1_13_Mid/6.png", "Image/P30_D2_P1_Medium_T1_13_Mid/7.png", "Image/P30_D2_P1_Medium_T1_13_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the middle plant on the top shelf to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 60.18126000000001 seconds to 75.42291000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D1_P7_Medium_T1_22_Left", "images": ["Image/P6_D1_P7_Medium_T1_22_Left/1.png", "Image/P6_D1_P7_Medium_T1_22_Left/2.png", "Image/P6_D1_P7_Medium_T1_22_Left/3.png", "Image/P6_D1_P7_Medium_T1_22_Left/4.png", "Image/P6_D1_P7_Medium_T1_22_Left/5.png", "Image/P6_D1_P7_Medium_T1_22_Left/6.png", "Image/P6_D1_P7_Medium_T1_22_Left/7.png", "Image/P6_D1_P7_Medium_T1_22_Left/8.png", "Image/P6_D1_P7_Medium_T1_22_Left/9.png", "Image/P6_D1_P7_Medium_T1_22_Left/10.png", "Image/P6_D1_P7_Medium_T1_22_Left/11.png", "Image/P6_D1_P7_Medium_T1_22_Left/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start! Hey Friday\"\nThe frames roughly correspond to the speech interval from 2250.9797000000003 seconds to 2271.6773000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P25_D3_P5_High_T2_2_Left", "images": ["Image/P25_D3_P5_High_T2_2_Left/1.png", "Image/P25_D3_P5_High_T2_2_Left/2.png", "Image/P25_D3_P5_High_T2_2_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Pour water into the vase. Finish.\"\nThe frames roughly correspond to the speech interval from 2087.1588 seconds to 2095.5951 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P5_High_T2_22_Right", "images": ["Image/P9_D3_P5_High_T2_22_Right/1.png", "Image/P9_D3_P5_High_T2_22_Right/2.png", "Image/P9_D3_P5_High_T2_22_Right/3.png", "Image/P9_D3_P5_High_T2_22_Right/4.png", "Image/P9_D3_P5_High_T2_22_Right/5.png", "Image/P9_D3_P5_High_T2_22_Right/6.png", "Image/P9_D3_P5_High_T2_22_Right/7.png", "Image/P9_D3_P5_High_T2_22_Right/8.png", "Image/P9_D3_P5_High_T2_22_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to pour some water on the plant on the third layer? Finish.\"\nThe frames roughly correspond to the speech interval from 1064.0675 seconds to 1080.065 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D2_P3_Medium_T1_20_Right", "images": ["Image/P25_D2_P3_Medium_T1_20_Right/1.png", "Image/P25_D2_P3_Medium_T1_20_Right/2.png", "Image/P25_D2_P3_Medium_T1_20_Right/3.png", "Image/P25_D2_P3_Medium_T1_20_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the vase over here. Finish.\"\nThe frames roughly correspond to the speech interval from 1043.0737 seconds to 1052.1218 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P11_D1_P3_High_T3_3_Left", "images": ["Image/P11_D1_P3_High_T3_3_Left/1.png", "Image/P11_D1_P3_High_T3_3_Left/2.png", "Image/P11_D1_P3_High_T3_3_Left/3.png", "Image/P11_D1_P3_High_T3_3_Left/4.png", "Image/P11_D1_P3_High_T3_3_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean the green bottle on the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 2777.1092 seconds to 2789.8877999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P2_D1_P3_Low_T3_3_Right", "images": ["Image/P2_D1_P3_Low_T3_3_Right/1.png", "Image/P2_D1_P3_Low_T3_3_Right/2.png", "Image/P2_D1_P3_Low_T3_3_Right/3.png", "Image/P2_D1_P3_Low_T3_3_Right/4.png", "Image/P2_D1_P3_Low_T3_3_Right/5.png", "Image/P2_D1_P3_Low_T3_3_Right/6.png", "Image/P2_D1_P3_Low_T3_3_Right/7.png", "Image/P2_D1_P3_Low_T3_3_Right/8.png", "Image/P2_D1_P3_Low_T3_3_Right/9.png", "Image/P2_D1_P3_Low_T3_3_Right/10.png", "Image/P2_D1_P3_Low_T3_3_Right/11.png", "Image/P2_D1_P3_Low_T3_3_Right/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 590.4922999999999 seconds to 614.516 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P7_Medium_T1_11_Mid", "images": ["Image/P18_D2_P7_Medium_T1_11_Mid/1.png", "Image/P18_D2_P7_Medium_T1_11_Mid/2.png", "Image/P18_D2_P7_Medium_T1_11_Mid/3.png", "Image/P18_D2_P7_Medium_T1_11_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move that middle bottle to this top shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 495.8423 seconds to 505.476 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D3_P6_Medium_T1_24_Right", "images": ["Image/P16_D3_P6_Medium_T1_24_Right/1.png", "Image/P16_D3_P6_Medium_T1_24_Right/2.png", "Image/P16_D3_P6_Medium_T1_24_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Please move that plant over there. Finish.\"\nThe frames roughly correspond to the speech interval from 282.1636 seconds to 291.43449999999996 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P7_Low_T2_7_Right", "images": ["Image/P23_D1_P7_Low_T2_7_Right/1.png", "Image/P23_D1_P7_Low_T2_7_Right/2.png", "Image/P23_D1_P7_Low_T2_7_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1784.3179 seconds to 1793.4231 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D2_P4_Medium_T3_17_Mid", "images": ["Image/P8_D2_P4_Medium_T3_17_Mid/1.png", "Image/P8_D2_P4_Medium_T3_17_Mid/2.png", "Image/P8_D2_P4_Medium_T3_17_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Could you water the planet that is on the right? Could you clean the middle brown glass?\"\nThe frames roughly correspond to the speech interval from 2193.5759000000003 seconds to 2204.3628000000003 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P9_D3_P5_Medium_T3_25_Left", "images": ["Image/P9_D3_P5_Medium_T3_25_Left/1.png", "Image/P9_D3_P5_Medium_T3_25_Left/2.png", "Image/P9_D3_P5_Medium_T3_25_Left/3.png", "Image/P9_D3_P5_Medium_T3_25_Left/4.png", "Image/P9_D3_P5_Medium_T3_25_Left/5.png", "Image/P9_D3_P5_Medium_T3_25_Left/6.png", "Image/P9_D3_P5_Medium_T3_25_Left/7.png", "Image/P9_D3_P5_Medium_T3_25_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to clean the cup on the third layer? Finish.\"\nThe frames roughly correspond to the speech interval from 868.0873 seconds to 887.653 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D1_P2_Low_T3_15_Left", "images": ["Image/P28_D1_P2_Low_T3_15_Left/1.png", "Image/P28_D1_P2_Low_T3_15_Left/2.png", "Image/P28_D1_P2_Low_T3_15_Left/3.png", "Image/P28_D1_P2_Low_T3_15_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1421.6493 seconds to 1430.7937 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D3_P2_Medium_T2_5_Right", "images": ["Image/P3_D3_P2_Medium_T2_5_Right/1.png", "Image/P3_D3_P2_Medium_T2_5_Right/2.png", "Image/P3_D3_P2_Medium_T2_5_Right/3.png", "Image/P3_D3_P2_Medium_T2_5_Right/4.png", "Image/P3_D3_P2_Medium_T2_5_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you give some water to the third item? Finish.\"\nThe frames roughly correspond to the speech interval from 690.0482999999999 seconds to 701.3251 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the open shelf unit\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P10_D2_P4_Low_T3_18_Left", "images": ["Image/P10_D2_P4_Low_T3_18_Left/1.png", "Image/P10_D2_P4_Low_T3_18_Left/2.png", "Image/P10_D2_P4_Low_T3_18_Left/3.png", "Image/P10_D2_P4_Low_T3_18_Left/4.png", "Image/P10_D2_P4_Low_T3_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1648.7192800000005 seconds to 1664.14228 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P18_D2_P2_Low_T1_25_Left", "images": ["Image/P18_D2_P2_Low_T1_25_Left/1.png", "Image/P18_D2_P2_Low_T1_25_Left/2.png", "Image/P18_D2_P2_Low_T1_25_Left/3.png", "Image/P18_D2_P2_Low_T1_25_Left/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the cup to the bottom shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 103.02439000000004 seconds to 113.36664000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P28_D3_P7_Low_T3_17_Mid", "images": ["Image/P28_D3_P7_Low_T3_17_Mid/1.png", "Image/P28_D3_P7_Low_T3_17_Mid/2.png", "Image/P28_D3_P7_Low_T3_17_Mid/3.png", "Image/P28_D3_P7_Low_T3_17_Mid/4.png", "Image/P28_D3_P7_Low_T3_17_Mid/5.png", "Image/P28_D3_P7_Low_T3_17_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Please clean that vase under the desk. Finish.\"\nThe frames roughly correspond to the speech interval from 61.23020000000001 seconds to 74.78632000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Clean\"}"}]}
{"id": "P10_D2_P3_High_T2_10_Right", "images": ["Image/P10_D2_P3_High_T2_10_Right/1.png", "Image/P10_D2_P3_High_T2_10_Right/2.png", "Image/P10_D2_P3_High_T2_10_Right/3.png", "Image/P10_D2_P3_High_T2_10_Right/4.png", "Image/P10_D2_P3_High_T2_10_Right/5.png", "Image/P10_D2_P3_High_T2_10_Right/6.png", "Image/P10_D2_P3_High_T2_10_Right/7.png", "Image/P10_D2_P3_High_T2_10_Right/8.png", "Image/P10_D2_P3_High_T2_10_Right/9.png", "Image/P10_D2_P3_High_T2_10_Right/10.png", "Image/P10_D2_P3_High_T2_10_Right/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1110.3114 seconds to 1128.18423 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P14_D3_P4_Medium_T3_5_Right", "images": ["Image/P14_D3_P4_Medium_T3_5_Right/1.png", "Image/P14_D3_P4_Medium_T3_5_Right/2.png", "Image/P14_D3_P4_Medium_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1356.65186 seconds to 1365.45686 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Clean\"}"}]}
{"id": "P26_D1_P2_High_T1_8_Right", "images": ["Image/P26_D1_P2_High_T1_8_Right/1.png", "Image/P26_D1_P2_High_T1_8_Right/2.png", "Image/P26_D1_P2_High_T1_8_Right/3.png", "Image/P26_D1_P2_High_T1_8_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the green cup on the floor? Finish\"\nThe frames roughly correspond to the speech interval from 513.7804 seconds to 524.3456 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P27_D3_P5_High_T1_15_Left", "images": ["Image/P27_D3_P5_High_T1_15_Left/1.png", "Image/P27_D3_P5_High_T1_15_Left/2.png", "Image/P27_D3_P5_High_T1_15_Left/3.png", "Image/P27_D3_P5_High_T1_15_Left/4.png", "Image/P27_D3_P5_High_T1_15_Left/5.png", "Image/P27_D3_P5_High_T1_15_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you take that plant and put it on the middle shelf over there? Finish\"\nThe frames roughly correspond to the speech interval from 249.4354 seconds to 261.5599 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P14_D2_P3_High_T3_14_Left", "images": ["Image/P14_D2_P3_High_T3_14_Left/1.png", "Image/P14_D2_P3_High_T3_14_Left/2.png", "Image/P14_D2_P3_High_T3_14_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean up that plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1793.5958999999998 seconds to 1801.9022999999995 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P3_D2_P9_High_T2_27_Left", "images": ["Image/P3_D2_P9_High_T2_27_Left/1.png", "Image/P3_D2_P9_High_T2_27_Left/2.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image>\nThe user's spoken instruction is: \"Could you give some water to this item? Could you clean this item?\"\nThe frames roughly correspond to the speech interval from 2305.6249000000003 seconds to 2315.8141 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D3_P7_Low_T1_15_Right", "images": ["Image/P4_D3_P7_Low_T1_15_Right/1.png", "Image/P4_D3_P7_Low_T1_15_Right/2.png", "Image/P4_D3_P7_Low_T1_15_Right/3.png", "Image/P4_D3_P7_Low_T1_15_Right/4.png", "Image/P4_D3_P7_Low_T1_15_Right/5.png", "Image/P4_D3_P7_Low_T1_15_Right/6.png", "Image/P4_D3_P7_Low_T1_15_Right/7.png", "Image/P4_D3_P7_Low_T1_15_Right/8.png", "Image/P4_D3_P7_Low_T1_15_Right/9.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move the plants on the ground to the top of the shelter on the desk? Finished.\"\nThe frames roughly correspond to the speech interval from 495.0236999999999 seconds to 512.7303999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"under the desk\", \"Task Type\": \"Move\"}"}]}
{"id": "P8_D1_P4_High_T2_18_Left", "images": ["Image/P8_D1_P4_High_T2_18_Left/1.png", "Image/P8_D1_P4_High_T2_18_Left/2.png", "Image/P8_D1_P4_High_T2_18_Left/3.png", "Image/P8_D1_P4_High_T2_18_Left/4.png", "Image/P8_D1_P4_High_T2_18_Left/5.png", "Image/P8_D1_P4_High_T2_18_Left/6.png", "Image/P8_D1_P4_High_T2_18_Left/7.png", "Image/P8_D1_P4_High_T2_18_Left/8.png", "Image/P8_D1_P4_High_T2_18_Left/9.png", "Image/P8_D1_P4_High_T2_18_Left/10.png", "Image/P8_D1_P4_High_T2_18_Left/11.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"So there is a brown glass on the left side of me. So will you pour some water to that glass? Thank you. Finished.\"\nThe frames roughly correspond to the speech interval from 551.7287999999999 seconds to 571.3083999999999 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P23_D3_P9_High_T2_19_Left", "images": ["Image/P23_D3_P9_High_T2_19_Left/1.png", "Image/P23_D3_P9_High_T2_19_Left/2.png", "Image/P23_D3_P9_High_T2_19_Left/3.png", "Image/P23_D3_P9_High_T2_19_Left/4.png", "Image/P23_D3_P9_High_T2_19_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Put water into the white waste. Finish.\"\nThe frames roughly correspond to the speech interval from 1028.3181 seconds to 1039.5648 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P4_D1_P5_Low_T2_7_Mid", "images": ["Image/P4_D1_P5_Low_T2_7_Mid/1.png", "Image/P4_D1_P5_Low_T2_7_Mid/2.png", "Image/P4_D1_P5_Low_T2_7_Mid/3.png", "Image/P4_D1_P5_Low_T2_7_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you fill in this bottle? Finish.\"\nThe frames roughly correspond to the speech interval from 2585.1418 seconds to 2597.5975 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P24_D2_P2_High_T2_18_Right", "images": ["Image/P24_D2_P2_High_T2_18_Right/1.png", "Image/P24_D2_P2_High_T2_18_Right/2.png", "Image/P24_D2_P2_High_T2_18_Right/3.png", "Image/P24_D2_P2_High_T2_18_Right/4.png", "Image/P24_D2_P2_High_T2_18_Right/5.png", "Image/P24_D2_P2_High_T2_18_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 531.0788 seconds to 542.715 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P9_D3_P6_Low_T1_6_Mid", "images": ["Image/P9_D3_P6_Low_T1_6_Mid/1.png", "Image/P9_D3_P6_Low_T1_6_Mid/2.png", "Image/P9_D3_P6_Low_T1_6_Mid/3.png", "Image/P9_D3_P6_Low_T1_6_Mid/4.png", "Image/P9_D3_P6_Low_T1_6_Mid/5.png", "Image/P9_D3_P6_Low_T1_6_Mid/6.png", "Image/P9_D3_P6_Low_T1_6_Mid/7.png", "Image/P9_D3_P6_Low_T1_6_Mid/8.png", "Image/P9_D3_P6_Low_T1_6_Mid/9.png", "Image/P9_D3_P6_Low_T1_6_Mid/10.png", "Image/P9_D3_P6_Low_T1_6_Mid/11.png", "Image/P9_D3_P6_Low_T1_6_Mid/12.png", "Image/P9_D3_P6_Low_T1_6_Mid/13.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you help me to move the plant on the second layer to the desk on the left? Finish.\"\nThe frames roughly correspond to the speech interval from 802.3491 seconds to 822.2203 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P16_D2_P4_Medium_T3_26_Left", "images": ["Image/P16_D2_P4_Medium_T3_26_Left/1.png", "Image/P16_D2_P4_Medium_T3_26_Left/2.png", "Image/P16_D2_P4_Medium_T3_26_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start Please clean that mug. Finish.\"\nThe frames roughly correspond to the speech interval from 955.7036 seconds to 965.1287 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the middle shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P28_D3_P9_High_T1_17_Left", "images": ["Image/P28_D3_P9_High_T1_17_Left/1.png", "Image/P28_D3_P9_High_T1_17_Left/2.png", "Image/P28_D3_P9_High_T1_17_Left/3.png", "Image/P28_D3_P9_High_T1_17_Left/4.png", "Image/P28_D3_P9_High_T1_17_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Please move the orange vase to the top of the shelf. Finish.\"\nThe frames roughly correspond to the speech interval from 194.7212 seconds to 207.1515 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D2_P9_Medium_T2_26_Right", "images": ["Image/P17_D2_P9_Medium_T2_26_Right/1.png", "Image/P17_D2_P9_Medium_T2_26_Right/2.png", "Image/P17_D2_P9_Medium_T2_26_Right/3.png", "Image/P17_D2_P9_Medium_T2_26_Right/4.png", "Image/P17_D2_P9_Medium_T2_26_Right/5.png", "Image/P17_D2_P9_Medium_T2_26_Right/6.png", "Image/P17_D2_P9_Medium_T2_26_Right/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you pour some water in that mug down there on the right? Finish.\"\nThe frames roughly correspond to the speech interval from 183.7801 seconds to 196.8657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P6_D3_P4_Medium_T1_23_Mid", "images": ["Image/P6_D3_P4_Medium_T1_23_Mid/1.png", "Image/P6_D3_P4_Medium_T1_23_Mid/2.png", "Image/P6_D3_P4_Medium_T1_23_Mid/3.png", "Image/P6_D3_P4_Medium_T1_23_Mid/4.png", "Image/P6_D3_P4_Medium_T1_23_Mid/5.png", "Image/P6_D3_P4_Medium_T1_23_Mid/6.png", "Image/P6_D3_P4_Medium_T1_23_Mid/7.png", "Image/P6_D3_P4_Medium_T1_23_Mid/8.png", "Image/P6_D3_P4_Medium_T1_23_Mid/9.png", "Image/P6_D3_P4_Medium_T1_23_Mid/10.png", "Image/P6_D3_P4_Medium_T1_23_Mid/11.png", "Image/P6_D3_P4_Medium_T1_23_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Such. Hey Friday\"\nThe frames roughly correspond to the speech interval from 3382.8465 seconds to 3403.5022 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the desk surface\", \"Task Type\": \"Move\"}"}]}
{"id": "P23_D1_P4_High_T3_5_Right", "images": ["Image/P23_D1_P4_High_T3_5_Right/1.png", "Image/P23_D1_P4_High_T3_5_Right/2.png", "Image/P23_D1_P4_High_T3_5_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Not clean at the plant. Finish.\"\nThe frames roughly correspond to the speech interval from 1557.32592 seconds to 1566.0404 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P23_D3_P1_High_T1_11_Right", "images": ["Image/P23_D3_P1_High_T1_11_Right/1.png", "Image/P23_D3_P1_High_T1_11_Right/2.png", "Image/P23_D3_P1_High_T1_11_Right/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the red one over there. Finish.\"\nThe frames roughly correspond to the speech interval from 998.8775 seconds to 1007.6743 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D3_P5_Low_T3_24_Mid", "images": ["Image/P20_D3_P5_Low_T3_24_Mid/1.png", "Image/P20_D3_P5_Low_T3_24_Mid/2.png", "Image/P20_D3_P5_Low_T3_24_Mid/3.png", "Image/P20_D3_P5_Low_T3_24_Mid/4.png", "Image/P20_D3_P5_Low_T3_24_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey robot\"\nThe frames roughly correspond to the speech interval from 1539.6298 seconds to 1550.069 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the middle level of the open shelf unit\", \"Task Type\": \"Clean\"}"}]}
{"id": "P6_D3_P1_Low_T3_23_Right", "images": ["Image/P6_D3_P1_Low_T3_23_Right/1.png", "Image/P6_D3_P1_Low_T3_23_Right/2.png", "Image/P6_D3_P1_Low_T3_23_Right/3.png", "Image/P6_D3_P1_Low_T3_23_Right/4.png", "Image/P6_D3_P1_Low_T3_23_Right/5.png", "Image/P6_D3_P1_Low_T3_23_Right/6.png", "Image/P6_D3_P1_Low_T3_23_Right/7.png", "Image/P6_D3_P1_Low_T3_23_Right/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey\"\nThe frames roughly correspond to the speech interval from 3488.1356 seconds to 3502.0421 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P24_D1_P3_Low_T1_21_Mid", "images": ["Image/P24_D1_P3_Low_T1_21_Mid/1.png", "Image/P24_D1_P3_Low_T1_21_Mid/2.png", "Image/P24_D1_P3_Low_T1_21_Mid/3.png", "Image/P24_D1_P3_Low_T1_21_Mid/4.png", "Image/P24_D1_P3_Low_T1_21_Mid/5.png", "Image/P24_D1_P3_Low_T1_21_Mid/6.png", "Image/P24_D1_P3_Low_T1_21_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you move this bottle over there please? Finish.\"\nThe frames roughly correspond to the speech interval from 1072.6685 seconds to 1084.6657 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P24_D1_P1_Low_T2_5_Mid", "images": ["Image/P24_D1_P1_Low_T2_5_Mid/1.png", "Image/P24_D1_P1_Low_T2_5_Mid/2.png", "Image/P24_D1_P1_Low_T2_5_Mid/3.png", "Image/P24_D1_P1_Low_T2_5_Mid/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Can you please pour water in this planet? Finish.\"\nThe frames roughly correspond to the speech interval from 1461.3118 seconds to 1471.6812 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D3_P7_High_T2_27_Mid", "images": ["Image/P7_D3_P7_High_T2_27_Mid/1.png", "Image/P7_D3_P7_High_T2_27_Mid/2.png", "Image/P7_D3_P7_High_T2_27_Mid/3.png", "Image/P7_D3_P7_High_T2_27_Mid/4.png", "Image/P7_D3_P7_High_T2_27_Mid/5.png", "Image/P7_D3_P7_High_T2_27_Mid/6.png", "Image/P7_D3_P7_High_T2_27_Mid/7.png", "Image/P7_D3_P7_High_T2_27_Mid/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Friday\"\nThe frames roughly correspond to the speech interval from 1886.593 seconds to 1901.4457 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"under the desk\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P25_D1_P9_High_T3_11_Left", "images": ["Image/P25_D1_P9_High_T3_11_Left/1.png", "Image/P25_D1_P9_High_T3_11_Left/2.png", "Image/P25_D1_P9_High_T3_11_Left/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean this. Finish.\"\nThe frames roughly correspond to the speech interval from 364.1975 seconds to 372.3119 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D2_P6_High_T2_8_Left", "images": ["Image/P14_D2_P6_High_T2_8_Left/1.png", "Image/P14_D2_P6_High_T2_8_Left/2.png", "Image/P14_D2_P6_High_T2_8_Left/3.png", "Image/P14_D2_P6_High_T2_8_Left/4.png", "Image/P14_D2_P6_High_T2_8_Left/5.png", "Image/P14_D2_P6_High_T2_8_Left/6.png", "Image/P14_D2_P6_High_T2_8_Left/7.png", "Image/P14_D2_P6_High_T2_8_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 1632.6944 seconds to 1646.399 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P18_D2_P6_Medium_T2_6_Left", "images": ["Image/P18_D2_P6_Medium_T2_6_Left/1.png", "Image/P18_D2_P6_Medium_T2_6_Left/2.png", "Image/P18_D2_P6_Medium_T2_6_Left/3.png", "Image/P18_D2_P6_Medium_T2_6_Left/4.png", "Image/P18_D2_P6_Medium_T2_6_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Water the first plant that's separate from the other two. Finish.\"\nThe frames roughly correspond to the speech interval from 335.1118 seconds to 347.0403 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P27_D1_P7_High_T1_8_Mid", "images": ["Image/P27_D1_P7_High_T1_8_Mid/1.png", "Image/P27_D1_P7_High_T1_8_Mid/2.png", "Image/P27_D1_P7_High_T1_8_Mid/3.png", "Image/P27_D1_P7_High_T1_8_Mid/4.png", "Image/P27_D1_P7_High_T1_8_Mid/5.png", "Image/P27_D1_P7_High_T1_8_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you move that cup onto this shelf? Finish.\"\nThe frames roughly correspond to the speech interval from 741.8986600000001 seconds to 752.9098200000001 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P10_D2_P3_Low_T1_5_Mid", "images": ["Image/P10_D2_P3_Low_T1_5_Mid/1.png", "Image/P10_D2_P3_Low_T1_5_Mid/2.png", "Image/P10_D2_P3_Low_T1_5_Mid/3.png", "Image/P10_D2_P3_Low_T1_5_Mid/4.png", "Image/P10_D2_P3_Low_T1_5_Mid/5.png", "Image/P10_D2_P3_Low_T1_5_Mid/6.png", "Image/P10_D2_P3_Low_T1_5_Mid/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 1434.70208 seconds to 1446.7130800000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P17_D1_P5_High_T3_12_Left", "images": ["Image/P17_D1_P5_High_T3_12_Left/1.png", "Image/P17_D1_P5_High_T3_12_Left/2.png", "Image/P17_D1_P5_High_T3_12_Left/3.png", "Image/P17_D1_P5_High_T3_12_Left/4.png", "Image/P17_D1_P5_High_T3_12_Left/5.png", "Image/P17_D1_P5_High_T3_12_Left/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start Can you clean off this brown object next to the green mug? Finish\"\nThe frames roughly correspond to the speech interval from 1764.4428000000005 seconds to 1781.9257000000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Clean\"}"}]}
{"id": "P14_D3_P9_Medium_T1_16_Left", "images": ["Image/P14_D3_P9_Medium_T1_16_Left/1.png", "Image/P14_D3_P9_Medium_T1_16_Left/2.png", "Image/P14_D3_P9_Medium_T1_16_Left/3.png", "Image/P14_D3_P9_Medium_T1_16_Left/4.png", "Image/P14_D3_P9_Medium_T1_16_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start\"\nThe frames roughly correspond to the speech interval from 1290.96716 seconds to 1300.7297600000002 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P7_D2_P6_Low_T3_17_Left", "images": ["Image/P7_D2_P6_Low_T3_17_Left/1.png", "Image/P7_D2_P6_Low_T3_17_Left/2.png", "Image/P7_D2_P6_Low_T3_17_Left/3.png", "Image/P7_D2_P6_Low_T3_17_Left/4.png", "Image/P7_D2_P6_Low_T3_17_Left/5.png", "Image/P7_D2_P6_Low_T3_17_Left/6.png", "Image/P7_D2_P6_Low_T3_17_Left/7.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start. Hey Frederick\"\nThe frames roughly correspond to the speech interval from 1299.7324 seconds to 1314.1412 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P8_D2_P1_Medium_T2_21_Mid", "images": ["Image/P8_D2_P1_Medium_T2_21_Mid/1.png", "Image/P8_D2_P1_Medium_T2_21_Mid/2.png", "Image/P8_D2_P1_Medium_T2_21_Mid/3.png", "Image/P8_D2_P1_Medium_T2_21_Mid/4.png", "Image/P8_D2_P1_Medium_T2_21_Mid/5.png", "Image/P8_D2_P1_Medium_T2_21_Mid/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Could you pour some water to the white glass in the middle? Finish.\"\nThe frames roughly correspond to the speech interval from 2298.963 seconds to 2310.6688000000004 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P5_D2_P5_Medium_T3_27_Mid", "images": ["Image/P5_D2_P5_Medium_T3_27_Mid/1.png", "Image/P5_D2_P5_Medium_T3_27_Mid/2.png", "Image/P5_D2_P5_Medium_T3_27_Mid/3.png", "Image/P5_D2_P5_Medium_T3_27_Mid/4.png", "Image/P5_D2_P5_Medium_T3_27_Mid/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Clean the cup that's in the middle. Finish.\"\nThe frames roughly correspond to the speech interval from 777.452 seconds to 793.4874 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"white\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P1_D3_P8_High_T1_18_Left", "images": ["Image/P1_D3_P8_High_T1_18_Left/1.png", "Image/P1_D3_P8_High_T1_18_Left/2.png", "Image/P1_D3_P8_High_T1_18_Left/3.png", "Image/P1_D3_P8_High_T1_18_Left/4.png", "Image/P1_D3_P8_High_T1_18_Left/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start can help move the left cup to there\"\nThe frames roughly correspond to the speech interval from 1497.8638 seconds to 1509.6573 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the left one among the three objects\", \"Global Position\": \"under the open shelf unit\", \"Task Type\": \"Move\"}"}]}
{"id": "P26_D2_P6_Low_T3_20_Mid", "images": ["Image/P26_D2_P6_Low_T3_20_Mid/1.png", "Image/P26_D2_P6_Low_T3_20_Mid/2.png", "Image/P26_D2_P6_Low_T3_20_Mid/3.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you please clean this? Finish.\"\nThe frames roughly correspond to the speech interval from 1546.7891 seconds to 1555.8363 seconds."}, {"from": "assistant", "value": "{\"Type\": \"vase\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the very top shelf inside the cabinet\", \"Task Type\": \"Clean\"}"}]}
{"id": "P17_D2_P5_Medium_T2_4_Right", "images": ["Image/P17_D2_P5_Medium_T2_4_Right/1.png", "Image/P17_D2_P5_Medium_T2_4_Right/2.png", "Image/P17_D2_P5_Medium_T2_4_Right/3.png", "Image/P17_D2_P5_Medium_T2_4_Right/4.png", "Image/P17_D2_P5_Medium_T2_4_Right/5.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Can you put some water in this plant right here? Finish.\"\nThe frames roughly correspond to the speech interval from 630.3877 seconds to 644.0145 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the upper shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P8_D3_P3_Low_T1_14_Left", "images": ["Image/P8_D3_P3_Low_T1_14_Left/1.png", "Image/P8_D3_P3_Low_T1_14_Left/2.png", "Image/P8_D3_P3_Low_T1_14_Left/3.png", "Image/P8_D3_P3_Low_T1_14_Left/4.png", "Image/P8_D3_P3_Low_T1_14_Left/5.png", "Image/P8_D3_P3_Low_T1_14_Left/6.png", "Image/P8_D3_P3_Low_T1_14_Left/7.png", "Image/P8_D3_P3_Low_T1_14_Left/8.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Star. Could you move that planet just at the bottom of the shelf\"\nThe frames roughly correspond to the speech interval from 1783.0472 seconds to 1796.5472 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"rectangular\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on top of the cabinet\", \"Task Type\": \"Move\"}"}]}
{"id": "P6_D3_P1_Medium_T1_13_Mid", "images": ["Image/P6_D3_P1_Medium_T1_13_Mid/1.png", "Image/P6_D3_P1_Medium_T1_13_Mid/2.png", "Image/P6_D3_P1_Medium_T1_13_Mid/3.png", "Image/P6_D3_P1_Medium_T1_13_Mid/4.png", "Image/P6_D3_P1_Medium_T1_13_Mid/5.png", "Image/P6_D3_P1_Medium_T1_13_Mid/6.png", "Image/P6_D3_P1_Medium_T1_13_Mid/7.png", "Image/P6_D3_P1_Medium_T1_13_Mid/8.png", "Image/P6_D3_P1_Medium_T1_13_Mid/9.png", "Image/P6_D3_P1_Medium_T1_13_Mid/10.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" Start. Hey\"\nThe frames roughly correspond to the speech interval from 4659.1662 seconds to 4676.5422 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"on top of the desk-top shelf\", \"Task Type\": \"Move\"}"}]}
{"id": "P15_D2_P9_Medium_T2_23_Right", "images": ["Image/P15_D2_P9_Medium_T2_23_Right/1.png", "Image/P15_D2_P9_Medium_T2_23_Right/2.png", "Image/P15_D2_P9_Medium_T2_23_Right/3.png", "Image/P15_D2_P9_Medium_T2_23_Right/4.png", "Image/P15_D2_P9_Medium_T2_23_Right/5.png", "Image/P15_D2_P9_Medium_T2_23_Right/6.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\"Start\"\nThe frames roughly correspond to the speech interval from 2742.6761 seconds to 2755.3846 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"white\", \"Shape\": \"rectangular\", \"Local Position\": \"the right one among the three objects\", \"Global Position\": \"on the bottom shelf inside the cabinet\", \"Task Type\": \"Pour water\"}"}]}
{"id": "P7_D1_P7_Medium_T1_7_Mid", "images": ["Image/P7_D1_P7_Medium_T1_7_Mid/1.png", "Image/P7_D1_P7_Medium_T1_7_Mid/2.png", "Image/P7_D1_P7_Medium_T1_7_Mid/3.png", "Image/P7_D1_P7_Medium_T1_7_Mid/4.png", "Image/P7_D1_P7_Medium_T1_7_Mid/5.png", "Image/P7_D1_P7_Medium_T1_7_Mid/6.png", "Image/P7_D1_P7_Medium_T1_7_Mid/7.png", "Image/P7_D1_P7_Medium_T1_7_Mid/8.png", "Image/P7_D1_P7_Medium_T1_7_Mid/9.png", "Image/P7_D1_P7_Medium_T1_7_Mid/10.png", "Image/P7_D1_P7_Medium_T1_7_Mid/11.png", "Image/P7_D1_P7_Medium_T1_7_Mid/12.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image> <image>\nThe user's spoken instruction is: \"\" start. Hey Freddy\"\nThe frames roughly correspond to the speech interval from 156.1072 seconds to 176.1376 seconds."}, {"from": "assistant", "value": "{\"Type\": \"cup\", \"Color\": \"green\", \"Shape\": \"cylindrical\", \"Local Position\": \"the middle one among the three objects\", \"Global Position\": \"inside the storage box\", \"Task Type\": \"Move\"}"}]}
{"id": "P20_D1_P5_Low_T1_15_Right", "images": ["Image/P20_D1_P5_Low_T1_15_Right/1.png", "Image/P20_D1_P5_Low_T1_15_Right/2.png", "Image/P20_D1_P5_Low_T1_15_Right/3.png", "Image/P20_D1_P5_Low_T1_15_Right/4.png"], "conversations": [{"from": "user", "value": "You are a vision assistant that helps a robot interpret the user's instruction.\nThe user gives a natural language instruction, and the robot captures several frames (images) during the instruction period.\nThe images in this sample are ordered in time from early to late.\nPlease infer the most likely single target object that the user refers to, based on BOTH the instruction and all frames.\nAnswer ONLY in JSON with the following fields (in English):\n  - Type: object type (e.g., 'cup', 'vase', 'plant').\n  - Color: main color (e.g., 'white', 'green', 'brown').\n  - Shape: shape / appearance (e.g., 'cylindrical', 'rectangular', 'irregular-shaped').\n  - Local Position: local position relative to nearby objects (e.g., 'the left one among the three objects').\n  - Global Position: global position in the environment (e.g., 'on the wall-mounted shelf').\n  - Task Type: the type of task (e.g., 'Move', 'Pour water', 'Clean').\nDo not output any extra text outside the JSON.\n\n<image> <image> <image> <image>\nThe user's spoken instruction is: \"Start. Move the plant over here. Finish.\"\nThe frames roughly correspond to the speech interval from 778.0118 seconds to 790.0098 seconds."}, {"from": "assistant", "value": "{\"Type\": \"plant\", \"Color\": \"brown\", \"Shape\": \"irregular-shaped\", \"Local Position\": \"the only object in this area\", \"Global Position\": \"on the wall-mounted shelf\", \"Task Type\": \"Move\"}"}]}
